{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a211101b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../py-conjugated/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import morphology_networks as net\n",
    "import model_training as train\n",
    "import model_testing as test\n",
    "import physically_informed_loss_functions as pilf\n",
    "import network_utils as nuts\n",
    "\n",
    "torch.manual_seed(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/Volumes/Tatum_SSD-1/Grad_School/m2py/Morphology_labels/OPV_morph_maps/train_set/'\n",
    "test_data_path = '/Volumes/Tatum_SSD-1/Grad_School/m2py/Morphology_labels/OPV_morph_maps/test_set/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nuts.local_OPV_ImDataset(train_data_path)\n",
    "test_dataset = nuts.local_OPV_ImDataset(test_data_path)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 13)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvolutionalEncoder() relies on PyTorch API to convolve and compress image-like data\n",
    "    from SPM analyses. The stack of 2-dimensional SPM channels is encoded into a 1-dimensional\n",
    "    torch.Tensor(), which describes the image encoding into feature-space\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, im_z, fc_nodes):\n",
    "        super(ConvolutionalEncoder, self).__init__()\n",
    "        \n",
    "        self.z = im_z\n",
    "        self.fc_nodes = fc_nodes\n",
    "        \n",
    "        self.conv_pool1 = nn.Sequential(\n",
    "            nn.Conv2d(im_z, 32, kernel_size = 5, stride = 1, padding = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        \n",
    "        self.conv_pool2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        \n",
    "        self.conv_pool3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Dropout(),               #to avoid over-fitting\n",
    "            nn.Linear(self.fc_nodes, 5000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5000, 100),\n",
    "            nn.ReLu()\n",
    "        )\n",
    "        \n",
    "    def forward(self, im):\n",
    "        conv_enc = self.conv_pool1(im)\n",
    "        conv_enc = self.conv_pool2(conv_enc)\n",
    "        conv_enc = self.conv_pool3(conv_enc)\n",
    "        \n",
    "        linear_enc = self.linear_layers(conv_enc)\n",
    "        \n",
    "        return linear_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvolutionalDecoder() relies on PyTorch API to convolve and decompress encodings of\n",
    "    image-like data from SPM analyses. The stack of 2-dimensional SPM channels are decoded \n",
    "    from a 1-dimensional torch.Tensor(), which reconstructs the image encoding from feature-space\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, im_z, fc_nodes):\n",
    "        super(ConvolutionalDecoder, self).__init__()\n",
    "        \n",
    "        self.z = im_z\n",
    "        self.fc_nodes = fc_nodes\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(100, 5000),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(5000, self.fc_nodes),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 2, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 2, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, im_z, 2, stride = 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, linear_enc):\n",
    "        im_enc = self.linear_layers(linear_enc),\n",
    "        \n",
    "        im_enc = self.conv_pool3(im_enc),\n",
    "        im_enc = self.conv_pool2(im_enc),\n",
    "        decoded_image = self.conv_pool1(im_enc)\n",
    "        \n",
    "        return decoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder based on PyTorch module framework. VAE takes in SPM images or\n",
    "    m2py_labels as a stack of 2-dimensional channels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, im_z, im_x = 256, im_y = 256):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.x = im_x\n",
    "        self.y = im_y\n",
    "        self.z = im_z\n",
    "        \n",
    "        #After *3* deconvolution and activation layers (decreasing x and y by 50% each),\n",
    "        #there are *128* channels expanded from the input tensor of the image encoding.\n",
    "        #May need modification if, stride, padding, convolution layers, or number of\n",
    "        #channels in output of convolution and pooling are changed.\n",
    "        \n",
    "        self.fc_nodes = int((im_x/(2^3))*(im_y/(2^3))*128)\n",
    "        \n",
    "        self.encoder = ConvolutionalEncoder(self.z, self.fc_nodes)\n",
    "        self.decoder = ConvolutionalDecoder(self.z, self.fc_nodes)\n",
    "        \n",
    "    def forward(self, original_im):\n",
    "        encoding = self.encoder(original_im)\n",
    "        decoded_image = self.decoder(encoding)\n",
    "        \n",
    "        return decoded_image, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, training_dataset, criterion, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    loss_list = []\n",
    "    \n",
    "    batch_iterator = 0\n",
    "    for images, labels in training_data_set:\n",
    "        batch_iterator += 1\n",
    "        print(f'image # {batch_iterator}')\n",
    "        \n",
    "        images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "        # Run the forward pass     \n",
    "        optimizer.zero_grad()\n",
    "        decoded_images, encoding = model(images)\n",
    "        \n",
    "        loss = criterion(decoded_images, images)\n",
    "        torch.autograd.backward(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "    samples = len(loss_list)\n",
    "    epoch_loss = sum(loss_list)/samples\n",
    "    \n",
    "    return epoch_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        results_dict = {}\n",
    "\n",
    "        batch_iterator = 0\n",
    "        for images in training_data_set:\n",
    "            batch_iterator += 1\n",
    "            print(f'image # {batch_iterator}')\n",
    "            \n",
    "            images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "            # Run the forward pass     \n",
    "            optimizer.zero_grad()\n",
    "            decoded_images, encoding = model(images)\n",
    "            \n",
    "            results_dict[batch_iterator] = {'original': images,\n",
    "                                            'decoded': decoded_images}\n",
    "\n",
    "            loss = criterion(decoded_images, images)\n",
    "            torch.autograd.backward(loss)\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "    samples = len(loss_list)\n",
    "    epoch_loss = sum(loss_list)/samples\n",
    "    \n",
    "    return epoch_loss, results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(im_z = 2)\n",
    "criterion = BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(0, 10, 1)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_results = {}\n",
    "\n",
    "for ep in epochs:\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    test_loss, results_dict = test(model, test_dataloader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    test_results[ep] = results_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
