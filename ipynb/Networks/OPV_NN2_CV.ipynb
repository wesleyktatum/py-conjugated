{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a229d21b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../py-conjugated/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import morphology_networks as net\n",
    "import model_training as train\n",
    "import model_testing as test\n",
    "import physically_informed_loss_functions as pilf\n",
    "import network_utils as nuts\n",
    "\n",
    "torch.manual_seed(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPV NN2\n",
    "\n",
    "### This notebook to uses OPV processing conditions and summary morphology descriptions to predict device performance.\n",
    "\n",
    "\n",
    "# Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_df shape: (36, 60)\n",
      "test_df shape: (10, 60)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>p1_fraction</th>\n",
       "      <th>p2_fraction</th>\n",
       "      <th>p3_fraction</th>\n",
       "      <th>p1_area_avg</th>\n",
       "      <th>p1_area_stdev</th>\n",
       "      <th>p1_filled_area_avg</th>\n",
       "      <th>p1_filled_area_stdev</th>\n",
       "      <th>p1_extent_avg</th>\n",
       "      <th>p1_extent_stdev</th>\n",
       "      <th>...</th>\n",
       "      <th>p3_Perim_avg</th>\n",
       "      <th>p3_Perim_stdev</th>\n",
       "      <th>PCE</th>\n",
       "      <th>VocL</th>\n",
       "      <th>Jsc</th>\n",
       "      <th>FF</th>\n",
       "      <th>Substrate</th>\n",
       "      <th>Device</th>\n",
       "      <th>Anneal_time</th>\n",
       "      <th>Anneal_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.379892</td>\n",
       "      <td>0.741510</td>\n",
       "      <td>0.042227</td>\n",
       "      <td>0.747318</td>\n",
       "      <td>0.416992</td>\n",
       "      <td>0.747056</td>\n",
       "      <td>0.437669</td>\n",
       "      <td>0.371252</td>\n",
       "      <td>0.265335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385802</td>\n",
       "      <td>0.214615</td>\n",
       "      <td>0.833110</td>\n",
       "      <td>0.817731</td>\n",
       "      <td>0.950302</td>\n",
       "      <td>0.822343</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.299771</td>\n",
       "      <td>0.662438</td>\n",
       "      <td>0.354767</td>\n",
       "      <td>0.338403</td>\n",
       "      <td>0.119074</td>\n",
       "      <td>0.308619</td>\n",
       "      <td>0.108677</td>\n",
       "      <td>0.427280</td>\n",
       "      <td>0.413324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756401</td>\n",
       "      <td>0.786992</td>\n",
       "      <td>0.635534</td>\n",
       "      <td>0.739614</td>\n",
       "      <td>0.961763</td>\n",
       "      <td>0.685312</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.122273</td>\n",
       "      <td>0.555842</td>\n",
       "      <td>0.907236</td>\n",
       "      <td>0.637064</td>\n",
       "      <td>0.178141</td>\n",
       "      <td>0.581681</td>\n",
       "      <td>0.158955</td>\n",
       "      <td>0.172276</td>\n",
       "      <td>0.278782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771184</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863038</td>\n",
       "      <td>0.814898</td>\n",
       "      <td>0.947029</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.019406</td>\n",
       "      <td>0.848362</td>\n",
       "      <td>0.504491</td>\n",
       "      <td>0.832873</td>\n",
       "      <td>0.498779</td>\n",
       "      <td>0.868876</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.240255</td>\n",
       "      <td>0.557641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557095</td>\n",
       "      <td>0.404926</td>\n",
       "      <td>0.947300</td>\n",
       "      <td>0.806031</td>\n",
       "      <td>0.996387</td>\n",
       "      <td>0.904754</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384999</td>\n",
       "      <td>0.271485</td>\n",
       "      <td>0.175114</td>\n",
       "      <td>0.267725</td>\n",
       "      <td>0.171034</td>\n",
       "      <td>0.309962</td>\n",
       "      <td>0.288716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084425</td>\n",
       "      <td>0.865331</td>\n",
       "      <td>0.825047</td>\n",
       "      <td>0.903792</td>\n",
       "      <td>0.890140</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  p1_fraction  p2_fraction  p3_fraction  p1_area_avg  \\\n",
       "0           0     0.379892     0.741510     0.042227     0.747318   \n",
       "1           1     0.299771     0.662438     0.354767     0.338403   \n",
       "2           2     0.122273     0.555842     0.907236     0.637064   \n",
       "3           3     0.019406     0.848362     0.504491     0.832873   \n",
       "4           6     1.000000     0.000000     0.384999     0.271485   \n",
       "\n",
       "   p1_area_stdev  p1_filled_area_avg  p1_filled_area_stdev  p1_extent_avg  \\\n",
       "0       0.416992            0.747056              0.437669       0.371252   \n",
       "1       0.119074            0.308619              0.108677       0.427280   \n",
       "2       0.178141            0.581681              0.158955       0.172276   \n",
       "3       0.498779            0.868876              0.509000       0.240255   \n",
       "4       0.175114            0.267725              0.171034       0.309962   \n",
       "\n",
       "   p1_extent_stdev  ...  p3_Perim_avg  p3_Perim_stdev       PCE      VocL  \\\n",
       "0         0.265335  ...      0.385802        0.214615  0.833110  0.817731   \n",
       "1         0.413324  ...      0.756401        0.786992  0.635534  0.739614   \n",
       "2         0.278782  ...      0.771184        1.000000  0.863038  0.814898   \n",
       "3         0.557641  ...      0.557095        0.404926  0.947300  0.806031   \n",
       "4         0.288716  ...      0.000000        0.084425  0.865331  0.825047   \n",
       "\n",
       "        Jsc        FF  Substrate  Device  Anneal_time  Anneal_temp  \n",
       "0  0.950302  0.822343          4       2           15          100  \n",
       "1  0.961763  0.685312          4       4           15          100  \n",
       "2  0.947029  0.857800          4       7           15          100  \n",
       "3  0.996387  0.904754          7       2           30          100  \n",
       "4  0.903792  0.890140          1       3            5          100  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take in data as a dataframe for easy preprocessing\n",
    "train_df = pd.read_excel('/Users/wesleytatum/Desktop/py-conjugated/data/normed_OPV_train.xlsx')\n",
    "\n",
    "test_df = pd.read_excel('/Users/wesleytatum/Desktop/py-conjugated/data/normed_OPV_test.xlsx')\n",
    "\n",
    "total_df = pd.concat([train_df, test_df])\n",
    "\n",
    "print (f'total_df shape: {total_df.shape}')\n",
    "print (f'test_df shape: {test_df.shape}')\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 500\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = total_df.drop(['PCE', 'VocL', 'Jsc', 'FF', 'Unnamed: 0', 'Substrate', 'Device'], axis = 1)#input features used to make prediction\n",
    "y_train = total_df[['PCE', 'VocL', 'Jsc', 'FF']] #target features to be predicted\n",
    "\n",
    "x_test = test_df[['Anneal_time', 'Anneal_temp']]\n",
    "y_test = test_df[['PCE', 'VocL', 'Jsc', 'FF']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model with variety of learning rates and epochs to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "def fit(model, lr, epochs):\n",
    "    \n",
    "    #define the optimizer\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    #empty list to hold loss per epoch\n",
    "    train_epoch_losses = []\n",
    "    pce_train_epoch_losses = []\n",
    "    voc_train_epoch_losses = []\n",
    "    jsc_train_epoch_losses = []\n",
    "    ff_train_epoch_losses = []\n",
    "\n",
    "    test_epoch_losses = []\n",
    "    pce_test_epoch_losses = []\n",
    "    voc_test_epoch_losses = []\n",
    "    jsc_test_epoch_losses = []\n",
    "    ff_test_epoch_losses = []\n",
    "\n",
    "    pce_test_epoch_accuracies = []\n",
    "    voc_test_epoch_accuracies = []\n",
    "    jsc_test_epoch_accuracies = []\n",
    "    ff_test_epoch_accuracies = []\n",
    "    test_epoch_accuracies = []\n",
    "\n",
    "    pce_test_epoch_r2 = []\n",
    "    voc_test_epoch_r2 = []\n",
    "    jsc_test_epoch_r2 = []\n",
    "    ff_test_epoch_r2 = []\n",
    "    test_epoch_r2s = []\n",
    "\n",
    "    save_epochs = np.arange(0, num_epochs, 5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('On epoch ', epoch)\n",
    "\n",
    "    #     save_dir = \"/Users/wesleytatum/Desktop/model_states/OPV_NN2/\"\n",
    "    #     model_name = \"OPV_NN2\"\n",
    "    #     model_path = save_dir+model_name+'*.pt'\n",
    "    #     if epoch < 10:\n",
    "    #         save_path = save_dir + model_name + '_epoch0' + str(epoch) + '.pt'\n",
    "    #     else:\n",
    "    #         save_path = save_dir + model_name + '_epoch' + str(epoch) + '.pt'\n",
    "\n",
    "    #     if glob.glob(model_path) != []:\n",
    "    #         model_states = glob.glob(model_path)\n",
    "    #         model_states = sorted(model_states)\n",
    "    #         previous_model = model_states[-1]    \n",
    "\n",
    "    #         model, optimizer = nuts.load_trained_model(previous_model, model, optimizer)\n",
    "\n",
    "        model, train_loss, pce_train_loss, voc_train_loss, jsc_train_loss, ff_train_loss = train.train_OPV_df_model(model = model,                                                                                                     training_data_set = training_data_set,\n",
    "                                                                                                             optimizer = optimizer)\n",
    "        train_epoch_losses.append(train_loss)\n",
    "        pce_train_epoch_losses.append(pce_train_loss)\n",
    "        voc_train_epoch_losses.append(voc_train_loss)\n",
    "        jsc_train_epoch_losses.append(jsc_train_loss)\n",
    "        ff_train_epoch_losses.append(ff_train_loss)\n",
    "\n",
    "        test_losses, test_accs, test_r2s = test.eval_OPV_df_model(model = model,\n",
    "                                                                  testing_data_set = testing_data_set)\n",
    "        pce_test_epoch_losses.append(test_losses[0])\n",
    "        voc_test_epoch_losses.append(test_losses[1])\n",
    "        jsc_test_epoch_losses.append(test_losses[2])\n",
    "        ff_test_epoch_losses.append(test_losses[3])\n",
    "\n",
    "        tot_tst_loss = sum(test_losses)\n",
    "        test_epoch_losses.append(tot_tst_loss)\n",
    "\n",
    "        pce_test_epoch_accuracies.append(test_accs[0])\n",
    "        voc_test_epoch_accuracies.append(test_accs[1])\n",
    "        jsc_test_epoch_accuracies.append(test_accs[2])\n",
    "        ff_test_epoch_accuracies.append(test_accs[3])\n",
    "\n",
    "        tot_tst_acc = sum(test_accs)\n",
    "        test_epoch_accuracies.append(tot_tst_acc)\n",
    "\n",
    "        pce_test_epoch_r2.append(test_r2s[0])\n",
    "        voc_test_epoch_r2.append(test_r2s[1])\n",
    "        jsc_test_epoch_r2.append(test_r2s[2])\n",
    "        ff_test_epoch_r2.append(test_r2s[3])\n",
    "\n",
    "        tot_tst_r2 = sum(test_r2s)\n",
    "        test_epoch_r2s.append(tot_tst_r2)\n",
    "\n",
    "        #nuts.save_trained_model(save_path, epoch, model, optimizer)\n",
    "        \n",
    "        print('Finished epoch ', epoch)\n",
    "        \n",
    "    best_loss_indx = test_epoch_losses.index(min(test_epoch_losses))\n",
    "    best_acc_indx = test_epoch_accuracies.index(min(test_epoch_accuracies))\n",
    "    best_r2_indx = test_epoch_r2s.index(max(test_epoch_r2s))\n",
    "    \n",
    "    fit_results = {\n",
    "        'lr': lr,\n",
    "        'best_loss_epoch': best_loss_indx,\n",
    "        'best_acc_epoch': best_acc_indx,\n",
    "        'best_r2_epoch': best_r2_indx,\n",
    "        'pce_loss': pce_test_epoch_losses,\n",
    "        'voc_loss': voc_test_epoch_losses,\n",
    "        'jsc_loss': jsc_test_epoch_losses,\n",
    "        'ff_loss': ff_test_epoch_losses,\n",
    "        'test_losses': test_epoch_losses,        \n",
    "        'pce_acc': pce_test_epoch_accuracies,\n",
    "        'voc_acc': voc_test_epoch_accuracies,\n",
    "        'jsc_acc': jsc_test_epoch_accuracies,\n",
    "        'ff_acc': ff_test_epoch_accuracies,\n",
    "        'test_accs': test_epoch_accuracies,\n",
    "        'pce_r2': pce_test_epoch_r2,\n",
    "        'voc_r2': voc_test_epoch_r2,\n",
    "        'jsc_r2': jsc_test_epoch_r2,\n",
    "        'ff_r2': ff_test_epoch_r2,\n",
    "        'test_r2s': test_epoch_r2s,\n",
    "        'train_pce_loss': pce_train_epoch_losses,\n",
    "        'train_voc_loss': voc_train_epoch_losses,\n",
    "        'train_jsc_loss': jsc_train_epoch_losses,\n",
    "        'train_ff_loss': ff_train_epoch_losses\n",
    "    }\n",
    "\n",
    "    return fit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "in_dims = int(x_train.shape[1]) #number of x channels\n",
    "out_dims = y_test.shape[1] #number of predicted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 0\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 66.416259765625\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 55.396156311035156\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 47.86339569091797\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  2\n",
      "On epoch  3\n",
      "Total Epoch Testing MAPE: PCE = 45.55832290649414\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 46.72878646850586\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  4\n",
      "On epoch  5\n",
      "Total Epoch Testing MAPE: PCE = 43.337581634521484\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 99.49494934082031\n",
      "Finished epoch  5\n",
      "On epoch  6\n",
      "Total Epoch Testing MAPE: PCE = 43.62934112548828\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 91.71634674072266\n",
      "Finished epoch  6\n",
      "On epoch  7\n",
      "Total Epoch Testing MAPE: PCE = 41.898929595947266\n",
      "                              Voc = 99.68607330322266\n",
      "                              Jsc = 100.0\n",
      "                              FF = 81.25731658935547\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 41.57978439331055\n",
      "                              Voc = 92.26150512695312\n",
      "                              Jsc = 100.0\n",
      "                              FF = 73.39857482910156\n",
      "Finished epoch  8\n",
      "On epoch  9\n",
      "Total Epoch Testing MAPE: PCE = 41.303688049316406\n",
      "                              Voc = 84.9187240600586\n",
      "                              Jsc = 99.89290618896484\n",
      "                              FF = 60.775875091552734\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 39.160709381103516\n",
      "                              Voc = 76.74407958984375\n",
      "                              Jsc = 100.0\n",
      "                              FF = 51.66710662841797\n",
      "Finished epoch  10\n",
      "On epoch  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 44.03520202636719\n",
      "                              Voc = 69.38874816894531\n",
      "                              Jsc = 99.83577728271484\n",
      "                              FF = 46.63259506225586\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 43.03180694580078\n",
      "                              Voc = 63.56038284301758\n",
      "                              Jsc = 97.77171325683594\n",
      "                              FF = 41.002742767333984\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 44.626888275146484\n",
      "                              Voc = 58.94400405883789\n",
      "                              Jsc = 97.35261535644531\n",
      "                              FF = 33.365753173828125\n",
      "Finished epoch  13\n",
      "On epoch  14\n",
      "Total Epoch Testing MAPE: PCE = 45.27760314941406\n",
      "                              Voc = 55.49647903442383\n",
      "                              Jsc = 95.8457260131836\n",
      "                              FF = 32.784645080566406\n",
      "Finished epoch  14\n",
      "On epoch  15\n",
      "Total Epoch Testing MAPE: PCE = 50.20248794555664\n",
      "                              Voc = 52.4069709777832\n",
      "                              Jsc = 94.51205444335938\n",
      "                              FF = 31.785202026367188\n",
      "Finished epoch  15\n",
      "On epoch  16\n",
      "Total Epoch Testing MAPE: PCE = 52.41787338256836\n",
      "                              Voc = 48.33323669433594\n",
      "                              Jsc = 93.4320297241211\n",
      "                              FF = 27.54138946533203\n",
      "Finished epoch  16\n",
      "On epoch  17\n",
      "Total Epoch Testing MAPE: PCE = 59.67013168334961\n",
      "                              Voc = 42.20133590698242\n",
      "                              Jsc = 94.00799560546875\n",
      "                              FF = 24.328454971313477\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 64.48544311523438\n",
      "                              Voc = 39.598297119140625\n",
      "                              Jsc = 94.82213592529297\n",
      "                              FF = 22.887828826904297\n",
      "Finished epoch  18\n",
      "On epoch  19\n",
      "Total Epoch Testing MAPE: PCE = 69.15489196777344\n",
      "                              Voc = 37.31461715698242\n",
      "                              Jsc = 94.58658599853516\n",
      "                              FF = 20.462974548339844\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 69.02069091796875\n",
      "                              Voc = 34.74326705932617\n",
      "                              Jsc = 95.0678482055664\n",
      "                              FF = 19.284757614135742\n",
      "Finished epoch  20\n",
      "On epoch  21\n",
      "Total Epoch Testing MAPE: PCE = 69.21772003173828\n",
      "                              Voc = 34.62430953979492\n",
      "                              Jsc = 95.79029083251953\n",
      "                              FF = 17.69417953491211\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 68.98683166503906\n",
      "                              Voc = 29.560302734375\n",
      "                              Jsc = 96.1020736694336\n",
      "                              FF = 17.625764846801758\n",
      "Finished epoch  22\n",
      "On epoch  23\n",
      "Total Epoch Testing MAPE: PCE = 68.79444885253906\n",
      "                              Voc = 28.346168518066406\n",
      "                              Jsc = 95.82203674316406\n",
      "                              FF = 18.104047775268555\n",
      "Finished epoch  23\n",
      "On epoch  24\n",
      "Total Epoch Testing MAPE: PCE = 69.45986938476562\n",
      "                              Voc = 24.096782684326172\n",
      "                              Jsc = 96.06864929199219\n",
      "                              FF = 17.661357879638672\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 74.66808319091797\n",
      "                              Voc = 21.999656677246094\n",
      "                              Jsc = 96.01461029052734\n",
      "                              FF = 17.481130599975586\n",
      "Finished epoch  25\n",
      "On epoch  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 75.90562438964844\n",
      "                              Voc = 21.1434268951416\n",
      "                              Jsc = 95.87841033935547\n",
      "                              FF = 17.120540618896484\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 77.26820373535156\n",
      "                              Voc = 17.812543869018555\n",
      "                              Jsc = 95.91107940673828\n",
      "                              FF = 16.35757064819336\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 76.8387680053711\n",
      "                              Voc = 14.567737579345703\n",
      "                              Jsc = 96.29608154296875\n",
      "                              FF = 16.253522872924805\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 79.39427185058594\n",
      "                              Voc = 15.392213821411133\n",
      "                              Jsc = 95.07640838623047\n",
      "                              FF = 15.5335111618042\n",
      "Finished epoch  29\n",
      "On epoch  30\n",
      "Total Epoch Testing MAPE: PCE = 79.94857788085938\n",
      "                              Voc = 15.805191040039062\n",
      "                              Jsc = 96.05316925048828\n",
      "                              FF = 15.527534484863281\n",
      "Finished epoch  30\n",
      "On epoch  31\n",
      "Total Epoch Testing MAPE: PCE = 81.21063995361328\n",
      "                              Voc = 16.294235229492188\n",
      "                              Jsc = 96.17449188232422\n",
      "                              FF = 15.752763748168945\n",
      "Finished epoch  31\n",
      "On epoch  32\n",
      "Total Epoch Testing MAPE: PCE = 81.95722961425781\n",
      "                              Voc = 15.57193374633789\n",
      "                              Jsc = 96.2470932006836\n",
      "                              FF = 14.834968566894531\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 82.58503723144531\n",
      "                              Voc = 14.60576057434082\n",
      "                              Jsc = 96.39765930175781\n",
      "                              FF = 14.3801908493042\n",
      "Finished epoch  33\n",
      "On epoch  34\n",
      "Total Epoch Testing MAPE: PCE = 83.03215789794922\n",
      "                              Voc = 16.329530715942383\n",
      "                              Jsc = 96.3918228149414\n",
      "                              FF = 14.918686866760254\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 83.73262023925781\n",
      "                              Voc = 14.034798622131348\n",
      "                              Jsc = 96.91229248046875\n",
      "                              FF = 14.898877143859863\n",
      "Finished epoch  35\n",
      "On epoch  36\n",
      "Total Epoch Testing MAPE: PCE = 83.40967559814453\n",
      "                              Voc = 13.51773738861084\n",
      "                              Jsc = 97.16829681396484\n",
      "                              FF = 15.041353225708008\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 84.04743957519531\n",
      "                              Voc = 12.962677955627441\n",
      "                              Jsc = 97.23064422607422\n",
      "                              FF = 15.277835845947266\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 84.94068145751953\n",
      "                              Voc = 13.549320220947266\n",
      "                              Jsc = 96.63933563232422\n",
      "                              FF = 16.27971839904785\n",
      "Finished epoch  38\n",
      "On epoch  39\n",
      "Total Epoch Testing MAPE: PCE = 84.9511489868164\n",
      "                              Voc = 14.328058242797852\n",
      "                              Jsc = 96.8127670288086\n",
      "                              FF = 16.35540199279785\n",
      "Finished epoch  39\n",
      "On epoch  40\n",
      "Total Epoch Testing MAPE: PCE = 87.27222442626953\n",
      "                              Voc = 12.768793106079102\n",
      "                              Jsc = 97.03800964355469\n",
      "                              FF = 16.69770050048828\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 86.3096923828125\n",
      "                              Voc = 11.7489652633667\n",
      "                              Jsc = 96.85089874267578\n",
      "                              FF = 15.933680534362793\n",
      "Finished epoch  41\n",
      "On epoch  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 85.22382354736328\n",
      "                              Voc = 11.752686500549316\n",
      "                              Jsc = 96.48868560791016\n",
      "                              FF = 16.064390182495117\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 86.95897674560547\n",
      "                              Voc = 12.506973266601562\n",
      "                              Jsc = 96.30848693847656\n",
      "                              FF = 16.68940544128418\n",
      "Finished epoch  43\n",
      "On epoch  44\n",
      "Total Epoch Testing MAPE: PCE = 84.09278106689453\n",
      "                              Voc = 11.91316032409668\n",
      "                              Jsc = 96.73087310791016\n",
      "                              FF = 17.489864349365234\n",
      "Finished epoch  44\n",
      "On epoch  45\n",
      "Total Epoch Testing MAPE: PCE = 84.4097900390625\n",
      "                              Voc = 11.763596534729004\n",
      "                              Jsc = 96.67642974853516\n",
      "                              FF = 16.74712371826172\n",
      "Finished epoch  45\n",
      "On epoch  46\n",
      "Total Epoch Testing MAPE: PCE = 82.12826538085938\n",
      "                              Voc = 11.806909561157227\n",
      "                              Jsc = 96.60874938964844\n",
      "                              FF = 17.27316665649414\n",
      "Finished epoch  46\n",
      "On epoch  47\n",
      "Total Epoch Testing MAPE: PCE = 82.7439956665039\n",
      "                              Voc = 11.882686614990234\n",
      "                              Jsc = 96.69452667236328\n",
      "                              FF = 16.728836059570312\n",
      "Finished epoch  47\n",
      "On epoch  48\n",
      "Total Epoch Testing MAPE: PCE = 84.23697662353516\n",
      "                              Voc = 12.354463577270508\n",
      "                              Jsc = 96.5133056640625\n",
      "                              FF = 15.989279747009277\n",
      "Finished epoch  48\n",
      "On epoch  49\n",
      "Total Epoch Testing MAPE: PCE = 85.49856567382812\n",
      "                              Voc = 12.276531219482422\n",
      "                              Jsc = 96.55931854248047\n",
      "                              FF = 15.78177261352539\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 85.65507507324219\n",
      "                              Voc = 12.467501640319824\n",
      "                              Jsc = 96.60399627685547\n",
      "                              FF = 15.542028427124023\n",
      "Finished epoch  50\n",
      "On epoch  51\n",
      "Total Epoch Testing MAPE: PCE = 85.29208374023438\n",
      "                              Voc = 12.637153625488281\n",
      "                              Jsc = 96.96380615234375\n",
      "                              FF = 15.139697074890137\n",
      "Finished epoch  51\n",
      "On epoch  52\n",
      "Total Epoch Testing MAPE: PCE = 87.25371551513672\n",
      "                              Voc = 12.203054428100586\n",
      "                              Jsc = 97.0694351196289\n",
      "                              FF = 15.332990646362305\n",
      "Finished epoch  52\n",
      "On epoch  53\n",
      "Total Epoch Testing MAPE: PCE = 86.78783416748047\n",
      "                              Voc = 12.125198364257812\n",
      "                              Jsc = 96.38789367675781\n",
      "                              FF = 15.51479434967041\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 86.30696868896484\n",
      "                              Voc = 12.269383430480957\n",
      "                              Jsc = 96.53133392333984\n",
      "                              FF = 15.601195335388184\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 86.21162414550781\n",
      "                              Voc = 12.94596004486084\n",
      "                              Jsc = 96.6846923828125\n",
      "                              FF = 15.635207176208496\n",
      "Finished epoch  55\n",
      "On epoch  56\n",
      "Total Epoch Testing MAPE: PCE = 85.14733123779297\n",
      "                              Voc = 12.358808517456055\n",
      "                              Jsc = 97.0645523071289\n",
      "                              FF = 15.203722953796387\n",
      "Finished epoch  56\n",
      "On epoch  57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 85.44239044189453\n",
      "                              Voc = 12.467856407165527\n",
      "                              Jsc = 97.46965789794922\n",
      "                              FF = 15.013996124267578\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 83.90975952148438\n",
      "                              Voc = 12.779614448547363\n",
      "                              Jsc = 96.98908233642578\n",
      "                              FF = 15.03124713897705\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 84.39495086669922\n",
      "                              Voc = 12.618371963500977\n",
      "                              Jsc = 96.67350769042969\n",
      "                              FF = 14.920687675476074\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 84.67070007324219\n",
      "                              Voc = 12.685625076293945\n",
      "                              Jsc = 97.02278900146484\n",
      "                              FF = 14.845085144042969\n",
      "Finished epoch  60\n",
      "On epoch  61\n",
      "Total Epoch Testing MAPE: PCE = 84.6239242553711\n",
      "                              Voc = 12.589159965515137\n",
      "                              Jsc = 97.08882904052734\n",
      "                              FF = 15.572731018066406\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 85.68131256103516\n",
      "                              Voc = 12.480932235717773\n",
      "                              Jsc = 96.60452270507812\n",
      "                              FF = 15.516517639160156\n",
      "Finished epoch  62\n",
      "On epoch  63\n",
      "Total Epoch Testing MAPE: PCE = 82.84262084960938\n",
      "                              Voc = 12.751115798950195\n",
      "                              Jsc = 96.52542114257812\n",
      "                              FF = 15.576887130737305\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 79.81737518310547\n",
      "                              Voc = 12.607627868652344\n",
      "                              Jsc = 95.793212890625\n",
      "                              FF = 15.380549430847168\n",
      "Finished epoch  64\n",
      "On epoch  65\n",
      "Total Epoch Testing MAPE: PCE = 80.21707153320312\n",
      "                              Voc = 12.65969467163086\n",
      "                              Jsc = 95.87613677978516\n",
      "                              FF = 15.718277931213379\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 80.68021392822266\n",
      "                              Voc = 12.180316925048828\n",
      "                              Jsc = 94.22484588623047\n",
      "                              FF = 15.16689395904541\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 80.5426254272461\n",
      "                              Voc = 12.061630249023438\n",
      "                              Jsc = 93.4646224975586\n",
      "                              FF = 14.829599380493164\n",
      "Finished epoch  67\n",
      "On epoch  68\n",
      "Total Epoch Testing MAPE: PCE = 81.26769256591797\n",
      "                              Voc = 12.179025650024414\n",
      "                              Jsc = 94.32528686523438\n",
      "                              FF = 15.609830856323242\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 81.80935668945312\n",
      "                              Voc = 12.226739883422852\n",
      "                              Jsc = 94.81652069091797\n",
      "                              FF = 15.940962791442871\n",
      "Finished epoch  69\n",
      "On epoch  70\n",
      "Total Epoch Testing MAPE: PCE = 83.87682342529297\n",
      "                              Voc = 12.2949857711792\n",
      "                              Jsc = 95.16048431396484\n",
      "                              FF = 16.496252059936523\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 85.14688873291016\n",
      "                              Voc = 11.973175048828125\n",
      "                              Jsc = 95.48096466064453\n",
      "                              FF = 16.500530242919922\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 84.0225601196289\n",
      "                              Voc = 12.091226577758789\n",
      "                              Jsc = 95.17829132080078\n",
      "                              FF = 16.96906089782715\n",
      "Finished epoch  72\n",
      "On epoch  73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.78540802001953\n",
      "                              Voc = 11.451552391052246\n",
      "                              Jsc = 94.59467315673828\n",
      "                              FF = 16.55607032775879\n",
      "Finished epoch  73\n",
      "On epoch  74\n",
      "Total Epoch Testing MAPE: PCE = 82.40190887451172\n",
      "                              Voc = 11.506014823913574\n",
      "                              Jsc = 94.25677490234375\n",
      "                              FF = 15.726316452026367\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 84.98030090332031\n",
      "                              Voc = 11.991720199584961\n",
      "                              Jsc = 93.76295471191406\n",
      "                              FF = 16.399059295654297\n",
      "Finished epoch  75\n",
      "On epoch  76\n",
      "Total Epoch Testing MAPE: PCE = 85.47595977783203\n",
      "                              Voc = 11.57446575164795\n",
      "                              Jsc = 94.41744232177734\n",
      "                              FF = 16.131942749023438\n",
      "Finished epoch  76\n",
      "On epoch  77\n",
      "Total Epoch Testing MAPE: PCE = 86.566650390625\n",
      "                              Voc = 12.090337753295898\n",
      "                              Jsc = 94.33242797851562\n",
      "                              FF = 15.293410301208496\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 85.73894500732422\n",
      "                              Voc = 11.826976776123047\n",
      "                              Jsc = 95.45942687988281\n",
      "                              FF = 15.531126022338867\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 86.51823425292969\n",
      "                              Voc = 12.039925575256348\n",
      "                              Jsc = 95.52595520019531\n",
      "                              FF = 15.33662223815918\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 86.19892120361328\n",
      "                              Voc = 12.278711318969727\n",
      "                              Jsc = 96.28936767578125\n",
      "                              FF = 14.847171783447266\n",
      "Finished epoch  80\n",
      "On epoch  81\n",
      "Total Epoch Testing MAPE: PCE = 85.2229232788086\n",
      "                              Voc = 12.320516586303711\n",
      "                              Jsc = 96.35186004638672\n",
      "                              FF = 14.422674179077148\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 84.10247039794922\n",
      "                              Voc = 12.364174842834473\n",
      "                              Jsc = 96.86402130126953\n",
      "                              FF = 14.838427543640137\n",
      "Finished epoch  82\n",
      "On epoch  83\n",
      "Total Epoch Testing MAPE: PCE = 83.66376495361328\n",
      "                              Voc = 11.973370552062988\n",
      "                              Jsc = 96.8638687133789\n",
      "                              FF = 15.342068672180176\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 82.93403625488281\n",
      "                              Voc = 12.210798263549805\n",
      "                              Jsc = 97.03556060791016\n",
      "                              FF = 15.767439842224121\n",
      "Finished epoch  84\n",
      "On epoch  85\n",
      "Total Epoch Testing MAPE: PCE = 81.75457000732422\n",
      "                              Voc = 11.944458961486816\n",
      "                              Jsc = 97.65304565429688\n",
      "                              FF = 15.625140190124512\n",
      "Finished epoch  85\n",
      "On epoch  86\n",
      "Total Epoch Testing MAPE: PCE = 81.47176361083984\n",
      "                              Voc = 11.966458320617676\n",
      "                              Jsc = 97.55455017089844\n",
      "                              FF = 16.280292510986328\n",
      "Finished epoch  86\n",
      "On epoch  87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 80.92042541503906\n",
      "                              Voc = 12.308910369873047\n",
      "                              Jsc = 98.15304565429688\n",
      "                              FF = 16.01709747314453\n",
      "Finished epoch  87\n",
      "On epoch  88\n",
      "Total Epoch Testing MAPE: PCE = 81.31246948242188\n",
      "                              Voc = 11.76816463470459\n",
      "                              Jsc = 97.92117309570312\n",
      "                              FF = 16.138822555541992\n",
      "Finished epoch  88\n",
      "On epoch  89\n",
      "Total Epoch Testing MAPE: PCE = 82.2950668334961\n",
      "                              Voc = 11.934389114379883\n",
      "                              Jsc = 97.68140411376953\n",
      "                              FF = 16.234926223754883\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 84.54139709472656\n",
      "                              Voc = 11.819377899169922\n",
      "                              Jsc = 97.31521606445312\n",
      "                              FF = 15.947857856750488\n",
      "Finished epoch  90\n",
      "On epoch  91\n",
      "Total Epoch Testing MAPE: PCE = 84.02275085449219\n",
      "                              Voc = 11.973798751831055\n",
      "                              Jsc = 96.96727752685547\n",
      "                              FF = 15.874752044677734\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 81.93033599853516\n",
      "                              Voc = 11.668191909790039\n",
      "                              Jsc = 96.06317901611328\n",
      "                              FF = 15.79250431060791\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 81.84349060058594\n",
      "                              Voc = 13.302996635437012\n",
      "                              Jsc = 96.60899353027344\n",
      "                              FF = 14.807378768920898\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 81.56634521484375\n",
      "                              Voc = 12.54533863067627\n",
      "                              Jsc = 95.29159545898438\n",
      "                              FF = 13.76469898223877\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 82.20878601074219\n",
      "                              Voc = 13.338846206665039\n",
      "                              Jsc = 94.81153106689453\n",
      "                              FF = 14.638155937194824\n",
      "Finished epoch  95\n",
      "On epoch  96\n",
      "Total Epoch Testing MAPE: PCE = 83.01307678222656\n",
      "                              Voc = 13.567778587341309\n",
      "                              Jsc = 94.85990142822266\n",
      "                              FF = 14.27504825592041\n",
      "Finished epoch  96\n",
      "On epoch  97\n",
      "Total Epoch Testing MAPE: PCE = 83.00831604003906\n",
      "                              Voc = 12.971284866333008\n",
      "                              Jsc = 95.06082916259766\n",
      "                              FF = 14.548534393310547\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 81.14598846435547\n",
      "                              Voc = 12.806519508361816\n",
      "                              Jsc = 94.80230712890625\n",
      "                              FF = 13.93563175201416\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 81.86347198486328\n",
      "                              Voc = 12.39427375793457\n",
      "                              Jsc = 95.1490249633789\n",
      "                              FF = 15.030680656433105\n",
      "Finished epoch  99\n",
      "On epoch  100\n",
      "Total Epoch Testing MAPE: PCE = 81.70867919921875\n",
      "                              Voc = 12.760810852050781\n",
      "                              Jsc = 95.30221557617188\n",
      "                              FF = 15.647747039794922\n",
      "Finished epoch  100\n",
      "On epoch  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.44473266601562\n",
      "                              Voc = 13.454299926757812\n",
      "                              Jsc = 94.96278381347656\n",
      "                              FF = 16.289865493774414\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 84.79803466796875\n",
      "                              Voc = 14.073875427246094\n",
      "                              Jsc = 94.74518585205078\n",
      "                              FF = 15.909777641296387\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 84.90776062011719\n",
      "                              Voc = 14.170024871826172\n",
      "                              Jsc = 94.6725082397461\n",
      "                              FF = 15.469947814941406\n",
      "Finished epoch  103\n",
      "On epoch  104\n",
      "Total Epoch Testing MAPE: PCE = 84.4715576171875\n",
      "                              Voc = 14.200155258178711\n",
      "                              Jsc = 94.5435562133789\n",
      "                              FF = 15.65372085571289\n",
      "Finished epoch  104\n",
      "On epoch  105\n",
      "Total Epoch Testing MAPE: PCE = 83.70130920410156\n",
      "                              Voc = 15.96354866027832\n",
      "                              Jsc = 95.11837005615234\n",
      "                              FF = 16.64745330810547\n",
      "Finished epoch  105\n",
      "On epoch  106\n",
      "Total Epoch Testing MAPE: PCE = 83.72515106201172\n",
      "                              Voc = 14.488252639770508\n",
      "                              Jsc = 95.48546600341797\n",
      "                              FF = 16.37737274169922\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 85.31768035888672\n",
      "                              Voc = 14.734664916992188\n",
      "                              Jsc = 95.73599243164062\n",
      "                              FF = 16.533405303955078\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 84.70935821533203\n",
      "                              Voc = 15.277901649475098\n",
      "                              Jsc = 96.0489273071289\n",
      "                              FF = 16.71613311767578\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 84.28436279296875\n",
      "                              Voc = 13.502740859985352\n",
      "                              Jsc = 96.39521789550781\n",
      "                              FF = 17.023893356323242\n",
      "Finished epoch  109\n",
      "On epoch  110\n",
      "Total Epoch Testing MAPE: PCE = 86.0746078491211\n",
      "                              Voc = 13.431389808654785\n",
      "                              Jsc = 96.15618133544922\n",
      "                              FF = 16.639545440673828\n",
      "Finished epoch  110\n",
      "On epoch  111\n",
      "Total Epoch Testing MAPE: PCE = 85.94361877441406\n",
      "                              Voc = 13.347940444946289\n",
      "                              Jsc = 96.01776885986328\n",
      "                              FF = 16.44523811340332\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 86.87425231933594\n",
      "                              Voc = 12.223538398742676\n",
      "                              Jsc = 95.71247863769531\n",
      "                              FF = 17.295671463012695\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 88.07475280761719\n",
      "                              Voc = 12.970614433288574\n",
      "                              Jsc = 96.18824768066406\n",
      "                              FF = 17.420406341552734\n",
      "Finished epoch  113\n",
      "On epoch  114\n",
      "Total Epoch Testing MAPE: PCE = 87.64088439941406\n",
      "                              Voc = 13.14046859741211\n",
      "                              Jsc = 96.39620208740234\n",
      "                              FF = 17.099693298339844\n",
      "Finished epoch  114\n",
      "On epoch  115\n",
      "Total Epoch Testing MAPE: PCE = 86.97594451904297\n",
      "                              Voc = 13.36335277557373\n",
      "                              Jsc = 97.01380920410156\n",
      "                              FF = 17.154890060424805\n",
      "Finished epoch  115\n",
      "On epoch  116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 85.57733154296875\n",
      "                              Voc = 12.929132461547852\n",
      "                              Jsc = 96.78980255126953\n",
      "                              FF = 16.943296432495117\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 85.50823974609375\n",
      "                              Voc = 13.967233657836914\n",
      "                              Jsc = 96.41429901123047\n",
      "                              FF = 16.581270217895508\n",
      "Finished epoch  117\n",
      "On epoch  118\n",
      "Total Epoch Testing MAPE: PCE = 85.45390319824219\n",
      "                              Voc = 14.17357349395752\n",
      "                              Jsc = 96.39051818847656\n",
      "                              FF = 17.236909866333008\n",
      "Finished epoch  118\n",
      "On epoch  119\n",
      "Total Epoch Testing MAPE: PCE = 85.08859252929688\n",
      "                              Voc = 12.85453987121582\n",
      "                              Jsc = 95.93643188476562\n",
      "                              FF = 17.046161651611328\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 85.0671615600586\n",
      "                              Voc = 11.401205062866211\n",
      "                              Jsc = 95.5729751586914\n",
      "                              FF = 16.97589111328125\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 86.484619140625\n",
      "                              Voc = 11.444616317749023\n",
      "                              Jsc = 95.7384262084961\n",
      "                              FF = 16.64093589782715\n",
      "Finished epoch  121\n",
      "On epoch  122\n",
      "Total Epoch Testing MAPE: PCE = 85.63128662109375\n",
      "                              Voc = 10.985596656799316\n",
      "                              Jsc = 96.03370666503906\n",
      "                              FF = 16.985523223876953\n",
      "Finished epoch  122\n",
      "On epoch  123\n",
      "Total Epoch Testing MAPE: PCE = 85.29023742675781\n",
      "                              Voc = 11.017410278320312\n",
      "                              Jsc = 96.10408782958984\n",
      "                              FF = 16.19916343688965\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 83.3199691772461\n",
      "                              Voc = 11.672367095947266\n",
      "                              Jsc = 96.60279846191406\n",
      "                              FF = 16.175827026367188\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 80.25923156738281\n",
      "                              Voc = 11.012409210205078\n",
      "                              Jsc = 96.65789031982422\n",
      "                              FF = 15.50730037689209\n",
      "Finished epoch  125\n",
      "On epoch  126\n",
      "Total Epoch Testing MAPE: PCE = 80.37165069580078\n",
      "                              Voc = 10.899829864501953\n",
      "                              Jsc = 96.69892883300781\n",
      "                              FF = 16.19186019897461\n",
      "Finished epoch  126\n",
      "On epoch  127\n",
      "Total Epoch Testing MAPE: PCE = 78.989013671875\n",
      "                              Voc = 10.475092887878418\n",
      "                              Jsc = 96.81292724609375\n",
      "                              FF = 16.841232299804688\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 78.606201171875\n",
      "                              Voc = 10.425104141235352\n",
      "                              Jsc = 96.97378540039062\n",
      "                              FF = 16.655244827270508\n",
      "Finished epoch  128\n",
      "On epoch  129\n",
      "Total Epoch Testing MAPE: PCE = 80.26712799072266\n",
      "                              Voc = 11.525874137878418\n",
      "                              Jsc = 97.18869018554688\n",
      "                              FF = 15.692120552062988\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 80.78874969482422\n",
      "                              Voc = 11.681365966796875\n",
      "                              Jsc = 97.59683990478516\n",
      "                              FF = 16.040613174438477\n",
      "Finished epoch  130\n",
      "On epoch  131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.21267700195312\n",
      "                              Voc = 11.70694351196289\n",
      "                              Jsc = 97.16958618164062\n",
      "                              FF = 15.557321548461914\n",
      "Finished epoch  131\n",
      "On epoch  132\n",
      "Total Epoch Testing MAPE: PCE = 83.75616455078125\n",
      "                              Voc = 13.086870193481445\n",
      "                              Jsc = 97.27951049804688\n",
      "                              FF = 15.693285942077637\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 83.28997802734375\n",
      "                              Voc = 12.405519485473633\n",
      "                              Jsc = 97.4868392944336\n",
      "                              FF = 15.951648712158203\n",
      "Finished epoch  133\n",
      "On epoch  134\n",
      "Total Epoch Testing MAPE: PCE = 83.55401611328125\n",
      "                              Voc = 13.41385555267334\n",
      "                              Jsc = 97.63555145263672\n",
      "                              FF = 15.938896179199219\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 82.91246032714844\n",
      "                              Voc = 13.737581253051758\n",
      "                              Jsc = 97.76582336425781\n",
      "                              FF = 16.664379119873047\n",
      "Finished epoch  135\n",
      "On epoch  136\n",
      "Total Epoch Testing MAPE: PCE = 83.21920776367188\n",
      "                              Voc = 15.546777725219727\n",
      "                              Jsc = 97.64031982421875\n",
      "                              FF = 16.540332794189453\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 85.68497467041016\n",
      "                              Voc = 13.149359703063965\n",
      "                              Jsc = 97.01725769042969\n",
      "                              FF = 15.867006301879883\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 85.854248046875\n",
      "                              Voc = 13.379101753234863\n",
      "                              Jsc = 97.04859924316406\n",
      "                              FF = 15.73355484008789\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 84.24240112304688\n",
      "                              Voc = 14.459609985351562\n",
      "                              Jsc = 97.1361312866211\n",
      "                              FF = 15.578311920166016\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 85.12734985351562\n",
      "                              Voc = 15.76370906829834\n",
      "                              Jsc = 96.61820220947266\n",
      "                              FF = 15.352612495422363\n",
      "Finished epoch  140\n",
      "On epoch  141\n",
      "Total Epoch Testing MAPE: PCE = 85.08293914794922\n",
      "                              Voc = 15.341120719909668\n",
      "                              Jsc = 97.163818359375\n",
      "                              FF = 14.937835693359375\n",
      "Finished epoch  141\n",
      "On epoch  142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.99580383300781\n",
      "                              Voc = 14.707742691040039\n",
      "                              Jsc = 97.32591247558594\n",
      "                              FF = 14.75342845916748\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 82.68302154541016\n",
      "                              Voc = 14.377776145935059\n",
      "                              Jsc = 97.61185455322266\n",
      "                              FF = 14.026151657104492\n",
      "Finished epoch  143\n",
      "On epoch  144\n",
      "Total Epoch Testing MAPE: PCE = 83.50263977050781\n",
      "                              Voc = 15.083083152770996\n",
      "                              Jsc = 97.854248046875\n",
      "                              FF = 13.262128829956055\n",
      "Finished epoch  144\n",
      "On epoch  145\n",
      "Total Epoch Testing MAPE: PCE = 82.41241455078125\n",
      "                              Voc = 13.117106437683105\n",
      "                              Jsc = 97.81334686279297\n",
      "                              FF = 13.074117660522461\n",
      "Finished epoch  145\n",
      "On epoch  146\n",
      "Total Epoch Testing MAPE: PCE = 83.27820587158203\n",
      "                              Voc = 12.838491439819336\n",
      "                              Jsc = 98.01329040527344\n",
      "                              FF = 12.738607406616211\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 82.5009536743164\n",
      "                              Voc = 14.550142288208008\n",
      "                              Jsc = 97.77833557128906\n",
      "                              FF = 11.930017471313477\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 81.9808578491211\n",
      "                              Voc = 14.437058448791504\n",
      "                              Jsc = 97.44361877441406\n",
      "                              FF = 13.070456504821777\n",
      "Finished epoch  148\n",
      "On epoch  149\n",
      "Total Epoch Testing MAPE: PCE = 83.22329711914062\n",
      "                              Voc = 14.881070137023926\n",
      "                              Jsc = 97.67424774169922\n",
      "                              FF = 13.249567031860352\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 82.27477264404297\n",
      "                              Voc = 16.56065559387207\n",
      "                              Jsc = 97.5735855102539\n",
      "                              FF = 13.821511268615723\n",
      "Finished epoch  150\n",
      "On epoch  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "Total Epoch Testing MAPE: PCE = 82.88484191894531\n",
      "                              Voc = 16.4891357421875\n",
      "                              Jsc = 97.146240234375\n",
      "                              FF = 14.058820724487305\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 83.10073852539062\n",
      "                              Voc = 16.303709030151367\n",
      "                              Jsc = 96.93643188476562\n",
      "                              FF = 13.83236312866211\n",
      "Finished epoch  152\n",
      "On epoch  153\n",
      "Total Epoch Testing MAPE: PCE = 81.15790557861328\n",
      "                              Voc = 17.076343536376953\n",
      "                              Jsc = 96.82305145263672\n",
      "                              FF = 15.196832656860352\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 82.83841705322266\n",
      "                              Voc = 16.530109405517578\n",
      "                              Jsc = 96.96434783935547\n",
      "                              FF = 15.15820598602295\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 83.159912109375\n",
      "                              Voc = 17.276994705200195\n",
      "                              Jsc = 97.34404754638672\n",
      "                              FF = 15.116456031799316\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 84.47354125976562\n",
      "                              Voc = 17.343172073364258\n",
      "                              Jsc = 97.0743408203125\n",
      "                              FF = 14.201667785644531\n",
      "Finished epoch  156\n",
      "On epoch  157\n",
      "Total Epoch Testing MAPE: PCE = 85.38094329833984\n",
      "                              Voc = 17.980478286743164\n",
      "                              Jsc = 97.27371978759766\n",
      "                              FF = 13.99652099609375\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 85.67485809326172\n",
      "                              Voc = 19.242586135864258\n",
      "                              Jsc = 97.11978149414062\n",
      "                              FF = 14.226862907409668\n",
      "Finished epoch  158\n",
      "On epoch  159\n",
      "Total Epoch Testing MAPE: PCE = 84.11022186279297\n",
      "                              Voc = 19.321184158325195\n",
      "                              Jsc = 97.2450180053711\n",
      "                              FF = 14.354083061218262\n",
      "Finished epoch  159\n",
      "On epoch  160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.98477172851562\n",
      "                              Voc = 17.460790634155273\n",
      "                              Jsc = 97.03820037841797\n",
      "                              FF = 13.737380981445312\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 87.84285736083984\n",
      "                              Voc = 16.275218963623047\n",
      "                              Jsc = 96.90164947509766\n",
      "                              FF = 13.668164253234863\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 87.34510803222656\n",
      "                              Voc = 16.72930335998535\n",
      "                              Jsc = 97.13538360595703\n",
      "                              FF = 13.653712272644043\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 86.75816345214844\n",
      "                              Voc = 16.41557502746582\n",
      "                              Jsc = 97.4659652709961\n",
      "                              FF = 13.896356582641602\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 86.41804504394531\n",
      "                              Voc = 15.934608459472656\n",
      "                              Jsc = 97.30960845947266\n",
      "                              FF = 15.068467140197754\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 85.83470916748047\n",
      "                              Voc = 15.054207801818848\n",
      "                              Jsc = 97.3494873046875\n",
      "                              FF = 14.429224967956543\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 85.83475494384766\n",
      "                              Voc = 15.364286422729492\n",
      "                              Jsc = 96.921142578125\n",
      "                              FF = 14.790042877197266\n",
      "Finished epoch  166\n",
      "On epoch  167\n",
      "Total Epoch Testing MAPE: PCE = 83.9276123046875\n",
      "                              Voc = 16.102527618408203\n",
      "                              Jsc = 97.36186981201172\n",
      "                              FF = 14.548386573791504\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 85.3792724609375\n",
      "                              Voc = 15.762585639953613\n",
      "                              Jsc = 97.18484497070312\n",
      "                              FF = 14.08257007598877\n",
      "Finished epoch  168\n",
      "On epoch  169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 86.2625503540039\n",
      "                              Voc = 16.062225341796875\n",
      "                              Jsc = 96.90501403808594\n",
      "                              FF = 13.701531410217285\n",
      "Finished epoch  169\n",
      "On epoch  170\n",
      "Total Epoch Testing MAPE: PCE = 86.09716033935547\n",
      "                              Voc = 16.931631088256836\n",
      "                              Jsc = 96.79043579101562\n",
      "                              FF = 14.259044647216797\n",
      "Finished epoch  170\n",
      "On epoch  171\n",
      "Total Epoch Testing MAPE: PCE = 84.34352111816406\n",
      "                              Voc = 18.431787490844727\n",
      "                              Jsc = 96.84166717529297\n",
      "                              FF = 14.111627578735352\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 86.28955841064453\n",
      "                              Voc = 20.708675384521484\n",
      "                              Jsc = 97.4284439086914\n",
      "                              FF = 14.451873779296875\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 85.99116516113281\n",
      "                              Voc = 20.6907958984375\n",
      "                              Jsc = 97.25262451171875\n",
      "                              FF = 15.536676406860352\n",
      "Finished epoch  173\n",
      "On epoch  174\n",
      "Total Epoch Testing MAPE: PCE = 86.34813690185547\n",
      "                              Voc = 21.26238441467285\n",
      "                              Jsc = 97.0518569946289\n",
      "                              FF = 15.261590957641602\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 84.12286376953125\n",
      "                              Voc = 21.03019142150879\n",
      "                              Jsc = 96.82835388183594\n",
      "                              FF = 15.257051467895508\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 82.0652084350586\n",
      "                              Voc = 18.86571502685547\n",
      "                              Jsc = 96.59237670898438\n",
      "                              FF = 15.182039260864258\n",
      "Finished epoch  176\n",
      "On epoch  177\n",
      "Total Epoch Testing MAPE: PCE = 83.50517272949219\n",
      "                              Voc = 19.357742309570312\n",
      "                              Jsc = 96.37869262695312\n",
      "                              FF = 15.432601928710938\n",
      "Finished epoch  177\n",
      "On epoch  178\n",
      "Total Epoch Testing MAPE: PCE = 81.04486083984375\n",
      "                              Voc = 20.54876708984375\n",
      "                              Jsc = 96.10382843017578\n",
      "                              FF = 14.61602783203125\n",
      "Finished epoch  178\n",
      "On epoch  179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.38243103027344\n",
      "                              Voc = 19.966930389404297\n",
      "                              Jsc = 96.1455078125\n",
      "                              FF = 14.248353004455566\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 82.88069152832031\n",
      "                              Voc = 18.16181182861328\n",
      "                              Jsc = 96.55602264404297\n",
      "                              FF = 15.363868713378906\n",
      "Finished epoch  180\n",
      "On epoch  181\n",
      "Total Epoch Testing MAPE: PCE = 83.94969177246094\n",
      "                              Voc = 18.059406280517578\n",
      "                              Jsc = 96.29875946044922\n",
      "                              FF = 15.443218231201172\n",
      "Finished epoch  181\n",
      "On epoch  182\n",
      "Total Epoch Testing MAPE: PCE = 83.12361145019531\n",
      "                              Voc = 18.06134605407715\n",
      "                              Jsc = 96.62518310546875\n",
      "                              FF = 15.597007751464844\n",
      "Finished epoch  182\n",
      "On epoch  183\n",
      "Total Epoch Testing MAPE: PCE = 84.12855529785156\n",
      "                              Voc = 16.4370174407959\n",
      "                              Jsc = 96.65283203125\n",
      "                              FF = 15.529552459716797\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 83.25975799560547\n",
      "                              Voc = 15.803665161132812\n",
      "                              Jsc = 96.80540466308594\n",
      "                              FF = 16.05765151977539\n",
      "Finished epoch  184\n",
      "On epoch  185\n",
      "Total Epoch Testing MAPE: PCE = 83.72112274169922\n",
      "                              Voc = 15.80928897857666\n",
      "                              Jsc = 96.63045501708984\n",
      "                              FF = 16.355457305908203\n",
      "Finished epoch  185\n",
      "On epoch  186\n",
      "Total Epoch Testing MAPE: PCE = 84.26520538330078\n",
      "                              Voc = 14.867886543273926\n",
      "                              Jsc = 96.71263122558594\n",
      "                              FF = 16.341093063354492\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 84.23445892333984\n",
      "                              Voc = 14.872686386108398\n",
      "                              Jsc = 96.81580352783203\n",
      "                              FF = 16.7088680267334\n",
      "Finished epoch  187\n",
      "On epoch  188\n",
      "Total Epoch Testing MAPE: PCE = 85.6649398803711\n",
      "                              Voc = 14.292510032653809\n",
      "                              Jsc = 97.03009796142578\n",
      "                              FF = 16.582386016845703\n",
      "Finished epoch  188\n",
      "On epoch  189\n",
      "Total Epoch Testing MAPE: PCE = 85.98857116699219\n",
      "                              Voc = 14.213520050048828\n",
      "                              Jsc = 97.029052734375\n",
      "                              FF = 16.222814559936523\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 83.3985595703125\n",
      "                              Voc = 13.018501281738281\n",
      "                              Jsc = 97.19487762451172\n",
      "                              FF = 16.164649963378906\n",
      "Finished epoch  190\n",
      "On epoch  191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 81.97673034667969\n",
      "                              Voc = 16.041824340820312\n",
      "                              Jsc = 97.4273910522461\n",
      "                              FF = 16.473913192749023\n",
      "Finished epoch  191\n",
      "On epoch  192\n",
      "Total Epoch Testing MAPE: PCE = 83.29606628417969\n",
      "                              Voc = 14.577512741088867\n",
      "                              Jsc = 97.38758087158203\n",
      "                              FF = 16.5380859375\n",
      "Finished epoch  192\n",
      "On epoch  193\n",
      "Total Epoch Testing MAPE: PCE = 82.43388366699219\n",
      "                              Voc = 13.987104415893555\n",
      "                              Jsc = 97.70149993896484\n",
      "                              FF = 16.189945220947266\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 82.10803985595703\n",
      "                              Voc = 13.672775268554688\n",
      "                              Jsc = 97.30201721191406\n",
      "                              FF = 15.608498573303223\n",
      "Finished epoch  194\n",
      "On epoch  195\n",
      "Total Epoch Testing MAPE: PCE = 80.82071685791016\n",
      "                              Voc = 15.283785820007324\n",
      "                              Jsc = 97.75386047363281\n",
      "                              FF = 16.001468658447266\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 81.47804260253906\n",
      "                              Voc = 14.990385055541992\n",
      "                              Jsc = 97.75645446777344\n",
      "                              FF = 15.516814231872559\n",
      "Finished epoch  196\n",
      "On epoch  197\n",
      "Total Epoch Testing MAPE: PCE = 79.04512023925781\n",
      "                              Voc = 12.958441734313965\n",
      "                              Jsc = 97.53396606445312\n",
      "                              FF = 15.279478073120117\n",
      "Finished epoch  197\n",
      "On epoch  198\n",
      "Total Epoch Testing MAPE: PCE = 80.00096893310547\n",
      "                              Voc = 12.45976448059082\n",
      "                              Jsc = 97.18534851074219\n",
      "                              FF = 15.229853630065918\n",
      "Finished epoch  198\n",
      "On epoch  199\n",
      "Total Epoch Testing MAPE: PCE = 79.64252471923828\n",
      "                              Voc = 12.716447830200195\n",
      "                              Jsc = 97.10423278808594\n",
      "                              FF = 15.516406059265137\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 80.022705078125\n",
      "                              Voc = 14.1631441116333\n",
      "                              Jsc = 97.59258270263672\n",
      "                              FF = 14.783808708190918\n",
      "Finished epoch  200\n",
      "On epoch  201\n",
      "Total Epoch Testing MAPE: PCE = 80.53358459472656\n",
      "                              Voc = 15.192108154296875\n",
      "                              Jsc = 97.91571807861328\n",
      "                              FF = 14.999204635620117\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 81.67092895507812\n",
      "                              Voc = 14.099637985229492\n",
      "                              Jsc = 98.03822326660156\n",
      "                              FF = 15.509661674499512\n",
      "Finished epoch  202\n",
      "On epoch  203\n",
      "Total Epoch Testing MAPE: PCE = 80.11437225341797\n",
      "                              Voc = 13.572019577026367\n",
      "                              Jsc = 97.88079071044922\n",
      "                              FF = 15.271684646606445\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 80.16407775878906\n",
      "                              Voc = 13.82747745513916\n",
      "                              Jsc = 98.05143737792969\n",
      "                              FF = 15.364679336547852\n",
      "Finished epoch  204\n",
      "On epoch  205\n",
      "Total Epoch Testing MAPE: PCE = 81.34674072265625\n",
      "                              Voc = 13.814324378967285\n",
      "                              Jsc = 98.11448669433594\n",
      "                              FF = 15.506765365600586\n",
      "Finished epoch  205\n",
      "On epoch  206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 81.4212417602539\n",
      "                              Voc = 14.091489791870117\n",
      "                              Jsc = 97.91319274902344\n",
      "                              FF = 14.82398796081543\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 82.13549041748047\n",
      "                              Voc = 15.273432731628418\n",
      "                              Jsc = 97.81502532958984\n",
      "                              FF = 15.086889266967773\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 82.89390563964844\n",
      "                              Voc = 14.217403411865234\n",
      "                              Jsc = 97.78952026367188\n",
      "                              FF = 15.427106857299805\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 82.3536376953125\n",
      "                              Voc = 14.177038192749023\n",
      "                              Jsc = 97.84688568115234\n",
      "                              FF = 15.650062561035156\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 82.85509490966797\n",
      "                              Voc = 14.843036651611328\n",
      "                              Jsc = 97.63632202148438\n",
      "                              FF = 15.555328369140625\n",
      "Finished epoch  210\n",
      "On epoch  211\n",
      "Total Epoch Testing MAPE: PCE = 81.5553207397461\n",
      "                              Voc = 15.626363754272461\n",
      "                              Jsc = 97.5419921875\n",
      "                              FF = 16.03007698059082\n",
      "Finished epoch  211\n",
      "On epoch  212\n",
      "Total Epoch Testing MAPE: PCE = 80.27381134033203\n",
      "                              Voc = 13.748699188232422\n",
      "                              Jsc = 96.92547607421875\n",
      "                              FF = 16.008432388305664\n",
      "Finished epoch  212\n",
      "On epoch  213\n",
      "Total Epoch Testing MAPE: PCE = 79.73403930664062\n",
      "                              Voc = 13.141663551330566\n",
      "                              Jsc = 97.72947692871094\n",
      "                              FF = 15.88304615020752\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 80.48030853271484\n",
      "                              Voc = 15.549057960510254\n",
      "                              Jsc = 97.25643157958984\n",
      "                              FF = 16.189804077148438\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 80.35276794433594\n",
      "                              Voc = 16.475004196166992\n",
      "                              Jsc = 97.21141052246094\n",
      "                              FF = 16.369529724121094\n",
      "Finished epoch  215\n",
      "On epoch  216\n",
      "Total Epoch Testing MAPE: PCE = 81.61946105957031\n",
      "                              Voc = 16.20306396484375\n",
      "                              Jsc = 97.19575500488281\n",
      "                              FF = 16.51735496520996\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 80.0605697631836\n",
      "                              Voc = 16.55310821533203\n",
      "                              Jsc = 97.60780334472656\n",
      "                              FF = 16.331783294677734\n",
      "Finished epoch  217\n",
      "On epoch  218\n",
      "Total Epoch Testing MAPE: PCE = 81.30817413330078\n",
      "                              Voc = 17.087047576904297\n",
      "                              Jsc = 97.61001586914062\n",
      "                              FF = 16.645652770996094\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 80.85865783691406\n",
      "                              Voc = 15.82909107208252\n",
      "                              Jsc = 97.37555694580078\n",
      "                              FF = 16.1807918548584\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 83.29296112060547\n",
      "                              Voc = 16.146343231201172\n",
      "                              Jsc = 97.61563873291016\n",
      "                              FF = 17.201345443725586\n",
      "Finished epoch  220\n",
      "On epoch  221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 83.62834930419922\n",
      "                              Voc = 15.842448234558105\n",
      "                              Jsc = 97.55558776855469\n",
      "                              FF = 17.2627010345459\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 84.09684753417969\n",
      "                              Voc = 14.339455604553223\n",
      "                              Jsc = 97.65803527832031\n",
      "                              FF = 17.642684936523438\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 84.65324401855469\n",
      "                              Voc = 13.636941909790039\n",
      "                              Jsc = 97.71591186523438\n",
      "                              FF = 17.43402862548828\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 83.74712371826172\n",
      "                              Voc = 15.499259948730469\n",
      "                              Jsc = 97.74296569824219\n",
      "                              FF = 17.15115737915039\n",
      "Finished epoch  224\n",
      "On epoch  225\n",
      "Total Epoch Testing MAPE: PCE = 84.7083969116211\n",
      "                              Voc = 16.1129093170166\n",
      "                              Jsc = 97.59576416015625\n",
      "                              FF = 16.734731674194336\n",
      "Finished epoch  225\n",
      "On epoch  226\n",
      "Total Epoch Testing MAPE: PCE = 84.60820770263672\n",
      "                              Voc = 18.150775909423828\n",
      "                              Jsc = 97.5499038696289\n",
      "                              FF = 16.20591926574707\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 82.9966812133789\n",
      "                              Voc = 19.93022918701172\n",
      "                              Jsc = 97.50482177734375\n",
      "                              FF = 15.704605102539062\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 84.50419616699219\n",
      "                              Voc = 18.048032760620117\n",
      "                              Jsc = 97.5599365234375\n",
      "                              FF = 16.36571502685547\n",
      "Finished epoch  228\n",
      "On epoch  229\n",
      "Total Epoch Testing MAPE: PCE = 84.68926239013672\n",
      "                              Voc = 17.031145095825195\n",
      "                              Jsc = 97.7025375366211\n",
      "                              FF = 15.836530685424805\n",
      "Finished epoch  229\n",
      "On epoch  230\n",
      "Total Epoch Testing MAPE: PCE = 84.64013671875\n",
      "                              Voc = 16.227935791015625\n",
      "                              Jsc = 97.24201965332031\n",
      "                              FF = 15.761087417602539\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 85.07911682128906\n",
      "                              Voc = 16.071720123291016\n",
      "                              Jsc = 97.26533508300781\n",
      "                              FF = 15.55760383605957\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 84.6149673461914\n",
      "                              Voc = 13.708385467529297\n",
      "                              Jsc = 97.48131561279297\n",
      "                              FF = 16.118610382080078\n",
      "Finished epoch  232\n",
      "On epoch  233\n",
      "Total Epoch Testing MAPE: PCE = 84.11071014404297\n",
      "                              Voc = 13.844532012939453\n",
      "                              Jsc = 97.32784271240234\n",
      "                              FF = 16.975461959838867\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 83.17684173583984\n",
      "                              Voc = 12.1605806350708\n",
      "                              Jsc = 96.84695434570312\n",
      "                              FF = 16.906742095947266\n",
      "Finished epoch  234\n",
      "On epoch  235\n",
      "Total Epoch Testing MAPE: PCE = 82.18958282470703\n",
      "                              Voc = 11.328327178955078\n",
      "                              Jsc = 97.41927337646484\n",
      "                              FF = 16.042396545410156\n",
      "Finished epoch  235\n",
      "On epoch  236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.96273040771484\n",
      "                              Voc = 11.652152061462402\n",
      "                              Jsc = 97.28430938720703\n",
      "                              FF = 16.271976470947266\n",
      "Finished epoch  236\n",
      "On epoch  237\n",
      "Total Epoch Testing MAPE: PCE = 83.08687591552734\n",
      "                              Voc = 11.33139419555664\n",
      "                              Jsc = 97.54008483886719\n",
      "                              FF = 16.283802032470703\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 83.87735748291016\n",
      "                              Voc = 11.207164764404297\n",
      "                              Jsc = 97.38496398925781\n",
      "                              FF = 15.130940437316895\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 84.75985717773438\n",
      "                              Voc = 11.385507583618164\n",
      "                              Jsc = 97.20997619628906\n",
      "                              FF = 15.151039123535156\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 85.17120361328125\n",
      "                              Voc = 11.569649696350098\n",
      "                              Jsc = 97.4763412475586\n",
      "                              FF = 15.446064949035645\n",
      "Finished epoch  240\n",
      "On epoch  241\n",
      "Total Epoch Testing MAPE: PCE = 83.12480163574219\n",
      "                              Voc = 11.658397674560547\n",
      "                              Jsc = 97.70124816894531\n",
      "                              FF = 15.406438827514648\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 83.88417053222656\n",
      "                              Voc = 12.958964347839355\n",
      "                              Jsc = 97.18013763427734\n",
      "                              FF = 16.67493438720703\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 84.23841094970703\n",
      "                              Voc = 13.720780372619629\n",
      "                              Jsc = 97.22093200683594\n",
      "                              FF = 16.50370979309082\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 84.67315673828125\n",
      "                              Voc = 14.550664901733398\n",
      "                              Jsc = 96.95325469970703\n",
      "                              FF = 16.845687866210938\n",
      "Finished epoch  244\n",
      "On epoch  245\n",
      "Total Epoch Testing MAPE: PCE = 86.21343994140625\n",
      "                              Voc = 14.4649076461792\n",
      "                              Jsc = 96.58155822753906\n",
      "                              FF = 16.899385452270508\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 84.88213348388672\n",
      "                              Voc = 13.846336364746094\n",
      "                              Jsc = 96.91260528564453\n",
      "                              FF = 16.94460105895996\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 83.21978759765625\n",
      "                              Voc = 13.544082641601562\n",
      "                              Jsc = 96.25814819335938\n",
      "                              FF = 17.037841796875\n",
      "Finished epoch  247\n",
      "On epoch  248\n",
      "Total Epoch Testing MAPE: PCE = 84.90383911132812\n",
      "                              Voc = 14.168211936950684\n",
      "                              Jsc = 96.93787384033203\n",
      "                              FF = 16.571609497070312\n",
      "Finished epoch  248\n",
      "On epoch  249\n",
      "Total Epoch Testing MAPE: PCE = 84.02566528320312\n",
      "                              Voc = 13.978385925292969\n",
      "                              Jsc = 97.0135726928711\n",
      "                              FF = 16.508995056152344\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 85.28002166748047\n",
      "                              Voc = 14.445517539978027\n",
      "                              Jsc = 96.85152435302734\n",
      "                              FF = 16.358430862426758\n",
      "Finished epoch  250\n",
      "On epoch  251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 86.52777862548828\n",
      "                              Voc = 13.921058654785156\n",
      "                              Jsc = 96.65637969970703\n",
      "                              FF = 16.2808895111084\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 87.19363403320312\n",
      "                              Voc = 14.354125022888184\n",
      "                              Jsc = 96.96892547607422\n",
      "                              FF = 16.01483154296875\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 84.57256317138672\n",
      "                              Voc = 14.171266555786133\n",
      "                              Jsc = 96.94839477539062\n",
      "                              FF = 15.143205642700195\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 84.36315155029297\n",
      "                              Voc = 14.117613792419434\n",
      "                              Jsc = 97.12103271484375\n",
      "                              FF = 14.718877792358398\n",
      "Finished epoch  254\n",
      "On epoch  255\n",
      "Total Epoch Testing MAPE: PCE = 84.7177505493164\n",
      "                              Voc = 12.894251823425293\n",
      "                              Jsc = 97.27307891845703\n",
      "                              FF = 15.262118339538574\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 84.22298431396484\n",
      "                              Voc = 12.728485107421875\n",
      "                              Jsc = 97.60359191894531\n",
      "                              FF = 15.3591890335083\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 84.71363067626953\n",
      "                              Voc = 12.332265853881836\n",
      "                              Jsc = 97.6585693359375\n",
      "                              FF = 14.701947212219238\n",
      "Finished epoch  257\n",
      "On epoch  258\n",
      "Total Epoch Testing MAPE: PCE = 82.47723388671875\n",
      "                              Voc = 11.029609680175781\n",
      "                              Jsc = 97.66413879394531\n",
      "                              FF = 14.943700790405273\n",
      "Finished epoch  258\n",
      "On epoch  259\n",
      "Total Epoch Testing MAPE: PCE = 83.46392822265625\n",
      "                              Voc = 11.80302619934082\n",
      "                              Jsc = 97.53413391113281\n",
      "                              FF = 15.151812553405762\n",
      "Finished epoch  259\n",
      "On epoch  260\n",
      "Total Epoch Testing MAPE: PCE = 83.6347427368164\n",
      "                              Voc = 11.226484298706055\n",
      "                              Jsc = 97.46439361572266\n",
      "                              FF = 14.717531204223633\n",
      "Finished epoch  260\n",
      "On epoch  261\n",
      "Total Epoch Testing MAPE: PCE = 82.2191390991211\n",
      "                              Voc = 11.3566255569458\n",
      "                              Jsc = 97.23548889160156\n",
      "                              FF = 14.693376541137695\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 83.26579284667969\n",
      "                              Voc = 10.938589096069336\n",
      "                              Jsc = 97.49556732177734\n",
      "                              FF = 14.862422943115234\n",
      "Finished epoch  262\n",
      "On epoch  263\n",
      "Total Epoch Testing MAPE: PCE = 82.30967712402344\n",
      "                              Voc = 11.182228088378906\n",
      "                              Jsc = 97.76744079589844\n",
      "                              FF = 14.17215633392334\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 82.90760803222656\n",
      "                              Voc = 11.663724899291992\n",
      "                              Jsc = 97.56787109375\n",
      "                              FF = 14.616474151611328\n",
      "Finished epoch  264\n",
      "On epoch  265\n",
      "Total Epoch Testing MAPE: PCE = 83.9197006225586\n",
      "                              Voc = 11.494667053222656\n",
      "                              Jsc = 97.63545989990234\n",
      "                              FF = 15.40405559539795\n",
      "Finished epoch  265\n",
      "On epoch  266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 83.97367095947266\n",
      "                              Voc = 11.934089660644531\n",
      "                              Jsc = 97.77257537841797\n",
      "                              FF = 15.286734580993652\n",
      "Finished epoch  266\n",
      "On epoch  267\n",
      "Total Epoch Testing MAPE: PCE = 83.30126190185547\n",
      "                              Voc = 12.15600872039795\n",
      "                              Jsc = 97.95511627197266\n",
      "                              FF = 15.387776374816895\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 84.66165161132812\n",
      "                              Voc = 12.002405166625977\n",
      "                              Jsc = 97.71944427490234\n",
      "                              FF = 15.061744689941406\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 85.79566192626953\n",
      "                              Voc = 11.789016723632812\n",
      "                              Jsc = 97.34566497802734\n",
      "                              FF = 15.593537330627441\n",
      "Finished epoch  269\n",
      "On epoch  270\n",
      "Total Epoch Testing MAPE: PCE = 86.17172241210938\n",
      "                              Voc = 12.090290069580078\n",
      "                              Jsc = 97.42471313476562\n",
      "                              FF = 15.372074127197266\n",
      "Finished epoch  270\n",
      "On epoch  271\n",
      "Total Epoch Testing MAPE: PCE = 83.94174194335938\n",
      "                              Voc = 12.437206268310547\n",
      "                              Jsc = 97.45356750488281\n",
      "                              FF = 15.912466049194336\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 82.89460754394531\n",
      "                              Voc = 12.807149887084961\n",
      "                              Jsc = 97.39034271240234\n",
      "                              FF = 16.170724868774414\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 82.78559112548828\n",
      "                              Voc = 12.690576553344727\n",
      "                              Jsc = 97.19525909423828\n",
      "                              FF = 15.867167472839355\n",
      "Finished epoch  273\n",
      "On epoch  274\n",
      "Total Epoch Testing MAPE: PCE = 82.21863555908203\n",
      "                              Voc = 13.101970672607422\n",
      "                              Jsc = 97.05607604980469\n",
      "                              FF = 16.20187759399414\n",
      "Finished epoch  274\n",
      "On epoch  275\n",
      "Total Epoch Testing MAPE: PCE = 83.83929443359375\n",
      "                              Voc = 12.345917701721191\n",
      "                              Jsc = 96.48650360107422\n",
      "                              FF = 15.123804092407227\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 83.41254425048828\n",
      "                              Voc = 12.334712982177734\n",
      "                              Jsc = 96.74784851074219\n",
      "                              FF = 15.42381477355957\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 82.53506469726562\n",
      "                              Voc = 12.500596046447754\n",
      "                              Jsc = 96.75027465820312\n",
      "                              FF = 15.713292121887207\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 83.72899627685547\n",
      "                              Voc = 13.035667419433594\n",
      "                              Jsc = 96.50616455078125\n",
      "                              FF = 15.046117782592773\n",
      "Finished epoch  278\n",
      "On epoch  279\n",
      "Total Epoch Testing MAPE: PCE = 84.15782165527344\n",
      "                              Voc = 13.056905746459961\n",
      "                              Jsc = 96.64837646484375\n",
      "                              FF = 14.790834426879883\n",
      "Finished epoch  279\n",
      "On epoch  280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 83.23946380615234\n",
      "                              Voc = 12.712541580200195\n",
      "                              Jsc = 96.67835235595703\n",
      "                              FF = 15.50864028930664\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 84.38848114013672\n",
      "                              Voc = 12.890450477600098\n",
      "                              Jsc = 96.36808776855469\n",
      "                              FF = 14.914751052856445\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 85.15167999267578\n",
      "                              Voc = 12.719193458557129\n",
      "                              Jsc = 96.2624282836914\n",
      "                              FF = 13.900591850280762\n",
      "Finished epoch  282\n",
      "On epoch  283\n",
      "Total Epoch Testing MAPE: PCE = 85.21170043945312\n",
      "                              Voc = 12.661920547485352\n",
      "                              Jsc = 96.78043365478516\n",
      "                              FF = 13.826367378234863\n",
      "Finished epoch  283\n",
      "On epoch  284\n",
      "Total Epoch Testing MAPE: PCE = 85.2296371459961\n",
      "                              Voc = 12.609766006469727\n",
      "                              Jsc = 96.42167663574219\n",
      "                              FF = 13.533682823181152\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 82.51859283447266\n",
      "                              Voc = 12.562280654907227\n",
      "                              Jsc = 97.17719268798828\n",
      "                              FF = 13.773423194885254\n",
      "Finished epoch  285\n",
      "On epoch  286\n",
      "Total Epoch Testing MAPE: PCE = 82.64701080322266\n",
      "                              Voc = 13.274389266967773\n",
      "                              Jsc = 97.48136138916016\n",
      "                              FF = 14.424936294555664\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 83.18313598632812\n",
      "                              Voc = 12.118640899658203\n",
      "                              Jsc = 97.23370361328125\n",
      "                              FF = 13.83242416381836\n",
      "Finished epoch  287\n",
      "On epoch  288\n",
      "Total Epoch Testing MAPE: PCE = 82.56594848632812\n",
      "                              Voc = 11.971659660339355\n",
      "                              Jsc = 97.51133728027344\n",
      "                              FF = 13.766956329345703\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 82.59967041015625\n",
      "                              Voc = 12.008834838867188\n",
      "                              Jsc = 97.45275115966797\n",
      "                              FF = 13.847233772277832\n",
      "Finished epoch  289\n",
      "On epoch  290\n",
      "Total Epoch Testing MAPE: PCE = 82.21836853027344\n",
      "                              Voc = 12.504837989807129\n",
      "                              Jsc = 97.30171966552734\n",
      "                              FF = 13.105876922607422\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 80.67735290527344\n",
      "                              Voc = 12.32556438446045\n",
      "                              Jsc = 97.40853881835938\n",
      "                              FF = 13.523444175720215\n",
      "Finished epoch  291\n",
      "On epoch  292\n",
      "Total Epoch Testing MAPE: PCE = 81.24363708496094\n",
      "                              Voc = 12.298232078552246\n",
      "                              Jsc = 97.46273040771484\n",
      "                              FF = 13.089763641357422\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 79.82110595703125\n",
      "                              Voc = 12.638163566589355\n",
      "                              Jsc = 97.68746185302734\n",
      "                              FF = 13.316588401794434\n",
      "Finished epoch  293\n",
      "On epoch  294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 79.50021362304688\n",
      "                              Voc = 12.886187553405762\n",
      "                              Jsc = 97.37089538574219\n",
      "                              FF = 14.2181978225708\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 80.52848815917969\n",
      "                              Voc = 12.828664779663086\n",
      "                              Jsc = 97.53376007080078\n",
      "                              FF = 14.970715522766113\n",
      "Finished epoch  295\n",
      "On epoch  296\n",
      "Total Epoch Testing MAPE: PCE = 80.4277114868164\n",
      "                              Voc = 13.12157917022705\n",
      "                              Jsc = 97.54348754882812\n",
      "                              FF = 14.431962013244629\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 79.31816101074219\n",
      "                              Voc = 13.214315414428711\n",
      "                              Jsc = 97.77503967285156\n",
      "                              FF = 15.341764450073242\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 79.81156921386719\n",
      "                              Voc = 13.411224365234375\n",
      "                              Jsc = 97.7341537475586\n",
      "                              FF = 15.384225845336914\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 80.71025085449219\n",
      "                              Voc = 12.975422859191895\n",
      "                              Jsc = 97.56566619873047\n",
      "                              FF = 14.995277404785156\n",
      "Finished epoch  299\n",
      "On epoch  300\n",
      "Total Epoch Testing MAPE: PCE = 79.96643829345703\n",
      "                              Voc = 13.101187705993652\n",
      "                              Jsc = 97.8429183959961\n",
      "                              FF = 14.776084899902344\n",
      "Finished epoch  300\n",
      "On epoch  301\n",
      "Total Epoch Testing MAPE: PCE = 79.55012512207031\n",
      "                              Voc = 12.603534698486328\n",
      "                              Jsc = 97.67961883544922\n",
      "                              FF = 13.689501762390137\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 79.1650390625\n",
      "                              Voc = 12.902003288269043\n",
      "                              Jsc = 97.78218841552734\n",
      "                              FF = 13.308103561401367\n",
      "Finished epoch  302\n",
      "On epoch  303\n",
      "Total Epoch Testing MAPE: PCE = 81.13688659667969\n",
      "                              Voc = 13.60744857788086\n",
      "                              Jsc = 97.57431030273438\n",
      "                              FF = 13.887736320495605\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 80.90196228027344\n",
      "                              Voc = 12.460693359375\n",
      "                              Jsc = 97.39614868164062\n",
      "                              FF = 12.7028169631958\n",
      "Finished epoch  304\n",
      "On epoch  305\n",
      "Total Epoch Testing MAPE: PCE = 80.77455139160156\n",
      "                              Voc = 12.739227294921875\n",
      "                              Jsc = 97.58126068115234\n",
      "                              FF = 13.80339527130127\n",
      "Finished epoch  305\n",
      "On epoch  306\n",
      "Total Epoch Testing MAPE: PCE = 81.11609649658203\n",
      "                              Voc = 12.721880912780762\n",
      "                              Jsc = 97.59308624267578\n",
      "                              FF = 13.636019706726074\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 80.71129608154297\n",
      "                              Voc = 12.900334358215332\n",
      "                              Jsc = 97.74998474121094\n",
      "                              FF = 13.824978828430176\n",
      "Finished epoch  307\n",
      "On epoch  308\n",
      "Total Epoch Testing MAPE: PCE = 81.98966979980469\n",
      "                              Voc = 12.724018096923828\n",
      "                              Jsc = 97.57304382324219\n",
      "                              FF = 14.34047794342041\n",
      "Finished epoch  308\n",
      "On epoch  309\n",
      "Total Epoch Testing MAPE: PCE = 81.61339569091797\n",
      "                              Voc = 12.6435546875\n",
      "                              Jsc = 97.82948303222656\n",
      "                              FF = 15.024561882019043\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 83.46206665039062\n",
      "                              Voc = 12.47127914428711\n",
      "                              Jsc = 97.78529357910156\n",
      "                              FF = 14.92428207397461\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 83.14352416992188\n",
      "                              Voc = 12.429088592529297\n",
      "                              Jsc = 97.57244873046875\n",
      "                              FF = 14.733335494995117\n",
      "Finished epoch  311\n",
      "On epoch  312\n",
      "Total Epoch Testing MAPE: PCE = 83.76983642578125\n",
      "                              Voc = 12.437164306640625\n",
      "                              Jsc = 97.28145599365234\n",
      "                              FF = 15.627164840698242\n",
      "Finished epoch  312\n",
      "On epoch  313\n",
      "Total Epoch Testing MAPE: PCE = 83.61238098144531\n",
      "                              Voc = 12.523433685302734\n",
      "                              Jsc = 97.29496765136719\n",
      "                              FF = 16.356996536254883\n",
      "Finished epoch  313\n",
      "On epoch  314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.8427963256836\n",
      "                              Voc = 12.021666526794434\n",
      "                              Jsc = 97.4806900024414\n",
      "                              FF = 15.6857328414917\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 87.66963958740234\n",
      "                              Voc = 12.092581748962402\n",
      "                              Jsc = 97.6765365600586\n",
      "                              FF = 16.05121421813965\n",
      "Finished epoch  315\n",
      "On epoch  316\n",
      "Total Epoch Testing MAPE: PCE = 88.09967041015625\n",
      "                              Voc = 12.4498929977417\n",
      "                              Jsc = 97.28276062011719\n",
      "                              FF = 14.984016418457031\n",
      "Finished epoch  316\n",
      "On epoch  317\n",
      "Total Epoch Testing MAPE: PCE = 87.81908416748047\n",
      "                              Voc = 12.471988677978516\n",
      "                              Jsc = 97.2471694946289\n",
      "                              FF = 15.044135093688965\n",
      "Finished epoch  317\n",
      "On epoch  318\n",
      "Total Epoch Testing MAPE: PCE = 85.27310943603516\n",
      "                              Voc = 12.61763858795166\n",
      "                              Jsc = 97.27046966552734\n",
      "                              FF = 15.730303764343262\n",
      "Finished epoch  318\n",
      "On epoch  319\n",
      "Total Epoch Testing MAPE: PCE = 85.66920471191406\n",
      "                              Voc = 13.970084190368652\n",
      "                              Jsc = 97.49454498291016\n",
      "                              FF = 15.587313652038574\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 84.76158905029297\n",
      "                              Voc = 14.05193042755127\n",
      "                              Jsc = 97.4812240600586\n",
      "                              FF = 15.525467872619629\n",
      "Finished epoch  320\n",
      "On epoch  321\n",
      "Total Epoch Testing MAPE: PCE = 85.02256774902344\n",
      "                              Voc = 12.775641441345215\n",
      "                              Jsc = 97.59378051757812\n",
      "                              FF = 15.482352256774902\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 83.6776351928711\n",
      "                              Voc = 12.399371147155762\n",
      "                              Jsc = 97.79652404785156\n",
      "                              FF = 15.491840362548828\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 84.34293365478516\n",
      "                              Voc = 12.230203628540039\n",
      "                              Jsc = 97.88850402832031\n",
      "                              FF = 16.629859924316406\n",
      "Finished epoch  323\n",
      "On epoch  324\n",
      "Total Epoch Testing MAPE: PCE = 84.80962371826172\n",
      "                              Voc = 12.173760414123535\n",
      "                              Jsc = 97.7761001586914\n",
      "                              FF = 16.438095092773438\n",
      "Finished epoch  324\n",
      "On epoch  325\n",
      "Total Epoch Testing MAPE: PCE = 86.232421875\n",
      "                              Voc = 12.31935977935791\n",
      "                              Jsc = 97.75845336914062\n",
      "                              FF = 16.305557250976562\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 87.253662109375\n",
      "                              Voc = 11.912031173706055\n",
      "                              Jsc = 97.28926086425781\n",
      "                              FF = 15.546292304992676\n",
      "Finished epoch  326\n",
      "On epoch  327\n",
      "Total Epoch Testing MAPE: PCE = 83.83563995361328\n",
      "                              Voc = 12.183812141418457\n",
      "                              Jsc = 97.63093566894531\n",
      "                              FF = 15.738248825073242\n",
      "Finished epoch  327\n",
      "On epoch  328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.58454895019531\n",
      "                              Voc = 11.88947868347168\n",
      "                              Jsc = 97.74442291259766\n",
      "                              FF = 15.091863632202148\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 83.713623046875\n",
      "                              Voc = 12.25473690032959\n",
      "                              Jsc = 97.61309814453125\n",
      "                              FF = 15.869714736938477\n",
      "Finished epoch  329\n",
      "On epoch  330\n",
      "Total Epoch Testing MAPE: PCE = 84.72904968261719\n",
      "                              Voc = 11.935233116149902\n",
      "                              Jsc = 97.38309478759766\n",
      "                              FF = 15.927555084228516\n",
      "Finished epoch  330\n",
      "On epoch  331\n",
      "Total Epoch Testing MAPE: PCE = 85.47924041748047\n",
      "                              Voc = 11.941621780395508\n",
      "                              Jsc = 97.2044677734375\n",
      "                              FF = 15.222765922546387\n",
      "Finished epoch  331\n",
      "On epoch  332\n",
      "Total Epoch Testing MAPE: PCE = 84.24806213378906\n",
      "                              Voc = 11.983026504516602\n",
      "                              Jsc = 97.33245849609375\n",
      "                              FF = 14.801817893981934\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 83.74983215332031\n",
      "                              Voc = 12.181396484375\n",
      "                              Jsc = 97.48841857910156\n",
      "                              FF = 14.452261924743652\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 82.656005859375\n",
      "                              Voc = 12.625604629516602\n",
      "                              Jsc = 97.26300048828125\n",
      "                              FF = 15.463602066040039\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 83.02173614501953\n",
      "                              Voc = 12.669290542602539\n",
      "                              Jsc = 97.06709289550781\n",
      "                              FF = 14.638884544372559\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 82.99683380126953\n",
      "                              Voc = 12.472806930541992\n",
      "                              Jsc = 96.97520446777344\n",
      "                              FF = 14.374235153198242\n",
      "Finished epoch  336\n",
      "On epoch  337\n",
      "Total Epoch Testing MAPE: PCE = 82.95449829101562\n",
      "                              Voc = 12.763818740844727\n",
      "                              Jsc = 97.35724639892578\n",
      "                              FF = 14.541228294372559\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 83.6410140991211\n",
      "                              Voc = 13.533841133117676\n",
      "                              Jsc = 97.62559509277344\n",
      "                              FF = 14.603188514709473\n",
      "Finished epoch  338\n",
      "On epoch  339\n",
      "Total Epoch Testing MAPE: PCE = 83.81596374511719\n",
      "                              Voc = 13.9647855758667\n",
      "                              Jsc = 97.580810546875\n",
      "                              FF = 15.438851356506348\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 82.92820739746094\n",
      "                              Voc = 15.765836715698242\n",
      "                              Jsc = 97.50574493408203\n",
      "                              FF = 15.766853332519531\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 81.28499603271484\n",
      "                              Voc = 16.01289176940918\n",
      "                              Jsc = 97.5850830078125\n",
      "                              FF = 15.237578392028809\n",
      "Finished epoch  341\n",
      "On epoch  342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 81.6240005493164\n",
      "                              Voc = 17.009632110595703\n",
      "                              Jsc = 97.55390930175781\n",
      "                              FF = 15.727252960205078\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 81.74690246582031\n",
      "                              Voc = 15.938459396362305\n",
      "                              Jsc = 97.80718231201172\n",
      "                              FF = 15.411565780639648\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 82.9333267211914\n",
      "                              Voc = 15.802190780639648\n",
      "                              Jsc = 97.62516784667969\n",
      "                              FF = 15.189212799072266\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 82.75611877441406\n",
      "                              Voc = 16.106718063354492\n",
      "                              Jsc = 97.41899871826172\n",
      "                              FF = 15.935598373413086\n",
      "Finished epoch  345\n",
      "On epoch  346\n",
      "Total Epoch Testing MAPE: PCE = 83.29749298095703\n",
      "                              Voc = 14.65019416809082\n",
      "                              Jsc = 97.28038024902344\n",
      "                              FF = 16.424110412597656\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 82.28186798095703\n",
      "                              Voc = 13.999795913696289\n",
      "                              Jsc = 97.17974853515625\n",
      "                              FF = 16.1724853515625\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 83.66834259033203\n",
      "                              Voc = 13.456744194030762\n",
      "                              Jsc = 97.28378295898438\n",
      "                              FF = 16.102357864379883\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 83.85601806640625\n",
      "                              Voc = 13.349061965942383\n",
      "                              Jsc = 97.30442810058594\n",
      "                              FF = 16.524993896484375\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 83.69351196289062\n",
      "                              Voc = 13.247262954711914\n",
      "                              Jsc = 97.37113952636719\n",
      "                              FF = 16.136442184448242\n",
      "Finished epoch  350\n",
      "On epoch  351\n",
      "Total Epoch Testing MAPE: PCE = 82.3122787475586\n",
      "                              Voc = 12.636322021484375\n",
      "                              Jsc = 97.30059814453125\n",
      "                              FF = 16.27296257019043\n",
      "Finished epoch  351\n",
      "On epoch  352\n",
      "Total Epoch Testing MAPE: PCE = 82.7795181274414\n",
      "                              Voc = 12.29489803314209\n",
      "                              Jsc = 97.61243438720703\n",
      "                              FF = 15.738720893859863\n",
      "Finished epoch  352\n",
      "On epoch  353\n",
      "Total Epoch Testing MAPE: PCE = 83.04179382324219\n",
      "                              Voc = 12.453767776489258\n",
      "                              Jsc = 97.60236358642578\n",
      "                              FF = 15.867132186889648\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 84.2279281616211\n",
      "                              Voc = 13.147086143493652\n",
      "                              Jsc = 97.7281265258789\n",
      "                              FF = 16.176408767700195\n",
      "Finished epoch  354\n",
      "On epoch  355\n",
      "Total Epoch Testing MAPE: PCE = 83.92527770996094\n",
      "                              Voc = 12.539895057678223\n",
      "                              Jsc = 97.67403411865234\n",
      "                              FF = 15.682201385498047\n",
      "Finished epoch  355\n",
      "On epoch  356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.88839721679688\n",
      "                              Voc = 12.52188491821289\n",
      "                              Jsc = 97.73786926269531\n",
      "                              FF = 16.228023529052734\n",
      "Finished epoch  356\n",
      "On epoch  357\n",
      "Total Epoch Testing MAPE: PCE = 85.75640106201172\n",
      "                              Voc = 13.0418119430542\n",
      "                              Jsc = 97.73004913330078\n",
      "                              FF = 16.15360450744629\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 85.62783813476562\n",
      "                              Voc = 12.480758666992188\n",
      "                              Jsc = 97.815185546875\n",
      "                              FF = 16.77464485168457\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 85.40021514892578\n",
      "                              Voc = 12.183478355407715\n",
      "                              Jsc = 97.75515747070312\n",
      "                              FF = 16.452224731445312\n",
      "Finished epoch  359\n",
      "On epoch  360\n",
      "Total Epoch Testing MAPE: PCE = 85.41310119628906\n",
      "                              Voc = 12.169118881225586\n",
      "                              Jsc = 97.41693878173828\n",
      "                              FF = 15.866494178771973\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 84.42383575439453\n",
      "                              Voc = 11.911325454711914\n",
      "                              Jsc = 97.7266616821289\n",
      "                              FF = 15.86031723022461\n",
      "Finished epoch  361\n",
      "On epoch  362\n",
      "Total Epoch Testing MAPE: PCE = 83.48454284667969\n",
      "                              Voc = 12.184311866760254\n",
      "                              Jsc = 97.7785415649414\n",
      "                              FF = 15.734436988830566\n",
      "Finished epoch  362\n",
      "On epoch  363\n",
      "Total Epoch Testing MAPE: PCE = 81.51708221435547\n",
      "                              Voc = 12.386350631713867\n",
      "                              Jsc = 97.47882843017578\n",
      "                              FF = 15.896248817443848\n",
      "Finished epoch  363\n",
      "On epoch  364\n",
      "Total Epoch Testing MAPE: PCE = 81.53128814697266\n",
      "                              Voc = 12.862003326416016\n",
      "                              Jsc = 97.4838638305664\n",
      "                              FF = 16.100688934326172\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 80.05830383300781\n",
      "                              Voc = 13.282086372375488\n",
      "                              Jsc = 97.52391815185547\n",
      "                              FF = 16.28573226928711\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 81.33644104003906\n",
      "                              Voc = 12.569597244262695\n",
      "                              Jsc = 97.17927551269531\n",
      "                              FF = 16.046289443969727\n",
      "Finished epoch  366\n",
      "On epoch  367\n",
      "Total Epoch Testing MAPE: PCE = 79.44368743896484\n",
      "                              Voc = 12.704041481018066\n",
      "                              Jsc = 97.20863342285156\n",
      "                              FF = 16.160297393798828\n",
      "Finished epoch  367\n",
      "On epoch  368\n",
      "Total Epoch Testing MAPE: PCE = 78.97005462646484\n",
      "                              Voc = 13.191503524780273\n",
      "                              Jsc = 97.04299926757812\n",
      "                              FF = 16.543004989624023\n",
      "Finished epoch  368\n",
      "On epoch  369\n",
      "Total Epoch Testing MAPE: PCE = 76.17586517333984\n",
      "                              Voc = 13.382599830627441\n",
      "                              Jsc = 96.61837768554688\n",
      "                              FF = 15.784194946289062\n",
      "Finished epoch  369\n",
      "On epoch  370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 78.32243347167969\n",
      "                              Voc = 12.318536758422852\n",
      "                              Jsc = 96.92859649658203\n",
      "                              FF = 15.661531448364258\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 79.37625122070312\n",
      "                              Voc = 12.61005973815918\n",
      "                              Jsc = 97.39433288574219\n",
      "                              FF = 14.90065860748291\n",
      "Finished epoch  371\n",
      "On epoch  372\n",
      "Total Epoch Testing MAPE: PCE = 81.66043090820312\n",
      "                              Voc = 12.868660926818848\n",
      "                              Jsc = 97.59358978271484\n",
      "                              FF = 13.506144523620605\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 80.51749420166016\n",
      "                              Voc = 14.27268123626709\n",
      "                              Jsc = 97.42090606689453\n",
      "                              FF = 14.210997581481934\n",
      "Finished epoch  373\n",
      "On epoch  374\n",
      "Total Epoch Testing MAPE: PCE = 82.06869506835938\n",
      "                              Voc = 14.232322692871094\n",
      "                              Jsc = 96.9930648803711\n",
      "                              FF = 13.853466033935547\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 82.84766387939453\n",
      "                              Voc = 15.112214088439941\n",
      "                              Jsc = 97.3194808959961\n",
      "                              FF = 14.471175193786621\n",
      "Finished epoch  375\n",
      "On epoch  376\n",
      "Total Epoch Testing MAPE: PCE = 83.74182891845703\n",
      "                              Voc = 14.631295204162598\n",
      "                              Jsc = 97.53517150878906\n",
      "                              FF = 14.354151725769043\n",
      "Finished epoch  376\n",
      "On epoch  377\n",
      "Total Epoch Testing MAPE: PCE = 84.24203491210938\n",
      "                              Voc = 14.299978256225586\n",
      "                              Jsc = 97.43309783935547\n",
      "                              FF = 14.892777442932129\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 83.8588638305664\n",
      "                              Voc = 14.699116706848145\n",
      "                              Jsc = 97.74227142333984\n",
      "                              FF = 15.149295806884766\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 83.64030456542969\n",
      "                              Voc = 14.620134353637695\n",
      "                              Jsc = 97.40618896484375\n",
      "                              FF = 15.179567337036133\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 82.44949340820312\n",
      "                              Voc = 14.784442901611328\n",
      "                              Jsc = 97.18163299560547\n",
      "                              FF = 14.477354049682617\n",
      "Finished epoch  380\n",
      "On epoch  381\n",
      "Total Epoch Testing MAPE: PCE = 83.5347900390625\n",
      "                              Voc = 14.514495849609375\n",
      "                              Jsc = 97.00382232666016\n",
      "                              FF = 14.042804718017578\n",
      "Finished epoch  381\n",
      "On epoch  382\n",
      "Total Epoch Testing MAPE: PCE = 83.66998291015625\n",
      "                              Voc = 14.190755844116211\n",
      "                              Jsc = 96.82563781738281\n",
      "                              FF = 14.159175872802734\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 83.8009262084961\n",
      "                              Voc = 13.335577964782715\n",
      "                              Jsc = 96.97503662109375\n",
      "                              FF = 15.153543472290039\n",
      "Finished epoch  383\n",
      "On epoch  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.97229766845703\n",
      "                              Voc = 12.691707611083984\n",
      "                              Jsc = 96.24369049072266\n",
      "                              FF = 14.496207237243652\n",
      "Finished epoch  384\n",
      "On epoch  385\n",
      "Total Epoch Testing MAPE: PCE = 84.756103515625\n",
      "                              Voc = 12.355706214904785\n",
      "                              Jsc = 96.87459564208984\n",
      "                              FF = 15.283266067504883\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 84.70420837402344\n",
      "                              Voc = 12.917876243591309\n",
      "                              Jsc = 96.88672637939453\n",
      "                              FF = 15.900167465209961\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 84.13359832763672\n",
      "                              Voc = 13.541605949401855\n",
      "                              Jsc = 97.0190200805664\n",
      "                              FF = 14.940511703491211\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 82.99954223632812\n",
      "                              Voc = 14.044992446899414\n",
      "                              Jsc = 96.81351470947266\n",
      "                              FF = 15.227301597595215\n",
      "Finished epoch  388\n",
      "On epoch  389\n",
      "Total Epoch Testing MAPE: PCE = 83.92860412597656\n",
      "                              Voc = 13.344051361083984\n",
      "                              Jsc = 96.27876281738281\n",
      "                              FF = 14.943634033203125\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 84.33985137939453\n",
      "                              Voc = 15.006491661071777\n",
      "                              Jsc = 96.62040710449219\n",
      "                              FF = 14.972116470336914\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 83.78485107421875\n",
      "                              Voc = 13.047896385192871\n",
      "                              Jsc = 96.8246078491211\n",
      "                              FF = 15.11109447479248\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 84.34471130371094\n",
      "                              Voc = 12.330395698547363\n",
      "                              Jsc = 97.16637420654297\n",
      "                              FF = 15.301937103271484\n",
      "Finished epoch  392\n",
      "On epoch  393\n",
      "Total Epoch Testing MAPE: PCE = 82.5762939453125\n",
      "                              Voc = 12.260354995727539\n",
      "                              Jsc = 97.12841796875\n",
      "                              FF = 15.145709037780762\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 81.78842163085938\n",
      "                              Voc = 12.265397071838379\n",
      "                              Jsc = 97.55965423583984\n",
      "                              FF = 14.85015869140625\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 82.08429718017578\n",
      "                              Voc = 13.289773941040039\n",
      "                              Jsc = 96.97372436523438\n",
      "                              FF = 15.21943187713623\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 81.84020233154297\n",
      "                              Voc = 13.062949180603027\n",
      "                              Jsc = 96.67617797851562\n",
      "                              FF = 15.302217483520508\n",
      "Finished epoch  396\n",
      "On epoch  397\n",
      "Total Epoch Testing MAPE: PCE = 81.04579162597656\n",
      "                              Voc = 12.99394416809082\n",
      "                              Jsc = 96.81681060791016\n",
      "                              FF = 14.397811889648438\n",
      "Finished epoch  397\n",
      "On epoch  398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 81.67581939697266\n",
      "                              Voc = 12.155251502990723\n",
      "                              Jsc = 96.77102661132812\n",
      "                              FF = 15.177353858947754\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 81.51438903808594\n",
      "                              Voc = 12.343892097473145\n",
      "                              Jsc = 96.75957489013672\n",
      "                              FF = 15.228409767150879\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 83.06512451171875\n",
      "                              Voc = 11.98324966430664\n",
      "                              Jsc = 96.91638946533203\n",
      "                              FF = 14.820104598999023\n",
      "Finished epoch  400\n",
      "On epoch  401\n",
      "Total Epoch Testing MAPE: PCE = 82.49414825439453\n",
      "                              Voc = 11.757901191711426\n",
      "                              Jsc = 97.19024658203125\n",
      "                              FF = 14.94714641571045\n",
      "Finished epoch  401\n",
      "On epoch  402\n",
      "Total Epoch Testing MAPE: PCE = 82.67198944091797\n",
      "                              Voc = 11.505109786987305\n",
      "                              Jsc = 97.2518539428711\n",
      "                              FF = 14.448129653930664\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 84.75331115722656\n",
      "                              Voc = 11.571693420410156\n",
      "                              Jsc = 97.00817108154297\n",
      "                              FF = 14.430244445800781\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 84.739013671875\n",
      "                              Voc = 11.420960426330566\n",
      "                              Jsc = 96.89129638671875\n",
      "                              FF = 14.21467399597168\n",
      "Finished epoch  404\n",
      "On epoch  405\n",
      "Total Epoch Testing MAPE: PCE = 83.42478942871094\n",
      "                              Voc = 11.732702255249023\n",
      "                              Jsc = 96.46060180664062\n",
      "                              FF = 14.505204200744629\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 81.845703125\n",
      "                              Voc = 11.880526542663574\n",
      "                              Jsc = 95.86112213134766\n",
      "                              FF = 13.722655296325684\n",
      "Finished epoch  406\n",
      "On epoch  407\n",
      "Total Epoch Testing MAPE: PCE = 82.91253662109375\n",
      "                              Voc = 12.091806411743164\n",
      "                              Jsc = 96.2638931274414\n",
      "                              FF = 13.471841812133789\n",
      "Finished epoch  407\n",
      "On epoch  408\n",
      "Total Epoch Testing MAPE: PCE = 81.50540924072266\n",
      "                              Voc = 12.28358268737793\n",
      "                              Jsc = 96.76307678222656\n",
      "                              FF = 13.805474281311035\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 82.31128692626953\n",
      "                              Voc = 13.02634334564209\n",
      "                              Jsc = 96.98053741455078\n",
      "                              FF = 14.825998306274414\n",
      "Finished epoch  409\n",
      "On epoch  410\n",
      "Total Epoch Testing MAPE: PCE = 81.49304962158203\n",
      "                              Voc = 11.845356941223145\n",
      "                              Jsc = 96.40904235839844\n",
      "                              FF = 14.427663803100586\n",
      "Finished epoch  410\n",
      "On epoch  411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 81.10191345214844\n",
      "                              Voc = 13.491111755371094\n",
      "                              Jsc = 96.18106842041016\n",
      "                              FF = 14.697305679321289\n",
      "Finished epoch  411\n",
      "On epoch  412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.94591522216797\n",
      "                              Voc = 12.018111228942871\n",
      "                              Jsc = 96.06587982177734\n",
      "                              FF = 14.423589706420898\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 82.50894165039062\n",
      "                              Voc = 12.240602493286133\n",
      "                              Jsc = 96.15711212158203\n",
      "                              FF = 14.34634017944336\n",
      "Finished epoch  413\n",
      "On epoch  414\n",
      "Total Epoch Testing MAPE: PCE = 84.68680572509766\n",
      "                              Voc = 12.045523643493652\n",
      "                              Jsc = 96.09161376953125\n",
      "                              FF = 14.934857368469238\n",
      "Finished epoch  414\n",
      "On epoch  415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.25006866455078\n",
      "                              Voc = 12.381331443786621\n",
      "                              Jsc = 96.39849853515625\n",
      "                              FF = 15.033056259155273\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 84.2584228515625\n",
      "                              Voc = 13.485713005065918\n",
      "                              Jsc = 96.53560638427734\n",
      "                              FF = 15.740182876586914\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 83.67901611328125\n",
      "                              Voc = 13.103538513183594\n",
      "                              Jsc = 96.77749633789062\n",
      "                              FF = 15.814946174621582\n",
      "Finished epoch  417\n",
      "On epoch  418\n",
      "Total Epoch Testing MAPE: PCE = 83.97151947021484\n",
      "                              Voc = 14.108914375305176\n",
      "                              Jsc = 96.33367919921875\n",
      "                              FF = 15.334847450256348\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 83.99237823486328\n",
      "                              Voc = 12.35382080078125\n",
      "                              Jsc = 96.64159393310547\n",
      "                              FF = 15.183008193969727\n",
      "Finished epoch  419\n",
      "On epoch  420\n",
      "Total Epoch Testing MAPE: PCE = 84.523681640625\n",
      "                              Voc = 13.062919616699219\n",
      "                              Jsc = 96.71761322021484\n",
      "                              FF = 15.095928192138672\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 84.9717788696289\n",
      "                              Voc = 14.698561668395996\n",
      "                              Jsc = 97.33164978027344\n",
      "                              FF = 15.404558181762695\n",
      "Finished epoch  421\n",
      "On epoch  422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.92647552490234\n",
      "                              Voc = 13.851216316223145\n",
      "                              Jsc = 97.4354248046875\n",
      "                              FF = 15.06042194366455\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 86.03089904785156\n",
      "                              Voc = 15.645120620727539\n",
      "                              Jsc = 97.10044860839844\n",
      "                              FF = 15.298138618469238\n",
      "Finished epoch  423\n",
      "On epoch  424\n",
      "Total Epoch Testing MAPE: PCE = 84.90636444091797\n",
      "                              Voc = 15.544137001037598\n",
      "                              Jsc = 96.85240173339844\n",
      "                              FF = 14.73110294342041\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 86.77851867675781\n",
      "                              Voc = 17.375537872314453\n",
      "                              Jsc = 96.85733032226562\n",
      "                              FF = 15.662701606750488\n",
      "Finished epoch  425\n",
      "On epoch  426\n",
      "Total Epoch Testing MAPE: PCE = 87.58704376220703\n",
      "                              Voc = 16.56731605529785\n",
      "                              Jsc = 96.8492660522461\n",
      "                              FF = 15.422415733337402\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 86.78253936767578\n",
      "                              Voc = 15.988579750061035\n",
      "                              Jsc = 96.68658447265625\n",
      "                              FF = 15.357354164123535\n",
      "Finished epoch  427\n",
      "On epoch  428\n",
      "Total Epoch Testing MAPE: PCE = 85.46165466308594\n",
      "                              Voc = 17.326919555664062\n",
      "                              Jsc = 96.94793701171875\n",
      "                              FF = 15.847912788391113\n",
      "Finished epoch  428\n",
      "On epoch  429\n",
      "Total Epoch Testing MAPE: PCE = 86.06033325195312\n",
      "                              Voc = 17.203304290771484\n",
      "                              Jsc = 97.09173583984375\n",
      "                              FF = 15.407669067382812\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 86.15866088867188\n",
      "                              Voc = 16.54793357849121\n",
      "                              Jsc = 97.147216796875\n",
      "                              FF = 15.725713729858398\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 87.99700927734375\n",
      "                              Voc = 16.366636276245117\n",
      "                              Jsc = 96.87263488769531\n",
      "                              FF = 15.809524536132812\n",
      "Finished epoch  431\n",
      "On epoch  432\n",
      "Total Epoch Testing MAPE: PCE = 88.22362518310547\n",
      "                              Voc = 15.55639362335205\n",
      "                              Jsc = 96.53942108154297\n",
      "                              FF = 16.097553253173828\n",
      "Finished epoch  432\n",
      "On epoch  433\n",
      "Total Epoch Testing MAPE: PCE = 87.5904769897461\n",
      "                              Voc = 17.457971572875977\n",
      "                              Jsc = 96.89041900634766\n",
      "                              FF = 17.317138671875\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 88.59200286865234\n",
      "                              Voc = 17.246273040771484\n",
      "                              Jsc = 97.3070297241211\n",
      "                              FF = 16.89059066772461\n",
      "Finished epoch  434\n",
      "On epoch  435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 87.11212921142578\n",
      "                              Voc = 17.724035263061523\n",
      "                              Jsc = 97.0499496459961\n",
      "                              FF = 16.123899459838867\n",
      "Finished epoch  435\n",
      "On epoch  436\n",
      "Total Epoch Testing MAPE: PCE = 87.33268737792969\n",
      "                              Voc = 18.813156127929688\n",
      "                              Jsc = 96.58656311035156\n",
      "                              FF = 16.737239837646484\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 85.96080780029297\n",
      "                              Voc = 17.983583450317383\n",
      "                              Jsc = 96.81095123291016\n",
      "                              FF = 16.98699378967285\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 84.15106201171875\n",
      "                              Voc = 17.254295349121094\n",
      "                              Jsc = 96.86915588378906\n",
      "                              FF = 17.477890014648438\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 83.71398162841797\n",
      "                              Voc = 17.553977966308594\n",
      "                              Jsc = 97.37614440917969\n",
      "                              FF = 17.99761199951172\n",
      "Finished epoch  439\n",
      "On epoch  440\n",
      "Total Epoch Testing MAPE: PCE = 83.52021789550781\n",
      "                              Voc = 16.5317325592041\n",
      "                              Jsc = 97.4899673461914\n",
      "                              FF = 17.640052795410156\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 84.61712646484375\n",
      "                              Voc = 16.74887466430664\n",
      "                              Jsc = 97.64879608154297\n",
      "                              FF = 16.679277420043945\n",
      "Finished epoch  441\n",
      "On epoch  442\n",
      "Total Epoch Testing MAPE: PCE = 85.818359375\n",
      "                              Voc = 18.488950729370117\n",
      "                              Jsc = 97.64189910888672\n",
      "                              FF = 16.358686447143555\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 86.55184173583984\n",
      "                              Voc = 18.148113250732422\n",
      "                              Jsc = 97.70658874511719\n",
      "                              FF = 15.947288513183594\n",
      "Finished epoch  443\n",
      "On epoch  444\n",
      "Total Epoch Testing MAPE: PCE = 86.603271484375\n",
      "                              Voc = 16.458337783813477\n",
      "                              Jsc = 97.4223403930664\n",
      "                              FF = 16.10162925720215\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 86.62379455566406\n",
      "                              Voc = 15.67264461517334\n",
      "                              Jsc = 97.61182403564453\n",
      "                              FF = 16.082826614379883\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 85.64540100097656\n",
      "                              Voc = 14.853480339050293\n",
      "                              Jsc = 97.49207305908203\n",
      "                              FF = 16.944984436035156\n",
      "Finished epoch  446\n",
      "On epoch  447\n",
      "Total Epoch Testing MAPE: PCE = 84.1159439086914\n",
      "                              Voc = 15.827581405639648\n",
      "                              Jsc = 97.13056182861328\n",
      "                              FF = 16.970720291137695\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 84.88142395019531\n",
      "                              Voc = 14.404709815979004\n",
      "                              Jsc = 97.55425262451172\n",
      "                              FF = 17.26704216003418\n",
      "Finished epoch  448\n",
      "On epoch  449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 85.44642639160156\n",
      "                              Voc = 14.741908073425293\n",
      "                              Jsc = 97.75255584716797\n",
      "                              FF = 16.146459579467773\n",
      "Finished epoch  449\n",
      "On epoch  450\n",
      "Total Epoch Testing MAPE: PCE = 82.51304626464844\n",
      "                              Voc = 15.254727363586426\n",
      "                              Jsc = 97.8014144897461\n",
      "                              FF = 15.822129249572754\n",
      "Finished epoch  450\n",
      "On epoch  451\n",
      "Total Epoch Testing MAPE: PCE = 83.84526062011719\n",
      "                              Voc = 15.660402297973633\n",
      "                              Jsc = 97.80176544189453\n",
      "                              FF = 16.304115295410156\n",
      "Finished epoch  451\n",
      "On epoch  452\n",
      "Total Epoch Testing MAPE: PCE = 85.63584899902344\n",
      "                              Voc = 17.45371437072754\n",
      "                              Jsc = 97.49554443359375\n",
      "                              FF = 16.05457305908203\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 84.3936767578125\n",
      "                              Voc = 17.359657287597656\n",
      "                              Jsc = 97.51750183105469\n",
      "                              FF = 15.471256256103516\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 85.43499755859375\n",
      "                              Voc = 18.17107582092285\n",
      "                              Jsc = 97.78825378417969\n",
      "                              FF = 15.58664321899414\n",
      "Finished epoch  454\n",
      "On epoch  455\n",
      "Total Epoch Testing MAPE: PCE = 84.45928192138672\n",
      "                              Voc = 19.882034301757812\n",
      "                              Jsc = 97.33773803710938\n",
      "                              FF = 15.413681030273438\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 83.79995727539062\n",
      "                              Voc = 21.04738998413086\n",
      "                              Jsc = 97.47145080566406\n",
      "                              FF = 14.59915828704834\n",
      "Finished epoch  456\n",
      "On epoch  457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.02216339111328\n",
      "                              Voc = 18.715484619140625\n",
      "                              Jsc = 97.53845977783203\n",
      "                              FF = 15.246784210205078\n",
      "Finished epoch  457\n",
      "On epoch  458\n",
      "Total Epoch Testing MAPE: PCE = 81.95468139648438\n",
      "                              Voc = 17.53444480895996\n",
      "                              Jsc = 97.38431549072266\n",
      "                              FF = 14.65191650390625\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 82.13357543945312\n",
      "                              Voc = 18.44743537902832\n",
      "                              Jsc = 97.298828125\n",
      "                              FF = 15.40865707397461\n",
      "Finished epoch  459\n",
      "On epoch  460\n",
      "Total Epoch Testing MAPE: PCE = 82.86434173583984\n",
      "                              Voc = 18.712312698364258\n",
      "                              Jsc = 96.77928924560547\n",
      "                              FF = 15.428845405578613\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 81.1343994140625\n",
      "                              Voc = 19.146759033203125\n",
      "                              Jsc = 96.75503540039062\n",
      "                              FF = 14.382007598876953\n",
      "Finished epoch  461\n",
      "On epoch  462\n",
      "Total Epoch Testing MAPE: PCE = 82.1184310913086\n",
      "                              Voc = 18.823766708374023\n",
      "                              Jsc = 97.08283233642578\n",
      "                              FF = 15.263748168945312\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 81.55549621582031\n",
      "                              Voc = 18.077072143554688\n",
      "                              Jsc = 97.27937316894531\n",
      "                              FF = 15.89034652709961\n",
      "Finished epoch  463\n",
      "On epoch  464\n",
      "Total Epoch Testing MAPE: PCE = 81.15287780761719\n",
      "                              Voc = 18.996564865112305\n",
      "                              Jsc = 97.25591278076172\n",
      "                              FF = 15.380154609680176\n",
      "Finished epoch  464\n",
      "On epoch  465\n",
      "Total Epoch Testing MAPE: PCE = 81.12503051757812\n",
      "                              Voc = 19.151153564453125\n",
      "                              Jsc = 97.31900024414062\n",
      "                              FF = 15.323623657226562\n",
      "Finished epoch  465\n",
      "On epoch  466\n",
      "Total Epoch Testing MAPE: PCE = 81.25770568847656\n",
      "                              Voc = 17.50278091430664\n",
      "                              Jsc = 97.17646026611328\n",
      "                              FF = 14.543994903564453\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 80.76197052001953\n",
      "                              Voc = 16.717239379882812\n",
      "                              Jsc = 97.12586975097656\n",
      "                              FF = 14.151904106140137\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 80.70614624023438\n",
      "                              Voc = 15.585556983947754\n",
      "                              Jsc = 96.8675765991211\n",
      "                              FF = 14.182683944702148\n",
      "Finished epoch  468\n",
      "On epoch  469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.6558837890625\n",
      "                              Voc = 14.003020286560059\n",
      "                              Jsc = 96.93896484375\n",
      "                              FF = 14.293699264526367\n",
      "Finished epoch  469\n",
      "On epoch  470\n",
      "Total Epoch Testing MAPE: PCE = 83.12220001220703\n",
      "                              Voc = 10.949824333190918\n",
      "                              Jsc = 96.99176025390625\n",
      "                              FF = 15.335074424743652\n",
      "Finished epoch  470\n",
      "On epoch  471\n",
      "Total Epoch Testing MAPE: PCE = 83.05191040039062\n",
      "                              Voc = 10.3723783493042\n",
      "                              Jsc = 96.45858764648438\n",
      "                              FF = 15.445905685424805\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 83.49322509765625\n",
      "                              Voc = 10.72513484954834\n",
      "                              Jsc = 95.99790954589844\n",
      "                              FF = 15.140785217285156\n",
      "Finished epoch  472\n",
      "On epoch  473\n",
      "Total Epoch Testing MAPE: PCE = 84.35733032226562\n",
      "                              Voc = 11.22711181640625\n",
      "                              Jsc = 96.26797485351562\n",
      "                              FF = 14.931386947631836\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 83.37849426269531\n",
      "                              Voc = 12.622603416442871\n",
      "                              Jsc = 95.72573852539062\n",
      "                              FF = 14.54005241394043\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 82.4787368774414\n",
      "                              Voc = 13.547572135925293\n",
      "                              Jsc = 95.79306030273438\n",
      "                              FF = 14.04610824584961\n",
      "Finished epoch  475\n",
      "On epoch  476\n",
      "Total Epoch Testing MAPE: PCE = 81.9053726196289\n",
      "                              Voc = 13.376334190368652\n",
      "                              Jsc = 95.93763732910156\n",
      "                              FF = 13.906227111816406\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 81.28422546386719\n",
      "                              Voc = 13.977355003356934\n",
      "                              Jsc = 96.11441802978516\n",
      "                              FF = 13.176705360412598\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 81.8525161743164\n",
      "                              Voc = 14.088079452514648\n",
      "                              Jsc = 96.39341735839844\n",
      "                              FF = 14.037163734436035\n",
      "Finished epoch  478\n",
      "On epoch  479\n",
      "Total Epoch Testing MAPE: PCE = 82.26607513427734\n",
      "                              Voc = 14.2916259765625\n",
      "                              Jsc = 96.22344207763672\n",
      "                              FF = 14.951333045959473\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 83.08733367919922\n",
      "                              Voc = 12.527972221374512\n",
      "                              Jsc = 96.6353530883789\n",
      "                              FF = 15.152931213378906\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 83.59690856933594\n",
      "                              Voc = 13.251291275024414\n",
      "                              Jsc = 96.61966705322266\n",
      "                              FF = 15.663662910461426\n",
      "Finished epoch  481\n",
      "On epoch  482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.22400665283203\n",
      "                              Voc = 13.922673225402832\n",
      "                              Jsc = 96.78266906738281\n",
      "                              FF = 15.823592185974121\n",
      "Finished epoch  482\n",
      "On epoch  483\n",
      "Total Epoch Testing MAPE: PCE = 81.06490325927734\n",
      "                              Voc = 13.26625919342041\n",
      "                              Jsc = 96.97981262207031\n",
      "                              FF = 15.743239402770996\n",
      "Finished epoch  483\n",
      "On epoch  484\n",
      "Total Epoch Testing MAPE: PCE = 80.94579315185547\n",
      "                              Voc = 14.718219757080078\n",
      "                              Jsc = 96.89948272705078\n",
      "                              FF = 16.018720626831055\n",
      "Finished epoch  484\n",
      "On epoch  485\n",
      "Total Epoch Testing MAPE: PCE = 79.77854919433594\n",
      "                              Voc = 15.345928192138672\n",
      "                              Jsc = 97.0881118774414\n",
      "                              FF = 16.786893844604492\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 81.73667907714844\n",
      "                              Voc = 17.983346939086914\n",
      "                              Jsc = 97.02041625976562\n",
      "                              FF = 16.30748748779297\n",
      "Finished epoch  486\n",
      "On epoch  487\n",
      "Total Epoch Testing MAPE: PCE = 82.0388412475586\n",
      "                              Voc = 15.792054176330566\n",
      "                              Jsc = 96.92155456542969\n",
      "                              FF = 15.722441673278809\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 83.15684509277344\n",
      "                              Voc = 15.005147933959961\n",
      "                              Jsc = 96.83808898925781\n",
      "                              FF = 15.506924629211426\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 81.79119110107422\n",
      "                              Voc = 14.46657943725586\n",
      "                              Jsc = 97.11563873291016\n",
      "                              FF = 15.0758056640625\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 82.20279693603516\n",
      "                              Voc = 14.520771980285645\n",
      "                              Jsc = 96.71990966796875\n",
      "                              FF = 14.739215850830078\n",
      "Finished epoch  490\n",
      "On epoch  491\n",
      "Total Epoch Testing MAPE: PCE = 82.74700927734375\n",
      "                              Voc = 15.500922203063965\n",
      "                              Jsc = 97.2059097290039\n",
      "                              FF = 14.930194854736328\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 83.1026611328125\n",
      "                              Voc = 14.311549186706543\n",
      "                              Jsc = 97.24171447753906\n",
      "                              FF = 15.247854232788086\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 83.8041763305664\n",
      "                              Voc = 14.745672225952148\n",
      "                              Jsc = 97.19656372070312\n",
      "                              FF = 15.393349647521973\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 83.69393920898438\n",
      "                              Voc = 16.885395050048828\n",
      "                              Jsc = 97.54254913330078\n",
      "                              FF = 14.510194778442383\n",
      "Finished epoch  494\n",
      "On epoch  495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 83.38430786132812\n",
      "                              Voc = 16.620708465576172\n",
      "                              Jsc = 97.87757873535156\n",
      "                              FF = 13.920622825622559\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 83.02915954589844\n",
      "                              Voc = 17.91602325439453\n",
      "                              Jsc = 97.5295181274414\n",
      "                              FF = 13.994728088378906\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 83.43090057373047\n",
      "                              Voc = 17.27887725830078\n",
      "                              Jsc = 96.79798126220703\n",
      "                              FF = 13.683950424194336\n",
      "Finished epoch  497\n",
      "On epoch  498\n",
      "Total Epoch Testing MAPE: PCE = 83.14654541015625\n",
      "                              Voc = 18.59975242614746\n",
      "                              Jsc = 96.64668273925781\n",
      "                              FF = 13.477418899536133\n",
      "Finished epoch  498\n",
      "On epoch  499\n",
      "Total Epoch Testing MAPE: PCE = 81.72634887695312\n",
      "                              Voc = 18.748394012451172\n",
      "                              Jsc = 96.64713287353516\n",
      "                              FF = 12.738127708435059\n",
      "Finished epoch  499\n",
      "Fold # 1\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 82.81202697753906\n",
      "                              Voc = 533.2200317382812\n",
      "                              Jsc = 450.9501037597656\n",
      "                              FF = 129.8175506591797\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 83.47443389892578\n",
      "                              Voc = 411.23358154296875\n",
      "                              Jsc = 292.2119140625\n",
      "                              FF = 83.08020782470703\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 83.10357666015625\n",
      "                              Voc = 352.7123718261719\n",
      "                              Jsc = 210.03244018554688\n",
      "                              FF = 57.464141845703125\n",
      "Finished epoch  2\n",
      "On epoch  3\n",
      "Total Epoch Testing MAPE: PCE = 79.14817810058594\n",
      "                              Voc = 291.3120422363281\n",
      "                              Jsc = 158.8765869140625\n",
      "                              FF = 42.6324348449707\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 72.2005386352539\n",
      "                              Voc = 256.07720947265625\n",
      "                              Jsc = 119.48453521728516\n",
      "                              FF = 24.507526397705078\n",
      "Finished epoch  4\n",
      "On epoch  5\n",
      "Total Epoch Testing MAPE: PCE = 69.09680938720703\n",
      "                              Voc = 229.0374755859375\n",
      "                              Jsc = 88.2861557006836\n",
      "                              FF = 14.199773788452148\n",
      "Finished epoch  5\n",
      "On epoch  6\n",
      "Total Epoch Testing MAPE: PCE = 64.41555786132812\n",
      "                              Voc = 200.66253662109375\n",
      "                              Jsc = 61.91825866699219\n",
      "                              FF = 10.68366813659668\n",
      "Finished epoch  6\n",
      "On epoch  7\n",
      "Total Epoch Testing MAPE: PCE = 63.07448196411133\n",
      "                              Voc = 179.6377410888672\n",
      "                              Jsc = 41.875362396240234\n",
      "                              FF = 6.084762096405029\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 61.089473724365234\n",
      "                              Voc = 162.79820251464844\n",
      "                              Jsc = 24.577857971191406\n",
      "                              FF = 5.316501617431641\n",
      "Finished epoch  8\n",
      "On epoch  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.034549713134766\n",
      "                              Voc = 144.70843505859375\n",
      "                              Jsc = 13.069989204406738\n",
      "                              FF = 5.261505126953125\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 58.151859283447266\n",
      "                              Voc = 134.72010803222656\n",
      "                              Jsc = 3.3592371940612793\n",
      "                              FF = 5.683196544647217\n",
      "Finished epoch  10\n",
      "On epoch  11\n",
      "Total Epoch Testing MAPE: PCE = 57.59648513793945\n",
      "                              Voc = 123.47325134277344\n",
      "                              Jsc = 12.765937805175781\n",
      "                              FF = 6.260250568389893\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 56.65333938598633\n",
      "                              Voc = 110.93126678466797\n",
      "                              Jsc = 17.578752517700195\n",
      "                              FF = 9.187037467956543\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 56.7711181640625\n",
      "                              Voc = 100.30099487304688\n",
      "                              Jsc = 26.98027229309082\n",
      "                              FF = 10.228567123413086\n",
      "Finished epoch  13\n",
      "On epoch  14\n",
      "Total Epoch Testing MAPE: PCE = 56.112152099609375\n",
      "                              Voc = 87.73846435546875\n",
      "                              Jsc = 31.566497802734375\n",
      "                              FF = 11.288384437561035\n",
      "Finished epoch  14\n",
      "On epoch  15\n",
      "Total Epoch Testing MAPE: PCE = 56.135589599609375\n",
      "                              Voc = 80.43636322021484\n",
      "                              Jsc = 37.57992172241211\n",
      "                              FF = 11.79419994354248\n",
      "Finished epoch  15\n",
      "On epoch  16\n",
      "Total Epoch Testing MAPE: PCE = 56.019718170166016\n",
      "                              Voc = 75.46858215332031\n",
      "                              Jsc = 40.788944244384766\n",
      "                              FF = 13.768423080444336\n",
      "Finished epoch  16\n",
      "On epoch  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.1881217956543\n",
      "                              Voc = 70.09815216064453\n",
      "                              Jsc = 46.65526580810547\n",
      "                              FF = 14.459864616394043\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 56.3603401184082\n",
      "                              Voc = 67.20884704589844\n",
      "                              Jsc = 52.15300369262695\n",
      "                              FF = 15.713669776916504\n",
      "Finished epoch  18\n",
      "On epoch  19\n",
      "Total Epoch Testing MAPE: PCE = 57.60264587402344\n",
      "                              Voc = 61.605655670166016\n",
      "                              Jsc = 56.38618469238281\n",
      "                              FF = 18.68205451965332\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 55.912078857421875\n",
      "                              Voc = 56.810707092285156\n",
      "                              Jsc = 59.01724624633789\n",
      "                              FF = 18.806610107421875\n",
      "Finished epoch  20\n",
      "On epoch  21\n",
      "Total Epoch Testing MAPE: PCE = 53.77595901489258\n",
      "                              Voc = 52.4585075378418\n",
      "                              Jsc = 59.597599029541016\n",
      "                              FF = 21.066205978393555\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 54.32987976074219\n",
      "                              Voc = 45.94997787475586\n",
      "                              Jsc = 61.35031509399414\n",
      "                              FF = 22.913558959960938\n",
      "Finished epoch  22\n",
      "On epoch  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.801265716552734\n",
      "                              Voc = 41.8535270690918\n",
      "                              Jsc = 64.81410217285156\n",
      "                              FF = 23.526222229003906\n",
      "Finished epoch  23\n",
      "On epoch  24\n",
      "Total Epoch Testing MAPE: PCE = 54.90630340576172\n",
      "                              Voc = 38.29674530029297\n",
      "                              Jsc = 67.99417877197266\n",
      "                              FF = 24.525142669677734\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 55.704254150390625\n",
      "                              Voc = 33.41009521484375\n",
      "                              Jsc = 69.59339904785156\n",
      "                              FF = 25.365665435791016\n",
      "Finished epoch  25\n",
      "On epoch  26\n",
      "Total Epoch Testing MAPE: PCE = 54.38162612915039\n",
      "                              Voc = 32.347251892089844\n",
      "                              Jsc = 70.36018371582031\n",
      "                              FF = 26.564863204956055\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 53.19123840332031\n",
      "                              Voc = 29.2850399017334\n",
      "                              Jsc = 72.8757095336914\n",
      "                              FF = 28.283388137817383\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 52.77025604248047\n",
      "                              Voc = 27.16144371032715\n",
      "                              Jsc = 72.34896087646484\n",
      "                              FF = 27.199295043945312\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 52.597415924072266\n",
      "                              Voc = 27.832374572753906\n",
      "                              Jsc = 73.71678924560547\n",
      "                              FF = 27.40350341796875\n",
      "Finished epoch  29\n",
      "On epoch  30\n",
      "Total Epoch Testing MAPE: PCE = 52.776092529296875\n",
      "                              Voc = 26.1037654876709\n",
      "                              Jsc = 75.3066635131836\n",
      "                              FF = 27.331117630004883\n",
      "Finished epoch  30\n",
      "On epoch  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.052528381347656\n",
      "                              Voc = 27.490785598754883\n",
      "                              Jsc = 79.14835357666016\n",
      "                              FF = 28.12783432006836\n",
      "Finished epoch  31\n",
      "On epoch  32\n",
      "Total Epoch Testing MAPE: PCE = 51.9958610534668\n",
      "                              Voc = 26.95305633544922\n",
      "                              Jsc = 79.72425842285156\n",
      "                              FF = 27.60686492919922\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 51.50929641723633\n",
      "                              Voc = 24.801597595214844\n",
      "                              Jsc = 79.4370346069336\n",
      "                              FF = 28.0206298828125\n",
      "Finished epoch  33\n",
      "On epoch  34\n",
      "Total Epoch Testing MAPE: PCE = 51.12223815917969\n",
      "                              Voc = 26.10592269897461\n",
      "                              Jsc = 80.41473388671875\n",
      "                              FF = 26.253992080688477\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 50.7962760925293\n",
      "                              Voc = 21.52466583251953\n",
      "                              Jsc = 80.55367279052734\n",
      "                              FF = 25.664966583251953\n",
      "Finished epoch  35\n",
      "On epoch  36\n",
      "Total Epoch Testing MAPE: PCE = 51.15271759033203\n",
      "                              Voc = 18.825061798095703\n",
      "                              Jsc = 79.71276092529297\n",
      "                              FF = 26.663150787353516\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 51.0289306640625\n",
      "                              Voc = 16.581825256347656\n",
      "                              Jsc = 81.13702392578125\n",
      "                              FF = 27.792068481445312\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 51.76390838623047\n",
      "                              Voc = 15.919987678527832\n",
      "                              Jsc = 80.98975372314453\n",
      "                              FF = 29.002660751342773\n",
      "Finished epoch  38\n",
      "On epoch  39\n",
      "Total Epoch Testing MAPE: PCE = 51.30015182495117\n",
      "                              Voc = 14.36180305480957\n",
      "                              Jsc = 81.75318908691406\n",
      "                              FF = 30.390256881713867\n",
      "Finished epoch  39\n",
      "On epoch  40\n",
      "Total Epoch Testing MAPE: PCE = 51.807891845703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Voc = 15.212156295776367\n",
      "                              Jsc = 81.92755889892578\n",
      "                              FF = 30.847190856933594\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 53.257896423339844\n",
      "                              Voc = 14.30677604675293\n",
      "                              Jsc = 81.75483703613281\n",
      "                              FF = 31.394094467163086\n",
      "Finished epoch  41\n",
      "On epoch  42\n",
      "Total Epoch Testing MAPE: PCE = 53.135841369628906\n",
      "                              Voc = 15.439457893371582\n",
      "                              Jsc = 81.04222869873047\n",
      "                              FF = 31.25477409362793\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 53.20991134643555\n",
      "                              Voc = 13.390229225158691\n",
      "                              Jsc = 82.08688354492188\n",
      "                              FF = 32.10103988647461\n",
      "Finished epoch  43\n",
      "On epoch  44\n",
      "Total Epoch Testing MAPE: PCE = 53.887115478515625\n",
      "                              Voc = 14.681048393249512\n",
      "                              Jsc = 82.74131774902344\n",
      "                              FF = 32.29054641723633\n",
      "Finished epoch  44\n",
      "On epoch  45\n",
      "Total Epoch Testing MAPE: PCE = 53.693634033203125\n",
      "                              Voc = 14.000454902648926\n",
      "                              Jsc = 83.70826721191406\n",
      "                              FF = 31.799036026000977\n",
      "Finished epoch  45\n",
      "On epoch  46\n",
      "Total Epoch Testing MAPE: PCE = 54.074256896972656\n",
      "                              Voc = 17.387332916259766\n",
      "                              Jsc = 85.38501739501953\n",
      "                              FF = 31.523242950439453\n",
      "Finished epoch  46\n",
      "On epoch  47\n",
      "Total Epoch Testing MAPE: PCE = 53.939788818359375\n",
      "                              Voc = 15.791397094726562\n",
      "                              Jsc = 84.95392608642578\n",
      "                              FF = 32.18060302734375\n",
      "Finished epoch  47\n",
      "On epoch  48\n",
      "Total Epoch Testing MAPE: PCE = 53.26668167114258\n",
      "                              Voc = 16.302291870117188\n",
      "                              Jsc = 87.66753387451172\n",
      "                              FF = 31.024070739746094\n",
      "Finished epoch  48\n",
      "On epoch  49\n",
      "Total Epoch Testing MAPE: PCE = 53.17049026489258\n",
      "                              Voc = 14.90678596496582\n",
      "                              Jsc = 87.6762924194336\n",
      "                              FF = 32.35455322265625\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 53.16469192504883\n",
      "                              Voc = 13.857471466064453\n",
      "                              Jsc = 87.41048431396484\n",
      "                              FF = 31.848711013793945\n",
      "Finished epoch  50\n",
      "On epoch  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.803688049316406\n",
      "                              Voc = 14.526542663574219\n",
      "                              Jsc = 88.6545181274414\n",
      "                              FF = 32.31913757324219\n",
      "Finished epoch  51\n",
      "On epoch  52\n",
      "Total Epoch Testing MAPE: PCE = 54.364784240722656\n",
      "                              Voc = 13.484029769897461\n",
      "                              Jsc = 90.23468780517578\n",
      "                              FF = 33.956764221191406\n",
      "Finished epoch  52\n",
      "On epoch  53\n",
      "Total Epoch Testing MAPE: PCE = 54.1917610168457\n",
      "                              Voc = 17.9066104888916\n",
      "                              Jsc = 89.45686340332031\n",
      "                              FF = 34.216468811035156\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 53.985923767089844\n",
      "                              Voc = 18.53955078125\n",
      "                              Jsc = 89.74757385253906\n",
      "                              FF = 33.076927185058594\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 54.129493713378906\n",
      "                              Voc = 17.950664520263672\n",
      "                              Jsc = 89.08440399169922\n",
      "                              FF = 33.86785125732422\n",
      "Finished epoch  55\n",
      "On epoch  56\n",
      "Total Epoch Testing MAPE: PCE = 54.6976432800293\n",
      "                              Voc = 18.3835506439209\n",
      "                              Jsc = 89.86888885498047\n",
      "                              FF = 35.26243209838867\n",
      "Finished epoch  56\n",
      "On epoch  57\n",
      "Total Epoch Testing MAPE: PCE = 55.0269775390625\n",
      "                              Voc = 18.08714485168457\n",
      "                              Jsc = 90.0100326538086\n",
      "                              FF = 32.74778747558594\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 54.73686981201172\n",
      "                              Voc = 16.892765045166016\n",
      "                              Jsc = 88.41510772705078\n",
      "                              FF = 32.07627868652344\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 53.3358154296875\n",
      "                              Voc = 16.632970809936523\n",
      "                              Jsc = 88.2086410522461\n",
      "                              FF = 32.33954620361328\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 54.14316177368164\n",
      "                              Voc = 12.92258071899414\n",
      "                              Jsc = 88.19442749023438\n",
      "                              FF = 32.177982330322266\n",
      "Finished epoch  60\n",
      "On epoch  61\n",
      "Total Epoch Testing MAPE: PCE = 54.00214767456055\n",
      "                              Voc = 7.858486652374268\n",
      "                              Jsc = 88.35943603515625\n",
      "                              FF = 32.39891052246094\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 52.26141357421875\n",
      "                              Voc = 7.87756872177124\n",
      "                              Jsc = 87.14105224609375\n",
      "                              FF = 30.653085708618164\n",
      "Finished epoch  62\n",
      "On epoch  63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.79964065551758\n",
      "                              Voc = 9.131537437438965\n",
      "                              Jsc = 86.68318939208984\n",
      "                              FF = 31.13465690612793\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 53.39408493041992\n",
      "                              Voc = 6.948991775512695\n",
      "                              Jsc = 86.47611999511719\n",
      "                              FF = 29.724905014038086\n",
      "Finished epoch  64\n",
      "On epoch  65\n",
      "Total Epoch Testing MAPE: PCE = 52.46635055541992\n",
      "                              Voc = 5.930331707000732\n",
      "                              Jsc = 86.30180358886719\n",
      "                              FF = 29.96585464477539\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 51.24264907836914\n",
      "                              Voc = 5.7735276222229\n",
      "                              Jsc = 86.09064483642578\n",
      "                              FF = 29.498586654663086\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 52.82923126220703\n",
      "                              Voc = 5.949789047241211\n",
      "                              Jsc = 86.98993682861328\n",
      "                              FF = 28.609460830688477\n",
      "Finished epoch  67\n",
      "On epoch  68\n",
      "Total Epoch Testing MAPE: PCE = 54.498779296875\n",
      "                              Voc = 5.137564659118652\n",
      "                              Jsc = 87.13668060302734\n",
      "                              FF = 28.95858383178711\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 54.44425964355469\n",
      "                              Voc = 5.203614711761475\n",
      "                              Jsc = 85.80712127685547\n",
      "                              FF = 27.82047462463379\n",
      "Finished epoch  69\n",
      "On epoch  70\n",
      "Total Epoch Testing MAPE: PCE = 54.060752868652344\n",
      "                              Voc = 5.397180557250977\n",
      "                              Jsc = 85.67449188232422\n",
      "                              FF = 27.755577087402344\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 53.10748291015625\n",
      "                              Voc = 6.881961345672607\n",
      "                              Jsc = 85.89285278320312\n",
      "                              FF = 27.05046272277832\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 53.08688735961914\n",
      "                              Voc = 8.2643461227417\n",
      "                              Jsc = 85.56729888916016\n",
      "                              FF = 28.97437286376953\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 51.63008499145508\n",
      "                              Voc = 7.768853664398193\n",
      "                              Jsc = 84.6968994140625\n",
      "                              FF = 30.941513061523438\n",
      "Finished epoch  73\n",
      "On epoch  74\n",
      "Total Epoch Testing MAPE: PCE = 51.46563720703125\n",
      "                              Voc = 6.936326026916504\n",
      "                              Jsc = 83.01896667480469\n",
      "                              FF = 30.325820922851562\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 51.582008361816406\n",
      "                              Voc = 5.034544944763184\n",
      "                              Jsc = 82.48881530761719\n",
      "                              FF = 29.2276611328125\n",
      "Finished epoch  75\n",
      "On epoch  76\n",
      "Total Epoch Testing MAPE: PCE = 53.120338439941406\n",
      "                              Voc = 5.0725812911987305\n",
      "                              Jsc = 83.57862091064453\n",
      "                              FF = 27.233198165893555\n",
      "Finished epoch  76\n",
      "On epoch  77\n",
      "Total Epoch Testing MAPE: PCE = 52.400733947753906\n",
      "                              Voc = 5.752131938934326\n",
      "                              Jsc = 83.546630859375\n",
      "                              FF = 27.740808486938477\n",
      "Finished epoch  77\n",
      "On epoch  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.57590866088867\n",
      "                              Voc = 5.052394866943359\n",
      "                              Jsc = 85.15323638916016\n",
      "                              FF = 26.317087173461914\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 55.458003997802734\n",
      "                              Voc = 5.318227291107178\n",
      "                              Jsc = 86.20291137695312\n",
      "                              FF = 28.019908905029297\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 53.81382751464844\n",
      "                              Voc = 4.817967891693115\n",
      "                              Jsc = 85.75135803222656\n",
      "                              FF = 29.252498626708984\n",
      "Finished epoch  80\n",
      "On epoch  81\n",
      "Total Epoch Testing MAPE: PCE = 54.42308044433594\n",
      "                              Voc = 4.854164123535156\n",
      "                              Jsc = 86.25045776367188\n",
      "                              FF = 27.696290969848633\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 55.173744201660156\n",
      "                              Voc = 4.900629043579102\n",
      "                              Jsc = 84.57142639160156\n",
      "                              FF = 29.05797576904297\n",
      "Finished epoch  82\n",
      "On epoch  83\n",
      "Total Epoch Testing MAPE: PCE = 55.558692932128906\n",
      "                              Voc = 4.515669345855713\n",
      "                              Jsc = 86.27216339111328\n",
      "                              FF = 28.013107299804688\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 56.057228088378906\n",
      "                              Voc = 4.84339714050293\n",
      "                              Jsc = 87.24942016601562\n",
      "                              FF = 28.76918601989746\n",
      "Finished epoch  84\n",
      "On epoch  85\n",
      "Total Epoch Testing MAPE: PCE = 54.378273010253906\n",
      "                              Voc = 4.349493026733398\n",
      "                              Jsc = 87.18034362792969\n",
      "                              FF = 29.077390670776367\n",
      "Finished epoch  85\n",
      "On epoch  86\n",
      "Total Epoch Testing MAPE: PCE = 53.046852111816406\n",
      "                              Voc = 4.573940277099609\n",
      "                              Jsc = 87.9879379272461\n",
      "                              FF = 29.331928253173828\n",
      "Finished epoch  86\n",
      "On epoch  87\n",
      "Total Epoch Testing MAPE: PCE = 52.64665603637695\n",
      "                              Voc = 4.510181427001953\n",
      "                              Jsc = 86.04568481445312\n",
      "                              FF = 28.691011428833008\n",
      "Finished epoch  87\n",
      "On epoch  88\n",
      "Total Epoch Testing MAPE: PCE = 52.27159881591797\n",
      "                              Voc = 4.631970405578613\n",
      "                              Jsc = 84.41963958740234\n",
      "                              FF = 29.11209487915039\n",
      "Finished epoch  88\n",
      "On epoch  89\n",
      "Total Epoch Testing MAPE: PCE = 52.809791564941406\n",
      "                              Voc = 5.4727067947387695\n",
      "                              Jsc = 84.04790496826172\n",
      "                              FF = 30.218273162841797\n",
      "Finished epoch  89\n",
      "On epoch  90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.42707824707031\n",
      "                              Voc = 5.01763916015625\n",
      "                              Jsc = 83.54166412353516\n",
      "                              FF = 28.43964385986328\n",
      "Finished epoch  90\n",
      "On epoch  91\n",
      "Total Epoch Testing MAPE: PCE = 52.18147659301758\n",
      "                              Voc = 4.774364948272705\n",
      "                              Jsc = 84.32308959960938\n",
      "                              FF = 27.774890899658203\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 52.358863830566406\n",
      "                              Voc = 5.030820369720459\n",
      "                              Jsc = 84.62590026855469\n",
      "                              FF = 29.044559478759766\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 52.10203552246094\n",
      "                              Voc = 5.527146816253662\n",
      "                              Jsc = 85.21126556396484\n",
      "                              FF = 28.102705001831055\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 53.36159896850586\n",
      "                              Voc = 6.012996673583984\n",
      "                              Jsc = 86.07494354248047\n",
      "                              FF = 28.801456451416016\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 53.60421371459961\n",
      "                              Voc = 6.144290447235107\n",
      "                              Jsc = 87.50120544433594\n",
      "                              FF = 26.818567276000977\n",
      "Finished epoch  95\n",
      "On epoch  96\n",
      "Total Epoch Testing MAPE: PCE = 53.62479019165039\n",
      "                              Voc = 6.697650909423828\n",
      "                              Jsc = 88.14855194091797\n",
      "                              FF = 26.53249740600586\n",
      "Finished epoch  96\n",
      "On epoch  97\n",
      "Total Epoch Testing MAPE: PCE = 53.84653091430664\n",
      "                              Voc = 7.038741588592529\n",
      "                              Jsc = 89.29041290283203\n",
      "                              FF = 27.006301879882812\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 54.59747314453125\n",
      "                              Voc = 7.116458415985107\n",
      "                              Jsc = 89.76719665527344\n",
      "                              FF = 27.840417861938477\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 52.388179779052734\n",
      "                              Voc = 7.4735798835754395\n",
      "                              Jsc = 88.7620849609375\n",
      "                              FF = 28.424665451049805\n",
      "Finished epoch  99\n",
      "On epoch  100\n",
      "Total Epoch Testing MAPE: PCE = 50.91285705566406\n",
      "                              Voc = 7.72879695892334\n",
      "                              Jsc = 89.41133880615234\n",
      "                              FF = 27.574310302734375\n",
      "Finished epoch  100\n",
      "On epoch  101\n",
      "Total Epoch Testing MAPE: PCE = 52.02629089355469\n",
      "                              Voc = 7.538819313049316\n",
      "                              Jsc = 90.6135025024414\n",
      "                              FF = 27.359130859375\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 53.733402252197266\n",
      "                              Voc = 7.886473178863525\n",
      "                              Jsc = 91.09632110595703\n",
      "                              FF = 26.819950103759766\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 53.223018646240234\n",
      "                              Voc = 7.818889617919922\n",
      "                              Jsc = 90.958984375\n",
      "                              FF = 26.653289794921875\n",
      "Finished epoch  103\n",
      "On epoch  104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.93416213989258\n",
      "                              Voc = 8.03640365600586\n",
      "                              Jsc = 91.85700988769531\n",
      "                              FF = 26.69283676147461\n",
      "Finished epoch  104\n",
      "On epoch  105\n",
      "Total Epoch Testing MAPE: PCE = 53.29327392578125\n",
      "                              Voc = 8.265511512756348\n",
      "                              Jsc = 91.00180053710938\n",
      "                              FF = 26.94602394104004\n",
      "Finished epoch  105\n",
      "On epoch  106\n",
      "Total Epoch Testing MAPE: PCE = 52.41828155517578\n",
      "                              Voc = 8.692961692810059\n",
      "                              Jsc = 91.20320892333984\n",
      "                              FF = 26.532384872436523\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 50.2326545715332\n",
      "                              Voc = 9.092912673950195\n",
      "                              Jsc = 90.85494232177734\n",
      "                              FF = 26.364269256591797\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 50.20829391479492\n",
      "                              Voc = 8.822417259216309\n",
      "                              Jsc = 89.02555847167969\n",
      "                              FF = 26.981712341308594\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 49.4836540222168\n",
      "                              Voc = 8.883684158325195\n",
      "                              Jsc = 88.16217041015625\n",
      "                              FF = 26.61125373840332\n",
      "Finished epoch  109\n",
      "On epoch  110\n",
      "Total Epoch Testing MAPE: PCE = 50.24129867553711\n",
      "                              Voc = 9.578332901000977\n",
      "                              Jsc = 87.3968505859375\n",
      "                              FF = 27.965904235839844\n",
      "Finished epoch  110\n",
      "On epoch  111\n",
      "Total Epoch Testing MAPE: PCE = 49.92921447753906\n",
      "                              Voc = 9.188681602478027\n",
      "                              Jsc = 88.52982330322266\n",
      "                              FF = 29.03614616394043\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 49.67353439331055\n",
      "                              Voc = 10.89367389678955\n",
      "                              Jsc = 88.21306610107422\n",
      "                              FF = 29.464309692382812\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 49.55168151855469\n",
      "                              Voc = 10.412115097045898\n",
      "                              Jsc = 88.2053451538086\n",
      "                              FF = 29.517824172973633\n",
      "Finished epoch  113\n",
      "On epoch  114\n",
      "Total Epoch Testing MAPE: PCE = 49.5322380065918\n",
      "                              Voc = 10.703875541687012\n",
      "                              Jsc = 86.39039611816406\n",
      "                              FF = 28.642229080200195\n",
      "Finished epoch  114\n",
      "On epoch  115\n",
      "Total Epoch Testing MAPE: PCE = 50.61734390258789\n",
      "                              Voc = 10.593594551086426\n",
      "                              Jsc = 85.90198516845703\n",
      "                              FF = 27.8323974609375\n",
      "Finished epoch  115\n",
      "On epoch  116\n",
      "Total Epoch Testing MAPE: PCE = 50.38138198852539\n",
      "                              Voc = 10.20291519165039\n",
      "                              Jsc = 85.28314208984375\n",
      "                              FF = 28.25937843322754\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 52.047061920166016\n",
      "                              Voc = 10.414173126220703\n",
      "                              Jsc = 85.20065307617188\n",
      "                              FF = 28.063636779785156\n",
      "Finished epoch  117\n",
      "On epoch  118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.00649642944336\n",
      "                              Voc = 10.643852233886719\n",
      "                              Jsc = 84.68836212158203\n",
      "                              FF = 28.29637908935547\n",
      "Finished epoch  118\n",
      "On epoch  119\n",
      "Total Epoch Testing MAPE: PCE = 54.503787994384766\n",
      "                              Voc = 10.537619590759277\n",
      "                              Jsc = 85.49809265136719\n",
      "                              FF = 29.802248001098633\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 54.697566986083984\n",
      "                              Voc = 10.400575637817383\n",
      "                              Jsc = 86.02880096435547\n",
      "                              FF = 28.230449676513672\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 55.965274810791016\n",
      "                              Voc = 10.925664901733398\n",
      "                              Jsc = 87.49726867675781\n",
      "                              FF = 28.697919845581055\n",
      "Finished epoch  121\n",
      "On epoch  122\n",
      "Total Epoch Testing MAPE: PCE = 56.798397064208984\n",
      "                              Voc = 11.321002006530762\n",
      "                              Jsc = 88.892822265625\n",
      "                              FF = 28.500417709350586\n",
      "Finished epoch  122\n",
      "On epoch  123\n",
      "Total Epoch Testing MAPE: PCE = 55.449241638183594\n",
      "                              Voc = 11.230097770690918\n",
      "                              Jsc = 89.23477172851562\n",
      "                              FF = 26.640533447265625\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 55.448089599609375\n",
      "                              Voc = 11.095409393310547\n",
      "                              Jsc = 88.10169219970703\n",
      "                              FF = 26.065093994140625\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 56.06626892089844\n",
      "                              Voc = 10.813793182373047\n",
      "                              Jsc = 87.1577377319336\n",
      "                              FF = 25.929349899291992\n",
      "Finished epoch  125\n",
      "On epoch  126\n",
      "Total Epoch Testing MAPE: PCE = 54.52448272705078\n",
      "                              Voc = 11.361580848693848\n",
      "                              Jsc = 86.61235046386719\n",
      "                              FF = 26.356992721557617\n",
      "Finished epoch  126\n",
      "On epoch  127\n",
      "Total Epoch Testing MAPE: PCE = 55.61024475097656\n",
      "                              Voc = 11.233431816101074\n",
      "                              Jsc = 86.38489532470703\n",
      "                              FF = 28.32196044921875\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 53.84484100341797\n",
      "                              Voc = 11.023908615112305\n",
      "                              Jsc = 84.99861145019531\n",
      "                              FF = 27.031787872314453\n",
      "Finished epoch  128\n",
      "On epoch  129\n",
      "Total Epoch Testing MAPE: PCE = 54.1330451965332\n",
      "                              Voc = 11.359169006347656\n",
      "                              Jsc = 85.52928924560547\n",
      "                              FF = 27.091907501220703\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 53.216583251953125\n",
      "                              Voc = 12.466337203979492\n",
      "                              Jsc = 84.74242401123047\n",
      "                              FF = 27.26513671875\n",
      "Finished epoch  130\n",
      "On epoch  131\n",
      "Total Epoch Testing MAPE: PCE = 54.25291061401367\n",
      "                              Voc = 14.954984664916992\n",
      "                              Jsc = 85.5728988647461\n",
      "                              FF = 29.045639038085938\n",
      "Finished epoch  131\n",
      "On epoch  132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.697669982910156\n",
      "                              Voc = 13.695372581481934\n",
      "                              Jsc = 84.04125213623047\n",
      "                              FF = 29.458967208862305\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 54.832489013671875\n",
      "                              Voc = 13.830465316772461\n",
      "                              Jsc = 84.15523529052734\n",
      "                              FF = 29.352781295776367\n",
      "Finished epoch  133\n",
      "On epoch  134\n",
      "Total Epoch Testing MAPE: PCE = 54.97609329223633\n",
      "                              Voc = 12.812082290649414\n",
      "                              Jsc = 85.61730194091797\n",
      "                              FF = 30.97001838684082\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 55.18193817138672\n",
      "                              Voc = 13.47637939453125\n",
      "                              Jsc = 85.79277038574219\n",
      "                              FF = 29.2908935546875\n",
      "Finished epoch  135\n",
      "On epoch  136\n",
      "Total Epoch Testing MAPE: PCE = 55.49908447265625\n",
      "                              Voc = 12.566208839416504\n",
      "                              Jsc = 84.5103759765625\n",
      "                              FF = 31.301651000976562\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 55.81184005737305\n",
      "                              Voc = 12.106602668762207\n",
      "                              Jsc = 84.34782409667969\n",
      "                              FF = 31.531482696533203\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 55.45486068725586\n",
      "                              Voc = 12.039990425109863\n",
      "                              Jsc = 83.77330017089844\n",
      "                              FF = 31.711292266845703\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 55.03068542480469\n",
      "                              Voc = 11.756608963012695\n",
      "                              Jsc = 83.27527618408203\n",
      "                              FF = 30.66161346435547\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 54.86697006225586\n",
      "                              Voc = 11.712543487548828\n",
      "                              Jsc = 83.95564270019531\n",
      "                              FF = 31.924278259277344\n",
      "Finished epoch  140\n",
      "On epoch  141\n",
      "Total Epoch Testing MAPE: PCE = 53.2674446105957\n",
      "                              Voc = 11.57129955291748\n",
      "                              Jsc = 83.91090393066406\n",
      "                              FF = 31.58515167236328\n",
      "Finished epoch  141\n",
      "On epoch  142\n",
      "Total Epoch Testing MAPE: PCE = 56.313262939453125\n",
      "                              Voc = 11.303526878356934\n",
      "                              Jsc = 85.97197723388672\n",
      "                              FF = 33.106300354003906\n",
      "Finished epoch  142\n",
      "On epoch  143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.74724197387695\n",
      "                              Voc = 12.161965370178223\n",
      "                              Jsc = 85.71502685546875\n",
      "                              FF = 34.34925842285156\n",
      "Finished epoch  143\n",
      "On epoch  144\n",
      "Total Epoch Testing MAPE: PCE = 56.13884353637695\n",
      "                              Voc = 13.03837776184082\n",
      "                              Jsc = 84.82591247558594\n",
      "                              FF = 33.04473876953125\n",
      "Finished epoch  144\n",
      "On epoch  145\n",
      "Total Epoch Testing MAPE: PCE = 55.9581298828125\n",
      "                              Voc = 12.964816093444824\n",
      "                              Jsc = 86.06607055664062\n",
      "                              FF = 33.46046829223633\n",
      "Finished epoch  145\n",
      "On epoch  146\n",
      "Total Epoch Testing MAPE: PCE = 52.92936706542969\n",
      "                              Voc = 12.302416801452637\n",
      "                              Jsc = 83.74949645996094\n",
      "                              FF = 33.43417739868164\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 52.411277770996094\n",
      "                              Voc = 12.207724571228027\n",
      "                              Jsc = 83.16515350341797\n",
      "                              FF = 34.3770637512207\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 53.63760757446289\n",
      "                              Voc = 13.604799270629883\n",
      "                              Jsc = 84.21209716796875\n",
      "                              FF = 33.85773468017578\n",
      "Finished epoch  148\n",
      "On epoch  149\n",
      "Total Epoch Testing MAPE: PCE = 53.05939865112305\n",
      "                              Voc = 13.431803703308105\n",
      "                              Jsc = 85.14295959472656\n",
      "                              FF = 33.304569244384766\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 52.02273178100586\n",
      "                              Voc = 15.816216468811035\n",
      "                              Jsc = 84.27925109863281\n",
      "                              FF = 32.669471740722656\n",
      "Finished epoch  150\n",
      "On epoch  151\n",
      "Total Epoch Testing MAPE: PCE = 52.91555404663086\n",
      "                              Voc = 13.269613265991211\n",
      "                              Jsc = 84.0669174194336\n",
      "                              FF = 31.213268280029297\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 51.64224624633789\n",
      "                              Voc = 12.906877517700195\n",
      "                              Jsc = 86.41775512695312\n",
      "                              FF = 30.29073715209961\n",
      "Finished epoch  152\n",
      "On epoch  153\n",
      "Total Epoch Testing MAPE: PCE = 49.413055419921875\n",
      "                              Voc = 14.902637481689453\n",
      "                              Jsc = 87.0324935913086\n",
      "                              FF = 30.901094436645508\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 50.872676849365234\n",
      "                              Voc = 13.949470520019531\n",
      "                              Jsc = 86.61625671386719\n",
      "                              FF = 31.348094940185547\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 51.17279815673828\n",
      "                              Voc = 13.664499282836914\n",
      "                              Jsc = 87.54066467285156\n",
      "                              FF = 31.765058517456055\n",
      "Finished epoch  155\n",
      "On epoch  156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.91006088256836\n",
      "                              Voc = 14.113612174987793\n",
      "                              Jsc = 88.84905242919922\n",
      "                              FF = 30.594310760498047\n",
      "Finished epoch  156\n",
      "On epoch  157\n",
      "Total Epoch Testing MAPE: PCE = 50.47639083862305\n",
      "                              Voc = 16.03739356994629\n",
      "                              Jsc = 88.12824249267578\n",
      "                              FF = 33.011714935302734\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 51.51704406738281\n",
      "                              Voc = 15.613601684570312\n",
      "                              Jsc = 88.1707534790039\n",
      "                              FF = 33.588504791259766\n",
      "Finished epoch  158\n",
      "On epoch  159\n",
      "Total Epoch Testing MAPE: PCE = 50.914363861083984\n",
      "                              Voc = 14.90058422088623\n",
      "                              Jsc = 89.24010467529297\n",
      "                              FF = 32.49320983886719\n",
      "Finished epoch  159\n",
      "On epoch  160\n",
      "Total Epoch Testing MAPE: PCE = 51.42768096923828\n",
      "                              Voc = 13.861759185791016\n",
      "                              Jsc = 89.63204956054688\n",
      "                              FF = 31.741539001464844\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 51.31298828125\n",
      "                              Voc = 14.369832992553711\n",
      "                              Jsc = 89.07611083984375\n",
      "                              FF = 31.269498825073242\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 51.4671745300293\n",
      "                              Voc = 13.968810081481934\n",
      "                              Jsc = 88.2098159790039\n",
      "                              FF = 30.825645446777344\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 51.320701599121094\n",
      "                              Voc = 13.628670692443848\n",
      "                              Jsc = 89.99301147460938\n",
      "                              FF = 30.182775497436523\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 51.50602340698242\n",
      "                              Voc = 13.621100425720215\n",
      "                              Jsc = 88.2060546875\n",
      "                              FF = 31.7504940032959\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 53.104801177978516\n",
      "                              Voc = 14.037592887878418\n",
      "                              Jsc = 87.25860595703125\n",
      "                              FF = 30.381492614746094\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 53.46339797973633\n",
      "                              Voc = 14.3353271484375\n",
      "                              Jsc = 88.74964141845703\n",
      "                              FF = 28.520124435424805\n",
      "Finished epoch  166\n",
      "On epoch  167\n",
      "Total Epoch Testing MAPE: PCE = 53.01675033569336\n",
      "                              Voc = 14.66983413696289\n",
      "                              Jsc = 87.72674560546875\n",
      "                              FF = 28.240446090698242\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 53.3184928894043\n",
      "                              Voc = 15.073473930358887\n",
      "                              Jsc = 88.03062438964844\n",
      "                              FF = 29.084991455078125\n",
      "Finished epoch  168\n",
      "On epoch  169\n",
      "Total Epoch Testing MAPE: PCE = 55.06325912475586\n",
      "                              Voc = 15.2318696975708\n",
      "                              Jsc = 88.78892517089844\n",
      "                              FF = 28.503387451171875\n",
      "Finished epoch  169\n",
      "On epoch  170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.221435546875\n",
      "                              Voc = 14.983227729797363\n",
      "                              Jsc = 90.3100814819336\n",
      "                              FF = 29.135005950927734\n",
      "Finished epoch  170\n",
      "On epoch  171\n",
      "Total Epoch Testing MAPE: PCE = 52.25571823120117\n",
      "                              Voc = 15.531129837036133\n",
      "                              Jsc = 89.85741424560547\n",
      "                              FF = 28.14096450805664\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 51.3213996887207\n",
      "                              Voc = 16.574249267578125\n",
      "                              Jsc = 90.28913116455078\n",
      "                              FF = 29.259910583496094\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 51.448673248291016\n",
      "                              Voc = 16.45517349243164\n",
      "                              Jsc = 90.94768524169922\n",
      "                              FF = 29.06402587890625\n",
      "Finished epoch  173\n",
      "On epoch  174\n",
      "Total Epoch Testing MAPE: PCE = 50.53947448730469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Voc = 16.622323989868164\n",
      "                              Jsc = 89.2897720336914\n",
      "                              FF = 29.855844497680664\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 51.09062957763672\n",
      "                              Voc = 16.947011947631836\n",
      "                              Jsc = 88.87637329101562\n",
      "                              FF = 29.46892738342285\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 51.2765998840332\n",
      "                              Voc = 15.93015193939209\n",
      "                              Jsc = 88.02862548828125\n",
      "                              FF = 29.04021453857422\n",
      "Finished epoch  176\n",
      "On epoch  177\n",
      "Total Epoch Testing MAPE: PCE = 51.1374397277832\n",
      "                              Voc = 16.8867130279541\n",
      "                              Jsc = 88.33793640136719\n",
      "                              FF = 27.962282180786133\n",
      "Finished epoch  177\n",
      "On epoch  178\n",
      "Total Epoch Testing MAPE: PCE = 50.236480712890625\n",
      "                              Voc = 18.0408935546875\n",
      "                              Jsc = 89.78866577148438\n",
      "                              FF = 26.66376495361328\n",
      "Finished epoch  178\n",
      "On epoch  179\n",
      "Total Epoch Testing MAPE: PCE = 49.527740478515625\n",
      "                              Voc = 18.74687385559082\n",
      "                              Jsc = 89.04039001464844\n",
      "                              FF = 26.762399673461914\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 48.79154586791992\n",
      "                              Voc = 18.981822967529297\n",
      "                              Jsc = 88.96186828613281\n",
      "                              FF = 26.36035919189453\n",
      "Finished epoch  180\n",
      "On epoch  181\n",
      "Total Epoch Testing MAPE: PCE = 47.91440200805664\n",
      "                              Voc = 22.686059951782227\n",
      "                              Jsc = 88.7234878540039\n",
      "                              FF = 27.057249069213867\n",
      "Finished epoch  181\n",
      "On epoch  182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.36064147949219\n",
      "                              Voc = 23.985567092895508\n",
      "                              Jsc = 87.90591430664062\n",
      "                              FF = 26.70697784423828\n",
      "Finished epoch  182\n",
      "On epoch  183\n",
      "Total Epoch Testing MAPE: PCE = 46.15122604370117\n",
      "                              Voc = 26.34630584716797\n",
      "                              Jsc = 88.0576400756836\n",
      "                              FF = 26.856781005859375\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 47.40098571777344\n",
      "                              Voc = 27.442724227905273\n",
      "                              Jsc = 88.97796630859375\n",
      "                              FF = 26.37680435180664\n",
      "Finished epoch  184\n",
      "On epoch  185\n",
      "Total Epoch Testing MAPE: PCE = 47.502811431884766\n",
      "                              Voc = 31.68445587158203\n",
      "                              Jsc = 88.5951156616211\n",
      "                              FF = 25.905210494995117\n",
      "Finished epoch  185\n",
      "On epoch  186\n",
      "Total Epoch Testing MAPE: PCE = 47.56432342529297\n",
      "                              Voc = 29.844934463500977\n",
      "                              Jsc = 89.4358901977539\n",
      "                              FF = 26.062110900878906\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 47.0577507019043\n",
      "                              Voc = 28.926414489746094\n",
      "                              Jsc = 88.4858627319336\n",
      "                              FF = 25.82709503173828\n",
      "Finished epoch  187\n",
      "On epoch  188\n",
      "Total Epoch Testing MAPE: PCE = 46.027801513671875\n",
      "                              Voc = 29.869409561157227\n",
      "                              Jsc = 90.00582885742188\n",
      "                              FF = 25.099149703979492\n",
      "Finished epoch  188\n",
      "On epoch  189\n",
      "Total Epoch Testing MAPE: PCE = 46.16053009033203\n",
      "                              Voc = 30.1329345703125\n",
      "                              Jsc = 91.1231460571289\n",
      "                              FF = 26.683706283569336\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 46.73270797729492\n",
      "                              Voc = 30.737442016601562\n",
      "                              Jsc = 91.25391387939453\n",
      "                              FF = 28.481821060180664\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 46.62506103515625\n",
      "                              Voc = 30.053810119628906\n",
      "                              Jsc = 91.39253234863281\n",
      "                              FF = 29.162019729614258\n",
      "Finished epoch  191\n",
      "On epoch  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 46.61594772338867\n",
      "                              Voc = 30.289844512939453\n",
      "                              Jsc = 91.61843872070312\n",
      "                              FF = 29.586347579956055\n",
      "Finished epoch  192\n",
      "On epoch  193\n",
      "Total Epoch Testing MAPE: PCE = 47.25657272338867\n",
      "                              Voc = 28.08795928955078\n",
      "                              Jsc = 93.16056060791016\n",
      "                              FF = 30.024534225463867\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 47.67494201660156\n",
      "                              Voc = 28.20375633239746\n",
      "                              Jsc = 93.65805053710938\n",
      "                              FF = 30.867332458496094\n",
      "Finished epoch  194\n",
      "On epoch  195\n",
      "Total Epoch Testing MAPE: PCE = 48.06809616088867\n",
      "                              Voc = 26.70513916015625\n",
      "                              Jsc = 93.1752700805664\n",
      "                              FF = 33.0762939453125\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 49.12153625488281\n",
      "                              Voc = 26.20372200012207\n",
      "                              Jsc = 93.61802673339844\n",
      "                              FF = 32.563297271728516\n",
      "Finished epoch  196\n",
      "On epoch  197\n",
      "Total Epoch Testing MAPE: PCE = 50.20240020751953\n",
      "                              Voc = 28.734830856323242\n",
      "                              Jsc = 92.53077697753906\n",
      "                              FF = 33.28852844238281\n",
      "Finished epoch  197\n",
      "On epoch  198\n",
      "Total Epoch Testing MAPE: PCE = 50.51854705810547\n",
      "                              Voc = 28.95996856689453\n",
      "                              Jsc = 91.19155883789062\n",
      "                              FF = 32.421234130859375\n",
      "Finished epoch  198\n",
      "On epoch  199\n",
      "Total Epoch Testing MAPE: PCE = 50.28853988647461\n",
      "                              Voc = 29.539798736572266\n",
      "                              Jsc = 91.17473602294922\n",
      "                              FF = 32.719749450683594\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 49.73814010620117\n",
      "                              Voc = 31.120681762695312\n",
      "                              Jsc = 92.48591613769531\n",
      "                              FF = 32.505680084228516\n",
      "Finished epoch  200\n",
      "On epoch  201\n",
      "Total Epoch Testing MAPE: PCE = 49.27043151855469\n",
      "                              Voc = 29.641454696655273\n",
      "                              Jsc = 89.94695281982422\n",
      "                              FF = 33.43757629394531\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 49.51898956298828\n",
      "                              Voc = 30.11249542236328\n",
      "                              Jsc = 89.24018859863281\n",
      "                              FF = 31.4328670501709\n",
      "Finished epoch  202\n",
      "On epoch  203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.88317108154297\n",
      "                              Voc = 29.4742488861084\n",
      "                              Jsc = 88.84725952148438\n",
      "                              FF = 33.8010368347168\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 49.960941314697266\n",
      "                              Voc = 31.382274627685547\n",
      "                              Jsc = 88.84928894042969\n",
      "                              FF = 33.81485366821289\n",
      "Finished epoch  204\n",
      "On epoch  205\n",
      "Total Epoch Testing MAPE: PCE = 49.25770568847656\n",
      "                              Voc = 31.3048152923584\n",
      "                              Jsc = 88.49405670166016\n",
      "                              FF = 32.861663818359375\n",
      "Finished epoch  205\n",
      "On epoch  206\n",
      "Total Epoch Testing MAPE: PCE = 48.45420455932617\n",
      "                              Voc = 29.47830581665039\n",
      "                              Jsc = 88.16400909423828\n",
      "                              FF = 31.980579376220703\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 47.385475158691406\n",
      "                              Voc = 30.795455932617188\n",
      "                              Jsc = 88.05602264404297\n",
      "                              FF = 31.68523406982422\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 48.17096710205078\n",
      "                              Voc = 29.104814529418945\n",
      "                              Jsc = 88.7970962524414\n",
      "                              FF = 30.714508056640625\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 48.68218231201172\n",
      "                              Voc = 27.078834533691406\n",
      "                              Jsc = 90.77605438232422\n",
      "                              FF = 30.905431747436523\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 50.53474426269531\n",
      "                              Voc = 27.710407257080078\n",
      "                              Jsc = 90.77555084228516\n",
      "                              FF = 32.574073791503906\n",
      "Finished epoch  210\n",
      "On epoch  211\n",
      "Total Epoch Testing MAPE: PCE = 49.64338684082031\n",
      "                              Voc = 27.048168182373047\n",
      "                              Jsc = 91.05615997314453\n",
      "                              FF = 31.767133712768555\n",
      "Finished epoch  211\n",
      "On epoch  212\n",
      "Total Epoch Testing MAPE: PCE = 51.21356201171875\n",
      "                              Voc = 24.734596252441406\n",
      "                              Jsc = 89.60566711425781\n",
      "                              FF = 31.09026336669922\n",
      "Finished epoch  212\n",
      "On epoch  213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.858192443847656\n",
      "                              Voc = 26.075172424316406\n",
      "                              Jsc = 89.9386978149414\n",
      "                              FF = 33.7513542175293\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 51.38722610473633\n",
      "                              Voc = 24.642786026000977\n",
      "                              Jsc = 88.76839447021484\n",
      "                              FF = 32.78963851928711\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 51.00885772705078\n",
      "                              Voc = 25.06049346923828\n",
      "                              Jsc = 86.56172943115234\n",
      "                              FF = 33.36821365356445\n",
      "Finished epoch  215\n",
      "On epoch  216\n",
      "Total Epoch Testing MAPE: PCE = 51.274532318115234\n",
      "                              Voc = 27.075904846191406\n",
      "                              Jsc = 87.35441589355469\n",
      "                              FF = 32.017398834228516\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 50.48178482055664\n",
      "                              Voc = 28.19906997680664\n",
      "                              Jsc = 87.79190826416016\n",
      "                              FF = 30.666845321655273\n",
      "Finished epoch  217\n",
      "On epoch  218\n",
      "Total Epoch Testing MAPE: PCE = 51.921661376953125\n",
      "                              Voc = 31.81658172607422\n",
      "                              Jsc = 86.94393157958984\n",
      "                              FF = 28.961753845214844\n",
      "Finished epoch  218\n",
      "On epoch  219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.12641525268555\n",
      "                              Voc = 32.21424102783203\n",
      "                              Jsc = 85.78251647949219\n",
      "                              FF = 30.425983428955078\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 50.507198333740234\n",
      "                              Voc = 33.1655158996582\n",
      "                              Jsc = 85.01576232910156\n",
      "                              FF = 30.935304641723633\n",
      "Finished epoch  220\n",
      "On epoch  221\n",
      "Total Epoch Testing MAPE: PCE = 51.726985931396484\n",
      "                              Voc = 31.216304779052734\n",
      "                              Jsc = 86.8231430053711\n",
      "                              FF = 30.58230209350586\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 50.628108978271484\n",
      "                              Voc = 28.450315475463867\n",
      "                              Jsc = 86.54943084716797\n",
      "                              FF = 30.125125885009766\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 49.70026779174805\n",
      "                              Voc = 28.60481071472168\n",
      "                              Jsc = 86.67237091064453\n",
      "                              FF = 30.34101104736328\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 50.39313888549805\n",
      "                              Voc = 33.518367767333984\n",
      "                              Jsc = 85.79299926757812\n",
      "                              FF = 30.512672424316406\n",
      "Finished epoch  224\n",
      "On epoch  225\n",
      "Total Epoch Testing MAPE: PCE = 51.36561584472656\n",
      "                              Voc = 31.746789932250977\n",
      "                              Jsc = 84.77606964111328\n",
      "                              FF = 31.038591384887695\n",
      "Finished epoch  225\n",
      "On epoch  226\n",
      "Total Epoch Testing MAPE: PCE = 50.463199615478516\n",
      "                              Voc = 32.767295837402344\n",
      "                              Jsc = 83.99629211425781\n",
      "                              FF = 31.85248374938965\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 51.1552619934082\n",
      "                              Voc = 33.12022018432617\n",
      "                              Jsc = 85.0649185180664\n",
      "                              FF = 32.196983337402344\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 49.99102020263672\n",
      "                              Voc = 28.64906120300293\n",
      "                              Jsc = 84.0867691040039\n",
      "                              FF = 32.353519439697266\n",
      "Finished epoch  228\n",
      "On epoch  229\n",
      "Total Epoch Testing MAPE: PCE = 49.547096252441406\n",
      "                              Voc = 28.43400001525879\n",
      "                              Jsc = 84.92146301269531\n",
      "                              FF = 33.34663772583008\n",
      "Finished epoch  229\n",
      "On epoch  230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.13270950317383\n",
      "                              Voc = 29.547521591186523\n",
      "                              Jsc = 86.32402038574219\n",
      "                              FF = 31.037822723388672\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 51.02669143676758\n",
      "                              Voc = 31.135377883911133\n",
      "                              Jsc = 85.83673095703125\n",
      "                              FF = 28.709020614624023\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 51.10313415527344\n",
      "                              Voc = 29.42318344116211\n",
      "                              Jsc = 85.24853515625\n",
      "                              FF = 28.204423904418945\n",
      "Finished epoch  232\n",
      "On epoch  233\n",
      "Total Epoch Testing MAPE: PCE = 50.46611404418945\n",
      "                              Voc = 30.27028465270996\n",
      "                              Jsc = 84.49411010742188\n",
      "                              FF = 28.31833267211914\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 49.80134963989258\n",
      "                              Voc = 29.914113998413086\n",
      "                              Jsc = 84.82176208496094\n",
      "                              FF = 29.58799934387207\n",
      "Finished epoch  234\n",
      "On epoch  235\n",
      "Total Epoch Testing MAPE: PCE = 49.98020935058594\n",
      "                              Voc = 30.670347213745117\n",
      "                              Jsc = 85.11343383789062\n",
      "                              FF = 28.617366790771484\n",
      "Finished epoch  235\n",
      "On epoch  236\n",
      "Total Epoch Testing MAPE: PCE = 50.11268997192383\n",
      "                              Voc = 31.977771759033203\n",
      "                              Jsc = 83.62332916259766\n",
      "                              FF = 28.701290130615234\n",
      "Finished epoch  236\n",
      "On epoch  237\n",
      "Total Epoch Testing MAPE: PCE = 50.01881790161133\n",
      "                              Voc = 31.253314971923828\n",
      "                              Jsc = 84.74750518798828\n",
      "                              FF = 27.670156478881836\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 51.2880973815918\n",
      "                              Voc = 29.151517868041992\n",
      "                              Jsc = 85.08273315429688\n",
      "                              FF = 28.737178802490234\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 51.51044845581055\n",
      "                              Voc = 28.676387786865234\n",
      "                              Jsc = 84.99625396728516\n",
      "                              FF = 27.632469177246094\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 51.250694274902344\n",
      "                              Voc = 25.406068801879883\n",
      "                              Jsc = 87.09878540039062\n",
      "                              FF = 27.992626190185547\n",
      "Finished epoch  240\n",
      "On epoch  241\n",
      "Total Epoch Testing MAPE: PCE = 50.747352600097656\n",
      "                              Voc = 24.041168212890625\n",
      "                              Jsc = 88.76870727539062\n",
      "                              FF = 28.705841064453125\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 50.71038055419922\n",
      "                              Voc = 25.202713012695312\n",
      "                              Jsc = 89.83537292480469\n",
      "                              FF = 29.091934204101562\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 50.727783203125\n",
      "                              Voc = 26.47995948791504\n",
      "                              Jsc = 90.50621795654297\n",
      "                              FF = 27.929264068603516\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 49.902374267578125\n",
      "                              Voc = 24.79159927368164\n",
      "                              Jsc = 90.35263061523438\n",
      "                              FF = 28.86691665649414\n",
      "Finished epoch  244\n",
      "On epoch  245\n",
      "Total Epoch Testing MAPE: PCE = 49.948760986328125\n",
      "                              Voc = 24.11001205444336\n",
      "                              Jsc = 89.72161102294922\n",
      "                              FF = 29.028419494628906\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 51.38874053955078\n",
      "                              Voc = 27.860942840576172\n",
      "                              Jsc = 88.1576156616211\n",
      "                              FF = 30.5242919921875\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 50.675601959228516\n",
      "                              Voc = 24.806039810180664\n",
      "                              Jsc = 88.31607055664062\n",
      "                              FF = 31.43242835998535\n",
      "Finished epoch  247\n",
      "On epoch  248\n",
      "Total Epoch Testing MAPE: PCE = 50.439090728759766\n",
      "                              Voc = 25.097259521484375\n",
      "                              Jsc = 88.54569244384766\n",
      "                              FF = 31.08454132080078\n",
      "Finished epoch  248\n",
      "On epoch  249\n",
      "Total Epoch Testing MAPE: PCE = 50.71881866455078\n",
      "                              Voc = 27.405733108520508\n",
      "                              Jsc = 88.81695556640625\n",
      "                              FF = 32.233272552490234\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 50.558929443359375\n",
      "                              Voc = 28.8248291015625\n",
      "                              Jsc = 88.78591918945312\n",
      "                              FF = 30.488828659057617\n",
      "Finished epoch  250\n",
      "On epoch  251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.42093276977539\n",
      "                              Voc = 29.559589385986328\n",
      "                              Jsc = 89.16592407226562\n",
      "                              FF = 29.627138137817383\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 50.25221252441406\n",
      "                              Voc = 30.677932739257812\n",
      "                              Jsc = 90.32672882080078\n",
      "                              FF = 30.999040603637695\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 50.48843765258789\n",
      "                              Voc = 29.634052276611328\n",
      "                              Jsc = 89.69462585449219\n",
      "                              FF = 33.403785705566406\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 48.26908874511719\n",
      "                              Voc = 32.21236038208008\n",
      "                              Jsc = 90.73544311523438\n",
      "                              FF = 33.51786422729492\n",
      "Finished epoch  254\n",
      "On epoch  255\n",
      "Total Epoch Testing MAPE: PCE = 48.41048812866211\n",
      "                              Voc = 31.652381896972656\n",
      "                              Jsc = 91.07929992675781\n",
      "                              FF = 35.07987976074219\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 48.37982940673828\n",
      "                              Voc = 34.52211380004883\n",
      "                              Jsc = 91.51851654052734\n",
      "                              FF = 37.4445686340332\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 48.901710510253906\n",
      "                              Voc = 33.506710052490234\n",
      "                              Jsc = 89.4873046875\n",
      "                              FF = 37.80815124511719\n",
      "Finished epoch  257\n",
      "On epoch  258\n",
      "Total Epoch Testing MAPE: PCE = 48.750213623046875\n",
      "                              Voc = 32.71494674682617\n",
      "                              Jsc = 90.92987060546875\n",
      "                              FF = 37.526466369628906\n",
      "Finished epoch  258\n",
      "On epoch  259\n",
      "Total Epoch Testing MAPE: PCE = 49.64488983154297\n",
      "                              Voc = 29.79002571105957\n",
      "                              Jsc = 91.86901092529297\n",
      "                              FF = 36.4361686706543\n",
      "Finished epoch  259\n",
      "On epoch  260\n",
      "Total Epoch Testing MAPE: PCE = 50.079078674316406\n",
      "                              Voc = 29.638391494750977\n",
      "                              Jsc = 90.76363372802734\n",
      "                              FF = 36.24102783203125\n",
      "Finished epoch  260\n",
      "On epoch  261\n",
      "Total Epoch Testing MAPE: PCE = 50.108367919921875\n",
      "                              Voc = 29.408899307250977\n",
      "                              Jsc = 90.71517944335938\n",
      "                              FF = 35.91694641113281\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 51.4348258972168\n",
      "                              Voc = 29.99466896057129\n",
      "                              Jsc = 91.3628158569336\n",
      "                              FF = 34.4292106628418\n",
      "Finished epoch  262\n",
      "On epoch  263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.806610107421875\n",
      "                              Voc = 30.445465087890625\n",
      "                              Jsc = 91.6829605102539\n",
      "                              FF = 34.77045440673828\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 50.93934631347656\n",
      "                              Voc = 31.655078887939453\n",
      "                              Jsc = 90.06434631347656\n",
      "                              FF = 35.484745025634766\n",
      "Finished epoch  264\n",
      "On epoch  265\n",
      "Total Epoch Testing MAPE: PCE = 50.42469024658203\n",
      "                              Voc = 33.124046325683594\n",
      "                              Jsc = 89.47998046875\n",
      "                              FF = 35.55241012573242\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 50.31654357910156\n",
      "                              Voc = 35.377960205078125\n",
      "                              Jsc = 90.55479431152344\n",
      "                              FF = 36.249061584472656\n",
      "Finished epoch  266\n",
      "On epoch  267\n",
      "Total Epoch Testing MAPE: PCE = 49.144134521484375\n",
      "                              Voc = 34.87441635131836\n",
      "                              Jsc = 90.83109283447266\n",
      "                              FF = 36.478538513183594\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 50.196285247802734\n",
      "                              Voc = 35.865692138671875\n",
      "                              Jsc = 89.12922668457031\n",
      "                              FF = 36.376914978027344\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 50.0256462097168\n",
      "                              Voc = 34.750343322753906\n",
      "                              Jsc = 90.00247955322266\n",
      "                              FF = 35.69802474975586\n",
      "Finished epoch  269\n",
      "On epoch  270\n",
      "Total Epoch Testing MAPE: PCE = 49.76766586303711\n",
      "                              Voc = 34.26490020751953\n",
      "                              Jsc = 90.31283569335938\n",
      "                              FF = 34.3975830078125\n",
      "Finished epoch  270\n",
      "On epoch  271\n",
      "Total Epoch Testing MAPE: PCE = 48.32209014892578\n",
      "                              Voc = 33.41733932495117\n",
      "                              Jsc = 91.00452423095703\n",
      "                              FF = 35.12717819213867\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 47.497955322265625\n",
      "                              Voc = 31.62250518798828\n",
      "                              Jsc = 91.650146484375\n",
      "                              FF = 35.83359909057617\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 47.31815719604492\n",
      "                              Voc = 34.44206237792969\n",
      "                              Jsc = 89.84928131103516\n",
      "                              FF = 34.31623077392578\n",
      "Finished epoch  273\n",
      "On epoch  274\n",
      "Total Epoch Testing MAPE: PCE = 47.6301383972168\n",
      "                              Voc = 32.0323600769043\n",
      "                              Jsc = 90.73129272460938\n",
      "                              FF = 33.76696014404297\n",
      "Finished epoch  274\n",
      "On epoch  275\n",
      "Total Epoch Testing MAPE: PCE = 47.944583892822266\n",
      "                              Voc = 33.21660614013672\n",
      "                              Jsc = 90.01253509521484\n",
      "                              FF = 32.253597259521484\n",
      "Finished epoch  275\n",
      "On epoch  276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.74310302734375\n",
      "                              Voc = 33.93848419189453\n",
      "                              Jsc = 88.85079956054688\n",
      "                              FF = 33.17687225341797\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 49.29836654663086\n",
      "                              Voc = 34.291988372802734\n",
      "                              Jsc = 89.22315979003906\n",
      "                              FF = 33.14486312866211\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 48.93653869628906\n",
      "                              Voc = 35.987274169921875\n",
      "                              Jsc = 89.62391662597656\n",
      "                              FF = 31.650060653686523\n",
      "Finished epoch  278\n",
      "On epoch  279\n",
      "Total Epoch Testing MAPE: PCE = 49.2923698425293\n",
      "                              Voc = 36.0389518737793\n",
      "                              Jsc = 89.76278686523438\n",
      "                              FF = 31.370445251464844\n",
      "Finished epoch  279\n",
      "On epoch  280\n",
      "Total Epoch Testing MAPE: PCE = 48.18265151977539\n",
      "                              Voc = 35.34496307373047\n",
      "                              Jsc = 89.67684173583984\n",
      "                              FF = 30.74118423461914\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 49.057769775390625\n",
      "                              Voc = 36.22168731689453\n",
      "                              Jsc = 90.1222915649414\n",
      "                              FF = 30.771955490112305\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 48.19594192504883\n",
      "                              Voc = 34.9956169128418\n",
      "                              Jsc = 92.53695678710938\n",
      "                              FF = 31.611173629760742\n",
      "Finished epoch  282\n",
      "On epoch  283\n",
      "Total Epoch Testing MAPE: PCE = 47.96236038208008\n",
      "                              Voc = 34.234066009521484\n",
      "                              Jsc = 90.47364807128906\n",
      "                              FF = 31.42426872253418\n",
      "Finished epoch  283\n",
      "On epoch  284\n",
      "Total Epoch Testing MAPE: PCE = 49.11217498779297\n",
      "                              Voc = 34.395713806152344\n",
      "                              Jsc = 90.07038116455078\n",
      "                              FF = 31.73923110961914\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 48.82747268676758\n",
      "                              Voc = 33.319950103759766\n",
      "                              Jsc = 90.11416625976562\n",
      "                              FF = 31.734949111938477\n",
      "Finished epoch  285\n",
      "On epoch  286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 47.773189544677734\n",
      "                              Voc = 35.0193977355957\n",
      "                              Jsc = 90.27064514160156\n",
      "                              FF = 30.783018112182617\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 47.140384674072266\n",
      "                              Voc = 32.48115921020508\n",
      "                              Jsc = 89.78775787353516\n",
      "                              FF = 32.065589904785156\n",
      "Finished epoch  287\n",
      "On epoch  288\n",
      "Total Epoch Testing MAPE: PCE = 45.180442810058594\n",
      "                              Voc = 33.21097946166992\n",
      "                              Jsc = 87.86433410644531\n",
      "                              FF = 31.381839752197266\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 48.03512954711914\n",
      "                              Voc = 33.67850112915039\n",
      "                              Jsc = 87.74019622802734\n",
      "                              FF = 31.281524658203125\n",
      "Finished epoch  289\n",
      "On epoch  290\n",
      "Total Epoch Testing MAPE: PCE = 47.19828796386719\n",
      "                              Voc = 33.98131561279297\n",
      "                              Jsc = 88.45731353759766\n",
      "                              FF = 29.48923683166504\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 48.0893669128418\n",
      "                              Voc = 33.85382080078125\n",
      "                              Jsc = 88.98138427734375\n",
      "                              FF = 28.60504150390625\n",
      "Finished epoch  291\n",
      "On epoch  292\n",
      "Total Epoch Testing MAPE: PCE = 49.74321746826172\n",
      "                              Voc = 34.59601593017578\n",
      "                              Jsc = 90.03776550292969\n",
      "                              FF = 30.792665481567383\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 49.42106628417969\n",
      "                              Voc = 34.040252685546875\n",
      "                              Jsc = 88.41763305664062\n",
      "                              FF = 29.342750549316406\n",
      "Finished epoch  293\n",
      "On epoch  294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.07509231567383\n",
      "                              Voc = 34.695762634277344\n",
      "                              Jsc = 89.07798767089844\n",
      "                              FF = 30.4691162109375\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 47.99119567871094\n",
      "                              Voc = 33.164756774902344\n",
      "                              Jsc = 87.56893157958984\n",
      "                              FF = 31.09796142578125\n",
      "Finished epoch  295\n",
      "On epoch  296\n",
      "Total Epoch Testing MAPE: PCE = 48.551937103271484\n",
      "                              Voc = 32.410400390625\n",
      "                              Jsc = 85.39253234863281\n",
      "                              FF = 31.236549377441406\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 47.55635070800781\n",
      "                              Voc = 32.99427032470703\n",
      "                              Jsc = 85.72480010986328\n",
      "                              FF = 31.09380340576172\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 48.404273986816406\n",
      "                              Voc = 34.26339340209961\n",
      "                              Jsc = 85.66429138183594\n",
      "                              FF = 30.938989639282227\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 50.36752700805664\n",
      "                              Voc = 32.99738311767578\n",
      "                              Jsc = 84.60709381103516\n",
      "                              FF = 32.00961685180664\n",
      "Finished epoch  299\n",
      "On epoch  300\n",
      "Total Epoch Testing MAPE: PCE = 50.89218521118164\n",
      "                              Voc = 32.472957611083984\n",
      "                              Jsc = 85.08321380615234\n",
      "                              FF = 32.072410583496094\n",
      "Finished epoch  300\n",
      "On epoch  301\n",
      "Total Epoch Testing MAPE: PCE = 51.95711135864258\n",
      "                              Voc = 32.631710052490234\n",
      "                              Jsc = 86.41459655761719\n",
      "                              FF = 33.08270263671875\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 52.71774673461914\n",
      "                              Voc = 34.05439376831055\n",
      "                              Jsc = 88.2773208618164\n",
      "                              FF = 33.35761260986328\n",
      "Finished epoch  302\n",
      "On epoch  303\n",
      "Total Epoch Testing MAPE: PCE = 52.48192596435547\n",
      "                              Voc = 32.648277282714844\n",
      "                              Jsc = 89.83654022216797\n",
      "                              FF = 32.6396598815918\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 51.84715270996094\n",
      "                              Voc = 30.956735610961914\n",
      "                              Jsc = 88.73296356201172\n",
      "                              FF = 32.01840591430664\n",
      "Finished epoch  304\n",
      "On epoch  305\n",
      "Total Epoch Testing MAPE: PCE = 51.25088119506836\n",
      "                              Voc = 32.93811798095703\n",
      "                              Jsc = 87.59622192382812\n",
      "                              FF = 31.55350112915039\n",
      "Finished epoch  305\n",
      "On epoch  306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.3454475402832\n",
      "                              Voc = 30.268733978271484\n",
      "                              Jsc = 86.3974609375\n",
      "                              FF = 31.24353790283203\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 51.94536590576172\n",
      "                              Voc = 29.6899356842041\n",
      "                              Jsc = 86.91951751708984\n",
      "                              FF = 30.75527572631836\n",
      "Finished epoch  307\n",
      "On epoch  308\n",
      "Total Epoch Testing MAPE: PCE = 52.05098342895508\n",
      "                              Voc = 29.383281707763672\n",
      "                              Jsc = 90.1693344116211\n",
      "                              FF = 31.279420852661133\n",
      "Finished epoch  308\n",
      "On epoch  309\n",
      "Total Epoch Testing MAPE: PCE = 49.88125991821289\n",
      "                              Voc = 28.568622589111328\n",
      "                              Jsc = 90.09752655029297\n",
      "                              FF = 30.16887664794922\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 51.04068374633789\n",
      "                              Voc = 25.494226455688477\n",
      "                              Jsc = 89.55521392822266\n",
      "                              FF = 29.912263870239258\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 51.95577621459961\n",
      "                              Voc = 24.31596565246582\n",
      "                              Jsc = 89.31065368652344\n",
      "                              FF = 29.526033401489258\n",
      "Finished epoch  311\n",
      "On epoch  312\n",
      "Total Epoch Testing MAPE: PCE = 51.55162811279297\n",
      "                              Voc = 26.38578987121582\n",
      "                              Jsc = 86.81311798095703\n",
      "                              FF = 29.215272903442383\n",
      "Finished epoch  312\n",
      "On epoch  313\n",
      "Total Epoch Testing MAPE: PCE = 54.0731086730957\n",
      "                              Voc = 26.18499755859375\n",
      "                              Jsc = 86.44967651367188\n",
      "                              FF = 30.675220489501953\n",
      "Finished epoch  313\n",
      "On epoch  314\n",
      "Total Epoch Testing MAPE: PCE = 52.513145446777344\n",
      "                              Voc = 26.056522369384766\n",
      "                              Jsc = 88.32654571533203\n",
      "                              FF = 31.277633666992188\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 52.72758483886719\n",
      "                              Voc = 24.833999633789062\n",
      "                              Jsc = 90.39445495605469\n",
      "                              FF = 31.720224380493164\n",
      "Finished epoch  315\n",
      "On epoch  316\n",
      "Total Epoch Testing MAPE: PCE = 54.35305404663086\n",
      "                              Voc = 22.999814987182617\n",
      "                              Jsc = 91.85472869873047\n",
      "                              FF = 31.996627807617188\n",
      "Finished epoch  316\n",
      "On epoch  317\n",
      "Total Epoch Testing MAPE: PCE = 53.49723815917969\n",
      "                              Voc = 24.143699645996094\n",
      "                              Jsc = 92.36674499511719\n",
      "                              FF = 31.093603134155273\n",
      "Finished epoch  317\n",
      "On epoch  318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.40336608886719\n",
      "                              Voc = 25.80180549621582\n",
      "                              Jsc = 91.7718505859375\n",
      "                              FF = 29.22739601135254\n",
      "Finished epoch  318\n",
      "On epoch  319\n",
      "Total Epoch Testing MAPE: PCE = 50.013362884521484\n",
      "                              Voc = 24.884929656982422\n",
      "                              Jsc = 89.78485107421875\n",
      "                              FF = 29.121524810791016\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 49.5010986328125\n",
      "                              Voc = 26.494583129882812\n",
      "                              Jsc = 89.91073608398438\n",
      "                              FF = 27.835237503051758\n",
      "Finished epoch  320\n",
      "On epoch  321\n",
      "Total Epoch Testing MAPE: PCE = 48.85637283325195\n",
      "                              Voc = 27.275657653808594\n",
      "                              Jsc = 90.43509674072266\n",
      "                              FF = 28.11184310913086\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 49.295650482177734\n",
      "                              Voc = 26.793045043945312\n",
      "                              Jsc = 89.12757873535156\n",
      "                              FF = 28.131484985351562\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 48.407657623291016\n",
      "                              Voc = 26.811126708984375\n",
      "                              Jsc = 88.7855453491211\n",
      "                              FF = 26.89272117614746\n",
      "Finished epoch  323\n",
      "On epoch  324\n",
      "Total Epoch Testing MAPE: PCE = 47.3166389465332\n",
      "                              Voc = 28.242006301879883\n",
      "                              Jsc = 89.81741333007812\n",
      "                              FF = 29.430622100830078\n",
      "Finished epoch  324\n",
      "On epoch  325\n",
      "Total Epoch Testing MAPE: PCE = 46.73954391479492\n",
      "                              Voc = 28.556520462036133\n",
      "                              Jsc = 89.51799011230469\n",
      "                              FF = 30.164554595947266\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 47.30233383178711\n",
      "                              Voc = 27.16650390625\n",
      "                              Jsc = 89.13926696777344\n",
      "                              FF = 28.602249145507812\n",
      "Finished epoch  326\n",
      "On epoch  327\n",
      "Total Epoch Testing MAPE: PCE = 49.38090515136719\n",
      "                              Voc = 24.664573669433594\n",
      "                              Jsc = 89.86360168457031\n",
      "                              FF = 27.724313735961914\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 49.51280212402344\n",
      "                              Voc = 24.507274627685547\n",
      "                              Jsc = 89.92633819580078\n",
      "                              FF = 28.85199546813965\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 49.62333297729492\n",
      "                              Voc = 22.82622528076172\n",
      "                              Jsc = 91.65397644042969\n",
      "                              FF = 27.228897094726562\n",
      "Finished epoch  329\n",
      "On epoch  330\n",
      "Total Epoch Testing MAPE: PCE = 49.1463508605957\n",
      "                              Voc = 24.261192321777344\n",
      "                              Jsc = 92.70753479003906\n",
      "                              FF = 27.644121170043945\n",
      "Finished epoch  330\n",
      "On epoch  331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 46.9705924987793\n",
      "                              Voc = 25.27293586730957\n",
      "                              Jsc = 90.29591369628906\n",
      "                              FF = 28.871503829956055\n",
      "Finished epoch  331\n",
      "On epoch  332\n",
      "Total Epoch Testing MAPE: PCE = 47.046512603759766\n",
      "                              Voc = 26.298606872558594\n",
      "                              Jsc = 89.5206298828125\n",
      "                              FF = 28.174686431884766\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 46.3209342956543\n",
      "                              Voc = 26.47454071044922\n",
      "                              Jsc = 89.33332824707031\n",
      "                              FF = 28.19268798828125\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 46.84254837036133\n",
      "                              Voc = 24.567874908447266\n",
      "                              Jsc = 89.41744232177734\n",
      "                              FF = 28.652402877807617\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 47.26546096801758\n",
      "                              Voc = 22.818275451660156\n",
      "                              Jsc = 88.04227447509766\n",
      "                              FF = 28.525394439697266\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 47.2215461730957\n",
      "                              Voc = 23.68376922607422\n",
      "                              Jsc = 87.79965209960938\n",
      "                              FF = 27.568265914916992\n",
      "Finished epoch  336\n",
      "On epoch  337\n",
      "Total Epoch Testing MAPE: PCE = 47.461795806884766\n",
      "                              Voc = 23.69900131225586\n",
      "                              Jsc = 87.37550354003906\n",
      "                              FF = 30.114099502563477\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 48.31489944458008\n",
      "                              Voc = 27.074331283569336\n",
      "                              Jsc = 85.71153259277344\n",
      "                              FF = 31.670974731445312\n",
      "Finished epoch  338\n",
      "On epoch  339\n",
      "Total Epoch Testing MAPE: PCE = 48.65616226196289\n",
      "                              Voc = 26.750940322875977\n",
      "                              Jsc = 87.2340087890625\n",
      "                              FF = 30.7442569732666\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 49.11435317993164\n",
      "                              Voc = 25.39060401916504\n",
      "                              Jsc = 88.35404205322266\n",
      "                              FF = 31.49455451965332\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 49.11201858520508\n",
      "                              Voc = 24.5841007232666\n",
      "                              Jsc = 88.90913391113281\n",
      "                              FF = 33.25007629394531\n",
      "Finished epoch  341\n",
      "On epoch  342\n",
      "Total Epoch Testing MAPE: PCE = 47.53696823120117\n",
      "                              Voc = 25.231138229370117\n",
      "                              Jsc = 89.5480728149414\n",
      "                              FF = 32.40201950073242\n",
      "Finished epoch  342\n",
      "On epoch  343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 47.2684440612793\n",
      "                              Voc = 25.880983352661133\n",
      "                              Jsc = 88.66327667236328\n",
      "                              FF = 30.118045806884766\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 46.49542999267578\n",
      "                              Voc = 24.75729751586914\n",
      "                              Jsc = 89.36566925048828\n",
      "                              FF = 30.970829010009766\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 45.98774719238281\n",
      "                              Voc = 24.160606384277344\n",
      "                              Jsc = 91.91487121582031\n",
      "                              FF = 31.427431106567383\n",
      "Finished epoch  345\n",
      "On epoch  346\n",
      "Total Epoch Testing MAPE: PCE = 46.0325813293457\n",
      "                              Voc = 23.802852630615234\n",
      "                              Jsc = 90.88249206542969\n",
      "                              FF = 30.831876754760742\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 44.87633514404297\n",
      "                              Voc = 24.937702178955078\n",
      "                              Jsc = 91.768798828125\n",
      "                              FF = 30.831417083740234\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 45.865234375\n",
      "                              Voc = 25.094688415527344\n",
      "                              Jsc = 91.28314971923828\n",
      "                              FF = 28.95382308959961\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 46.71935272216797\n",
      "                              Voc = 25.811115264892578\n",
      "                              Jsc = 89.56417083740234\n",
      "                              FF = 28.176774978637695\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 46.89141845703125\n",
      "                              Voc = 27.280197143554688\n",
      "                              Jsc = 89.02589416503906\n",
      "                              FF = 27.87623405456543\n",
      "Finished epoch  350\n",
      "On epoch  351\n",
      "Total Epoch Testing MAPE: PCE = 48.30687713623047\n",
      "                              Voc = 26.184621810913086\n",
      "                              Jsc = 90.02117919921875\n",
      "                              FF = 27.231178283691406\n",
      "Finished epoch  351\n",
      "On epoch  352\n",
      "Total Epoch Testing MAPE: PCE = 48.28876495361328\n",
      "                              Voc = 24.884138107299805\n",
      "                              Jsc = 89.43231964111328\n",
      "                              FF = 27.420696258544922\n",
      "Finished epoch  352\n",
      "On epoch  353\n",
      "Total Epoch Testing MAPE: PCE = 48.87936019897461\n",
      "                              Voc = 23.63883399963379\n",
      "                              Jsc = 87.72889709472656\n",
      "                              FF = 27.192312240600586\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 50.526607513427734\n",
      "                              Voc = 23.23716163635254\n",
      "                              Jsc = 89.3204116821289\n",
      "                              FF = 29.02027130126953\n",
      "Finished epoch  354\n",
      "On epoch  355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.97345733642578\n",
      "                              Voc = 22.621583938598633\n",
      "                              Jsc = 88.82603454589844\n",
      "                              FF = 29.527191162109375\n",
      "Finished epoch  355\n",
      "On epoch  356\n",
      "Total Epoch Testing MAPE: PCE = 50.08040237426758\n",
      "                              Voc = 24.564756393432617\n",
      "                              Jsc = 88.0853500366211\n",
      "                              FF = 28.670242309570312\n",
      "Finished epoch  356\n",
      "On epoch  357\n",
      "Total Epoch Testing MAPE: PCE = 49.77958679199219\n",
      "                              Voc = 22.85576057434082\n",
      "                              Jsc = 87.63194274902344\n",
      "                              FF = 28.437450408935547\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 50.94521713256836\n",
      "                              Voc = 27.258256912231445\n",
      "                              Jsc = 87.54032897949219\n",
      "                              FF = 29.439958572387695\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 48.48128890991211\n",
      "                              Voc = 27.551687240600586\n",
      "                              Jsc = 87.70579528808594\n",
      "                              FF = 30.032106399536133\n",
      "Finished epoch  359\n",
      "On epoch  360\n",
      "Total Epoch Testing MAPE: PCE = 50.008792877197266\n",
      "                              Voc = 25.852123260498047\n",
      "                              Jsc = 88.72119903564453\n",
      "                              FF = 29.59548568725586\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 49.69101333618164\n",
      "                              Voc = 25.324954986572266\n",
      "                              Jsc = 88.39603424072266\n",
      "                              FF = 29.494125366210938\n",
      "Finished epoch  361\n",
      "On epoch  362\n",
      "Total Epoch Testing MAPE: PCE = 49.605377197265625\n",
      "                              Voc = 26.67981719970703\n",
      "                              Jsc = 88.2197494506836\n",
      "                              FF = 30.184545516967773\n",
      "Finished epoch  362\n",
      "On epoch  363\n",
      "Total Epoch Testing MAPE: PCE = 49.111907958984375\n",
      "                              Voc = 25.918134689331055\n",
      "                              Jsc = 88.1150894165039\n",
      "                              FF = 29.68269920349121\n",
      "Finished epoch  363\n",
      "On epoch  364\n",
      "Total Epoch Testing MAPE: PCE = 49.58528137207031\n",
      "                              Voc = 25.895339965820312\n",
      "                              Jsc = 88.51371002197266\n",
      "                              FF = 29.785018920898438\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 49.568965911865234\n",
      "                              Voc = 25.68214225769043\n",
      "                              Jsc = 89.21599578857422\n",
      "                              FF = 31.067058563232422\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 49.34054946899414\n",
      "                              Voc = 26.332416534423828\n",
      "                              Jsc = 89.53121948242188\n",
      "                              FF = 29.68156623840332\n",
      "Finished epoch  366\n",
      "On epoch  367\n",
      "Total Epoch Testing MAPE: PCE = 49.210609436035156\n",
      "                              Voc = 27.443307876586914\n",
      "                              Jsc = 87.7215347290039\n",
      "                              FF = 28.369945526123047\n",
      "Finished epoch  367\n",
      "On epoch  368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.4234733581543\n",
      "                              Voc = 27.607282638549805\n",
      "                              Jsc = 86.6873550415039\n",
      "                              FF = 27.622095108032227\n",
      "Finished epoch  368\n",
      "On epoch  369\n",
      "Total Epoch Testing MAPE: PCE = 49.07661437988281\n",
      "                              Voc = 26.776334762573242\n",
      "                              Jsc = 88.00513458251953\n",
      "                              FF = 28.270050048828125\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 49.26477813720703\n",
      "                              Voc = 28.141822814941406\n",
      "                              Jsc = 88.69712829589844\n",
      "                              FF = 28.086814880371094\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 50.75644302368164\n",
      "                              Voc = 27.712940216064453\n",
      "                              Jsc = 89.06649017333984\n",
      "                              FF = 27.448997497558594\n",
      "Finished epoch  371\n",
      "On epoch  372\n",
      "Total Epoch Testing MAPE: PCE = 51.906883239746094\n",
      "                              Voc = 25.83236312866211\n",
      "                              Jsc = 87.92601013183594\n",
      "                              FF = 27.495315551757812\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 51.81495666503906\n",
      "                              Voc = 25.617345809936523\n",
      "                              Jsc = 87.78239440917969\n",
      "                              FF = 28.083620071411133\n",
      "Finished epoch  373\n",
      "On epoch  374\n",
      "Total Epoch Testing MAPE: PCE = 51.508480072021484\n",
      "                              Voc = 24.253551483154297\n",
      "                              Jsc = 87.1394271850586\n",
      "                              FF = 28.663331985473633\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 51.791236877441406\n",
      "                              Voc = 24.357070922851562\n",
      "                              Jsc = 87.83394622802734\n",
      "                              FF = 29.977638244628906\n",
      "Finished epoch  375\n",
      "On epoch  376\n",
      "Total Epoch Testing MAPE: PCE = 51.260929107666016\n",
      "                              Voc = 26.365657806396484\n",
      "                              Jsc = 89.06671142578125\n",
      "                              FF = 32.178260803222656\n",
      "Finished epoch  376\n",
      "On epoch  377\n",
      "Total Epoch Testing MAPE: PCE = 50.195068359375\n",
      "                              Voc = 28.472238540649414\n",
      "                              Jsc = 91.40978240966797\n",
      "                              FF = 32.055606842041016\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 50.48793411254883\n",
      "                              Voc = 26.963632583618164\n",
      "                              Jsc = 92.28640747070312\n",
      "                              FF = 32.189205169677734\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 50.15934753417969\n",
      "                              Voc = 27.944522857666016\n",
      "                              Jsc = 93.35562896728516\n",
      "                              FF = 31.174072265625\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 49.35362243652344\n",
      "                              Voc = 28.623109817504883\n",
      "                              Jsc = 93.0949478149414\n",
      "                              FF = 32.00621795654297\n",
      "Finished epoch  380\n",
      "On epoch  381\n",
      "Total Epoch Testing MAPE: PCE = 49.095314025878906\n",
      "                              Voc = 29.199203491210938\n",
      "                              Jsc = 92.42537689208984\n",
      "                              FF = 30.259183883666992\n",
      "Finished epoch  381\n",
      "On epoch  382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.71759796142578\n",
      "                              Voc = 28.66461944580078\n",
      "                              Jsc = 92.39369201660156\n",
      "                              FF = 28.189117431640625\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 50.89188766479492\n",
      "                              Voc = 26.069265365600586\n",
      "                              Jsc = 94.74899291992188\n",
      "                              FF = 29.10913848876953\n",
      "Finished epoch  383\n",
      "On epoch  384\n",
      "Total Epoch Testing MAPE: PCE = 51.017032623291016\n",
      "                              Voc = 25.75424575805664\n",
      "                              Jsc = 94.51838684082031\n",
      "                              FF = 29.64213752746582\n",
      "Finished epoch  384\n",
      "On epoch  385\n",
      "Total Epoch Testing MAPE: PCE = 50.86210250854492\n",
      "                              Voc = 24.73814582824707\n",
      "                              Jsc = 94.0184097290039\n",
      "                              FF = 30.602582931518555\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 50.920631408691406\n",
      "                              Voc = 24.42039680480957\n",
      "                              Jsc = 96.00321197509766\n",
      "                              FF = 32.76865005493164\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 51.141578674316406\n",
      "                              Voc = 25.137096405029297\n",
      "                              Jsc = 95.0516128540039\n",
      "                              FF = 32.41203308105469\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 49.92081832885742\n",
      "                              Voc = 23.90817642211914\n",
      "                              Jsc = 93.35623931884766\n",
      "                              FF = 31.125608444213867\n",
      "Finished epoch  388\n",
      "On epoch  389\n",
      "Total Epoch Testing MAPE: PCE = 50.4346923828125\n",
      "                              Voc = 25.109222412109375\n",
      "                              Jsc = 94.04944610595703\n",
      "                              FF = 31.842439651489258\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 50.232547760009766\n",
      "                              Voc = 24.650325775146484\n",
      "                              Jsc = 93.48179626464844\n",
      "                              FF = 31.04359245300293\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 49.71372985839844\n",
      "                              Voc = 23.53641700744629\n",
      "                              Jsc = 93.04080963134766\n",
      "                              FF = 31.512832641601562\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 50.46776580810547\n",
      "                              Voc = 23.517656326293945\n",
      "                              Jsc = 93.54175567626953\n",
      "                              FF = 31.790191650390625\n",
      "Finished epoch  392\n",
      "On epoch  393\n",
      "Total Epoch Testing MAPE: PCE = 50.448787689208984\n",
      "                              Voc = 22.92424774169922\n",
      "                              Jsc = 92.15937805175781\n",
      "                              FF = 31.615978240966797\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 48.56858444213867\n",
      "                              Voc = 22.038719177246094\n",
      "                              Jsc = 92.21366119384766\n",
      "                              FF = 33.26165771484375\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 48.69367218017578\n",
      "                              Voc = 21.949918746948242\n",
      "                              Jsc = 91.80769348144531\n",
      "                              FF = 33.44664001464844\n",
      "Finished epoch  395\n",
      "On epoch  396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.611698150634766\n",
      "                              Voc = 21.161161422729492\n",
      "                              Jsc = 91.70808410644531\n",
      "                              FF = 33.19975662231445\n",
      "Finished epoch  396\n",
      "On epoch  397\n",
      "Total Epoch Testing MAPE: PCE = 47.73910903930664\n",
      "                              Voc = 18.53523063659668\n",
      "                              Jsc = 91.55571746826172\n",
      "                              FF = 32.10363006591797\n",
      "Finished epoch  397\n",
      "On epoch  398\n",
      "Total Epoch Testing MAPE: PCE = 47.80815124511719\n",
      "                              Voc = 19.80878257751465\n",
      "                              Jsc = 91.76692962646484\n",
      "                              FF = 33.637603759765625\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 48.33767318725586\n",
      "                              Voc = 18.41329002380371\n",
      "                              Jsc = 91.77140045166016\n",
      "                              FF = 33.826717376708984\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 47.81410598754883\n",
      "                              Voc = 18.631574630737305\n",
      "                              Jsc = 91.80003356933594\n",
      "                              FF = 33.715267181396484\n",
      "Finished epoch  400\n",
      "On epoch  401\n",
      "Total Epoch Testing MAPE: PCE = 47.57110595703125\n",
      "                              Voc = 18.815345764160156\n",
      "                              Jsc = 91.78861999511719\n",
      "                              FF = 34.12682342529297\n",
      "Finished epoch  401\n",
      "On epoch  402\n",
      "Total Epoch Testing MAPE: PCE = 48.56563949584961\n",
      "                              Voc = 18.97970962524414\n",
      "                              Jsc = 90.69268035888672\n",
      "                              FF = 35.12117385864258\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 49.4830436706543\n",
      "                              Voc = 19.519039154052734\n",
      "                              Jsc = 90.60171508789062\n",
      "                              FF = 34.79471206665039\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 50.02046203613281\n",
      "                              Voc = 19.80193328857422\n",
      "                              Jsc = 91.12767791748047\n",
      "                              FF = 35.462276458740234\n",
      "Finished epoch  404\n",
      "On epoch  405\n",
      "Total Epoch Testing MAPE: PCE = 49.03426742553711\n",
      "                              Voc = 18.20313835144043\n",
      "                              Jsc = 92.2618408203125\n",
      "                              FF = 35.24057388305664\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 47.96855163574219\n",
      "                              Voc = 18.962507247924805\n",
      "                              Jsc = 92.40412902832031\n",
      "                              FF = 36.5609245300293\n",
      "Finished epoch  406\n",
      "On epoch  407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.468997955322266\n",
      "                              Voc = 18.825037002563477\n",
      "                              Jsc = 92.34126281738281\n",
      "                              FF = 35.55778503417969\n",
      "Finished epoch  407\n",
      "On epoch  408\n",
      "Total Epoch Testing MAPE: PCE = 48.5604362487793\n",
      "                              Voc = 17.01363754272461\n",
      "                              Jsc = 92.07524871826172\n",
      "                              FF = 34.46818161010742\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 47.8197021484375\n",
      "                              Voc = 15.659393310546875\n",
      "                              Jsc = 91.52584838867188\n",
      "                              FF = 34.466487884521484\n",
      "Finished epoch  409\n",
      "On epoch  410\n",
      "Total Epoch Testing MAPE: PCE = 47.5487174987793\n",
      "                              Voc = 16.274341583251953\n",
      "                              Jsc = 90.36476135253906\n",
      "                              FF = 35.60728073120117\n",
      "Finished epoch  410\n",
      "On epoch  411\n",
      "Total Epoch Testing MAPE: PCE = 46.66126251220703\n",
      "                              Voc = 16.851139068603516\n",
      "                              Jsc = 90.32815551757812\n",
      "                              FF = 36.32197570800781\n",
      "Finished epoch  411\n",
      "On epoch  412\n",
      "Total Epoch Testing MAPE: PCE = 48.56825256347656\n",
      "                              Voc = 18.367197036743164\n",
      "                              Jsc = 89.89096069335938\n",
      "                              FF = 35.659950256347656\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 48.18604278564453\n",
      "                              Voc = 21.023204803466797\n",
      "                              Jsc = 89.97441101074219\n",
      "                              FF = 35.28947067260742\n",
      "Finished epoch  413\n",
      "On epoch  414\n",
      "Total Epoch Testing MAPE: PCE = 49.07768630981445\n",
      "                              Voc = 21.642927169799805\n",
      "                              Jsc = 91.02977752685547\n",
      "                              FF = 35.10099411010742\n",
      "Finished epoch  414\n",
      "On epoch  415\n",
      "Total Epoch Testing MAPE: PCE = 49.676292419433594\n",
      "                              Voc = 19.887481689453125\n",
      "                              Jsc = 90.09281158447266\n",
      "                              FF = 33.9544563293457\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 49.87290573120117\n",
      "                              Voc = 19.581321716308594\n",
      "                              Jsc = 92.12974548339844\n",
      "                              FF = 33.774776458740234\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 49.39544677734375\n",
      "                              Voc = 18.351863861083984\n",
      "                              Jsc = 90.69982147216797\n",
      "                              FF = 35.4573860168457\n",
      "Finished epoch  417\n",
      "On epoch  418\n",
      "Total Epoch Testing MAPE: PCE = 49.99015808105469\n",
      "                              Voc = 18.911130905151367\n",
      "                              Jsc = 91.63450622558594\n",
      "                              FF = 35.430850982666016\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 50.141937255859375\n",
      "                              Voc = 20.049150466918945\n",
      "                              Jsc = 91.11643981933594\n",
      "                              FF = 35.66205978393555\n",
      "Finished epoch  419\n",
      "On epoch  420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.70411682128906\n",
      "                              Voc = 19.491580963134766\n",
      "                              Jsc = 90.37403106689453\n",
      "                              FF = 36.59367370605469\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 52.626426696777344\n",
      "                              Voc = 17.290916442871094\n",
      "                              Jsc = 89.50940704345703\n",
      "                              FF = 37.072486877441406\n",
      "Finished epoch  421\n",
      "On epoch  422\n",
      "Total Epoch Testing MAPE: PCE = 52.85495376586914\n",
      "                              Voc = 18.4989070892334\n",
      "                              Jsc = 90.2530517578125\n",
      "                              FF = 36.63066482543945\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 53.688716888427734\n",
      "                              Voc = 21.22591209411621\n",
      "                              Jsc = 89.2315902709961\n",
      "                              FF = 37.479454040527344\n",
      "Finished epoch  423\n",
      "On epoch  424\n",
      "Total Epoch Testing MAPE: PCE = 53.478973388671875\n",
      "                              Voc = 18.21074867248535\n",
      "                              Jsc = 88.90241241455078\n",
      "                              FF = 37.93235778808594\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 54.01720428466797\n",
      "                              Voc = 17.26910400390625\n",
      "                              Jsc = 89.69872283935547\n",
      "                              FF = 36.17271423339844\n",
      "Finished epoch  425\n",
      "On epoch  426\n",
      "Total Epoch Testing MAPE: PCE = 54.27592468261719\n",
      "                              Voc = 17.40926742553711\n",
      "                              Jsc = 90.83341979980469\n",
      "                              FF = 35.673614501953125\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 56.2748908996582\n",
      "                              Voc = 19.241241455078125\n",
      "                              Jsc = 90.5810546875\n",
      "                              FF = 35.62643051147461\n",
      "Finished epoch  427\n",
      "On epoch  428\n",
      "Total Epoch Testing MAPE: PCE = 55.699951171875\n",
      "                              Voc = 18.264209747314453\n",
      "                              Jsc = 88.71672821044922\n",
      "                              FF = 35.188602447509766\n",
      "Finished epoch  428\n",
      "On epoch  429\n",
      "Total Epoch Testing MAPE: PCE = 55.99342727661133\n",
      "                              Voc = 19.457347869873047\n",
      "                              Jsc = 87.43108367919922\n",
      "                              FF = 35.19889450073242\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 54.14311218261719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Voc = 20.450332641601562\n",
      "                              Jsc = 87.26165771484375\n",
      "                              FF = 33.68772888183594\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 54.109012603759766\n",
      "                              Voc = 20.2860050201416\n",
      "                              Jsc = 88.0033950805664\n",
      "                              FF = 35.070098876953125\n",
      "Finished epoch  431\n",
      "On epoch  432\n",
      "Total Epoch Testing MAPE: PCE = 52.91358184814453\n",
      "                              Voc = 22.050823211669922\n",
      "                              Jsc = 89.57167053222656\n",
      "                              FF = 33.65593719482422\n",
      "Finished epoch  432\n",
      "On epoch  433\n",
      "Total Epoch Testing MAPE: PCE = 54.35659408569336\n",
      "                              Voc = 22.621522903442383\n",
      "                              Jsc = 91.65560150146484\n",
      "                              FF = 32.61441421508789\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 53.708351135253906\n",
      "                              Voc = 24.002492904663086\n",
      "                              Jsc = 90.88939666748047\n",
      "                              FF = 33.26547622680664\n",
      "Finished epoch  434\n",
      "On epoch  435\n",
      "Total Epoch Testing MAPE: PCE = 54.08381271362305\n",
      "                              Voc = 24.128700256347656\n",
      "                              Jsc = 90.2574691772461\n",
      "                              FF = 32.46540832519531\n",
      "Finished epoch  435\n",
      "On epoch  436\n",
      "Total Epoch Testing MAPE: PCE = 52.31776809692383\n",
      "                              Voc = 23.976369857788086\n",
      "                              Jsc = 89.52374267578125\n",
      "                              FF = 32.50246047973633\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 53.39954376220703\n",
      "                              Voc = 24.35211753845215\n",
      "                              Jsc = 89.19567108154297\n",
      "                              FF = 32.03887939453125\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 54.96445846557617\n",
      "                              Voc = 24.722667694091797\n",
      "                              Jsc = 89.98815155029297\n",
      "                              FF = 31.092796325683594\n",
      "Finished epoch  438\n",
      "On epoch  439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.0107421875\n",
      "                              Voc = 23.944374084472656\n",
      "                              Jsc = 89.45105743408203\n",
      "                              FF = 33.15078353881836\n",
      "Finished epoch  439\n",
      "On epoch  440\n",
      "Total Epoch Testing MAPE: PCE = 57.245689392089844\n",
      "                              Voc = 21.41257667541504\n",
      "                              Jsc = 90.12942504882812\n",
      "                              FF = 31.40740966796875\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 57.2108268737793\n",
      "                              Voc = 22.51288414001465\n",
      "                              Jsc = 89.10202026367188\n",
      "                              FF = 31.87592315673828\n",
      "Finished epoch  441\n",
      "On epoch  442\n",
      "Total Epoch Testing MAPE: PCE = 56.11587142944336\n",
      "                              Voc = 22.095745086669922\n",
      "                              Jsc = 89.32011413574219\n",
      "                              FF = 31.969186782836914\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 55.93015670776367\n",
      "                              Voc = 20.317398071289062\n",
      "                              Jsc = 89.78873443603516\n",
      "                              FF = 31.138334274291992\n",
      "Finished epoch  443\n",
      "On epoch  444\n",
      "Total Epoch Testing MAPE: PCE = 54.37126922607422\n",
      "                              Voc = 20.81314468383789\n",
      "                              Jsc = 89.10115814208984\n",
      "                              FF = 30.539941787719727\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 53.76469421386719\n",
      "                              Voc = 21.85719871520996\n",
      "                              Jsc = 88.88003540039062\n",
      "                              FF = 31.40576171875\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 52.173500061035156\n",
      "                              Voc = 22.807971954345703\n",
      "                              Jsc = 87.01066589355469\n",
      "                              FF = 32.419830322265625\n",
      "Finished epoch  446\n",
      "On epoch  447\n",
      "Total Epoch Testing MAPE: PCE = 50.83672332763672\n",
      "                              Voc = 21.585960388183594\n",
      "                              Jsc = 86.86737060546875\n",
      "                              FF = 31.42154312133789\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 51.1525993347168\n",
      "                              Voc = 19.159324645996094\n",
      "                              Jsc = 85.89991760253906\n",
      "                              FF = 32.514747619628906\n",
      "Finished epoch  448\n",
      "On epoch  449\n",
      "Total Epoch Testing MAPE: PCE = 49.35309982299805\n",
      "                              Voc = 18.472639083862305\n",
      "                              Jsc = 84.57131958007812\n",
      "                              FF = 32.03512191772461\n",
      "Finished epoch  449\n",
      "On epoch  450\n",
      "Total Epoch Testing MAPE: PCE = 48.560302734375\n",
      "                              Voc = 19.12020492553711\n",
      "                              Jsc = 84.2586898803711\n",
      "                              FF = 32.66291809082031\n",
      "Finished epoch  450\n",
      "On epoch  451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.18232727050781\n",
      "                              Voc = 17.469663619995117\n",
      "                              Jsc = 84.44032287597656\n",
      "                              FF = 32.33474349975586\n",
      "Finished epoch  451\n",
      "On epoch  452\n",
      "Total Epoch Testing MAPE: PCE = 49.32886505126953\n",
      "                              Voc = 17.458843231201172\n",
      "                              Jsc = 85.78394317626953\n",
      "                              FF = 29.91632843017578\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 49.68048095703125\n",
      "                              Voc = 17.541669845581055\n",
      "                              Jsc = 86.27458953857422\n",
      "                              FF = 29.7489013671875\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 48.82844924926758\n",
      "                              Voc = 17.455127716064453\n",
      "                              Jsc = 87.00846099853516\n",
      "                              FF = 30.14786148071289\n",
      "Finished epoch  454\n",
      "On epoch  455\n",
      "Total Epoch Testing MAPE: PCE = 47.80574035644531\n",
      "                              Voc = 18.782642364501953\n",
      "                              Jsc = 88.75592041015625\n",
      "                              FF = 30.57231330871582\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 48.33868408203125\n",
      "                              Voc = 17.69153594970703\n",
      "                              Jsc = 89.2157974243164\n",
      "                              FF = 31.316862106323242\n",
      "Finished epoch  456\n",
      "On epoch  457\n",
      "Total Epoch Testing MAPE: PCE = 47.615779876708984\n",
      "                              Voc = 17.272613525390625\n",
      "                              Jsc = 88.2612533569336\n",
      "                              FF = 31.743091583251953\n",
      "Finished epoch  457\n",
      "On epoch  458\n",
      "Total Epoch Testing MAPE: PCE = 48.410728454589844\n",
      "                              Voc = 17.371082305908203\n",
      "                              Jsc = 88.690673828125\n",
      "                              FF = 32.828609466552734\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 48.246131896972656\n",
      "                              Voc = 17.43972396850586\n",
      "                              Jsc = 88.60993957519531\n",
      "                              FF = 33.048404693603516\n",
      "Finished epoch  459\n",
      "On epoch  460\n",
      "Total Epoch Testing MAPE: PCE = 48.38972473144531\n",
      "                              Voc = 20.477407455444336\n",
      "                              Jsc = 89.66900634765625\n",
      "                              FF = 30.688732147216797\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 48.329105377197266\n",
      "                              Voc = 19.331321716308594\n",
      "                              Jsc = 90.70138549804688\n",
      "                              FF = 30.40049934387207\n",
      "Finished epoch  461\n",
      "On epoch  462\n",
      "Total Epoch Testing MAPE: PCE = 47.09934997558594\n",
      "                              Voc = 19.673152923583984\n",
      "                              Jsc = 91.18928527832031\n",
      "                              FF = 29.89969825744629\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 46.513206481933594\n",
      "                              Voc = 20.403453826904297\n",
      "                              Jsc = 90.5908203125\n",
      "                              FF = 29.320106506347656\n",
      "Finished epoch  463\n",
      "On epoch  464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 47.43387985229492\n",
      "                              Voc = 18.770347595214844\n",
      "                              Jsc = 92.14698028564453\n",
      "                              FF = 30.317861557006836\n",
      "Finished epoch  464\n",
      "On epoch  465\n",
      "Total Epoch Testing MAPE: PCE = 48.05105209350586\n",
      "                              Voc = 18.624881744384766\n",
      "                              Jsc = 91.13969421386719\n",
      "                              FF = 30.049875259399414\n",
      "Finished epoch  465\n",
      "On epoch  466\n",
      "Total Epoch Testing MAPE: PCE = 47.53728103637695\n",
      "                              Voc = 19.67844009399414\n",
      "                              Jsc = 91.11334228515625\n",
      "                              FF = 30.479446411132812\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 45.24835968017578\n",
      "                              Voc = 21.27277183532715\n",
      "                              Jsc = 90.5455093383789\n",
      "                              FF = 32.395751953125\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 46.590389251708984\n",
      "                              Voc = 20.09905433654785\n",
      "                              Jsc = 91.98939514160156\n",
      "                              FF = 34.59497833251953\n",
      "Finished epoch  468\n",
      "On epoch  469\n",
      "Total Epoch Testing MAPE: PCE = 46.18855285644531\n",
      "                              Voc = 20.2115535736084\n",
      "                              Jsc = 91.0353012084961\n",
      "                              FF = 34.37955856323242\n",
      "Finished epoch  469\n",
      "On epoch  470\n",
      "Total Epoch Testing MAPE: PCE = 46.68912887573242\n",
      "                              Voc = 18.755428314208984\n",
      "                              Jsc = 92.872314453125\n",
      "                              FF = 33.97720718383789\n",
      "Finished epoch  470\n",
      "On epoch  471\n",
      "Total Epoch Testing MAPE: PCE = 47.73148727416992\n",
      "                              Voc = 16.52484893798828\n",
      "                              Jsc = 92.80460357666016\n",
      "                              FF = 33.66691970825195\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 49.50592041015625\n",
      "                              Voc = 15.972905158996582\n",
      "                              Jsc = 90.6289291381836\n",
      "                              FF = 34.24824905395508\n",
      "Finished epoch  472\n",
      "On epoch  473\n",
      "Total Epoch Testing MAPE: PCE = 48.433345794677734\n",
      "                              Voc = 17.977067947387695\n",
      "                              Jsc = 91.06121063232422\n",
      "                              FF = 32.983558654785156\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 48.737205505371094\n",
      "                              Voc = 18.373943328857422\n",
      "                              Jsc = 91.87359619140625\n",
      "                              FF = 32.595375061035156\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 47.976829528808594\n",
      "                              Voc = 17.224084854125977\n",
      "                              Jsc = 92.66148376464844\n",
      "                              FF = 33.260189056396484\n",
      "Finished epoch  475\n",
      "On epoch  476\n",
      "Total Epoch Testing MAPE: PCE = 47.638126373291016\n",
      "                              Voc = 13.913043022155762\n",
      "                              Jsc = 93.0792236328125\n",
      "                              FF = 33.02645492553711\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 48.75749206542969\n",
      "                              Voc = 14.93222427368164\n",
      "                              Jsc = 94.19012451171875\n",
      "                              FF = 31.98694610595703\n",
      "Finished epoch  477\n",
      "On epoch  478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.00133514404297\n",
      "                              Voc = 15.546183586120605\n",
      "                              Jsc = 93.98179626464844\n",
      "                              FF = 33.156673431396484\n",
      "Finished epoch  478\n",
      "On epoch  479\n",
      "Total Epoch Testing MAPE: PCE = 49.89518737792969\n",
      "                              Voc = 13.684887886047363\n",
      "                              Jsc = 92.29882049560547\n",
      "                              FF = 33.97278594970703\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 49.986900329589844\n",
      "                              Voc = 12.608613014221191\n",
      "                              Jsc = 91.61595153808594\n",
      "                              FF = 34.28733825683594\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 49.057437896728516\n",
      "                              Voc = 12.450995445251465\n",
      "                              Jsc = 90.8530044555664\n",
      "                              FF = 34.25899887084961\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 47.4983024597168\n",
      "                              Voc = 12.364632606506348\n",
      "                              Jsc = 90.84772491455078\n",
      "                              FF = 33.12839889526367\n",
      "Finished epoch  482\n",
      "On epoch  483\n",
      "Total Epoch Testing MAPE: PCE = 47.418418884277344\n",
      "                              Voc = 13.65544319152832\n",
      "                              Jsc = 91.2737808227539\n",
      "                              FF = 32.70233154296875\n",
      "Finished epoch  483\n",
      "On epoch  484\n",
      "Total Epoch Testing MAPE: PCE = 47.17616271972656\n",
      "                              Voc = 14.819613456726074\n",
      "                              Jsc = 90.21268463134766\n",
      "                              FF = 33.59977722167969\n",
      "Finished epoch  484\n",
      "On epoch  485\n",
      "Total Epoch Testing MAPE: PCE = 47.41323471069336\n",
      "                              Voc = 13.995461463928223\n",
      "                              Jsc = 90.08303833007812\n",
      "                              FF = 32.659366607666016\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 48.565956115722656\n",
      "                              Voc = 14.929636001586914\n",
      "                              Jsc = 91.05902862548828\n",
      "                              FF = 33.337833404541016\n",
      "Finished epoch  486\n",
      "On epoch  487\n",
      "Total Epoch Testing MAPE: PCE = 47.529541015625\n",
      "                              Voc = 15.802023887634277\n",
      "                              Jsc = 90.88813018798828\n",
      "                              FF = 33.66606903076172\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 47.079410552978516\n",
      "                              Voc = 16.128759384155273\n",
      "                              Jsc = 91.49935913085938\n",
      "                              FF = 33.40260314941406\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 47.39588165283203\n",
      "                              Voc = 15.235856056213379\n",
      "                              Jsc = 92.26341247558594\n",
      "                              FF = 32.55853271484375\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 48.40310287475586\n",
      "                              Voc = 18.271268844604492\n",
      "                              Jsc = 93.07637786865234\n",
      "                              FF = 30.373844146728516\n",
      "Finished epoch  490\n",
      "On epoch  491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.093990325927734\n",
      "                              Voc = 18.063570022583008\n",
      "                              Jsc = 92.47029113769531\n",
      "                              FF = 29.09093475341797\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 49.6058235168457\n",
      "                              Voc = 20.3726863861084\n",
      "                              Jsc = 90.39381408691406\n",
      "                              FF = 30.437332153320312\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 50.13579559326172\n",
      "                              Voc = 20.30866241455078\n",
      "                              Jsc = 90.84539794921875\n",
      "                              FF = 30.957826614379883\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 51.6013069152832\n",
      "                              Voc = 19.3160400390625\n",
      "                              Jsc = 89.28502655029297\n",
      "                              FF = 30.46803092956543\n",
      "Finished epoch  494\n",
      "On epoch  495\n",
      "Total Epoch Testing MAPE: PCE = 51.15431594848633\n",
      "                              Voc = 18.3969669342041\n",
      "                              Jsc = 89.97746276855469\n",
      "                              FF = 32.39396286010742\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 50.41364288330078\n",
      "                              Voc = 16.18876838684082\n",
      "                              Jsc = 90.06640625\n",
      "                              FF = 30.985137939453125\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 50.6232795715332\n",
      "                              Voc = 16.079620361328125\n",
      "                              Jsc = 89.84183502197266\n",
      "                              FF = 32.03087615966797\n",
      "Finished epoch  497\n",
      "On epoch  498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.55788803100586\n",
      "                              Voc = 15.347509384155273\n",
      "                              Jsc = 90.31525421142578\n",
      "                              FF = 32.15681457519531\n",
      "Finished epoch  498\n",
      "On epoch  499\n",
      "Total Epoch Testing MAPE: PCE = 49.661888122558594\n",
      "                              Voc = 15.294466972351074\n",
      "                              Jsc = 89.67776489257812\n",
      "                              FF = 32.17655563354492\n",
      "Finished epoch  499\n",
      "Fold # 2\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 43263.625\n",
      "                              Voc = 42898.9921875\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 43187.21484375\n",
      "                              Voc = 42894.9140625\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 43129.72265625\n",
      "                              Voc = 42880.40234375\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  2\n",
      "On epoch  3\n",
      "Total Epoch Testing MAPE: PCE = 43087.92578125\n",
      "                              Voc = 42882.33203125\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 43066.58203125\n",
      "                              Voc = 42885.34765625\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  4\n",
      "On epoch  5\n",
      "Total Epoch Testing MAPE: PCE = 43045.125\n",
      "                              Voc = 42881.765625\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  5\n",
      "On epoch  6\n",
      "Total Epoch Testing MAPE: PCE = 43022.84375\n",
      "                              Voc = 42876.3203125\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  6\n",
      "On epoch  7\n",
      "Total Epoch Testing MAPE: PCE = 43004.9296875\n",
      "                              Voc = 42879.90234375\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 42991.63671875\n",
      "                              Voc = 42878.73828125\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  8\n",
      "On epoch  9\n",
      "Total Epoch Testing MAPE: PCE = 42980.18359375\n",
      "                              Voc = 42875.56640625\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 42969.703125\n",
      "                              Voc = 42877.36328125\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  10\n",
      "On epoch  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42963.36328125\n",
      "                              Voc = 42875.82421875\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 42956.37109375\n",
      "                              Voc = 42875.5546875\n",
      "                              Jsc = nan\n",
      "                              FF = nan\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 42945.2890625\n",
      "                              Voc = 42872.5234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42914.28125\n",
      "Finished epoch  13\n",
      "On epoch  14\n",
      "Total Epoch Testing MAPE: PCE = 42936.5078125\n",
      "                              Voc = 42873.7890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42914.28125\n",
      "Finished epoch  14\n",
      "On epoch  15\n",
      "Total Epoch Testing MAPE: PCE = 42932.1484375\n",
      "                              Voc = 42872.0625\n",
      "                              Jsc = nan\n",
      "                              FF = 42914.28515625\n",
      "Finished epoch  15\n",
      "On epoch  16\n",
      "Total Epoch Testing MAPE: PCE = 42926.8515625\n",
      "                              Voc = 42871.1171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42913.7890625\n",
      "Finished epoch  16\n",
      "On epoch  17\n",
      "Total Epoch Testing MAPE: PCE = 42924.0625\n",
      "                              Voc = 42871.93359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42912.25390625\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 42921.17578125\n",
      "                              Voc = 42870.125\n",
      "                              Jsc = nan\n",
      "                              FF = 42911.78125\n",
      "Finished epoch  18\n",
      "On epoch  19\n",
      "Total Epoch Testing MAPE: PCE = 42920.14453125\n",
      "                              Voc = 42869.1484375\n",
      "                              Jsc = nan\n",
      "                              FF = 42911.88671875\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 42916.34375\n",
      "                              Voc = 42867.68359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42911.7109375\n",
      "Finished epoch  20\n",
      "On epoch  21\n",
      "Total Epoch Testing MAPE: PCE = 42914.22265625\n",
      "                              Voc = 42867.58203125\n",
      "                              Jsc = nan\n",
      "                              FF = 42910.7578125\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 42911.08984375\n",
      "                              Voc = 42867.50390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42911.25\n",
      "Finished epoch  22\n",
      "On epoch  23\n",
      "Total Epoch Testing MAPE: PCE = 42909.90625\n",
      "                              Voc = 42867.3984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42909.515625\n",
      "Finished epoch  23\n",
      "On epoch  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42907.53515625\n",
      "                              Voc = 42867.6484375\n",
      "                              Jsc = nan\n",
      "                              FF = 42909.21484375\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 42906.3046875\n",
      "                              Voc = 42867.5546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42909.3203125\n",
      "Finished epoch  25\n",
      "On epoch  26\n",
      "Total Epoch Testing MAPE: PCE = 42904.92578125\n",
      "                              Voc = 42867.65234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42908.34765625\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 42903.5\n",
      "                              Voc = 42867.79296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.72265625\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 42904.3046875\n",
      "                              Voc = 42867.62890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.64453125\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 42904.77734375\n",
      "                              Voc = 42867.36328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.97265625\n",
      "Finished epoch  29\n",
      "On epoch  30\n",
      "Total Epoch Testing MAPE: PCE = 42899.89453125\n",
      "                              Voc = 42867.47265625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.4921875\n",
      "Finished epoch  30\n",
      "On epoch  31\n",
      "Total Epoch Testing MAPE: PCE = 42898.33203125\n",
      "                              Voc = 42868.14453125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.5625\n",
      "Finished epoch  31\n",
      "On epoch  32\n",
      "Total Epoch Testing MAPE: PCE = 42894.9296875\n",
      "                              Voc = 42867.5546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.74609375\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 42896.28125\n",
      "                              Voc = 42867.44921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.19140625\n",
      "Finished epoch  33\n",
      "On epoch  34\n",
      "Total Epoch Testing MAPE: PCE = 42897.109375\n",
      "                              Voc = 42867.55078125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.37109375\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 42896.45703125\n",
      "                              Voc = 42867.62890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.1015625\n",
      "Finished epoch  35\n",
      "On epoch  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42894.3828125\n",
      "                              Voc = 42867.55859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.7578125\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 42895.27734375\n",
      "                              Voc = 42867.65625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.45703125\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 42895.4140625\n",
      "                              Voc = 42867.65625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.6640625\n",
      "Finished epoch  38\n",
      "On epoch  39\n",
      "Total Epoch Testing MAPE: PCE = 42895.75\n",
      "                              Voc = 42867.6015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.125\n",
      "Finished epoch  39\n",
      "On epoch  40\n",
      "Total Epoch Testing MAPE: PCE = 42895.75\n",
      "                              Voc = 42867.58203125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.9375\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 42892.8984375\n",
      "                              Voc = 42867.625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.2109375\n",
      "Finished epoch  41\n",
      "On epoch  42\n",
      "Total Epoch Testing MAPE: PCE = 42889.578125\n",
      "                              Voc = 42867.79296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.02734375\n",
      "Finished epoch  42\n",
      "On epoch  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42889.796875\n",
      "                              Voc = 42867.9375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.8359375\n",
      "Finished epoch  43\n",
      "On epoch  44\n",
      "Total Epoch Testing MAPE: PCE = 42890.2578125\n",
      "                              Voc = 42867.1953125\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.25\n",
      "Finished epoch  44\n",
      "On epoch  45\n",
      "Total Epoch Testing MAPE: PCE = 42888.45703125\n",
      "                              Voc = 42867.1328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.00390625\n",
      "Finished epoch  45\n",
      "On epoch  46\n",
      "Total Epoch Testing MAPE: PCE = 42889.30078125\n",
      "                              Voc = 42867.265625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.95703125\n",
      "Finished epoch  46\n",
      "On epoch  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42889.1328125\n",
      "                              Voc = 42867.4296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.60546875\n",
      "Finished epoch  47\n",
      "On epoch  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42889.10546875\n",
      "                              Voc = 42867.14453125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.62109375\n",
      "Finished epoch  48\n",
      "On epoch  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42888.203125\n",
      "                              Voc = 42867.30859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.56640625\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 42888.11328125\n",
      "                              Voc = 42867.75\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.2578125\n",
      "Finished epoch  50\n",
      "On epoch  51\n",
      "Total Epoch Testing MAPE: PCE = 42886.6796875\n",
      "                              Voc = 42868.46484375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.0234375\n",
      "Finished epoch  51\n",
      "On epoch  52\n",
      "Total Epoch Testing MAPE: PCE = 42886.84375\n",
      "                              Voc = 42867.5\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.41796875\n",
      "Finished epoch  52\n",
      "On epoch  53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42887.546875\n",
      "                              Voc = 42867.50390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.83984375\n",
      "Finished epoch  53\n",
      "On epoch  54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42887.30078125\n",
      "                              Voc = 42869.92578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.01171875\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 42886.52734375\n",
      "                              Voc = 42868.91015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.83203125\n",
      "Finished epoch  55\n",
      "On epoch  56\n",
      "Total Epoch Testing MAPE: PCE = 42886.55859375\n",
      "                              Voc = 42867.46484375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.2421875\n",
      "Finished epoch  56\n",
      "On epoch  57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42886.05859375\n",
      "                              Voc = 42867.234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.71484375\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 42886.546875\n",
      "                              Voc = 42866.62109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.5390625\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 42886.80859375\n",
      "                              Voc = 42866.38671875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.6328125\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 42886.41015625\n",
      "                              Voc = 42866.2578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.73046875\n",
      "Finished epoch  60\n",
      "On epoch  61\n",
      "Total Epoch Testing MAPE: PCE = 42887.0078125\n",
      "                              Voc = 42866.16015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.5625\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 42888.3125\n",
      "                              Voc = 42866.16796875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.94140625\n",
      "Finished epoch  62\n",
      "On epoch  63\n",
      "Total Epoch Testing MAPE: PCE = 42887.93359375\n",
      "                              Voc = 42866.109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.80078125\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 42887.92578125\n",
      "                              Voc = 42866.21484375\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.44921875\n",
      "Finished epoch  64\n",
      "On epoch  65\n",
      "Total Epoch Testing MAPE: PCE = 42888.7890625\n",
      "                              Voc = 42866.31640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.26953125\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 42889.03515625\n",
      "                              Voc = 42866.140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.421875\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 42890.31640625\n",
      "                              Voc = 42865.69921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.29296875\n",
      "Finished epoch  67\n",
      "On epoch  68\n",
      "Total Epoch Testing MAPE: PCE = 42889.94140625\n",
      "                              Voc = 42865.83984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.40234375\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 42889.25390625\n",
      "                              Voc = 42865.7890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.0\n",
      "Finished epoch  69\n",
      "On epoch  70\n",
      "Total Epoch Testing MAPE: PCE = 42889.4296875\n",
      "                              Voc = 42866.0078125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.015625\n",
      "Finished epoch  70\n",
      "On epoch  71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42889.9375\n",
      "                              Voc = 42866.16015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.26171875\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 42891.84375\n",
      "                              Voc = 42866.3125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.09375\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 42889.2578125\n",
      "                              Voc = 42866.0390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.12109375\n",
      "Finished epoch  73\n",
      "On epoch  74\n",
      "Total Epoch Testing MAPE: PCE = 42889.94140625\n",
      "                              Voc = 42865.828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.9453125\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 42889.46875\n",
      "                              Voc = 42865.703125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.9375\n",
      "Finished epoch  75\n",
      "On epoch  76\n",
      "Total Epoch Testing MAPE: PCE = 42889.05859375\n",
      "                              Voc = 42865.78515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.3515625\n",
      "Finished epoch  76\n",
      "On epoch  77\n",
      "Total Epoch Testing MAPE: PCE = 42889.6328125\n",
      "                              Voc = 42865.73046875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.45703125\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 42888.8046875\n",
      "                              Voc = 42865.6640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.2734375\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 42888.90625\n",
      "                              Voc = 42865.62890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.953125\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 42888.90234375\n",
      "                              Voc = 42865.7890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.0546875\n",
      "Finished epoch  80\n",
      "On epoch  81\n",
      "Total Epoch Testing MAPE: PCE = 42888.77734375\n",
      "                              Voc = 42866.87109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.76953125\n",
      "Finished epoch  81\n",
      "On epoch  82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42889.75390625\n",
      "                              Voc = 42871.13671875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.546875\n",
      "Finished epoch  82\n",
      "On epoch  83\n",
      "Total Epoch Testing MAPE: PCE = 42889.30859375\n",
      "                              Voc = 42871.6328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.8828125\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 42888.2734375\n",
      "                              Voc = 42869.4140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.1953125\n",
      "Finished epoch  84\n",
      "On epoch  85\n",
      "Total Epoch Testing MAPE: PCE = 42886.86328125\n",
      "                              Voc = 42870.96875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.8359375\n",
      "Finished epoch  85\n",
      "On epoch  86\n",
      "Total Epoch Testing MAPE: PCE = 42886.6484375\n",
      "                              Voc = 42873.34765625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.34375\n",
      "Finished epoch  86\n",
      "On epoch  87\n",
      "Total Epoch Testing MAPE: PCE = 42886.80859375\n",
      "                              Voc = 42872.359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.61328125\n",
      "Finished epoch  87\n",
      "On epoch  88\n",
      "Total Epoch Testing MAPE: PCE = 42887.296875\n",
      "                              Voc = 42871.609375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.1484375\n",
      "Finished epoch  88\n",
      "On epoch  89\n",
      "Total Epoch Testing MAPE: PCE = 42886.86328125\n",
      "                              Voc = 42870.9453125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.35546875\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 42887.296875\n",
      "                              Voc = 42868.828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.81640625\n",
      "Finished epoch  90\n",
      "On epoch  91\n",
      "Total Epoch Testing MAPE: PCE = 42887.9609375\n",
      "                              Voc = 42867.375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.67578125\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 42886.9296875\n",
      "                              Voc = 42866.4765625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.515625\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 42887.2265625\n",
      "                              Voc = 42867.40625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.63671875\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 42887.23046875\n",
      "                              Voc = 42866.7734375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.5078125\n",
      "Finished epoch  94\n",
      "On epoch  95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42886.6796875\n",
      "                              Voc = 42867.921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.4453125\n",
      "Finished epoch  95\n",
      "On epoch  96\n",
      "Total Epoch Testing MAPE: PCE = 42886.30078125\n",
      "                              Voc = 42866.80078125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.41015625\n",
      "Finished epoch  96\n",
      "On epoch  97\n",
      "Total Epoch Testing MAPE: PCE = 42887.03125\n",
      "                              Voc = 42867.33984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.08203125\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 42886.83203125\n",
      "                              Voc = 42868.125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.79296875\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 42886.46484375\n",
      "                              Voc = 42868.72265625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.29296875\n",
      "Finished epoch  99\n",
      "On epoch  100\n",
      "Total Epoch Testing MAPE: PCE = 42886.7265625\n",
      "                              Voc = 42867.390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.00390625\n",
      "Finished epoch  100\n",
      "On epoch  101\n",
      "Total Epoch Testing MAPE: PCE = 42887.328125\n",
      "                              Voc = 42866.9140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.25\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 42887.5703125\n",
      "                              Voc = 42867.2109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.98046875\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 42887.3046875\n",
      "                              Voc = 42866.8203125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.73828125\n",
      "Finished epoch  103\n",
      "On epoch  104\n",
      "Total Epoch Testing MAPE: PCE = 42889.21484375\n",
      "                              Voc = 42866.25\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.6484375\n",
      "Finished epoch  104\n",
      "On epoch  105\n",
      "Total Epoch Testing MAPE: PCE = 42889.59765625\n",
      "                              Voc = 42866.19140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.6796875\n",
      "Finished epoch  105\n",
      "On epoch  106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42889.046875\n",
      "                              Voc = 42865.6640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.21484375\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 42889.45703125\n",
      "                              Voc = 42865.71875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.2578125\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 42889.15625\n",
      "                              Voc = 42865.90234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.80078125\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 42888.03125\n",
      "                              Voc = 42865.96484375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.4765625\n",
      "Finished epoch  109\n",
      "On epoch  110\n",
      "Total Epoch Testing MAPE: PCE = 42888.6640625\n",
      "                              Voc = 42865.8828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.87890625\n",
      "Finished epoch  110\n",
      "On epoch  111\n",
      "Total Epoch Testing MAPE: PCE = 42889.328125\n",
      "                              Voc = 42865.93359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.5390625\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 42889.1328125\n",
      "                              Voc = 42865.9921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.3515625\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 42887.78515625\n",
      "                              Voc = 42866.26953125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.57421875\n",
      "Finished epoch  113\n",
      "On epoch  114\n",
      "Total Epoch Testing MAPE: PCE = 42888.22265625\n",
      "                              Voc = 42866.0234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.53125\n",
      "Finished epoch  114\n",
      "On epoch  115\n",
      "Total Epoch Testing MAPE: PCE = 42888.0859375\n",
      "                              Voc = 42865.91015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.73828125\n",
      "Finished epoch  115\n",
      "On epoch  116\n",
      "Total Epoch Testing MAPE: PCE = 42886.921875\n",
      "                              Voc = 42866.0859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.54296875\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 42886.65625\n",
      "                              Voc = 42865.98046875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.67578125\n",
      "Finished epoch  117\n",
      "On epoch  118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42887.18359375\n",
      "                              Voc = 42865.9375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.88671875\n",
      "Finished epoch  118\n",
      "On epoch  119\n",
      "Total Epoch Testing MAPE: PCE = 42888.6796875\n",
      "                              Voc = 42865.875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.3046875\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 42887.73828125\n",
      "                              Voc = 42865.96875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.83203125\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 42887.3125\n",
      "                              Voc = 42865.92578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.59765625\n",
      "Finished epoch  121\n",
      "On epoch  122\n",
      "Total Epoch Testing MAPE: PCE = 42888.5234375\n",
      "                              Voc = 42866.0390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.27734375\n",
      "Finished epoch  122\n",
      "On epoch  123\n",
      "Total Epoch Testing MAPE: PCE = 42889.453125\n",
      "                              Voc = 42865.890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.6328125\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 42888.73046875\n",
      "                              Voc = 42866.015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.796875\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 42889.21875\n",
      "                              Voc = 42866.10546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.8359375\n",
      "Finished epoch  125\n",
      "On epoch  126\n",
      "Total Epoch Testing MAPE: PCE = 42888.97265625\n",
      "                              Voc = 42865.79296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.890625\n",
      "Finished epoch  126\n",
      "On epoch  127\n",
      "Total Epoch Testing MAPE: PCE = 42888.984375\n",
      "                              Voc = 42865.44921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.0703125\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 42888.02734375\n",
      "                              Voc = 42865.33984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.05078125\n",
      "Finished epoch  128\n",
      "On epoch  129\n",
      "Total Epoch Testing MAPE: PCE = 42888.8515625\n",
      "                              Voc = 42865.16796875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.3828125\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 42889.8046875\n",
      "                              Voc = 42865.37890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.60546875\n",
      "Finished epoch  130\n",
      "On epoch  131\n",
      "Total Epoch Testing MAPE: PCE = 42889.42578125\n",
      "                              Voc = 42865.36328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.1875\n",
      "Finished epoch  131\n",
      "On epoch  132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42887.8984375\n",
      "                              Voc = 42865.53515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.8203125\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 42886.6796875\n",
      "                              Voc = 42865.640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.13671875\n",
      "Finished epoch  133\n",
      "On epoch  134\n",
      "Total Epoch Testing MAPE: PCE = 42887.5078125\n",
      "                              Voc = 42865.60546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.80078125\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 42887.26171875\n",
      "                              Voc = 42865.58984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.109375\n",
      "Finished epoch  135\n",
      "On epoch  136\n",
      "Total Epoch Testing MAPE: PCE = 42889.21484375\n",
      "                              Voc = 42865.453125\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.98828125\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 42890.31640625\n",
      "                              Voc = 42865.4765625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.6640625\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 42890.30859375\n",
      "                              Voc = 42865.55859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42908.17578125\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 42890.203125\n",
      "                              Voc = 42866.0078125\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.25390625\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 42888.42578125\n",
      "                              Voc = 42865.48046875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.484375\n",
      "Finished epoch  140\n",
      "On epoch  141\n",
      "Total Epoch Testing MAPE: PCE = 42887.51953125\n",
      "                              Voc = 42865.390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.6015625\n",
      "Finished epoch  141\n",
      "On epoch  142\n",
      "Total Epoch Testing MAPE: PCE = 42888.39453125\n",
      "                              Voc = 42865.28125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.24609375\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 42888.49609375\n",
      "                              Voc = 42865.421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.08984375\n",
      "Finished epoch  143\n",
      "On epoch  144\n",
      "Total Epoch Testing MAPE: PCE = 42889.60546875\n",
      "                              Voc = 42865.28125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.25\n",
      "Finished epoch  144\n",
      "On epoch  145\n",
      "Total Epoch Testing MAPE: PCE = 42888.26171875\n",
      "                              Voc = 42865.390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.3984375\n",
      "Finished epoch  145\n",
      "On epoch  146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42887.36328125\n",
      "                              Voc = 42865.44921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.6015625\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 42885.984375\n",
      "                              Voc = 42865.62109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.2421875\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 42886.1796875\n",
      "                              Voc = 42865.62109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.2421875\n",
      "Finished epoch  148\n",
      "On epoch  149\n",
      "Total Epoch Testing MAPE: PCE = 42885.953125\n",
      "                              Voc = 42865.640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.51953125\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 42887.5078125\n",
      "                              Voc = 42865.4453125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.671875\n",
      "Finished epoch  150\n",
      "On epoch  151\n",
      "Total Epoch Testing MAPE: PCE = 42887.421875\n",
      "                              Voc = 42865.421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.19140625\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 42887.265625\n",
      "                              Voc = 42865.3359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.8515625\n",
      "Finished epoch  152\n",
      "On epoch  153\n",
      "Total Epoch Testing MAPE: PCE = 42885.984375\n",
      "                              Voc = 42865.48828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.55078125\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 42886.3046875\n",
      "                              Voc = 42865.69921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.38671875\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 42886.08984375\n",
      "                              Voc = 42865.4375\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.6796875\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 42886.34375\n",
      "                              Voc = 42865.3984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.9921875\n",
      "Finished epoch  156\n",
      "On epoch  157\n",
      "Total Epoch Testing MAPE: PCE = 42886.1015625\n",
      "                              Voc = 42865.48828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.7890625\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 42887.3203125\n",
      "                              Voc = 42865.36328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.76171875\n",
      "Finished epoch  158\n",
      "On epoch  159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42886.234375\n",
      "                              Voc = 42865.578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.9609375\n",
      "Finished epoch  159\n",
      "On epoch  160\n",
      "Total Epoch Testing MAPE: PCE = 42886.2109375\n",
      "                              Voc = 42865.578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.23046875\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 42886.02734375\n",
      "                              Voc = 42865.6015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.27734375\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 42886.015625\n",
      "                              Voc = 42865.640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.44140625\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 42885.54296875\n",
      "                              Voc = 42865.671875\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.52734375\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 42886.5390625\n",
      "                              Voc = 42865.6328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.80859375\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 42886.94921875\n",
      "                              Voc = 42865.7890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.66796875\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 42887.5\n",
      "                              Voc = 42865.4375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.2890625\n",
      "Finished epoch  166\n",
      "On epoch  167\n",
      "Total Epoch Testing MAPE: PCE = 42887.9921875\n",
      "                              Voc = 42865.03125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.2265625\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 42887.8203125\n",
      "                              Voc = 42864.70703125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.25\n",
      "Finished epoch  168\n",
      "On epoch  169\n",
      "Total Epoch Testing MAPE: PCE = 42887.20703125\n",
      "                              Voc = 42864.92578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.55859375\n",
      "Finished epoch  169\n",
      "On epoch  170\n",
      "Total Epoch Testing MAPE: PCE = 42887.234375\n",
      "                              Voc = 42864.9609375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.53125\n",
      "Finished epoch  170\n",
      "On epoch  171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42886.0546875\n",
      "                              Voc = 42864.796875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.68359375\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 42886.08203125\n",
      "                              Voc = 42864.83984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.2890625\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 42885.89453125\n",
      "                              Voc = 42864.640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.34765625\n",
      "Finished epoch  173\n",
      "On epoch  174\n",
      "Total Epoch Testing MAPE: PCE = 42886.109375\n",
      "                              Voc = 42864.80859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.0234375\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 42885.63671875\n",
      "                              Voc = 42865.01171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.6796875\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 42885.7890625\n",
      "                              Voc = 42864.84765625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.26953125\n",
      "Finished epoch  176\n",
      "On epoch  177\n",
      "Total Epoch Testing MAPE: PCE = 42885.46875\n",
      "                              Voc = 42864.6875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.1640625\n",
      "Finished epoch  177\n",
      "On epoch  178\n",
      "Total Epoch Testing MAPE: PCE = 42885.359375\n",
      "                              Voc = 42864.99609375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.1015625\n",
      "Finished epoch  178\n",
      "On epoch  179\n",
      "Total Epoch Testing MAPE: PCE = 42885.72265625\n",
      "                              Voc = 42864.93359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.5390625\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 42887.1875\n",
      "                              Voc = 42865.05859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.40234375\n",
      "Finished epoch  180\n",
      "On epoch  181\n",
      "Total Epoch Testing MAPE: PCE = 42889.11328125\n",
      "                              Voc = 42865.046875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.06640625\n",
      "Finished epoch  181\n",
      "On epoch  182\n",
      "Total Epoch Testing MAPE: PCE = 42889.296875\n",
      "                              Voc = 42865.19140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.38671875\n",
      "Finished epoch  182\n",
      "On epoch  183\n",
      "Total Epoch Testing MAPE: PCE = 42887.5546875\n",
      "                              Voc = 42865.29296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.984375\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 42886.47265625\n",
      "                              Voc = 42865.3515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.9140625\n",
      "Finished epoch  184\n",
      "On epoch  185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42887.0546875\n",
      "                              Voc = 42865.421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.9453125\n",
      "Finished epoch  185\n",
      "On epoch  186\n",
      "Total Epoch Testing MAPE: PCE = 42887.58203125\n",
      "                              Voc = 42865.60546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.18359375\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 42886.15234375\n",
      "                              Voc = 42865.6328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.0078125\n",
      "Finished epoch  187\n",
      "On epoch  188\n",
      "Total Epoch Testing MAPE: PCE = 42885.44140625\n",
      "                              Voc = 42865.55859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.29296875\n",
      "Finished epoch  188\n",
      "On epoch  189\n",
      "Total Epoch Testing MAPE: PCE = 42885.56640625\n",
      "                              Voc = 42865.62109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.421875\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 42886.23828125\n",
      "                              Voc = 42865.3828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.88671875\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 42885.7890625\n",
      "                              Voc = 42865.3515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.5390625\n",
      "Finished epoch  191\n",
      "On epoch  192\n",
      "Total Epoch Testing MAPE: PCE = 42886.5546875\n",
      "                              Voc = 42865.15625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.16015625\n",
      "Finished epoch  192\n",
      "On epoch  193\n",
      "Total Epoch Testing MAPE: PCE = 42885.8203125\n",
      "                              Voc = 42865.36328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.6328125\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 42885.3046875\n",
      "                              Voc = 42865.4375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.47265625\n",
      "Finished epoch  194\n",
      "On epoch  195\n",
      "Total Epoch Testing MAPE: PCE = 42885.109375\n",
      "                              Voc = 42865.12109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.83984375\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 42885.00390625\n",
      "                              Voc = 42865.08984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.83203125\n",
      "Finished epoch  196\n",
      "On epoch  197\n",
      "Total Epoch Testing MAPE: PCE = 42885.40625\n",
      "                              Voc = 42865.25\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.53515625\n",
      "Finished epoch  197\n",
      "On epoch  198\n",
      "Total Epoch Testing MAPE: PCE = 42885.50390625\n",
      "                              Voc = 42865.1875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.9765625\n",
      "Finished epoch  198\n",
      "On epoch  199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42885.06640625\n",
      "                              Voc = 42865.37109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.4609375\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 42885.56640625\n",
      "                              Voc = 42865.37109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.7421875\n",
      "Finished epoch  200\n",
      "On epoch  201\n",
      "Total Epoch Testing MAPE: PCE = 42886.42578125\n",
      "                              Voc = 42865.44921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.984375\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 42886.44140625\n",
      "                              Voc = 42865.484375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.7109375\n",
      "Finished epoch  202\n",
      "On epoch  203\n",
      "Total Epoch Testing MAPE: PCE = 42887.66015625\n",
      "                              Voc = 42865.74609375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.0859375\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 42889.25\n",
      "                              Voc = 42865.7265625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.6953125\n",
      "Finished epoch  204\n",
      "On epoch  205\n",
      "Total Epoch Testing MAPE: PCE = 42887.03125\n",
      "                              Voc = 42865.98828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.8515625\n",
      "Finished epoch  205\n",
      "On epoch  206\n",
      "Total Epoch Testing MAPE: PCE = 42887.01171875\n",
      "                              Voc = 42865.64453125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.05859375\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 42887.484375\n",
      "                              Voc = 42865.48046875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.73046875\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 42887.578125\n",
      "                              Voc = 42865.53125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.375\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 42886.5859375\n",
      "                              Voc = 42865.43359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.015625\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 42887.44921875\n",
      "                              Voc = 42865.80078125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.14453125\n",
      "Finished epoch  210\n",
      "On epoch  211\n",
      "Total Epoch Testing MAPE: PCE = 42886.734375\n",
      "                              Voc = 42865.7421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.98828125\n",
      "Finished epoch  211\n",
      "On epoch  212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42887.46484375\n",
      "                              Voc = 42865.4140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.96484375\n",
      "Finished epoch  212\n",
      "On epoch  213\n",
      "Total Epoch Testing MAPE: PCE = 42888.7265625\n",
      "                              Voc = 42865.3671875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.0390625\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 42887.3046875\n",
      "                              Voc = 42865.4765625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.28125\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 42886.1796875\n",
      "                              Voc = 42865.41015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.12890625\n",
      "Finished epoch  215\n",
      "On epoch  216\n",
      "Total Epoch Testing MAPE: PCE = 42886.015625\n",
      "                              Voc = 42865.4296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.01953125\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 42886.35546875\n",
      "                              Voc = 42865.625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.55859375\n",
      "Finished epoch  217\n",
      "On epoch  218\n",
      "Total Epoch Testing MAPE: PCE = 42886.1875\n",
      "                              Voc = 42865.47265625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.55078125\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 42885.5390625\n",
      "                              Voc = 42865.38671875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.6328125\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 42885.5625\n",
      "                              Voc = 42865.32421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.81640625\n",
      "Finished epoch  220\n",
      "On epoch  221\n",
      "Total Epoch Testing MAPE: PCE = 42885.46875\n",
      "                              Voc = 42865.3515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.20703125\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 42885.734375\n",
      "                              Voc = 42865.546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.3359375\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 42887.21484375\n",
      "                              Voc = 42865.671875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.41796875\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 42888.06640625\n",
      "                              Voc = 42865.890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.87109375\n",
      "Finished epoch  224\n",
      "On epoch  225\n",
      "Total Epoch Testing MAPE: PCE = 42888.69921875\n",
      "                              Voc = 42865.80078125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.109375\n",
      "Finished epoch  225\n",
      "On epoch  226\n",
      "Total Epoch Testing MAPE: PCE = 42888.3984375\n",
      "                              Voc = 42865.8359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.171875\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 42886.28515625\n",
      "                              Voc = 42865.640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.72265625\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 42885.3203125\n",
      "                              Voc = 42865.5703125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.09765625\n",
      "Finished epoch  228\n",
      "On epoch  229\n",
      "Total Epoch Testing MAPE: PCE = 42885.70703125\n",
      "                              Voc = 42865.66796875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.92578125\n",
      "Finished epoch  229\n",
      "On epoch  230\n",
      "Total Epoch Testing MAPE: PCE = 42884.99609375\n",
      "                              Voc = 42865.32421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.953125\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 42884.81640625\n",
      "                              Voc = 42865.21484375\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.1796875\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 42884.69140625\n",
      "                              Voc = 42865.1015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.11328125\n",
      "Finished epoch  232\n",
      "On epoch  233\n",
      "Total Epoch Testing MAPE: PCE = 42884.58984375\n",
      "                              Voc = 42865.1796875\n",
      "                              Jsc = nan\n",
      "                              FF = 42907.328125\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 42884.92578125\n",
      "                              Voc = 42865.41015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.765625\n",
      "Finished epoch  234\n",
      "On epoch  235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42884.30078125\n",
      "                              Voc = 42865.578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.1796875\n",
      "Finished epoch  235\n",
      "On epoch  236\n",
      "Total Epoch Testing MAPE: PCE = 42883.75\n",
      "                              Voc = 42865.65234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.94921875\n",
      "Finished epoch  236\n",
      "On epoch  237\n",
      "Total Epoch Testing MAPE: PCE = 42883.296875\n",
      "                              Voc = 42865.44921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.34765625\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 42883.53515625\n",
      "                              Voc = 42865.3125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.41015625\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 42883.3671875\n",
      "                              Voc = 42865.10546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.87109375\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 42883.12890625\n",
      "                              Voc = 42864.86328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.6640625\n",
      "Finished epoch  240\n",
      "On epoch  241\n",
      "Total Epoch Testing MAPE: PCE = 42883.73046875\n",
      "                              Voc = 42865.12109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.203125\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 42883.8359375\n",
      "                              Voc = 42864.890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.6171875\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 42883.6484375\n",
      "                              Voc = 42864.58984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42906.2421875\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 42884.46484375\n",
      "                              Voc = 42864.8046875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.984375\n",
      "Finished epoch  244\n",
      "On epoch  245\n",
      "Total Epoch Testing MAPE: PCE = 42884.80078125\n",
      "                              Voc = 42864.94140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.890625\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 42884.83203125\n",
      "                              Voc = 42865.19140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.71875\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 42884.89453125\n",
      "                              Voc = 42864.984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.58203125\n",
      "Finished epoch  247\n",
      "On epoch  248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42885.21484375\n",
      "                              Voc = 42865.07421875\n",
      "                              Jsc = 42882.37890625\n",
      "                              FF = 42904.8359375\n",
      "Finished epoch  248\n",
      "On epoch  249\n",
      "Total Epoch Testing MAPE: PCE = 42885.23046875\n",
      "                              Voc = 42864.97265625\n",
      "                              Jsc = 42883.15625\n",
      "                              FF = 42904.4453125\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 42885.0390625\n",
      "                              Voc = 42864.828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.15234375\n",
      "Finished epoch  250\n",
      "On epoch  251\n",
      "Total Epoch Testing MAPE: PCE = 42885.30859375\n",
      "                              Voc = 42864.921875\n",
      "                              Jsc = 42882.0234375\n",
      "                              FF = 42904.34765625\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 42885.46875\n",
      "                              Voc = 42864.8515625\n",
      "                              Jsc = 42881.9296875\n",
      "                              FF = 42904.86328125\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 42885.203125\n",
      "                              Voc = 42864.73828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.4453125\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 42884.73046875\n",
      "                              Voc = 42864.58203125\n",
      "                              Jsc = 42880.9765625\n",
      "                              FF = 42904.0625\n",
      "Finished epoch  254\n",
      "On epoch  255\n",
      "Total Epoch Testing MAPE: PCE = 42884.33203125\n",
      "                              Voc = 42864.4375\n",
      "                              Jsc = 42880.5859375\n",
      "                              FF = 42903.86328125\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 42884.78515625\n",
      "                              Voc = 42864.61328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.4453125\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 42884.66015625\n",
      "                              Voc = 42864.77734375\n",
      "                              Jsc = 42881.7890625\n",
      "                              FF = 42905.09375\n",
      "Finished epoch  257\n",
      "On epoch  258\n",
      "Total Epoch Testing MAPE: PCE = 42884.6875\n",
      "                              Voc = 42864.71875\n",
      "                              Jsc = 42881.328125\n",
      "                              FF = 42904.45703125\n",
      "Finished epoch  258\n",
      "On epoch  259\n",
      "Total Epoch Testing MAPE: PCE = 42884.98046875\n",
      "                              Voc = 42864.58984375\n",
      "                              Jsc = 42881.640625\n",
      "                              FF = 42904.70703125\n",
      "Finished epoch  259\n",
      "On epoch  260\n",
      "Total Epoch Testing MAPE: PCE = 42884.8125\n",
      "                              Voc = 42864.8359375\n",
      "                              Jsc = 42878.7421875\n",
      "                              FF = 42904.38671875\n",
      "Finished epoch  260\n",
      "On epoch  261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42885.95703125\n",
      "                              Voc = 42864.66796875\n",
      "                              Jsc = 42879.625\n",
      "                              FF = 42903.765625\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 42884.94140625\n",
      "                              Voc = 42864.59375\n",
      "                              Jsc = 42880.53515625\n",
      "                              FF = 42903.375\n",
      "Finished epoch  262\n",
      "On epoch  263\n",
      "Total Epoch Testing MAPE: PCE = 42884.8046875\n",
      "                              Voc = 42864.5\n",
      "                              Jsc = 42879.08203125\n",
      "                              FF = 42902.92578125\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 42885.796875\n",
      "                              Voc = 42864.5234375\n",
      "                              Jsc = 42879.94921875\n",
      "                              FF = 42902.91015625\n",
      "Finished epoch  264\n",
      "On epoch  265\n",
      "Total Epoch Testing MAPE: PCE = 42884.6875\n",
      "                              Voc = 42864.79296875\n",
      "                              Jsc = 42881.8359375\n",
      "                              FF = 42903.48046875\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 42884.6015625\n",
      "                              Voc = 42864.515625\n",
      "                              Jsc = 42881.10546875\n",
      "                              FF = 42903.3828125\n",
      "Finished epoch  266\n",
      "On epoch  267\n",
      "Total Epoch Testing MAPE: PCE = 42885.0078125\n",
      "                              Voc = 42864.4609375\n",
      "                              Jsc = 42882.21875\n",
      "                              FF = 42904.40234375\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 42885.09765625\n",
      "                              Voc = 42864.48046875\n",
      "                              Jsc = 42882.45703125\n",
      "                              FF = 42904.421875\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 42885.890625\n",
      "                              Voc = 42864.5\n",
      "                              Jsc = 42881.7734375\n",
      "                              FF = 42904.46875\n",
      "Finished epoch  269\n",
      "On epoch  270\n",
      "Total Epoch Testing MAPE: PCE = 42884.8125\n",
      "                              Voc = 42864.29296875\n",
      "                              Jsc = 42880.01171875\n",
      "                              FF = 42904.37109375\n",
      "Finished epoch  270\n",
      "On epoch  271\n",
      "Total Epoch Testing MAPE: PCE = 42884.96875\n",
      "                              Voc = 42864.6796875\n",
      "                              Jsc = 42880.86328125\n",
      "                              FF = 42903.93359375\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 42884.91015625\n",
      "                              Voc = 42864.234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.24609375\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 42885.10546875\n",
      "                              Voc = 42864.54296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.6171875\n",
      "Finished epoch  273\n",
      "On epoch  274\n",
      "Total Epoch Testing MAPE: PCE = 42885.4765625\n",
      "                              Voc = 42866.44921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.92578125\n",
      "Finished epoch  274\n",
      "On epoch  275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42885.32421875\n",
      "                              Voc = 42864.4609375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.875\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 42885.55078125\n",
      "                              Voc = 42865.828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.58984375\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 42885.875\n",
      "                              Voc = 42865.73046875\n",
      "                              Jsc = 42885.765625\n",
      "                              FF = 42904.1640625\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 42885.91015625\n",
      "                              Voc = 42867.15234375\n",
      "                              Jsc = 42884.6640625\n",
      "                              FF = 42903.33984375\n",
      "Finished epoch  278\n",
      "On epoch  279\n",
      "Total Epoch Testing MAPE: PCE = 42885.5546875\n",
      "                              Voc = 42865.37890625\n",
      "                              Jsc = 42882.51171875\n",
      "                              FF = 42902.85546875\n",
      "Finished epoch  279\n",
      "On epoch  280\n",
      "Total Epoch Testing MAPE: PCE = 42885.25390625\n",
      "                              Voc = 42864.82421875\n",
      "                              Jsc = 42883.76953125\n",
      "                              FF = 42903.73046875\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 42885.46875\n",
      "                              Voc = 42864.43359375\n",
      "                              Jsc = 42884.26171875\n",
      "                              FF = 42903.16015625\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 42885.8125\n",
      "                              Voc = 42864.06640625\n",
      "                              Jsc = 42885.296875\n",
      "                              FF = 42903.84765625\n",
      "Finished epoch  282\n",
      "On epoch  283\n",
      "Total Epoch Testing MAPE: PCE = 42885.43359375\n",
      "                              Voc = 42864.23828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.5546875\n",
      "Finished epoch  283\n",
      "On epoch  284\n",
      "Total Epoch Testing MAPE: PCE = 42886.40234375\n",
      "                              Voc = 42864.2734375\n",
      "                              Jsc = 42884.80078125\n",
      "                              FF = 42901.84765625\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 42885.46875\n",
      "                              Voc = 42864.296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.68359375\n",
      "Finished epoch  285\n",
      "On epoch  286\n",
      "Total Epoch Testing MAPE: PCE = 42885.05078125\n",
      "                              Voc = 42864.3515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.8359375\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 42884.609375\n",
      "                              Voc = 42864.29296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.328125\n",
      "Finished epoch  287\n",
      "On epoch  288\n",
      "Total Epoch Testing MAPE: PCE = 42884.26953125\n",
      "                              Voc = 42864.33203125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.1171875\n",
      "Finished epoch  288\n",
      "On epoch  289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42884.58984375\n",
      "                              Voc = 42864.59765625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.56640625\n",
      "Finished epoch  289\n",
      "On epoch  290\n",
      "Total Epoch Testing MAPE: PCE = 42884.578125\n",
      "                              Voc = 42864.30859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.9140625\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 42885.06640625\n",
      "                              Voc = 42864.48828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.33203125\n",
      "Finished epoch  291\n",
      "On epoch  292\n",
      "Total Epoch Testing MAPE: PCE = 42884.70703125\n",
      "                              Voc = 42864.890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.2890625\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 42884.96484375\n",
      "                              Voc = 42864.6328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.88671875\n",
      "Finished epoch  293\n",
      "On epoch  294\n",
      "Total Epoch Testing MAPE: PCE = 42885.234375\n",
      "                              Voc = 42864.8359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.7890625\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 42885.33203125\n",
      "                              Voc = 42864.94921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.76953125\n",
      "Finished epoch  295\n",
      "On epoch  296\n",
      "Total Epoch Testing MAPE: PCE = 42885.34765625\n",
      "                              Voc = 42864.6171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.95703125\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 42886.61328125\n",
      "                              Voc = 42864.6171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.73828125\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 42885.21484375\n",
      "                              Voc = 42864.43359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.4375\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 42885.63671875\n",
      "                              Voc = 42864.5\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.4453125\n",
      "Finished epoch  299\n",
      "On epoch  300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42885.33203125\n",
      "                              Voc = 42864.4140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.77734375\n",
      "Finished epoch  300\n",
      "On epoch  301\n",
      "Total Epoch Testing MAPE: PCE = 42885.62890625\n",
      "                              Voc = 42864.4921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.05859375\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 42885.5\n",
      "                              Voc = 42865.5234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.08203125\n",
      "Finished epoch  302\n",
      "On epoch  303\n",
      "Total Epoch Testing MAPE: PCE = 42885.515625\n",
      "                              Voc = 42864.98828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.91015625\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 42885.65234375\n",
      "                              Voc = 42864.0546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.640625\n",
      "Finished epoch  304\n",
      "On epoch  305\n",
      "Total Epoch Testing MAPE: PCE = 42885.1484375\n",
      "                              Voc = 42864.1171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.69140625\n",
      "Finished epoch  305\n",
      "On epoch  306\n",
      "Total Epoch Testing MAPE: PCE = 42884.96875\n",
      "                              Voc = 42864.3359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.45703125\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 42884.21875\n",
      "                              Voc = 42864.2734375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.125\n",
      "Finished epoch  307\n",
      "On epoch  308\n",
      "Total Epoch Testing MAPE: PCE = 42884.078125\n",
      "                              Voc = 42864.3359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.8359375\n",
      "Finished epoch  308\n",
      "On epoch  309\n",
      "Total Epoch Testing MAPE: PCE = 42884.5625\n",
      "                              Voc = 42864.11328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.1171875\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 42885.234375\n",
      "                              Voc = 42864.12890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.04296875\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 42885.46484375\n",
      "                              Voc = 42864.30859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.52734375\n",
      "Finished epoch  311\n",
      "On epoch  312\n",
      "Total Epoch Testing MAPE: PCE = 42885.10546875\n",
      "                              Voc = 42864.2578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.984375\n",
      "Finished epoch  312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On epoch  313\n",
      "Total Epoch Testing MAPE: PCE = 42885.23046875\n",
      "                              Voc = 42864.3515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.0703125\n",
      "Finished epoch  313\n",
      "On epoch  314\n",
      "Total Epoch Testing MAPE: PCE = 42885.484375\n",
      "                              Voc = 42864.25390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.26953125\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 42885.4296875\n",
      "                              Voc = 42864.3515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.765625\n",
      "Finished epoch  315\n",
      "On epoch  316\n",
      "Total Epoch Testing MAPE: PCE = 42884.52734375\n",
      "                              Voc = 42864.57421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.91015625\n",
      "Finished epoch  316\n",
      "On epoch  317\n",
      "Total Epoch Testing MAPE: PCE = 42884.46484375\n",
      "                              Voc = 42864.58984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.046875\n",
      "Finished epoch  317\n",
      "On epoch  318\n",
      "Total Epoch Testing MAPE: PCE = 42885.0234375\n",
      "                              Voc = 42864.59375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.046875\n",
      "Finished epoch  318\n",
      "On epoch  319\n",
      "Total Epoch Testing MAPE: PCE = 42884.44140625\n",
      "                              Voc = 42864.57421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.31640625\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 42884.6328125\n",
      "                              Voc = 42864.36328125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.42578125\n",
      "Finished epoch  320\n",
      "On epoch  321\n",
      "Total Epoch Testing MAPE: PCE = 42885.1796875\n",
      "                              Voc = 42864.47265625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.8046875\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 42885.3515625\n",
      "                              Voc = 42864.015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.41015625\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 42885.30859375\n",
      "                              Voc = 42864.08984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.3515625\n",
      "Finished epoch  323\n",
      "On epoch  324\n",
      "Total Epoch Testing MAPE: PCE = 42886.20703125\n",
      "                              Voc = 42864.10546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.45703125\n",
      "Finished epoch  324\n",
      "On epoch  325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42885.9609375\n",
      "                              Voc = 42863.57421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.8515625\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 42886.48828125\n",
      "                              Voc = 42864.359375\n",
      "                              Jsc = 42886.08203125\n",
      "                              FF = 42903.5\n",
      "Finished epoch  326\n",
      "On epoch  327\n",
      "Total Epoch Testing MAPE: PCE = 42885.7109375\n",
      "                              Voc = 42864.6171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.1171875\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 42885.93359375\n",
      "                              Voc = 42864.6875\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.14453125\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 42887.109375\n",
      "                              Voc = 42866.97265625\n",
      "                              Jsc = 42886.74609375\n",
      "                              FF = 42904.26953125\n",
      "Finished epoch  329\n",
      "On epoch  330\n",
      "Total Epoch Testing MAPE: PCE = 42888.2734375\n",
      "                              Voc = 42867.234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.05078125\n",
      "Finished epoch  330\n",
      "On epoch  331\n",
      "Total Epoch Testing MAPE: PCE = 42888.0390625\n",
      "                              Voc = 42866.0234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.4453125\n",
      "Finished epoch  331\n",
      "On epoch  332\n",
      "Total Epoch Testing MAPE: PCE = 42888.453125\n",
      "                              Voc = 42867.53125\n",
      "                              Jsc = 42885.7421875\n",
      "                              FF = 42904.66796875\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 42889.47265625\n",
      "                              Voc = 42866.3515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.27734375\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 42887.94140625\n",
      "                              Voc = 42868.671875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.3515625\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 42886.8984375\n",
      "                              Voc = 42865.62109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.04296875\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 42885.8828125\n",
      "                              Voc = 42864.42578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.734375\n",
      "Finished epoch  336\n",
      "On epoch  337\n",
      "Total Epoch Testing MAPE: PCE = 42885.8203125\n",
      "                              Voc = 42863.4296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.21484375\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 42885.67578125\n",
      "                              Voc = 42863.62890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.58203125\n",
      "Finished epoch  338\n",
      "On epoch  339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42885.70703125\n",
      "                              Voc = 42863.859375\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.234375\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 42885.54296875\n",
      "                              Voc = 42863.9453125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.91796875\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 42885.17578125\n",
      "                              Voc = 42863.94140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.33984375\n",
      "Finished epoch  341\n",
      "On epoch  342\n",
      "Total Epoch Testing MAPE: PCE = 42885.22265625\n",
      "                              Voc = 42864.19140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.671875\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 42885.26953125\n",
      "                              Voc = 42864.1875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.96484375\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 42885.25390625\n",
      "                              Voc = 42865.04296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.59375\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 42884.5078125\n",
      "                              Voc = 42865.56640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.5703125\n",
      "Finished epoch  345\n",
      "On epoch  346\n",
      "Total Epoch Testing MAPE: PCE = 42884.6484375\n",
      "                              Voc = 42864.17578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.453125\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 42885.453125\n",
      "                              Voc = 42863.95703125\n",
      "                              Jsc = nan\n",
      "                              FF = 42905.15625\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 42885.55078125\n",
      "                              Voc = 42864.62109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.2734375\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 42885.01953125\n",
      "                              Voc = 42864.4296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.12890625\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 42885.30859375\n",
      "                              Voc = 42864.53125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.21484375\n",
      "Finished epoch  350\n",
      "On epoch  351\n",
      "Total Epoch Testing MAPE: PCE = 42884.84375\n",
      "                              Voc = 42863.57421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.23046875\n",
      "Finished epoch  351\n",
      "On epoch  352\n",
      "Total Epoch Testing MAPE: PCE = 42884.81640625\n",
      "                              Voc = 42863.45703125\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.546875\n",
      "Finished epoch  352\n",
      "On epoch  353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42884.56640625\n",
      "                              Voc = 42863.40625\n",
      "                              Jsc = 42885.02734375\n",
      "                              FF = 42901.66015625\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 42884.3515625\n",
      "                              Voc = 42863.37890625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.28125\n",
      "Finished epoch  354\n",
      "On epoch  355\n",
      "Total Epoch Testing MAPE: PCE = 42883.80859375\n",
      "                              Voc = 42863.40234375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.30859375\n",
      "Finished epoch  355\n",
      "On epoch  356\n",
      "Total Epoch Testing MAPE: PCE = 42884.296875\n",
      "                              Voc = 42863.76953125\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.48828125\n",
      "Finished epoch  356\n",
      "On epoch  357\n",
      "Total Epoch Testing MAPE: PCE = 42884.59765625\n",
      "                              Voc = 42863.82421875\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.890625\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 42884.359375\n",
      "                              Voc = 42863.984375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.125\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 42883.796875\n",
      "                              Voc = 42863.4609375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.3046875\n",
      "Finished epoch  359\n",
      "On epoch  360\n",
      "Total Epoch Testing MAPE: PCE = 42883.61328125\n",
      "                              Voc = 42863.69921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.64453125\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 42883.6875\n",
      "                              Voc = 42863.69140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.03125\n",
      "Finished epoch  361\n",
      "On epoch  362\n",
      "Total Epoch Testing MAPE: PCE = 42883.30859375\n",
      "                              Voc = 42863.74609375\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.37109375\n",
      "Finished epoch  362\n",
      "On epoch  363\n",
      "Total Epoch Testing MAPE: PCE = 42883.96484375\n",
      "                              Voc = 42863.84765625\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.2421875\n",
      "Finished epoch  363\n",
      "On epoch  364\n",
      "Total Epoch Testing MAPE: PCE = 42883.671875\n",
      "                              Voc = 42863.87109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42899.94140625\n",
      "Finished epoch  364\n",
      "On epoch  365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42883.640625\n",
      "                              Voc = 42863.796875\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.53125\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 42883.3828125\n",
      "                              Voc = 42863.98828125\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.73828125\n",
      "Finished epoch  366\n",
      "On epoch  367\n",
      "Total Epoch Testing MAPE: PCE = 42883.2421875\n",
      "                              Voc = 42863.94140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.5703125\n",
      "Finished epoch  367\n",
      "On epoch  368\n",
      "Total Epoch Testing MAPE: PCE = 42883.84375\n",
      "                              Voc = 42863.9296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.4453125\n",
      "Finished epoch  368\n",
      "On epoch  369\n",
      "Total Epoch Testing MAPE: PCE = 42884.06640625\n",
      "                              Voc = 42863.8359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.84765625\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 42884.1640625\n",
      "                              Voc = 42863.94921875\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.86328125\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 42884.1328125\n",
      "                              Voc = 42864.12109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.23046875\n",
      "Finished epoch  371\n",
      "On epoch  372\n",
      "Total Epoch Testing MAPE: PCE = 42884.08203125\n",
      "                              Voc = 42864.01171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.9140625\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 42884.03125\n",
      "                              Voc = 42863.9609375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.88671875\n",
      "Finished epoch  373\n",
      "On epoch  374\n",
      "Total Epoch Testing MAPE: PCE = 42884.953125\n",
      "                              Voc = 42864.1015625\n",
      "                              Jsc = 42885.49609375\n",
      "                              FF = 42902.39453125\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 42884.80078125\n",
      "                              Voc = 42864.0546875\n",
      "                              Jsc = 42884.9375\n",
      "                              FF = 42901.87109375\n",
      "Finished epoch  375\n",
      "On epoch  376\n",
      "Total Epoch Testing MAPE: PCE = 42884.7109375\n",
      "                              Voc = 42864.0625\n",
      "                              Jsc = 42883.11328125\n",
      "                              FF = 42901.91796875\n",
      "Finished epoch  376\n",
      "On epoch  377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42885.20703125\n",
      "                              Voc = 42863.8984375\n",
      "                              Jsc = 42883.3359375\n",
      "                              FF = 42902.32421875\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 42885.05078125\n",
      "                              Voc = 42863.90234375\n",
      "                              Jsc = 42883.0859375\n",
      "                              FF = 42901.45703125\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 42885.4296875\n",
      "                              Voc = 42864.13671875\n",
      "                              Jsc = 42882.90625\n",
      "                              FF = 42901.359375\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 42885.15234375\n",
      "                              Voc = 42864.21875\n",
      "                              Jsc = 42882.6171875\n",
      "                              FF = 42901.39453125\n",
      "Finished epoch  380\n",
      "On epoch  381\n",
      "Total Epoch Testing MAPE: PCE = 42885.17578125\n",
      "                              Voc = 42864.30078125\n",
      "                              Jsc = 42882.69921875\n",
      "                              FF = 42901.50390625\n",
      "Finished epoch  381\n",
      "On epoch  382\n",
      "Total Epoch Testing MAPE: PCE = 42885.0546875\n",
      "                              Voc = 42864.35546875\n",
      "                              Jsc = 42886.83984375\n",
      "                              FF = 42901.875\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 42884.6015625\n",
      "                              Voc = 42864.19140625\n",
      "                              Jsc = 42885.56640625\n",
      "                              FF = 42902.28125\n",
      "Finished epoch  383\n",
      "On epoch  384\n",
      "Total Epoch Testing MAPE: PCE = 42884.33203125\n",
      "                              Voc = 42864.27734375\n",
      "                              Jsc = 42886.765625\n",
      "                              FF = 42901.90625\n",
      "Finished epoch  384\n",
      "On epoch  385\n",
      "Total Epoch Testing MAPE: PCE = 42883.9921875\n",
      "                              Voc = 42864.1796875\n",
      "                              Jsc = 42886.2734375\n",
      "                              FF = 42901.90625\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 42883.95703125\n",
      "                              Voc = 42864.2734375\n",
      "                              Jsc = 42887.5390625\n",
      "                              FF = 42902.41796875\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 42883.98046875\n",
      "                              Voc = 42864.4609375\n",
      "                              Jsc = 42885.6875\n",
      "                              FF = 42901.98828125\n",
      "Finished epoch  387\n",
      "On epoch  388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42884.12109375\n",
      "                              Voc = 42864.55859375\n",
      "                              Jsc = 42887.18359375\n",
      "                              FF = 42902.37890625\n",
      "Finished epoch  388\n",
      "On epoch  389\n",
      "Total Epoch Testing MAPE: PCE = 42883.953125\n",
      "                              Voc = 42864.73046875\n",
      "                              Jsc = 42886.8828125\n",
      "                              FF = 42902.19921875\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 42884.703125\n",
      "                              Voc = 42864.4609375\n",
      "                              Jsc = 42887.69140625\n",
      "                              FF = 42902.3671875\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 42884.53125\n",
      "                              Voc = 42864.421875\n",
      "                              Jsc = 42885.8828125\n",
      "                              FF = 42902.421875\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 42884.421875\n",
      "                              Voc = 42864.69921875\n",
      "                              Jsc = 42885.5546875\n",
      "                              FF = 42901.84765625\n",
      "Finished epoch  392\n",
      "On epoch  393\n",
      "Total Epoch Testing MAPE: PCE = 42885.5390625\n",
      "                              Voc = 42864.81640625\n",
      "                              Jsc = 42885.40234375\n",
      "                              FF = 42902.40625\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 42885.05859375\n",
      "                              Voc = 42864.6953125\n",
      "                              Jsc = 42885.421875\n",
      "                              FF = 42902.87890625\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 42885.0234375\n",
      "                              Voc = 42864.62109375\n",
      "                              Jsc = 42885.2734375\n",
      "                              FF = 42902.0234375\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 42884.7734375\n",
      "                              Voc = 42864.54296875\n",
      "                              Jsc = 42885.30859375\n",
      "                              FF = 42901.85546875\n",
      "Finished epoch  396\n",
      "On epoch  397\n",
      "Total Epoch Testing MAPE: PCE = 42884.53125\n",
      "                              Voc = 42864.62890625\n",
      "                              Jsc = 42885.69921875\n",
      "                              FF = 42902.4609375\n",
      "Finished epoch  397\n",
      "On epoch  398\n",
      "Total Epoch Testing MAPE: PCE = 42884.00390625\n",
      "                              Voc = 42864.21875\n",
      "                              Jsc = 42886.23828125\n",
      "                              FF = 42901.953125\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 42883.89453125\n",
      "                              Voc = 42864.03515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.6328125\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 42883.4765625\n",
      "                              Voc = 42863.90625\n",
      "                              Jsc = 42887.1640625\n",
      "                              FF = 42904.21875\n",
      "Finished epoch  400\n",
      "On epoch  401\n",
      "Total Epoch Testing MAPE: PCE = 42883.62890625\n",
      "                              Voc = 42863.88671875\n",
      "                              Jsc = 42886.3359375\n",
      "                              FF = 42904.64453125\n",
      "Finished epoch  401\n",
      "On epoch  402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42883.83203125\n",
      "                              Voc = 42863.77734375\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.60546875\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 42883.5859375\n",
      "                              Voc = 42864.1015625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.7265625\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 42883.890625\n",
      "                              Voc = 42864.53515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.96484375\n",
      "Finished epoch  404\n",
      "On epoch  405\n",
      "Total Epoch Testing MAPE: PCE = 42883.71484375\n",
      "                              Voc = 42864.1953125\n",
      "                              Jsc = nan\n",
      "                              FF = 42904.6640625\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 42883.80859375\n",
      "                              Voc = 42864.39453125\n",
      "                              Jsc = 42884.28515625\n",
      "                              FF = 42904.08203125\n",
      "Finished epoch  406\n",
      "On epoch  407\n",
      "Total Epoch Testing MAPE: PCE = 42883.59765625\n",
      "                              Voc = 42864.37109375\n",
      "                              Jsc = 42885.80859375\n",
      "                              FF = 42904.6875\n",
      "Finished epoch  407\n",
      "On epoch  408\n",
      "Total Epoch Testing MAPE: PCE = 42883.94140625\n",
      "                              Voc = 42864.18359375\n",
      "                              Jsc = 42885.2578125\n",
      "                              FF = 42904.609375\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 42883.6328125\n",
      "                              Voc = 42863.5546875\n",
      "                              Jsc = 42885.140625\n",
      "                              FF = 42904.38671875\n",
      "Finished epoch  409\n",
      "On epoch  410\n",
      "Total Epoch Testing MAPE: PCE = 42883.6015625\n",
      "                              Voc = 42867.60546875\n",
      "                              Jsc = 42885.33203125\n",
      "                              FF = 42902.8828125\n",
      "Finished epoch  410\n",
      "On epoch  411\n",
      "Total Epoch Testing MAPE: PCE = 42883.68359375\n",
      "                              Voc = 42864.8828125\n",
      "                              Jsc = 42884.609375\n",
      "                              FF = 42903.32421875\n",
      "Finished epoch  411\n",
      "On epoch  412\n",
      "Total Epoch Testing MAPE: PCE = 42883.93359375\n",
      "                              Voc = 42865.859375\n",
      "                              Jsc = 42884.67578125\n",
      "                              FF = 42902.51171875\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 42883.57421875\n",
      "                              Voc = 42867.97265625\n",
      "                              Jsc = 42884.9140625\n",
      "                              FF = 42901.484375\n",
      "Finished epoch  413\n",
      "On epoch  414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42883.921875\n",
      "                              Voc = 42866.2890625\n",
      "                              Jsc = 42887.015625\n",
      "                              FF = 42900.80078125\n",
      "Finished epoch  414\n",
      "On epoch  415\n",
      "Total Epoch Testing MAPE: PCE = 42884.05859375\n",
      "                              Voc = 42867.1796875\n",
      "                              Jsc = 42886.2734375\n",
      "                              FF = 42900.34375\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 42884.21484375\n",
      "                              Voc = 42864.48046875\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.67578125\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 42883.83984375\n",
      "                              Voc = 42864.1171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.2890625\n",
      "Finished epoch  417\n",
      "On epoch  418\n",
      "Total Epoch Testing MAPE: PCE = 42883.37890625\n",
      "                              Voc = 42863.88671875\n",
      "                              Jsc = 42886.53125\n",
      "                              FF = 42900.60546875\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 42883.1015625\n",
      "                              Voc = 42863.94921875\n",
      "                              Jsc = 42886.9921875\n",
      "                              FF = 42900.9609375\n",
      "Finished epoch  419\n",
      "On epoch  420\n",
      "Total Epoch Testing MAPE: PCE = 42882.4140625\n",
      "                              Voc = 42864.921875\n",
      "                              Jsc = 42885.71484375\n",
      "                              FF = 42900.77734375\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 42883.02734375\n",
      "                              Voc = 42866.1640625\n",
      "                              Jsc = 42884.76171875\n",
      "                              FF = 42902.43359375\n",
      "Finished epoch  421\n",
      "On epoch  422\n",
      "Total Epoch Testing MAPE: PCE = 42883.05078125\n",
      "                              Voc = 42864.9765625\n",
      "                              Jsc = 42882.87109375\n",
      "                              FF = 42901.515625\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 42883.06640625\n",
      "                              Voc = 42864.40625\n",
      "                              Jsc = 42885.203125\n",
      "                              FF = 42901.22265625\n",
      "Finished epoch  423\n",
      "On epoch  424\n",
      "Total Epoch Testing MAPE: PCE = 42883.046875\n",
      "                              Voc = 42865.37109375\n",
      "                              Jsc = 42883.44140625\n",
      "                              FF = 42900.4609375\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 42882.6953125\n",
      "                              Voc = 42864.03515625\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.01953125\n",
      "Finished epoch  425\n",
      "On epoch "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 426\n",
      "Total Epoch Testing MAPE: PCE = 42882.8125\n",
      "                              Voc = 42863.67578125\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.54296875\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 42882.7890625\n",
      "                              Voc = 42864.3046875\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.47265625\n",
      "Finished epoch  427\n",
      "On epoch  428\n",
      "Total Epoch Testing MAPE: PCE = 42881.953125\n",
      "                              Voc = 42864.109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.12890625\n",
      "Finished epoch  428\n",
      "On epoch  429\n",
      "Total Epoch Testing MAPE: PCE = 42882.765625\n",
      "                              Voc = 42864.0390625\n",
      "                              Jsc = 42886.89453125\n",
      "                              FF = 42902.1640625\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 42882.7890625\n",
      "                              Voc = 42864.44921875\n",
      "                              Jsc = 42885.26953125\n",
      "                              FF = 42901.77734375\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 42882.21875\n",
      "                              Voc = 42864.43359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.45703125\n",
      "Finished epoch  431\n",
      "On epoch  432\n",
      "Total Epoch Testing MAPE: PCE = 42882.34375\n",
      "                              Voc = 42864.359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.60546875\n",
      "Finished epoch  432\n",
      "On epoch  433\n",
      "Total Epoch Testing MAPE: PCE = 42882.55078125\n",
      "                              Voc = 42864.49609375\n",
      "                              Jsc = 42886.5234375\n",
      "                              FF = 42900.71875\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 42882.78515625\n",
      "                              Voc = 42863.7421875\n",
      "                              Jsc = 42884.48828125\n",
      "                              FF = 42901.1015625\n",
      "Finished epoch  434\n",
      "On epoch  435\n",
      "Total Epoch Testing MAPE: PCE = 42883.4921875\n",
      "                              Voc = 42863.453125\n",
      "                              Jsc = 42885.06640625\n",
      "                              FF = 42901.6171875\n",
      "Finished epoch  435\n",
      "On epoch  436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42883.9765625\n",
      "                              Voc = 42863.70703125\n",
      "                              Jsc = 42884.6484375\n",
      "                              FF = 42901.00390625\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 42883.59375\n",
      "                              Voc = 42863.71875\n",
      "                              Jsc = 42885.328125\n",
      "                              FF = 42900.7734375\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 42882.84765625\n",
      "                              Voc = 42863.8125\n",
      "                              Jsc = 42885.5625\n",
      "                              FF = 42901.60546875\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 42883.02734375\n",
      "                              Voc = 42863.7734375\n",
      "                              Jsc = 42886.0859375\n",
      "                              FF = 42901.09375\n",
      "Finished epoch  439\n",
      "On epoch  440\n",
      "Total Epoch Testing MAPE: PCE = 42883.12890625\n",
      "                              Voc = 42863.640625\n",
      "                              Jsc = 42885.0078125\n",
      "                              FF = 42901.47265625\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 42883.02734375\n",
      "                              Voc = 42863.9140625\n",
      "                              Jsc = 42884.6171875\n",
      "                              FF = 42901.7109375\n",
      "Finished epoch  441\n",
      "On epoch  442\n",
      "Total Epoch Testing MAPE: PCE = 42882.81640625\n",
      "                              Voc = 42864.13671875\n",
      "                              Jsc = 42883.14453125\n",
      "                              FF = 42901.1953125\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 42882.9375\n",
      "                              Voc = 42863.984375\n",
      "                              Jsc = 42883.2109375\n",
      "                              FF = 42901.4140625\n",
      "Finished epoch  443\n",
      "On epoch  444\n",
      "Total Epoch Testing MAPE: PCE = 42882.875\n",
      "                              Voc = 42864.109375\n",
      "                              Jsc = 42884.5625\n",
      "                              FF = 42900.7109375\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 42883.18359375\n",
      "                              Voc = 42863.91796875\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.921875\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 42883.29296875\n",
      "                              Voc = 42863.50390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.3125\n",
      "Finished epoch  446\n",
      "On epoch  447\n",
      "Total Epoch Testing MAPE: PCE = 42883.2109375\n",
      "                              Voc = 42864.14453125\n",
      "                              Jsc = 42887.60546875\n",
      "                              FF = 42901.203125\n",
      "Finished epoch  447\n",
      "On epoch  448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42884.02734375\n",
      "                              Voc = 42863.64453125\n",
      "                              Jsc = 42889.3046875\n",
      "                              FF = 42901.05859375\n",
      "Finished epoch  448\n",
      "On epoch  449\n",
      "Total Epoch Testing MAPE: PCE = 42884.53125\n",
      "                              Voc = 42864.1171875\n",
      "                              Jsc = 42886.57421875\n",
      "                              FF = 42900.75390625\n",
      "Finished epoch  449\n",
      "On epoch  450\n",
      "Total Epoch Testing MAPE: PCE = 42884.7265625\n",
      "                              Voc = 42863.4140625\n",
      "                              Jsc = 42884.85546875\n",
      "                              FF = 42900.5703125\n",
      "Finished epoch  450\n",
      "On epoch  451\n",
      "Total Epoch Testing MAPE: PCE = 42884.3671875\n",
      "                              Voc = 42864.1171875\n",
      "                              Jsc = 42886.3125\n",
      "                              FF = 42901.33203125\n",
      "Finished epoch  451\n",
      "On epoch  452\n",
      "Total Epoch Testing MAPE: PCE = 42883.9375\n",
      "                              Voc = 42864.921875\n",
      "                              Jsc = 42885.5390625\n",
      "                              FF = 42901.66796875\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 42884.40234375\n",
      "                              Voc = 42865.390625\n",
      "                              Jsc = 42884.7265625\n",
      "                              FF = 42902.296875\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 42884.984375\n",
      "                              Voc = 42865.46875\n",
      "                              Jsc = 42884.94140625\n",
      "                              FF = 42902.37109375\n",
      "Finished epoch  454\n",
      "On epoch  455\n",
      "Total Epoch Testing MAPE: PCE = 42884.328125\n",
      "                              Voc = 42865.80078125\n",
      "                              Jsc = 42883.87890625\n",
      "                              FF = 42901.83984375\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 42883.76171875\n",
      "                              Voc = 42865.828125\n",
      "                              Jsc = 42885.91015625\n",
      "                              FF = 42901.9375\n",
      "Finished epoch  456\n",
      "On epoch  457\n",
      "Total Epoch Testing MAPE: PCE = 42883.8828125\n",
      "                              Voc = 42867.40234375\n",
      "                              Jsc = 42887.60546875\n",
      "                              FF = 42902.4453125\n",
      "Finished epoch  457\n",
      "On epoch  458\n",
      "Total Epoch Testing MAPE: PCE = 42883.37109375\n",
      "                              Voc = 42866.6953125\n",
      "                              Jsc = 42887.66015625\n",
      "                              FF = 42902.65625\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 42883.515625\n",
      "                              Voc = 42866.72265625\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.9140625\n",
      "Finished epoch  459\n",
      "On epoch  460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42883.4765625\n",
      "                              Voc = 42867.9296875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.65625\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 42883.76171875\n",
      "                              Voc = 42867.94140625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.3984375\n",
      "Finished epoch  461\n",
      "On epoch  462\n",
      "Total Epoch Testing MAPE: PCE = 42883.54296875\n",
      "                              Voc = 42866.0390625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.41015625\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 42883.140625\n",
      "                              Voc = 42867.0546875\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.8828125\n",
      "Finished epoch  463\n",
      "On epoch  464\n",
      "Total Epoch Testing MAPE: PCE = 42882.70703125\n",
      "                              Voc = 42866.0078125\n",
      "                              Jsc = 42887.27734375\n",
      "                              FF = 42902.55859375\n",
      "Finished epoch  464\n",
      "On epoch  465\n",
      "Total Epoch Testing MAPE: PCE = 42883.1484375\n",
      "                              Voc = 42865.87890625\n",
      "                              Jsc = 42885.26171875\n",
      "                              FF = 42902.75\n",
      "Finished epoch  465\n",
      "On epoch  466\n",
      "Total Epoch Testing MAPE: PCE = 42882.8203125\n",
      "                              Voc = 42864.27734375\n",
      "                              Jsc = 42885.76953125\n",
      "                              FF = 42902.6328125\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 42882.8203125\n",
      "                              Voc = 42864.6640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42903.359375\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 42882.78125\n",
      "                              Voc = 42863.71875\n",
      "                              Jsc = 42886.91796875\n",
      "                              FF = 42903.546875\n",
      "Finished epoch  468\n",
      "On epoch  469\n",
      "Total Epoch Testing MAPE: PCE = 42882.890625\n",
      "                              Voc = 42863.234375\n",
      "                              Jsc = 42885.45703125\n",
      "                              FF = 42903.265625\n",
      "Finished epoch  469\n",
      "On epoch  470\n",
      "Total Epoch Testing MAPE: PCE = 42883.015625\n",
      "                              Voc = 42863.08203125\n",
      "                              Jsc = 42886.16015625\n",
      "                              FF = 42903.22265625\n",
      "Finished epoch  470\n",
      "On epoch  471\n",
      "Total Epoch Testing MAPE: PCE = 42882.8828125\n",
      "                              Voc = 42863.21875\n",
      "                              Jsc = 42887.98828125\n",
      "                              FF = 42903.0\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 42882.6953125\n",
      "                              Voc = 42863.1484375\n",
      "                              Jsc = 42887.7265625\n",
      "                              FF = 42903.65234375\n",
      "Finished epoch  472\n",
      "On epoch  473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42883.046875\n",
      "                              Voc = 42863.2578125\n",
      "                              Jsc = 42887.46875\n",
      "                              FF = 42901.90625\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 42882.73828125\n",
      "                              Voc = 42863.30078125\n",
      "                              Jsc = 42888.12109375\n",
      "                              FF = 42901.6796875\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 42883.1875\n",
      "                              Voc = 42863.51171875\n",
      "                              Jsc = 42885.390625\n",
      "                              FF = 42901.9140625\n",
      "Finished epoch  475\n",
      "On epoch  476\n",
      "Total Epoch Testing MAPE: PCE = 42883.49609375\n",
      "                              Voc = 42863.40625\n",
      "                              Jsc = 42884.8125\n",
      "                              FF = 42901.71875\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 42883.140625\n",
      "                              Voc = 42863.3984375\n",
      "                              Jsc = 42886.61328125\n",
      "                              FF = 42901.359375\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 42884.23828125\n",
      "                              Voc = 42864.06640625\n",
      "                              Jsc = 42886.90625\n",
      "                              FF = 42900.859375\n",
      "Finished epoch  478\n",
      "On epoch  479\n",
      "Total Epoch Testing MAPE: PCE = 42884.578125\n",
      "                              Voc = 42864.546875\n",
      "                              Jsc = 42886.11328125\n",
      "                              FF = 42900.0\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 42884.3359375\n",
      "                              Voc = 42864.4765625\n",
      "                              Jsc = 42884.5625\n",
      "                              FF = 42900.7890625\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 42883.6875\n",
      "                              Voc = 42868.73046875\n",
      "                              Jsc = 42883.578125\n",
      "                              FF = 42901.1640625\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 42883.3671875\n",
      "                              Voc = 42870.515625\n",
      "                              Jsc = 42885.34765625\n",
      "                              FF = 42901.68359375\n",
      "Finished epoch  482\n",
      "On epoch  483\n",
      "Total Epoch Testing MAPE: PCE = 42883.28515625\n",
      "                              Voc = 42867.66796875\n",
      "                              Jsc = 42888.203125\n",
      "                              FF = 42900.46875\n",
      "Finished epoch  483\n",
      "On epoch  484\n",
      "Total Epoch Testing MAPE: PCE = 42883.59375\n",
      "                              Voc = 42869.109375\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.66796875\n",
      "Finished epoch  484\n",
      "On epoch  485\n",
      "Total Epoch Testing MAPE: PCE = 42883.62890625\n",
      "                              Voc = 42866.57421875\n",
      "                              Jsc = 42888.078125\n",
      "                              FF = 42900.5390625\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 42883.2265625\n",
      "                              Voc = 42869.3671875\n",
      "                              Jsc = 42888.015625\n",
      "                              FF = 42900.2265625\n",
      "Finished epoch  486\n",
      "On epoch  487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42883.4375\n",
      "                              Voc = 42870.68359375\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.23046875\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 42883.6484375\n",
      "                              Voc = 42870.51171875\n",
      "                              Jsc = nan\n",
      "                              FF = 42900.69921875\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 42883.8828125\n",
      "                              Voc = 42868.4375\n",
      "                              Jsc = 42886.40234375\n",
      "                              FF = 42900.45703125\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 42884.44140625\n",
      "                              Voc = 42869.68359375\n",
      "                              Jsc = 42887.1015625\n",
      "                              FF = 42900.80859375\n",
      "Finished epoch  490\n",
      "On epoch  491\n",
      "Total Epoch Testing MAPE: PCE = 42884.60546875\n",
      "                              Voc = 42868.1171875\n",
      "                              Jsc = 42886.51953125\n",
      "                              FF = 42901.9765625\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 42884.84375\n",
      "                              Voc = 42866.1953125\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.34765625\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 42884.5546875\n",
      "                              Voc = 42865.2734375\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.23046875\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 42884.34375\n",
      "                              Voc = 42864.08203125\n",
      "                              Jsc = nan\n",
      "                              FF = 42902.65234375\n",
      "Finished epoch  494\n",
      "On epoch  495\n",
      "Total Epoch Testing MAPE: PCE = 42883.84375\n",
      "                              Voc = 42863.76171875\n",
      "                              Jsc = 42886.796875\n",
      "                              FF = 42902.671875\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 42883.5546875\n",
      "                              Voc = 42863.75\n",
      "                              Jsc = 42886.75\n",
      "                              FF = 42902.171875\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 42883.76953125\n",
      "                              Voc = 42863.6640625\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.79296875\n",
      "Finished epoch  497\n",
      "On epoch  498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42883.9765625\n",
      "                              Voc = 42863.70703125\n",
      "                              Jsc = nan\n",
      "                              FF = 42901.5703125\n",
      "Finished epoch  498\n",
      "On epoch  499\n",
      "Total Epoch Testing MAPE: PCE = 42884.4765625\n",
      "                              Voc = 42864.12890625\n",
      "                              Jsc = 42888.625\n",
      "                              FF = 42901.58203125\n",
      "Finished epoch  499\n",
      "Fold # 3\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 3232.432861328125\n",
      "                              Voc = 234.73696899414062\n",
      "                              Jsc = 100.0\n",
      "                              FF = 251.49459838867188\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 2154.1962890625\n",
      "                              Voc = 207.9479217529297\n",
      "                              Jsc = 100.0\n",
      "                              FF = 232.0157470703125\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 1375.995361328125\n",
      "                              Voc = 169.70928955078125\n",
      "                              Jsc = 100.0\n",
      "                              FF = 201.1729736328125\n",
      "Finished epoch  2\n",
      "On epoch  3\n",
      "Total Epoch Testing MAPE: PCE = 757.6536865234375\n",
      "                              Voc = 148.51437377929688\n",
      "                              Jsc = 100.0\n",
      "                              FF = 190.1772918701172\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 339.88494873046875\n",
      "                              Voc = 135.70989990234375\n",
      "                              Jsc = 100.0\n",
      "                              FF = 193.98681640625\n",
      "Finished epoch  4\n",
      "On epoch  5\n",
      "Total Epoch Testing MAPE: PCE = 236.44065856933594\n",
      "                              Voc = 122.0772705078125\n",
      "                              Jsc = 100.0\n",
      "                              FF = 203.25030517578125\n",
      "Finished epoch  5\n",
      "On epoch  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 172.9612274169922\n",
      "                              Voc = 114.25650024414062\n",
      "                              Jsc = 100.0\n",
      "                              FF = 208.8321990966797\n",
      "Finished epoch  6\n",
      "On epoch  7\n",
      "Total Epoch Testing MAPE: PCE = 164.34127807617188\n",
      "                              Voc = 108.4172592163086\n",
      "                              Jsc = 100.0\n",
      "                              FF = 218.8547821044922\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 151.23931884765625\n",
      "                              Voc = 101.6748275756836\n",
      "                              Jsc = 99.87557220458984\n",
      "                              FF = 223.1187744140625\n",
      "Finished epoch  8\n",
      "On epoch  9\n",
      "Total Epoch Testing MAPE: PCE = 138.15847778320312\n",
      "                              Voc = 94.70640563964844\n",
      "                              Jsc = 99.75528717041016\n",
      "                              FF = 221.5323944091797\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 128.21139526367188\n",
      "                              Voc = 88.67451477050781\n",
      "                              Jsc = 99.67669677734375\n",
      "                              FF = 223.30606079101562\n",
      "Finished epoch  10\n",
      "On epoch  11\n",
      "Total Epoch Testing MAPE: PCE = 118.59400177001953\n",
      "                              Voc = 84.44618225097656\n",
      "                              Jsc = 98.89501953125\n",
      "                              FF = 227.45925903320312\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 109.06597137451172\n",
      "                              Voc = 78.52774810791016\n",
      "                              Jsc = 99.14067077636719\n",
      "                              FF = 227.49827575683594\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 101.99144744873047\n",
      "                              Voc = 75.21484375\n",
      "                              Jsc = 99.16447448730469\n",
      "                              FF = 240.23182678222656\n",
      "Finished epoch  13\n",
      "On epoch  14\n",
      "Total Epoch Testing MAPE: PCE = 96.40255737304688\n",
      "                              Voc = 73.76925659179688\n",
      "                              Jsc = 99.08790588378906\n",
      "                              FF = 244.90597534179688\n",
      "Finished epoch  14\n",
      "On epoch  15\n",
      "Total Epoch Testing MAPE: PCE = 90.25684356689453\n",
      "                              Voc = 73.35786437988281\n",
      "                              Jsc = 98.99845886230469\n",
      "                              FF = 252.60443115234375\n",
      "Finished epoch  15\n",
      "On epoch  16\n",
      "Total Epoch Testing MAPE: PCE = 85.13442993164062\n",
      "                              Voc = 72.23748779296875\n",
      "                              Jsc = 98.9679183959961\n",
      "                              FF = 257.5056457519531\n",
      "Finished epoch  16\n",
      "On epoch  17\n",
      "Total Epoch Testing MAPE: PCE = 83.7981185913086\n",
      "                              Voc = 71.88130950927734\n",
      "                              Jsc = 99.13321685791016\n",
      "                              FF = 253.56021118164062\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 84.39527893066406\n",
      "                              Voc = 70.2572250366211\n",
      "                              Jsc = 99.2499771118164\n",
      "                              FF = 258.559814453125\n",
      "Finished epoch  18\n",
      "On epoch  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 84.96637725830078\n",
      "                              Voc = 68.80725860595703\n",
      "                              Jsc = 99.5540542602539\n",
      "                              FF = 256.54510498046875\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 84.10348510742188\n",
      "                              Voc = 68.6641616821289\n",
      "                              Jsc = 99.73768615722656\n",
      "                              FF = 271.2593078613281\n",
      "Finished epoch  20\n",
      "On epoch  21\n",
      "Total Epoch Testing MAPE: PCE = 83.48675537109375\n",
      "                              Voc = 68.64999389648438\n",
      "                              Jsc = 99.65086364746094\n",
      "                              FF = 274.2674255371094\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 83.58740997314453\n",
      "                              Voc = 67.6690673828125\n",
      "                              Jsc = 99.91847229003906\n",
      "                              FF = 275.54180908203125\n",
      "Finished epoch  22\n",
      "On epoch  23\n",
      "Total Epoch Testing MAPE: PCE = 82.64057159423828\n",
      "                              Voc = 67.37458038330078\n",
      "                              Jsc = 100.0\n",
      "                              FF = 277.1762390136719\n",
      "Finished epoch  23\n",
      "On epoch  24\n",
      "Total Epoch Testing MAPE: PCE = 82.37342834472656\n",
      "                              Voc = 66.67899322509766\n",
      "                              Jsc = 100.0\n",
      "                              FF = 276.94915771484375\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 81.79739379882812\n",
      "                              Voc = 66.65811157226562\n",
      "                              Jsc = 99.93608856201172\n",
      "                              FF = 282.3923645019531\n",
      "Finished epoch  25\n",
      "On epoch  26\n",
      "Total Epoch Testing MAPE: PCE = 82.0654067993164\n",
      "                              Voc = 66.53421783447266\n",
      "                              Jsc = 99.94602966308594\n",
      "                              FF = 284.02313232421875\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 82.79422760009766\n",
      "                              Voc = 66.39863586425781\n",
      "                              Jsc = 100.0\n",
      "                              FF = 291.5863037109375\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 82.6668701171875\n",
      "                              Voc = 66.34001159667969\n",
      "                              Jsc = 99.69973754882812\n",
      "                              FF = 287.5194091796875\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 81.56004333496094\n",
      "                              Voc = 66.394775390625\n",
      "                              Jsc = 99.46038055419922\n",
      "                              FF = 285.7720947265625\n",
      "Finished epoch  29\n",
      "On epoch  30\n",
      "Total Epoch Testing MAPE: PCE = 81.14069366455078\n",
      "                              Voc = 65.87125396728516\n",
      "                              Jsc = 99.27742004394531\n",
      "                              FF = 282.650634765625\n",
      "Finished epoch  30\n",
      "On epoch  31\n",
      "Total Epoch Testing MAPE: PCE = 80.32218933105469\n",
      "                              Voc = 66.07825469970703\n",
      "                              Jsc = 99.3487777709961\n",
      "                              FF = 276.3199462890625\n",
      "Finished epoch  31\n",
      "On epoch  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 80.14004516601562\n",
      "                              Voc = 65.90504455566406\n",
      "                              Jsc = 99.12246704101562\n",
      "                              FF = 277.470458984375\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 80.50431823730469\n",
      "                              Voc = 65.51912689208984\n",
      "                              Jsc = 99.21495056152344\n",
      "                              FF = 274.3658752441406\n",
      "Finished epoch  33\n",
      "On epoch  34\n",
      "Total Epoch Testing MAPE: PCE = 80.02922058105469\n",
      "                              Voc = 65.30239868164062\n",
      "                              Jsc = 99.12682342529297\n",
      "                              FF = 278.9483337402344\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 80.03751373291016\n",
      "                              Voc = 65.34269714355469\n",
      "                              Jsc = 99.19586181640625\n",
      "                              FF = 274.7765197753906\n",
      "Finished epoch  35\n",
      "On epoch  36\n",
      "Total Epoch Testing MAPE: PCE = 80.57040405273438\n",
      "                              Voc = 65.55099487304688\n",
      "                              Jsc = 99.30435180664062\n",
      "                              FF = 273.67022705078125\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 81.35682678222656\n",
      "                              Voc = 65.7536849975586\n",
      "                              Jsc = 99.7064208984375\n",
      "                              FF = 274.4765625\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 80.67985534667969\n",
      "                              Voc = 65.77210998535156\n",
      "                              Jsc = 99.76737213134766\n",
      "                              FF = 282.5345764160156\n",
      "Finished epoch  38\n",
      "On epoch  39\n",
      "Total Epoch Testing MAPE: PCE = 81.45295715332031\n",
      "                              Voc = 66.21402740478516\n",
      "                              Jsc = 99.63216400146484\n",
      "                              FF = 283.34832763671875\n",
      "Finished epoch  39\n",
      "On epoch  40\n",
      "Total Epoch Testing MAPE: PCE = 81.32083129882812\n",
      "                              Voc = 66.48133850097656\n",
      "                              Jsc = 99.49212646484375\n",
      "                              FF = 277.9310607910156\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 80.75584411621094\n",
      "                              Voc = 66.4736328125\n",
      "                              Jsc = 99.43302154541016\n",
      "                              FF = 280.7583312988281\n",
      "Finished epoch  41\n",
      "On epoch  42\n",
      "Total Epoch Testing MAPE: PCE = 80.575927734375\n",
      "                              Voc = 66.15291595458984\n",
      "                              Jsc = 99.79768371582031\n",
      "                              FF = 284.5880126953125\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 81.36184692382812\n",
      "                              Voc = 65.97096252441406\n",
      "                              Jsc = 99.91805267333984\n",
      "                              FF = 280.68109130859375\n",
      "Finished epoch  43\n",
      "On epoch  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 80.37110900878906\n",
      "                              Voc = 65.37450408935547\n",
      "                              Jsc = 99.98268127441406\n",
      "                              FF = 282.196044921875\n",
      "Finished epoch  44\n",
      "On epoch  45\n",
      "Total Epoch Testing MAPE: PCE = 79.70536041259766\n",
      "                              Voc = 65.35982513427734\n",
      "                              Jsc = 100.0\n",
      "                              FF = 286.9611511230469\n",
      "Finished epoch  45\n",
      "On epoch  46\n",
      "Total Epoch Testing MAPE: PCE = 79.42487335205078\n",
      "                              Voc = 65.38787078857422\n",
      "                              Jsc = 100.0\n",
      "                              FF = 291.1919250488281\n",
      "Finished epoch  46\n",
      "On epoch  47\n",
      "Total Epoch Testing MAPE: PCE = 79.05992126464844\n",
      "                              Voc = 65.57006072998047\n",
      "                              Jsc = 99.80060577392578\n",
      "                              FF = 288.49908447265625\n",
      "Finished epoch  47\n",
      "On epoch  48\n",
      "Total Epoch Testing MAPE: PCE = 79.84551239013672\n",
      "                              Voc = 65.69975280761719\n",
      "                              Jsc = 100.0\n",
      "                              FF = 290.33905029296875\n",
      "Finished epoch  48\n",
      "On epoch  49\n",
      "Total Epoch Testing MAPE: PCE = 80.172119140625\n",
      "                              Voc = 65.5394287109375\n",
      "                              Jsc = 100.0\n",
      "                              FF = 293.2360534667969\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 80.4739990234375\n",
      "                              Voc = 65.46307373046875\n",
      "                              Jsc = 100.0\n",
      "                              FF = 295.90399169921875\n",
      "Finished epoch  50\n",
      "On epoch  51\n",
      "Total Epoch Testing MAPE: PCE = 80.2466812133789\n",
      "                              Voc = 66.08079528808594\n",
      "                              Jsc = 100.0\n",
      "                              FF = 297.861328125\n",
      "Finished epoch  51\n",
      "On epoch  52\n",
      "Total Epoch Testing MAPE: PCE = 80.47447204589844\n",
      "                              Voc = 65.5338134765625\n",
      "                              Jsc = 100.0\n",
      "                              FF = 297.1744689941406\n",
      "Finished epoch  52\n",
      "On epoch  53\n",
      "Total Epoch Testing MAPE: PCE = 79.5452651977539\n",
      "                              Voc = 65.94270324707031\n",
      "                              Jsc = 100.0\n",
      "                              FF = 287.5801696777344\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 79.59441375732422\n",
      "                              Voc = 65.6514663696289\n",
      "                              Jsc = 99.9910659790039\n",
      "                              FF = 285.48931884765625\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 79.15510559082031\n",
      "                              Voc = 65.7113037109375\n",
      "                              Jsc = 99.91019439697266\n",
      "                              FF = 282.47479248046875\n",
      "Finished epoch  55\n",
      "On epoch  56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 80.3371810913086\n",
      "                              Voc = 65.90594482421875\n",
      "                              Jsc = 99.7542495727539\n",
      "                              FF = 281.094482421875\n",
      "Finished epoch  56\n",
      "On epoch  57\n",
      "Total Epoch Testing MAPE: PCE = 79.11404418945312\n",
      "                              Voc = 65.87257385253906\n",
      "                              Jsc = 99.77336883544922\n",
      "                              FF = 285.4710693359375\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 79.01602172851562\n",
      "                              Voc = 65.64461517333984\n",
      "                              Jsc = 100.0\n",
      "                              FF = 287.7795104980469\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 79.21367645263672\n",
      "                              Voc = 65.81698608398438\n",
      "                              Jsc = 100.0\n",
      "                              FF = 284.9691162109375\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 78.45844268798828\n",
      "                              Voc = 65.0472183227539\n",
      "                              Jsc = 100.0\n",
      "                              FF = 280.16278076171875\n",
      "Finished epoch  60\n",
      "On epoch  61\n",
      "Total Epoch Testing MAPE: PCE = 77.87290954589844\n",
      "                              Voc = 65.37515258789062\n",
      "                              Jsc = 100.0\n",
      "                              FF = 279.83001708984375\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 76.29026794433594\n",
      "                              Voc = 64.99276733398438\n",
      "                              Jsc = 100.0\n",
      "                              FF = 273.1396484375\n",
      "Finished epoch  62\n",
      "On epoch  63\n",
      "Total Epoch Testing MAPE: PCE = 78.51465606689453\n",
      "                              Voc = 65.05648803710938\n",
      "                              Jsc = 99.90705871582031\n",
      "                              FF = 269.0729064941406\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 78.32146453857422\n",
      "                              Voc = 65.68622589111328\n",
      "                              Jsc = 100.0\n",
      "                              FF = 269.9835510253906\n",
      "Finished epoch  64\n",
      "On epoch  65\n",
      "Total Epoch Testing MAPE: PCE = 79.10698699951172\n",
      "                              Voc = 65.55642700195312\n",
      "                              Jsc = 100.0\n",
      "                              FF = 276.13836669921875\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 77.5474853515625\n",
      "                              Voc = 65.11308288574219\n",
      "                              Jsc = 100.0\n",
      "                              FF = 277.5689392089844\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 76.86663818359375\n",
      "                              Voc = 66.24227905273438\n",
      "                              Jsc = 100.0\n",
      "                              FF = 281.96221923828125\n",
      "Finished epoch  67\n",
      "On epoch  68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 75.42082214355469\n",
      "                              Voc = 66.42383575439453\n",
      "                              Jsc = 100.0\n",
      "                              FF = 288.6434326171875\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 66.55517578125\n",
      "                              Voc = 66.61007690429688\n",
      "                              Jsc = 100.0\n",
      "                              FF = 286.5953369140625\n",
      "Finished epoch  69\n",
      "On epoch  70\n",
      "Total Epoch Testing MAPE: PCE = 65.72604370117188\n",
      "                              Voc = 66.73409271240234\n",
      "                              Jsc = 100.0\n",
      "                              FF = 291.6591796875\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 79.52108001708984\n",
      "                              Voc = 66.97452545166016\n",
      "                              Jsc = 100.0\n",
      "                              FF = 293.2520751953125\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 64.04183959960938\n",
      "                              Voc = 67.31136322021484\n",
      "                              Jsc = 100.0\n",
      "                              FF = 300.36761474609375\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 80.3004150390625\n",
      "                              Voc = 67.1888656616211\n",
      "                              Jsc = 100.0\n",
      "                              FF = 297.65185546875\n",
      "Finished epoch  73\n",
      "On epoch  74\n",
      "Total Epoch Testing MAPE: PCE = 82.19700622558594\n",
      "                              Voc = 66.33332824707031\n",
      "                              Jsc = 100.0\n",
      "                              FF = 295.8834228515625\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 115.66365814208984\n",
      "                              Voc = 65.49209594726562\n",
      "                              Jsc = 100.0\n",
      "                              FF = 297.5635681152344\n",
      "Finished epoch  75\n",
      "On epoch  76\n",
      "Total Epoch Testing MAPE: PCE = 105.74829864501953\n",
      "                              Voc = 65.53335571289062\n",
      "                              Jsc = 99.57372283935547\n",
      "                              FF = 296.1060791015625\n",
      "Finished epoch  76\n",
      "On epoch  77\n",
      "Total Epoch Testing MAPE: PCE = 109.13497161865234\n",
      "                              Voc = 65.01946258544922\n",
      "                              Jsc = 99.40470886230469\n",
      "                              FF = 293.28765869140625\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 89.53260803222656\n",
      "                              Voc = 65.22795867919922\n",
      "                              Jsc = 99.45085906982422\n",
      "                              FF = 292.21112060546875\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 90.42204284667969\n",
      "                              Voc = 65.07634735107422\n",
      "                              Jsc = 99.4508285522461\n",
      "                              FF = 293.376220703125\n",
      "Finished epoch  79\n",
      "On epoch  80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 97.93834686279297\n",
      "                              Voc = 64.6768569946289\n",
      "                              Jsc = 99.62953186035156\n",
      "                              FF = 298.7142333984375\n",
      "Finished epoch  80\n",
      "On epoch  81\n",
      "Total Epoch Testing MAPE: PCE = 90.762939453125\n",
      "                              Voc = 65.55172729492188\n",
      "                              Jsc = 99.4022216796875\n",
      "                              FF = 297.09027099609375\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 77.52877807617188\n",
      "                              Voc = 65.84491729736328\n",
      "                              Jsc = 99.16194152832031\n",
      "                              FF = 297.860107421875\n",
      "Finished epoch  82\n",
      "On epoch  83\n",
      "Total Epoch Testing MAPE: PCE = 77.54686737060547\n",
      "                              Voc = 65.77764129638672\n",
      "                              Jsc = 99.11582946777344\n",
      "                              FF = 298.253173828125\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 77.75301361083984\n",
      "                              Voc = 65.46348571777344\n",
      "                              Jsc = 99.07776641845703\n",
      "                              FF = 295.0811767578125\n",
      "Finished epoch  84\n",
      "On epoch  85\n",
      "Total Epoch Testing MAPE: PCE = 76.95471954345703\n",
      "                              Voc = 65.25692749023438\n",
      "                              Jsc = 98.96489715576172\n",
      "                              FF = 291.169189453125\n",
      "Finished epoch  85\n",
      "On epoch  86\n",
      "Total Epoch Testing MAPE: PCE = 70.58464050292969\n",
      "                              Voc = 65.96183776855469\n",
      "                              Jsc = 98.681640625\n",
      "                              FF = 292.85540771484375\n",
      "Finished epoch  86\n",
      "On epoch  87\n",
      "Total Epoch Testing MAPE: PCE = 76.23506927490234\n",
      "                              Voc = 65.78567504882812\n",
      "                              Jsc = 98.87698364257812\n",
      "                              FF = 292.8362121582031\n",
      "Finished epoch  87\n",
      "On epoch  88\n",
      "Total Epoch Testing MAPE: PCE = 76.55894470214844\n",
      "                              Voc = 65.82357025146484\n",
      "                              Jsc = 98.94837951660156\n",
      "                              FF = 287.04229736328125\n",
      "Finished epoch  88\n",
      "On epoch  89\n",
      "Total Epoch Testing MAPE: PCE = 78.98163604736328\n",
      "                              Voc = 64.69892883300781\n",
      "                              Jsc = 99.04048156738281\n",
      "                              FF = 284.6778259277344\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 78.54568481445312\n",
      "                              Voc = 64.71599578857422\n",
      "                              Jsc = 99.39842987060547\n",
      "                              FF = 288.79425048828125\n",
      "Finished epoch  90\n",
      "On epoch  91\n",
      "Total Epoch Testing MAPE: PCE = 79.02922058105469\n",
      "                              Voc = 64.94461059570312\n",
      "                              Jsc = 99.47628021240234\n",
      "                              FF = 291.7059020996094\n",
      "Finished epoch  91\n",
      "On epoch  92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 79.67816925048828\n",
      "                              Voc = 64.7769546508789\n",
      "                              Jsc = 99.306396484375\n",
      "                              FF = 295.8254699707031\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 79.86463165283203\n",
      "                              Voc = 64.8298568725586\n",
      "                              Jsc = 99.58320617675781\n",
      "                              FF = 299.0686340332031\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 80.44065856933594\n",
      "                              Voc = 64.90451049804688\n",
      "                              Jsc = 99.65559387207031\n",
      "                              FF = 297.55938720703125\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 80.71290588378906\n",
      "                              Voc = 64.81356048583984\n",
      "                              Jsc = 99.74870300292969\n",
      "                              FF = 298.9850158691406\n",
      "Finished epoch  95\n",
      "On epoch  96\n",
      "Total Epoch Testing MAPE: PCE = 79.99547576904297\n",
      "                              Voc = 65.42991638183594\n",
      "                              Jsc = 99.8376693725586\n",
      "                              FF = 299.43743896484375\n",
      "Finished epoch  96\n",
      "On epoch  97\n",
      "Total Epoch Testing MAPE: PCE = 78.7196273803711\n",
      "                              Voc = 65.69026184082031\n",
      "                              Jsc = 99.39838409423828\n",
      "                              FF = 296.7361755371094\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 80.54859924316406\n",
      "                              Voc = 65.5108642578125\n",
      "                              Jsc = 99.65437316894531\n",
      "                              FF = 299.5578918457031\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 80.01641082763672\n",
      "                              Voc = 65.71305847167969\n",
      "                              Jsc = 99.83690643310547\n",
      "                              FF = 299.85675048828125\n",
      "Finished epoch  99\n",
      "On epoch  100\n",
      "Total Epoch Testing MAPE: PCE = 77.67547607421875\n",
      "                              Voc = 65.72264099121094\n",
      "                              Jsc = 99.83394622802734\n",
      "                              FF = 294.7686767578125\n",
      "Finished epoch  100\n",
      "On epoch  101\n",
      "Total Epoch Testing MAPE: PCE = 79.16337585449219\n",
      "                              Voc = 65.31022644042969\n",
      "                              Jsc = 99.5541000366211\n",
      "                              FF = 297.20819091796875\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 79.10388946533203\n",
      "                              Voc = 65.624755859375\n",
      "                              Jsc = 99.32898712158203\n",
      "                              FF = 301.4552001953125\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 78.42572021484375\n",
      "                              Voc = 65.10746765136719\n",
      "                              Jsc = 99.0655288696289\n",
      "                              FF = 299.9554748535156\n",
      "Finished epoch  103\n",
      "On epoch  104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 78.6471176147461\n",
      "                              Voc = 65.80438995361328\n",
      "                              Jsc = 99.15758514404297\n",
      "                              FF = 298.0191345214844\n",
      "Finished epoch  104\n",
      "On epoch  105\n",
      "Total Epoch Testing MAPE: PCE = 79.15438079833984\n",
      "                              Voc = 65.67155456542969\n",
      "                              Jsc = 99.00377655029297\n",
      "                              FF = 298.24139404296875\n",
      "Finished epoch  105\n",
      "On epoch  106\n",
      "Total Epoch Testing MAPE: PCE = 76.735595703125\n",
      "                              Voc = 65.66389465332031\n",
      "                              Jsc = 99.21595764160156\n",
      "                              FF = 300.2753601074219\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 61.26746368408203\n",
      "                              Voc = 65.66398620605469\n",
      "                              Jsc = 99.4519271850586\n",
      "                              FF = 298.9607238769531\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 69.53022003173828\n",
      "                              Voc = 65.72376251220703\n",
      "                              Jsc = 98.99482727050781\n",
      "                              FF = 298.14263916015625\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 61.29131317138672\n",
      "                              Voc = 65.67476654052734\n",
      "                              Jsc = 98.70484924316406\n",
      "                              FF = 290.1474609375\n",
      "Finished epoch  109\n",
      "On epoch  110\n",
      "Total Epoch Testing MAPE: PCE = 70.81890106201172\n",
      "                              Voc = 65.8757553100586\n",
      "                              Jsc = 98.47394561767578\n",
      "                              FF = 285.5425109863281\n",
      "Finished epoch  110\n",
      "On epoch  111\n",
      "Total Epoch Testing MAPE: PCE = 84.75634765625\n",
      "                              Voc = 65.93112182617188\n",
      "                              Jsc = 98.4087142944336\n",
      "                              FF = 284.20831298828125\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 65.74188232421875\n",
      "                              Voc = 65.23795318603516\n",
      "                              Jsc = 98.57405090332031\n",
      "                              FF = 284.64208984375\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 57.984310150146484\n",
      "                              Voc = 65.46411895751953\n",
      "                              Jsc = 98.8171157836914\n",
      "                              FF = 285.6950378417969\n",
      "Finished epoch  113\n",
      "On epoch  114\n",
      "Total Epoch Testing MAPE: PCE = 64.96148681640625\n",
      "                              Voc = 66.11665344238281\n",
      "                              Jsc = 98.9688491821289\n",
      "                              FF = 281.8807067871094\n",
      "Finished epoch  114\n",
      "On epoch  115\n",
      "Total Epoch Testing MAPE: PCE = 84.23356628417969\n",
      "                              Voc = 66.28295135498047\n",
      "                              Jsc = 98.69000244140625\n",
      "                              FF = 280.2974853515625\n",
      "Finished epoch  115\n",
      "On epoch  116\n",
      "Total Epoch Testing MAPE: PCE = 103.52651977539062\n",
      "                              Voc = 66.26138305664062\n",
      "                              Jsc = 99.19224548339844\n",
      "                              FF = 279.3736572265625\n",
      "Finished epoch  116\n",
      "On epoch  117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 106.56712341308594\n",
      "                              Voc = 67.17031860351562\n",
      "                              Jsc = 99.03985595703125\n",
      "                              FF = 276.3670349121094\n",
      "Finished epoch  117\n",
      "On epoch  118\n",
      "Total Epoch Testing MAPE: PCE = 105.52407836914062\n",
      "                              Voc = 66.02010345458984\n",
      "                              Jsc = 98.9871597290039\n",
      "                              FF = 276.7349548339844\n",
      "Finished epoch  118\n",
      "On epoch  119\n",
      "Total Epoch Testing MAPE: PCE = 107.9290542602539\n",
      "                              Voc = 66.09256744384766\n",
      "                              Jsc = 99.25357055664062\n",
      "                              FF = 274.9931945800781\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 84.82510375976562\n",
      "                              Voc = 66.20306396484375\n",
      "                              Jsc = 99.22492218017578\n",
      "                              FF = 278.4264221191406\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 92.53304290771484\n",
      "                              Voc = 66.02264404296875\n",
      "                              Jsc = 98.9308853149414\n",
      "                              FF = 288.6300964355469\n",
      "Finished epoch  121\n",
      "On epoch  122\n",
      "Total Epoch Testing MAPE: PCE = 113.88480377197266\n",
      "                              Voc = 65.70667266845703\n",
      "                              Jsc = 98.94886016845703\n",
      "                              FF = 290.9477844238281\n",
      "Finished epoch  122\n",
      "On epoch  123\n",
      "Total Epoch Testing MAPE: PCE = 111.76922607421875\n",
      "                              Voc = 66.11036682128906\n",
      "                              Jsc = 98.85486602783203\n",
      "                              FF = 288.74200439453125\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 87.22241973876953\n",
      "                              Voc = 66.2961654663086\n",
      "                              Jsc = 99.21947479248047\n",
      "                              FF = 285.2259521484375\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 86.78328704833984\n",
      "                              Voc = 66.50572967529297\n",
      "                              Jsc = 99.54827117919922\n",
      "                              FF = 288.4261779785156\n",
      "Finished epoch  125\n",
      "On epoch  126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 97.16307830810547\n",
      "                              Voc = 66.61485290527344\n",
      "                              Jsc = 99.45408630371094\n",
      "                              FF = 294.3046875\n",
      "Finished epoch  126\n",
      "On epoch  127\n",
      "Total Epoch Testing MAPE: PCE = 84.17259979248047\n",
      "                              Voc = 65.9186019897461\n",
      "                              Jsc = 99.42500305175781\n",
      "                              FF = 299.8898010253906\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 83.21292877197266\n",
      "                              Voc = 65.66976165771484\n",
      "                              Jsc = 99.130859375\n",
      "                              FF = 296.9646911621094\n",
      "Finished epoch  128\n",
      "On epoch  129\n",
      "Total Epoch Testing MAPE: PCE = 64.57771301269531\n",
      "                              Voc = 66.28596496582031\n",
      "                              Jsc = 98.86641693115234\n",
      "                              FF = 302.2086486816406\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 65.29275512695312\n",
      "                              Voc = 66.6019058227539\n",
      "                              Jsc = 98.93571472167969\n",
      "                              FF = 299.4240417480469\n",
      "Finished epoch  130\n",
      "On epoch  131\n",
      "Total Epoch Testing MAPE: PCE = 63.06156539916992\n",
      "                              Voc = 66.33103942871094\n",
      "                              Jsc = 99.02337646484375\n",
      "                              FF = 293.3000183105469\n",
      "Finished epoch  131\n",
      "On epoch  132\n",
      "Total Epoch Testing MAPE: PCE = 57.83256149291992\n",
      "                              Voc = 66.64938354492188\n",
      "                              Jsc = 98.59756469726562\n",
      "                              FF = 301.1813659667969\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 60.73862838745117\n",
      "                              Voc = 66.88025665283203\n",
      "                              Jsc = 98.42301940917969\n",
      "                              FF = 298.26300048828125\n",
      "Finished epoch  133\n",
      "On epoch  134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 76.75897216796875\n",
      "                              Voc = 66.70199584960938\n",
      "                              Jsc = 98.57593536376953\n",
      "                              FF = 294.2242431640625\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 75.38402557373047\n",
      "                              Voc = 66.41655731201172\n",
      "                              Jsc = 98.5536117553711\n",
      "                              FF = 292.489013671875\n",
      "Finished epoch  135\n",
      "On epoch  136\n",
      "Total Epoch Testing MAPE: PCE = 55.577796936035156\n",
      "                              Voc = 66.59735107421875\n",
      "                              Jsc = 98.45061492919922\n",
      "                              FF = 295.02923583984375\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 77.24352264404297\n",
      "                              Voc = 65.66699981689453\n",
      "                              Jsc = 98.49964141845703\n",
      "                              FF = 292.83050537109375\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 73.0577621459961\n",
      "                              Voc = 65.46350860595703\n",
      "                              Jsc = 98.39353942871094\n",
      "                              FF = 290.852294921875\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 82.33094024658203\n",
      "                              Voc = 65.57437133789062\n",
      "                              Jsc = 98.34075927734375\n",
      "                              FF = 301.43701171875\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 69.76264953613281\n",
      "                              Voc = 65.63207244873047\n",
      "                              Jsc = 98.78457641601562\n",
      "                              FF = 305.08709716796875\n",
      "Finished epoch  140\n",
      "On epoch  141\n",
      "Total Epoch Testing MAPE: PCE = 91.5093765258789\n",
      "                              Voc = 65.01702117919922\n",
      "                              Jsc = 98.9032211303711\n",
      "                              FF = 304.5672912597656\n",
      "Finished epoch  141\n",
      "On epoch  142\n",
      "Total Epoch Testing MAPE: PCE = 111.56478881835938\n",
      "                              Voc = 65.02486419677734\n",
      "                              Jsc = 98.69989013671875\n",
      "                              FF = 300.3441467285156\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 139.6837921142578\n",
      "                              Voc = 65.36897277832031\n",
      "                              Jsc = 99.26131439208984\n",
      "                              FF = 294.20587158203125\n",
      "Finished epoch  143\n",
      "On epoch  144\n",
      "Total Epoch Testing MAPE: PCE = 141.84579467773438\n",
      "                              Voc = 65.19659423828125\n",
      "                              Jsc = 99.01568603515625\n",
      "                              FF = 301.6960754394531\n",
      "Finished epoch  144\n",
      "On epoch  145\n",
      "Total Epoch Testing MAPE: PCE = 118.52677154541016\n",
      "                              Voc = 65.07044219970703\n",
      "                              Jsc = 98.78740692138672\n",
      "                              FF = 302.32623291015625\n",
      "Finished epoch  145\n",
      "On epoch  146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 151.1597442626953\n",
      "                              Voc = 65.60226440429688\n",
      "                              Jsc = 98.86771392822266\n",
      "                              FF = 300.0127868652344\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 147.58364868164062\n",
      "                              Voc = 65.37300872802734\n",
      "                              Jsc = 99.06636810302734\n",
      "                              FF = 298.6236572265625\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 133.0078887939453\n",
      "                              Voc = 64.40591430664062\n",
      "                              Jsc = 98.6922836303711\n",
      "                              FF = 300.5706787109375\n",
      "Finished epoch  148\n",
      "On epoch  149\n",
      "Total Epoch Testing MAPE: PCE = 80.74784088134766\n",
      "                              Voc = 64.35093688964844\n",
      "                              Jsc = 98.68795013427734\n",
      "                              FF = 299.4827880859375\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 125.38017272949219\n",
      "                              Voc = 64.66111755371094\n",
      "                              Jsc = 98.38391876220703\n",
      "                              FF = 299.1197204589844\n",
      "Finished epoch  150\n",
      "On epoch  151\n",
      "Total Epoch Testing MAPE: PCE = 93.77518463134766\n",
      "                              Voc = 64.34373474121094\n",
      "                              Jsc = 98.40604400634766\n",
      "                              FF = 301.4095458984375\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 81.82340240478516\n",
      "                              Voc = 64.18528747558594\n",
      "                              Jsc = 98.18609619140625\n",
      "                              FF = 303.75164794921875\n",
      "Finished epoch  152\n",
      "On epoch  153\n",
      "Total Epoch Testing MAPE: PCE = 81.78561401367188\n",
      "                              Voc = 64.50265502929688\n",
      "                              Jsc = 98.02559661865234\n",
      "                              FF = 309.1090087890625\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 111.40161895751953\n",
      "                              Voc = 64.62450408935547\n",
      "                              Jsc = 98.36965942382812\n",
      "                              FF = 308.8397216796875\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 98.98348999023438\n",
      "                              Voc = 64.85098266601562\n",
      "                              Jsc = 98.38533020019531\n",
      "                              FF = 310.08087158203125\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 107.61799621582031\n",
      "                              Voc = 64.80738830566406\n",
      "                              Jsc = 98.36154174804688\n",
      "                              FF = 313.9657897949219\n",
      "Finished epoch  156\n",
      "On epoch  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 137.05955505371094\n",
      "                              Voc = 64.68140411376953\n",
      "                              Jsc = 98.43516540527344\n",
      "                              FF = 320.5272521972656\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 133.35472106933594\n",
      "                              Voc = 64.57349395751953\n",
      "                              Jsc = 98.49168395996094\n",
      "                              FF = 320.3955078125\n",
      "Finished epoch  158\n",
      "On epoch  159\n",
      "Total Epoch Testing MAPE: PCE = 110.2484130859375\n",
      "                              Voc = 64.93811798095703\n",
      "                              Jsc = 98.64759063720703\n",
      "                              FF = 312.2245788574219\n",
      "Finished epoch  159\n",
      "On epoch  160\n",
      "Total Epoch Testing MAPE: PCE = 125.36622619628906\n",
      "                              Voc = 64.59493255615234\n",
      "                              Jsc = 98.65911865234375\n",
      "                              FF = 307.52392578125\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 124.54527282714844\n",
      "                              Voc = 64.77379608154297\n",
      "                              Jsc = 99.00875854492188\n",
      "                              FF = 311.0287170410156\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 109.20606994628906\n",
      "                              Voc = 65.02653503417969\n",
      "                              Jsc = 99.26873016357422\n",
      "                              FF = 308.2129821777344\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 80.70913696289062\n",
      "                              Voc = 64.4893798828125\n",
      "                              Jsc = 99.30082702636719\n",
      "                              FF = 308.55218505859375\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 68.70209503173828\n",
      "                              Voc = 64.76591491699219\n",
      "                              Jsc = 99.43190002441406\n",
      "                              FF = 309.9212951660156\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 93.04217529296875\n",
      "                              Voc = 64.44857788085938\n",
      "                              Jsc = 99.4686508178711\n",
      "                              FF = 315.70416259765625\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 110.086181640625\n",
      "                              Voc = 65.2453842163086\n",
      "                              Jsc = 99.59611511230469\n",
      "                              FF = 318.2577819824219\n",
      "Finished epoch  166\n",
      "On epoch  167\n",
      "Total Epoch Testing MAPE: PCE = 131.72560119628906\n",
      "                              Voc = 65.3846435546875\n",
      "                              Jsc = 99.887939453125\n",
      "                              FF = 312.5880126953125\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 116.90718078613281\n",
      "                              Voc = 65.72551727294922\n",
      "                              Jsc = 99.37757110595703\n",
      "                              FF = 312.9327697753906\n",
      "Finished epoch  168\n",
      "On epoch  169\n",
      "Total Epoch Testing MAPE: PCE = 118.43862915039062\n",
      "                              Voc = 65.09384155273438\n",
      "                              Jsc = 99.44544982910156\n",
      "                              FF = 316.13128662109375\n",
      "Finished epoch  169\n",
      "On epoch  170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 111.12905883789062\n",
      "                              Voc = 65.0530776977539\n",
      "                              Jsc = 99.68464660644531\n",
      "                              FF = 311.191162109375\n",
      "Finished epoch  170\n",
      "On epoch  171\n",
      "Total Epoch Testing MAPE: PCE = 76.853515625\n",
      "                              Voc = 65.04318237304688\n",
      "                              Jsc = 99.85639190673828\n",
      "                              FF = 311.7486572265625\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 83.0994873046875\n",
      "                              Voc = 64.74691009521484\n",
      "                              Jsc = 99.62905883789062\n",
      "                              FF = 314.8795471191406\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 121.28113555908203\n",
      "                              Voc = 64.52931213378906\n",
      "                              Jsc = 99.72579193115234\n",
      "                              FF = 307.8327331542969\n",
      "Finished epoch  173\n",
      "On epoch  174\n",
      "Total Epoch Testing MAPE: PCE = 137.5724639892578\n",
      "                              Voc = 64.64189147949219\n",
      "                              Jsc = 99.62503051757812\n",
      "                              FF = 308.5168151855469\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 188.58314514160156\n",
      "                              Voc = 65.34910583496094\n",
      "                              Jsc = 99.45067596435547\n",
      "                              FF = 312.470947265625\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 211.04718017578125\n",
      "                              Voc = 65.05084228515625\n",
      "                              Jsc = 99.6474838256836\n",
      "                              FF = 314.15618896484375\n",
      "Finished epoch  176\n",
      "On epoch  177\n",
      "Total Epoch Testing MAPE: PCE = 219.366455078125\n",
      "                              Voc = 65.05641174316406\n",
      "                              Jsc = 99.69666290283203\n",
      "                              FF = 313.0702819824219\n",
      "Finished epoch  177\n",
      "On epoch  178\n",
      "Total Epoch Testing MAPE: PCE = 253.96099853515625\n",
      "                              Voc = 64.82452392578125\n",
      "                              Jsc = 99.45829772949219\n",
      "                              FF = 308.5429992675781\n",
      "Finished epoch  178\n",
      "On epoch  179\n",
      "Total Epoch Testing MAPE: PCE = 245.8813934326172\n",
      "                              Voc = 65.05717468261719\n",
      "                              Jsc = 99.68804168701172\n",
      "                              FF = 307.5534362792969\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 242.8348846435547\n",
      "                              Voc = 64.332763671875\n",
      "                              Jsc = 99.1065444946289\n",
      "                              FF = 301.8126220703125\n",
      "Finished epoch  180\n",
      "On epoch  181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 219.03662109375\n",
      "                              Voc = 64.23107147216797\n",
      "                              Jsc = 99.39531707763672\n",
      "                              FF = 300.85723876953125\n",
      "Finished epoch  181\n",
      "On epoch  182\n",
      "Total Epoch Testing MAPE: PCE = 233.52816772460938\n",
      "                              Voc = 64.33930969238281\n",
      "                              Jsc = 99.40494537353516\n",
      "                              FF = 302.5465087890625\n",
      "Finished epoch  182\n",
      "On epoch  183\n",
      "Total Epoch Testing MAPE: PCE = 232.550048828125\n",
      "                              Voc = 64.23351287841797\n",
      "                              Jsc = 99.48076629638672\n",
      "                              FF = 298.6573791503906\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 235.9615020751953\n",
      "                              Voc = 64.47318267822266\n",
      "                              Jsc = 99.3594741821289\n",
      "                              FF = 299.2334899902344\n",
      "Finished epoch  184\n",
      "On epoch  185\n",
      "Total Epoch Testing MAPE: PCE = 232.24703979492188\n",
      "                              Voc = 64.82610321044922\n",
      "                              Jsc = 99.2330093383789\n",
      "                              FF = 297.62567138671875\n",
      "Finished epoch  185\n",
      "On epoch  186\n",
      "Total Epoch Testing MAPE: PCE = 218.4610137939453\n",
      "                              Voc = 64.92444610595703\n",
      "                              Jsc = 99.22228240966797\n",
      "                              FF = 305.72674560546875\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 217.40354919433594\n",
      "                              Voc = 65.29193878173828\n",
      "                              Jsc = 99.41907501220703\n",
      "                              FF = 299.7760009765625\n",
      "Finished epoch  187\n",
      "On epoch  188\n",
      "Total Epoch Testing MAPE: PCE = 187.24142456054688\n",
      "                              Voc = 64.92340850830078\n",
      "                              Jsc = 99.4484634399414\n",
      "                              FF = 295.9313049316406\n",
      "Finished epoch  188\n",
      "On epoch  189\n",
      "Total Epoch Testing MAPE: PCE = 155.08177185058594\n",
      "                              Voc = 64.6353530883789\n",
      "                              Jsc = 99.3846206665039\n",
      "                              FF = 293.1482238769531\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 174.82296752929688\n",
      "                              Voc = 65.01493072509766\n",
      "                              Jsc = 99.41283416748047\n",
      "                              FF = 285.5099182128906\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 153.06666564941406\n",
      "                              Voc = 65.46697998046875\n",
      "                              Jsc = 99.77464294433594\n",
      "                              FF = 290.65087890625\n",
      "Finished epoch  191\n",
      "On epoch  192\n",
      "Total Epoch Testing MAPE: PCE = 162.51502990722656\n",
      "                              Voc = 65.21562194824219\n",
      "                              Jsc = 99.8983383178711\n",
      "                              FF = 289.32470703125\n",
      "Finished epoch  192\n",
      "On epoch  193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 147.9690704345703\n",
      "                              Voc = 65.27847290039062\n",
      "                              Jsc = 99.54185485839844\n",
      "                              FF = 292.1605529785156\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 132.9071044921875\n",
      "                              Voc = 65.647216796875\n",
      "                              Jsc = 99.57963562011719\n",
      "                              FF = 295.0686950683594\n",
      "Finished epoch  194\n",
      "On epoch  195\n",
      "Total Epoch Testing MAPE: PCE = 121.96501159667969\n",
      "                              Voc = 65.66645812988281\n",
      "                              Jsc = 99.50020599365234\n",
      "                              FF = 296.2931213378906\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 126.47706604003906\n",
      "                              Voc = 65.6583251953125\n",
      "                              Jsc = 99.67160034179688\n",
      "                              FF = 291.8934020996094\n",
      "Finished epoch  196\n",
      "On epoch  197\n",
      "Total Epoch Testing MAPE: PCE = 118.9337387084961\n",
      "                              Voc = 65.8036117553711\n",
      "                              Jsc = 99.44032287597656\n",
      "                              FF = 286.1478576660156\n",
      "Finished epoch  197\n",
      "On epoch  198\n",
      "Total Epoch Testing MAPE: PCE = 121.69149017333984\n",
      "                              Voc = 65.96321868896484\n",
      "                              Jsc = 99.28883361816406\n",
      "                              FF = 283.7315673828125\n",
      "Finished epoch  198\n",
      "On epoch  199\n",
      "Total Epoch Testing MAPE: PCE = 115.50157928466797\n",
      "                              Voc = 66.1298828125\n",
      "                              Jsc = 99.08589172363281\n",
      "                              FF = 284.5278625488281\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 140.7406005859375\n",
      "                              Voc = 66.67477416992188\n",
      "                              Jsc = 99.31000518798828\n",
      "                              FF = 285.4413146972656\n",
      "Finished epoch  200\n",
      "On epoch  201\n",
      "Total Epoch Testing MAPE: PCE = 116.62928771972656\n",
      "                              Voc = 65.92247009277344\n",
      "                              Jsc = 99.13970947265625\n",
      "                              FF = 286.8994140625\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 127.36952209472656\n",
      "                              Voc = 65.99854278564453\n",
      "                              Jsc = 99.43817138671875\n",
      "                              FF = 289.8536376953125\n",
      "Finished epoch  202\n",
      "On epoch  203\n",
      "Total Epoch Testing MAPE: PCE = 152.8019561767578\n",
      "                              Voc = 65.96126556396484\n",
      "                              Jsc = 99.48894500732422\n",
      "                              FF = 285.6070861816406\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 182.49676513671875\n",
      "                              Voc = 65.46443176269531\n",
      "                              Jsc = 99.49186706542969\n",
      "                              FF = 290.14190673828125\n",
      "Finished epoch  204\n",
      "On epoch  205\n",
      "Total Epoch Testing MAPE: PCE = 211.6687469482422\n",
      "                              Voc = 65.67517852783203\n",
      "                              Jsc = 99.48377227783203\n",
      "                              FF = 288.8709411621094\n",
      "Finished epoch  205\n",
      "On epoch  206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 208.5186309814453\n",
      "                              Voc = 65.66139221191406\n",
      "                              Jsc = 99.85952758789062\n",
      "                              FF = 292.4136657714844\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 230.55763244628906\n",
      "                              Voc = 65.35420227050781\n",
      "                              Jsc = 99.90181732177734\n",
      "                              FF = 302.0754699707031\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 257.4754333496094\n",
      "                              Voc = 65.41571044921875\n",
      "                              Jsc = 99.74247741699219\n",
      "                              FF = 301.6283264160156\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 255.36557006835938\n",
      "                              Voc = 65.52803802490234\n",
      "                              Jsc = 99.68732452392578\n",
      "                              FF = 301.6330871582031\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 266.0709533691406\n",
      "                              Voc = 65.03902435302734\n",
      "                              Jsc = 99.22565460205078\n",
      "                              FF = 285.336669921875\n",
      "Finished epoch  210\n",
      "On epoch  211\n",
      "Total Epoch Testing MAPE: PCE = 290.62646484375\n",
      "                              Voc = 65.18629455566406\n",
      "                              Jsc = 99.28916931152344\n",
      "                              FF = 284.9067077636719\n",
      "Finished epoch  211\n",
      "On epoch  212\n",
      "Total Epoch Testing MAPE: PCE = 255.0345001220703\n",
      "                              Voc = 64.5434341430664\n",
      "                              Jsc = 99.58838653564453\n",
      "                              FF = 280.6305236816406\n",
      "Finished epoch  212\n",
      "On epoch  213\n",
      "Total Epoch Testing MAPE: PCE = 302.58447265625\n",
      "                              Voc = 65.07384490966797\n",
      "                              Jsc = 99.53204345703125\n",
      "                              FF = 285.8869934082031\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 316.8327331542969\n",
      "                              Voc = 65.06180572509766\n",
      "                              Jsc = 99.70085144042969\n",
      "                              FF = 291.5058288574219\n",
      "Finished epoch  214\n",
      "On epoch  215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 342.2789306640625\n",
      "                              Voc = 64.67534637451172\n",
      "                              Jsc = 99.65447998046875\n",
      "                              FF = 291.0408630371094\n",
      "Finished epoch  215\n",
      "On epoch  216\n",
      "Total Epoch Testing MAPE: PCE = 294.4337463378906\n",
      "                              Voc = 64.10597229003906\n",
      "                              Jsc = 99.50407409667969\n",
      "                              FF = 291.1495056152344\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 310.94781494140625\n",
      "                              Voc = 64.07369995117188\n",
      "                              Jsc = 99.4980697631836\n",
      "                              FF = 287.6489562988281\n",
      "Finished epoch  217\n",
      "On epoch  218\n",
      "Total Epoch Testing MAPE: PCE = 277.6219482421875\n",
      "                              Voc = 64.28240203857422\n",
      "                              Jsc = 99.55628967285156\n",
      "                              FF = 288.5360412597656\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 268.2527160644531\n",
      "                              Voc = 64.6654281616211\n",
      "                              Jsc = 99.15208435058594\n",
      "                              FF = 290.03271484375\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 257.5511474609375\n",
      "                              Voc = 64.68324279785156\n",
      "                              Jsc = 99.67223358154297\n",
      "                              FF = 291.05230712890625\n",
      "Finished epoch  220\n",
      "On epoch  221\n",
      "Total Epoch Testing MAPE: PCE = 250.07876586914062\n",
      "                              Voc = 64.73755645751953\n",
      "                              Jsc = 99.904541015625\n",
      "                              FF = 289.58251953125\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 261.24700927734375\n",
      "                              Voc = 64.54798889160156\n",
      "                              Jsc = 99.75341796875\n",
      "                              FF = 291.47259521484375\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 269.12518310546875\n",
      "                              Voc = 64.56610107421875\n",
      "                              Jsc = 99.35201263427734\n",
      "                              FF = 281.6378479003906\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 238.05032348632812\n",
      "                              Voc = 64.38465881347656\n",
      "                              Jsc = 99.14649963378906\n",
      "                              FF = 278.99163818359375\n",
      "Finished epoch  224\n",
      "On epoch  225\n",
      "Total Epoch Testing MAPE: PCE = 222.507568359375\n",
      "                              Voc = 65.38046264648438\n",
      "                              Jsc = 99.16859436035156\n",
      "                              FF = 280.0776062011719\n",
      "Finished epoch  225\n",
      "On epoch  226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 204.7356414794922\n",
      "                              Voc = 65.60301971435547\n",
      "                              Jsc = 99.07451629638672\n",
      "                              FF = 277.6942138671875\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 231.13885498046875\n",
      "                              Voc = 65.66078186035156\n",
      "                              Jsc = 99.14125061035156\n",
      "                              FF = 276.4752502441406\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 225.90374755859375\n",
      "                              Voc = 65.51283264160156\n",
      "                              Jsc = 99.20698547363281\n",
      "                              FF = 282.9842224121094\n",
      "Finished epoch  228\n",
      "On epoch  229\n",
      "Total Epoch Testing MAPE: PCE = 243.5194091796875\n",
      "                              Voc = 65.42516326904297\n",
      "                              Jsc = 98.77074432373047\n",
      "                              FF = 286.93902587890625\n",
      "Finished epoch  229\n",
      "On epoch  230\n",
      "Total Epoch Testing MAPE: PCE = 207.35401916503906\n",
      "                              Voc = 64.94020080566406\n",
      "                              Jsc = 98.92788696289062\n",
      "                              FF = 283.31884765625\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 227.26673889160156\n",
      "                              Voc = 64.68632507324219\n",
      "                              Jsc = 99.57207489013672\n",
      "                              FF = 278.0025329589844\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 228.62257385253906\n",
      "                              Voc = 64.6657943725586\n",
      "                              Jsc = 99.21373748779297\n",
      "                              FF = 278.6934509277344\n",
      "Finished epoch  232\n",
      "On epoch  233\n",
      "Total Epoch Testing MAPE: PCE = 221.7247772216797\n",
      "                              Voc = 64.87718200683594\n",
      "                              Jsc = 99.80072784423828\n",
      "                              FF = 279.68603515625\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 203.56228637695312\n",
      "                              Voc = 64.07721710205078\n",
      "                              Jsc = 99.92604064941406\n",
      "                              FF = 280.5288391113281\n",
      "Finished epoch  234\n",
      "On epoch  235\n",
      "Total Epoch Testing MAPE: PCE = 222.8555908203125\n",
      "                              Voc = 64.7982177734375\n",
      "                              Jsc = 99.82701110839844\n",
      "                              FF = 282.3625793457031\n",
      "Finished epoch  235\n",
      "On epoch  236\n",
      "Total Epoch Testing MAPE: PCE = 246.20578002929688\n",
      "                              Voc = 65.88416290283203\n",
      "                              Jsc = 100.0\n",
      "                              FF = 282.57562255859375\n",
      "Finished epoch  236\n",
      "On epoch  237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 252.11642456054688\n",
      "                              Voc = 66.0375747680664\n",
      "                              Jsc = 100.0\n",
      "                              FF = 279.54217529296875\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 256.0302734375\n",
      "                              Voc = 66.26740264892578\n",
      "                              Jsc = 99.99246215820312\n",
      "                              FF = 280.6231994628906\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 247.32601928710938\n",
      "                              Voc = 65.20256042480469\n",
      "                              Jsc = 99.83958435058594\n",
      "                              FF = 280.75921630859375\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 289.54571533203125\n",
      "                              Voc = 65.24720001220703\n",
      "                              Jsc = 99.73747253417969\n",
      "                              FF = 290.7327575683594\n",
      "Finished epoch  240\n",
      "On epoch  241\n",
      "Total Epoch Testing MAPE: PCE = 313.83209228515625\n",
      "                              Voc = 64.75547790527344\n",
      "                              Jsc = 100.0\n",
      "                              FF = 285.84906005859375\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 295.56622314453125\n",
      "                              Voc = 64.43402099609375\n",
      "                              Jsc = 100.0\n",
      "                              FF = 286.38653564453125\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 268.1449890136719\n",
      "                              Voc = 64.56060028076172\n",
      "                              Jsc = 100.0\n",
      "                              FF = 285.2289733886719\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 259.4622497558594\n",
      "                              Voc = 64.40563201904297\n",
      "                              Jsc = 100.0\n",
      "                              FF = 285.32891845703125\n",
      "Finished epoch  244\n",
      "On epoch  245\n",
      "Total Epoch Testing MAPE: PCE = 250.98971557617188\n",
      "                              Voc = 64.59612274169922\n",
      "                              Jsc = 100.0\n",
      "                              FF = 290.6570739746094\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 264.7167663574219\n",
      "                              Voc = 64.10945129394531\n",
      "                              Jsc = 99.94732666015625\n",
      "                              FF = 287.4400939941406\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 275.746337890625\n",
      "                              Voc = 64.32575225830078\n",
      "                              Jsc = 99.80712127685547\n",
      "                              FF = 289.3237609863281\n",
      "Finished epoch  247\n",
      "On epoch  248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 292.2740783691406\n",
      "                              Voc = 64.21459197998047\n",
      "                              Jsc = 99.94789123535156\n",
      "                              FF = 295.77777099609375\n",
      "Finished epoch  248\n",
      "On epoch  249\n",
      "Total Epoch Testing MAPE: PCE = 292.1646728515625\n",
      "                              Voc = 63.931358337402344\n",
      "                              Jsc = 99.74668884277344\n",
      "                              FF = 290.7305603027344\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 312.4761962890625\n",
      "                              Voc = 64.01738739013672\n",
      "                              Jsc = 99.38896179199219\n",
      "                              FF = 287.0756530761719\n",
      "Finished epoch  250\n",
      "On epoch  251\n",
      "Total Epoch Testing MAPE: PCE = 327.5570068359375\n",
      "                              Voc = 64.60981750488281\n",
      "                              Jsc = 98.93798065185547\n",
      "                              FF = 283.6814270019531\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 358.88140869140625\n",
      "                              Voc = 64.1158676147461\n",
      "                              Jsc = 98.91252899169922\n",
      "                              FF = 277.035400390625\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 334.1043395996094\n",
      "                              Voc = 64.25270080566406\n",
      "                              Jsc = 98.80184936523438\n",
      "                              FF = 276.4367370605469\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 348.0553894042969\n",
      "                              Voc = 63.55080032348633\n",
      "                              Jsc = 98.72122192382812\n",
      "                              FF = 274.2470397949219\n",
      "Finished epoch  254\n",
      "On epoch  255\n",
      "Total Epoch Testing MAPE: PCE = 354.9443054199219\n",
      "                              Voc = 63.587677001953125\n",
      "                              Jsc = 98.91194152832031\n",
      "                              FF = 275.0657958984375\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 367.5960693359375\n",
      "                              Voc = 63.74615478515625\n",
      "                              Jsc = 99.15350341796875\n",
      "                              FF = 278.65576171875\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 390.8776550292969\n",
      "                              Voc = 63.85108947753906\n",
      "                              Jsc = 99.07816314697266\n",
      "                              FF = 272.5286560058594\n",
      "Finished epoch  257\n",
      "On epoch  258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 410.79010009765625\n",
      "                              Voc = 63.950984954833984\n",
      "                              Jsc = 99.27377319335938\n",
      "                              FF = 265.671875\n",
      "Finished epoch  258\n",
      "On epoch  259\n",
      "Total Epoch Testing MAPE: PCE = 420.2106018066406\n",
      "                              Voc = 64.01123046875\n",
      "                              Jsc = 99.17919158935547\n",
      "                              FF = 267.9096984863281\n",
      "Finished epoch  259\n",
      "On epoch  260\n",
      "Total Epoch Testing MAPE: PCE = 380.8050537109375\n",
      "                              Voc = 63.74946975708008\n",
      "                              Jsc = 99.29457092285156\n",
      "                              FF = 275.5450744628906\n",
      "Finished epoch  260\n",
      "On epoch  261\n",
      "Total Epoch Testing MAPE: PCE = 410.6798095703125\n",
      "                              Voc = 63.738525390625\n",
      "                              Jsc = 99.63505554199219\n",
      "                              FF = 270.1284484863281\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 362.0332336425781\n",
      "                              Voc = 63.2730598449707\n",
      "                              Jsc = 99.60700225830078\n",
      "                              FF = 274.65399169921875\n",
      "Finished epoch  262\n",
      "On epoch  263\n",
      "Total Epoch Testing MAPE: PCE = 408.1907653808594\n",
      "                              Voc = 63.11906433105469\n",
      "                              Jsc = 99.81971740722656\n",
      "                              FF = 275.9071350097656\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 393.502197265625\n",
      "                              Voc = 62.83270263671875\n",
      "                              Jsc = 99.94306182861328\n",
      "                              FF = 283.7027893066406\n",
      "Finished epoch  264\n",
      "On epoch  265\n",
      "Total Epoch Testing MAPE: PCE = 409.3009338378906\n",
      "                              Voc = 63.50079345703125\n",
      "                              Jsc = 99.9873275756836\n",
      "                              FF = 287.7606201171875\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 399.79669189453125\n",
      "                              Voc = 63.803558349609375\n",
      "                              Jsc = 99.76215362548828\n",
      "                              FF = 287.0159606933594\n",
      "Finished epoch  266\n",
      "On epoch  267\n",
      "Total Epoch Testing MAPE: PCE = 390.008544921875\n",
      "                              Voc = 64.14146423339844\n",
      "                              Jsc = 99.90129089355469\n",
      "                              FF = 291.70172119140625\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 370.96527099609375\n",
      "                              Voc = 64.36962890625\n",
      "                              Jsc = 99.82910919189453\n",
      "                              FF = 292.9994201660156\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 359.1871643066406\n",
      "                              Voc = 64.1755599975586\n",
      "                              Jsc = 99.6438217163086\n",
      "                              FF = 296.111328125\n",
      "Finished epoch  269\n",
      "On epoch  270\n",
      "Total Epoch Testing MAPE: PCE = 341.4537353515625\n",
      "                              Voc = 64.44561004638672\n",
      "                              Jsc = 99.29291534423828\n",
      "                              FF = 289.58123779296875\n",
      "Finished epoch  270\n",
      "On epoch  271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 337.6731872558594\n",
      "                              Voc = 64.30592346191406\n",
      "                              Jsc = 99.58866882324219\n",
      "                              FF = 289.23175048828125\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 307.9892578125\n",
      "                              Voc = 64.2398681640625\n",
      "                              Jsc = 99.61225128173828\n",
      "                              FF = 286.2550964355469\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 298.503662109375\n",
      "                              Voc = 64.08280944824219\n",
      "                              Jsc = 99.55339050292969\n",
      "                              FF = 284.87017822265625\n",
      "Finished epoch  273\n",
      "On epoch  274\n",
      "Total Epoch Testing MAPE: PCE = 327.6953125\n",
      "                              Voc = 64.30303192138672\n",
      "                              Jsc = 99.31069946289062\n",
      "                              FF = 285.83221435546875\n",
      "Finished epoch  274\n",
      "On epoch  275\n",
      "Total Epoch Testing MAPE: PCE = 326.6280517578125\n",
      "                              Voc = 64.58749389648438\n",
      "                              Jsc = 99.54125213623047\n",
      "                              FF = 275.981201171875\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 315.96673583984375\n",
      "                              Voc = 65.89114379882812\n",
      "                              Jsc = 99.71727752685547\n",
      "                              FF = 273.3964538574219\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 345.60137939453125\n",
      "                              Voc = 65.38078308105469\n",
      "                              Jsc = 99.39906311035156\n",
      "                              FF = 273.9488220214844\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 361.9776916503906\n",
      "                              Voc = 65.48492431640625\n",
      "                              Jsc = 99.28289794921875\n",
      "                              FF = 280.9804382324219\n",
      "Finished epoch  278\n",
      "On epoch  279\n",
      "Total Epoch Testing MAPE: PCE = 409.10687255859375\n",
      "                              Voc = 65.36101531982422\n",
      "                              Jsc = 99.27497100830078\n",
      "                              FF = 280.6133728027344\n",
      "Finished epoch  279\n",
      "On epoch  280\n",
      "Total Epoch Testing MAPE: PCE = 425.94476318359375\n",
      "                              Voc = 65.53248596191406\n",
      "                              Jsc = 99.07037353515625\n",
      "                              FF = 278.9375915527344\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 368.1798400878906\n",
      "                              Voc = 65.17724609375\n",
      "                              Jsc = 98.97726440429688\n",
      "                              FF = 282.9765930175781\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 396.68768310546875\n",
      "                              Voc = 65.55274963378906\n",
      "                              Jsc = 99.01490020751953\n",
      "                              FF = 286.8026123046875\n",
      "Finished epoch  282\n",
      "On epoch  283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 368.656005859375\n",
      "                              Voc = 65.34869384765625\n",
      "                              Jsc = 98.91445922851562\n",
      "                              FF = 281.29010009765625\n",
      "Finished epoch  283\n",
      "On epoch  284\n",
      "Total Epoch Testing MAPE: PCE = 375.2036437988281\n",
      "                              Voc = 64.91493225097656\n",
      "                              Jsc = 99.20414733886719\n",
      "                              FF = 281.4614562988281\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 383.3086853027344\n",
      "                              Voc = 64.76792907714844\n",
      "                              Jsc = 99.33208465576172\n",
      "                              FF = 291.02691650390625\n",
      "Finished epoch  285\n",
      "On epoch  286\n",
      "Total Epoch Testing MAPE: PCE = 397.6506042480469\n",
      "                              Voc = 65.1455078125\n",
      "                              Jsc = 99.31816864013672\n",
      "                              FF = 289.65972900390625\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 370.8052062988281\n",
      "                              Voc = 65.37259674072266\n",
      "                              Jsc = 99.41343688964844\n",
      "                              FF = 296.2528076171875\n",
      "Finished epoch  287\n",
      "On epoch  288\n",
      "Total Epoch Testing MAPE: PCE = 357.6695861816406\n",
      "                              Voc = 64.92471313476562\n",
      "                              Jsc = 99.10491943359375\n",
      "                              FF = 292.2399597167969\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 375.9576416015625\n",
      "                              Voc = 64.95948791503906\n",
      "                              Jsc = 98.72526550292969\n",
      "                              FF = 295.82659912109375\n",
      "Finished epoch  289\n",
      "On epoch  290\n",
      "Total Epoch Testing MAPE: PCE = 379.830810546875\n",
      "                              Voc = 64.92889404296875\n",
      "                              Jsc = 98.75994873046875\n",
      "                              FF = 294.95220947265625\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 404.0482177734375\n",
      "                              Voc = 65.65469360351562\n",
      "                              Jsc = 98.9848403930664\n",
      "                              FF = 293.1455993652344\n",
      "Finished epoch  291\n",
      "On epoch  292\n",
      "Total Epoch Testing MAPE: PCE = 397.4962158203125\n",
      "                              Voc = 65.70587921142578\n",
      "                              Jsc = 99.2170639038086\n",
      "                              FF = 293.62457275390625\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 404.7578430175781\n",
      "                              Voc = 65.7537612915039\n",
      "                              Jsc = 98.83705139160156\n",
      "                              FF = 295.9740905761719\n",
      "Finished epoch  293\n",
      "On epoch  294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 398.4504089355469\n",
      "                              Voc = 65.62178802490234\n",
      "                              Jsc = 98.82304382324219\n",
      "                              FF = 296.7017517089844\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 388.20953369140625\n",
      "                              Voc = 66.21452331542969\n",
      "                              Jsc = 99.26517486572266\n",
      "                              FF = 300.49041748046875\n",
      "Finished epoch  295\n",
      "On epoch  296\n",
      "Total Epoch Testing MAPE: PCE = 391.1348876953125\n",
      "                              Voc = 65.92768859863281\n",
      "                              Jsc = 99.33238983154297\n",
      "                              FF = 301.831787109375\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 394.09881591796875\n",
      "                              Voc = 65.63760375976562\n",
      "                              Jsc = 99.14208984375\n",
      "                              FF = 301.8786315917969\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 395.59161376953125\n",
      "                              Voc = 65.66233825683594\n",
      "                              Jsc = 99.21854400634766\n",
      "                              FF = 305.670654296875\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 396.4761657714844\n",
      "                              Voc = 65.23653411865234\n",
      "                              Jsc = 99.43383026123047\n",
      "                              FF = 309.3027648925781\n",
      "Finished epoch  299\n",
      "On epoch  300\n",
      "Total Epoch Testing MAPE: PCE = 367.3353576660156\n",
      "                              Voc = 64.91818237304688\n",
      "                              Jsc = 99.21733093261719\n",
      "                              FF = 307.6548767089844\n",
      "Finished epoch  300\n",
      "On epoch  301\n",
      "Total Epoch Testing MAPE: PCE = 411.6140441894531\n",
      "                              Voc = 64.78614044189453\n",
      "                              Jsc = 99.1517333984375\n",
      "                              FF = 309.7605285644531\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 446.5828857421875\n",
      "                              Voc = 64.51826477050781\n",
      "                              Jsc = 99.18819427490234\n",
      "                              FF = 310.51422119140625\n",
      "Finished epoch  302\n",
      "On epoch  303\n",
      "Total Epoch Testing MAPE: PCE = 432.27117919921875\n",
      "                              Voc = 64.5391616821289\n",
      "                              Jsc = 98.89999389648438\n",
      "                              FF = 305.9675598144531\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 435.12994384765625\n",
      "                              Voc = 64.8421630859375\n",
      "                              Jsc = 98.27446746826172\n",
      "                              FF = 307.7845764160156\n",
      "Finished epoch  304\n",
      "On epoch  305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 458.1940612792969\n",
      "                              Voc = 65.12522888183594\n",
      "                              Jsc = 98.10382080078125\n",
      "                              FF = 309.2173767089844\n",
      "Finished epoch  305\n",
      "On epoch  306\n",
      "Total Epoch Testing MAPE: PCE = 432.7782287597656\n",
      "                              Voc = 64.55770111083984\n",
      "                              Jsc = 98.35635375976562\n",
      "                              FF = 300.2928466796875\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 420.1354064941406\n",
      "                              Voc = 64.14488220214844\n",
      "                              Jsc = 98.18498229980469\n",
      "                              FF = 303.3870849609375\n",
      "Finished epoch  307\n",
      "On epoch  308\n",
      "Total Epoch Testing MAPE: PCE = 414.235595703125\n",
      "                              Voc = 63.91075897216797\n",
      "                              Jsc = 98.40482330322266\n",
      "                              FF = 302.5388488769531\n",
      "Finished epoch  308\n",
      "On epoch  309\n",
      "Total Epoch Testing MAPE: PCE = 419.94305419921875\n",
      "                              Voc = 63.974571228027344\n",
      "                              Jsc = 98.7306137084961\n",
      "                              FF = 291.7010192871094\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 455.0126953125\n",
      "                              Voc = 64.19281005859375\n",
      "                              Jsc = 98.46519470214844\n",
      "                              FF = 291.3262634277344\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 508.5194091796875\n",
      "                              Voc = 64.071533203125\n",
      "                              Jsc = 98.45580291748047\n",
      "                              FF = 290.78857421875\n",
      "Finished epoch  311\n",
      "On epoch  312\n",
      "Total Epoch Testing MAPE: PCE = 498.6876525878906\n",
      "                              Voc = 63.706260681152344\n",
      "                              Jsc = 98.80492401123047\n",
      "                              FF = 294.583740234375\n",
      "Finished epoch  312\n",
      "On epoch  313\n",
      "Total Epoch Testing MAPE: PCE = 492.51202392578125\n",
      "                              Voc = 63.4501953125\n",
      "                              Jsc = 99.01567077636719\n",
      "                              FF = 295.44500732421875\n",
      "Finished epoch  313\n",
      "On epoch  314\n",
      "Total Epoch Testing MAPE: PCE = 488.7266540527344\n",
      "                              Voc = 64.33878326416016\n",
      "                              Jsc = 99.37059020996094\n",
      "                              FF = 295.1319580078125\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 491.847900390625\n",
      "                              Voc = 64.60850524902344\n",
      "                              Jsc = 99.5487289428711\n",
      "                              FF = 294.6856689453125\n",
      "Finished epoch  315\n",
      "On epoch  316\n",
      "Total Epoch Testing MAPE: PCE = 500.5068664550781\n",
      "                              Voc = 64.64783477783203\n",
      "                              Jsc = 99.45606231689453\n",
      "                              FF = 303.8189392089844\n",
      "Finished epoch  316\n",
      "On epoch  317\n",
      "Total Epoch Testing MAPE: PCE = 485.6621398925781\n",
      "                              Voc = 65.13585662841797\n",
      "                              Jsc = 99.63494110107422\n",
      "                              FF = 304.05126953125\n",
      "Finished epoch  317\n",
      "On epoch  318\n",
      "Total Epoch Testing MAPE: PCE = 472.3577880859375\n",
      "                              Voc = 65.28631591796875\n",
      "                              Jsc = 99.74369049072266\n",
      "                              FF = 305.2854919433594\n",
      "Finished epoch  318\n",
      "On epoch  319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 446.1849670410156\n",
      "                              Voc = 65.01142120361328\n",
      "                              Jsc = 100.0\n",
      "                              FF = 303.0440673828125\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 468.427978515625\n",
      "                              Voc = 64.71304321289062\n",
      "                              Jsc = 99.99714660644531\n",
      "                              FF = 302.8524475097656\n",
      "Finished epoch  320\n",
      "On epoch  321\n",
      "Total Epoch Testing MAPE: PCE = 414.6878662109375\n",
      "                              Voc = 64.86339569091797\n",
      "                              Jsc = 99.96321868896484\n",
      "                              FF = 297.3305358886719\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 485.3329162597656\n",
      "                              Voc = 65.1983413696289\n",
      "                              Jsc = 99.90750885009766\n",
      "                              FF = 291.8982238769531\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 488.5810241699219\n",
      "                              Voc = 65.66621398925781\n",
      "                              Jsc = 99.87812042236328\n",
      "                              FF = 291.3321228027344\n",
      "Finished epoch  323\n",
      "On epoch  324\n",
      "Total Epoch Testing MAPE: PCE = 466.576904296875\n",
      "                              Voc = 65.76085662841797\n",
      "                              Jsc = 99.99984741210938\n",
      "                              FF = 292.789306640625\n",
      "Finished epoch  324\n",
      "On epoch  325\n",
      "Total Epoch Testing MAPE: PCE = 433.276123046875\n",
      "                              Voc = 66.27435302734375\n",
      "                              Jsc = 99.74829864501953\n",
      "                              FF = 294.0936584472656\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 414.650634765625\n",
      "                              Voc = 66.01441192626953\n",
      "                              Jsc = 99.88262939453125\n",
      "                              FF = 289.989013671875\n",
      "Finished epoch  326\n",
      "On epoch  327\n",
      "Total Epoch Testing MAPE: PCE = 425.40582275390625\n",
      "                              Voc = 65.97740173339844\n",
      "                              Jsc = 100.0\n",
      "                              FF = 286.1702575683594\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 389.25634765625\n",
      "                              Voc = 66.07882690429688\n",
      "                              Jsc = 100.0\n",
      "                              FF = 284.0699768066406\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 425.7978820800781\n",
      "                              Voc = 66.56675720214844\n",
      "                              Jsc = 100.0\n",
      "                              FF = 279.023681640625\n",
      "Finished epoch  329\n",
      "On epoch  330\n",
      "Total Epoch Testing MAPE: PCE = 446.03057861328125\n",
      "                              Voc = 66.1465072631836\n",
      "                              Jsc = 99.6150894165039\n",
      "                              FF = 281.4819030761719\n",
      "Finished epoch  330\n",
      "On epoch  331\n",
      "Total Epoch Testing MAPE: PCE = 457.82708740234375\n",
      "                              Voc = 65.97017669677734\n",
      "                              Jsc = 99.58168029785156\n",
      "                              FF = 273.75823974609375\n",
      "Finished epoch  331\n",
      "On epoch  332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 447.1382751464844\n",
      "                              Voc = 65.0187759399414\n",
      "                              Jsc = 99.51042938232422\n",
      "                              FF = 277.61505126953125\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 471.2760925292969\n",
      "                              Voc = 64.81267547607422\n",
      "                              Jsc = 100.0\n",
      "                              FF = 281.59881591796875\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 473.7803955078125\n",
      "                              Voc = 64.587890625\n",
      "                              Jsc = 99.99469757080078\n",
      "                              FF = 284.8900451660156\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 528.6248168945312\n",
      "                              Voc = 64.8077163696289\n",
      "                              Jsc = 99.89332580566406\n",
      "                              FF = 286.30908203125\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 534.7711181640625\n",
      "                              Voc = 64.87251281738281\n",
      "                              Jsc = 99.86373901367188\n",
      "                              FF = 287.7748107910156\n",
      "Finished epoch  336\n",
      "On epoch  337\n",
      "Total Epoch Testing MAPE: PCE = 572.7597045898438\n",
      "                              Voc = 64.93024444580078\n",
      "                              Jsc = 99.87107849121094\n",
      "                              FF = 280.61376953125\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 573.52099609375\n",
      "                              Voc = 65.37449645996094\n",
      "                              Jsc = 99.76296997070312\n",
      "                              FF = 283.7281799316406\n",
      "Finished epoch  338\n",
      "On epoch  339\n",
      "Total Epoch Testing MAPE: PCE = 499.4393310546875\n",
      "                              Voc = 65.52014923095703\n",
      "                              Jsc = 99.3748779296875\n",
      "                              FF = 272.7224426269531\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 476.1614685058594\n",
      "                              Voc = 64.77149963378906\n",
      "                              Jsc = 99.24756622314453\n",
      "                              FF = 277.0899353027344\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 452.5187072753906\n",
      "                              Voc = 64.62681579589844\n",
      "                              Jsc = 99.32238006591797\n",
      "                              FF = 278.2669677734375\n",
      "Finished epoch  341\n",
      "On epoch  342\n",
      "Total Epoch Testing MAPE: PCE = 461.2998962402344\n",
      "                              Voc = 64.7210922241211\n",
      "                              Jsc = 98.9704818725586\n",
      "                              FF = 282.9033203125\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 415.1900634765625\n",
      "                              Voc = 64.53369140625\n",
      "                              Jsc = 99.14293670654297\n",
      "                              FF = 284.9158020019531\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 375.72222900390625\n",
      "                              Voc = 64.56717681884766\n",
      "                              Jsc = 98.9015121459961\n",
      "                              FF = 287.5115661621094\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 393.5611267089844\n",
      "                              Voc = 65.16605377197266\n",
      "                              Jsc = 99.00531005859375\n",
      "                              FF = 281.6642150878906\n",
      "Finished epoch  345\n",
      "On epoch  346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 401.2377624511719\n",
      "                              Voc = 65.21987915039062\n",
      "                              Jsc = 98.9871826171875\n",
      "                              FF = 285.4627990722656\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 406.3238220214844\n",
      "                              Voc = 65.04202270507812\n",
      "                              Jsc = 99.05906677246094\n",
      "                              FF = 288.3586120605469\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 391.27294921875\n",
      "                              Voc = 64.91545867919922\n",
      "                              Jsc = 98.96238708496094\n",
      "                              FF = 291.2567443847656\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 423.31146240234375\n",
      "                              Voc = 64.72675323486328\n",
      "                              Jsc = 98.89462280273438\n",
      "                              FF = 289.04510498046875\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 432.8322448730469\n",
      "                              Voc = 65.261474609375\n",
      "                              Jsc = 98.76988220214844\n",
      "                              FF = 296.558349609375\n",
      "Finished epoch  350\n",
      "On epoch  351\n",
      "Total Epoch Testing MAPE: PCE = 479.52398681640625\n",
      "                              Voc = 65.00225067138672\n",
      "                              Jsc = 98.71931457519531\n",
      "                              FF = 295.2837219238281\n",
      "Finished epoch  351\n",
      "On epoch  352\n",
      "Total Epoch Testing MAPE: PCE = 514.845703125\n",
      "                              Voc = 64.86446380615234\n",
      "                              Jsc = 98.63909149169922\n",
      "                              FF = 302.25860595703125\n",
      "Finished epoch  352\n",
      "On epoch  353\n",
      "Total Epoch Testing MAPE: PCE = 585.8709716796875\n",
      "                              Voc = 64.37445068359375\n",
      "                              Jsc = 98.49978637695312\n",
      "                              FF = 301.5690002441406\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 565.2323608398438\n",
      "                              Voc = 64.42903137207031\n",
      "                              Jsc = 98.55915832519531\n",
      "                              FF = 300.0716552734375\n",
      "Finished epoch  354\n",
      "On epoch  355\n",
      "Total Epoch Testing MAPE: PCE = 531.0242919921875\n",
      "                              Voc = 64.3260269165039\n",
      "                              Jsc = 98.72477722167969\n",
      "                              FF = 299.23175048828125\n",
      "Finished epoch  355\n",
      "On epoch  356\n",
      "Total Epoch Testing MAPE: PCE = 541.2030029296875\n",
      "                              Voc = 64.19835662841797\n",
      "                              Jsc = 98.84303283691406\n",
      "                              FF = 299.35504150390625\n",
      "Finished epoch  356\n",
      "On epoch  357\n",
      "Total Epoch Testing MAPE: PCE = 523.6798095703125\n",
      "                              Voc = 65.1966781616211\n",
      "                              Jsc = 98.67863464355469\n",
      "                              FF = 297.261962890625\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 511.3753662109375\n",
      "                              Voc = 65.21961975097656\n",
      "                              Jsc = 98.64398193359375\n",
      "                              FF = 295.93328857421875\n",
      "Finished epoch  358\n",
      "On epoch  359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 542.8528442382812\n",
      "                              Voc = 65.59095764160156\n",
      "                              Jsc = 98.49010467529297\n",
      "                              FF = 291.4168701171875\n",
      "Finished epoch  359\n",
      "On epoch  360\n",
      "Total Epoch Testing MAPE: PCE = 546.0319213867188\n",
      "                              Voc = 64.75305938720703\n",
      "                              Jsc = 98.80097198486328\n",
      "                              FF = 295.78094482421875\n",
      "Finished epoch  360\n",
      "On epoch  361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 520.4244384765625\n",
      "                              Voc = 64.815673828125\n",
      "                              Jsc = 99.20008850097656\n",
      "                              FF = 293.75860595703125\n",
      "Finished epoch  361\n",
      "On epoch  362\n",
      "Total Epoch Testing MAPE: PCE = 498.5605773925781\n",
      "                              Voc = 64.92162322998047\n",
      "                              Jsc = 98.94547271728516\n",
      "                              FF = 293.2004699707031\n",
      "Finished epoch  362\n",
      "On epoch  363\n",
      "Total Epoch Testing MAPE: PCE = 463.65545654296875\n",
      "                              Voc = 65.0093002319336\n",
      "                              Jsc = 98.9745101928711\n",
      "                              FF = 294.3209228515625\n",
      "Finished epoch  363\n",
      "On epoch  364\n",
      "Total Epoch Testing MAPE: PCE = 479.10308837890625\n",
      "                              Voc = 65.46327209472656\n",
      "                              Jsc = 99.08605194091797\n",
      "                              FF = 296.6360168457031\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 490.96112060546875\n",
      "                              Voc = 64.46879577636719\n",
      "                              Jsc = 99.00110626220703\n",
      "                              FF = 296.9500427246094\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 495.7623596191406\n",
      "                              Voc = 64.49971771240234\n",
      "                              Jsc = 98.98043060302734\n",
      "                              FF = 298.2742614746094\n",
      "Finished epoch  366\n",
      "On epoch  367\n",
      "Total Epoch Testing MAPE: PCE = 554.2041015625\n",
      "                              Voc = 64.94754791259766\n",
      "                              Jsc = 98.89302062988281\n",
      "                              FF = 297.39520263671875\n",
      "Finished epoch  367\n",
      "On epoch  368\n",
      "Total Epoch Testing MAPE: PCE = 574.3710327148438\n",
      "                              Voc = 64.90997314453125\n",
      "                              Jsc = 98.83246612548828\n",
      "                              FF = 297.26397705078125\n",
      "Finished epoch  368\n",
      "On epoch  369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 569.0629272460938\n",
      "                              Voc = 64.603759765625\n",
      "                              Jsc = 98.63955688476562\n",
      "                              FF = 300.0314636230469\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 574.44873046875\n",
      "                              Voc = 64.26836395263672\n",
      "                              Jsc = 98.55856323242188\n",
      "                              FF = 299.0606994628906\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 580.1521606445312\n",
      "                              Voc = 64.02008819580078\n",
      "                              Jsc = 98.46430969238281\n",
      "                              FF = 306.7209777832031\n",
      "Finished epoch  371\n",
      "On epoch  372\n",
      "Total Epoch Testing MAPE: PCE = 593.1170043945312\n",
      "                              Voc = 64.62487030029297\n",
      "                              Jsc = 98.73145294189453\n",
      "                              FF = 296.04736328125\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 609.8927612304688\n",
      "                              Voc = 64.60527801513672\n",
      "                              Jsc = 98.75210571289062\n",
      "                              FF = 291.33282470703125\n",
      "Finished epoch  373\n",
      "On epoch  374\n",
      "Total Epoch Testing MAPE: PCE = 619.9053344726562\n",
      "                              Voc = 64.82638549804688\n",
      "                              Jsc = 98.47761535644531\n",
      "                              FF = 287.4322509765625\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 605.0472412109375\n",
      "                              Voc = 64.72225189208984\n",
      "                              Jsc = 98.1252212524414\n",
      "                              FF = 287.575927734375\n",
      "Finished epoch  375\n",
      "On epoch  376\n",
      "Total Epoch Testing MAPE: PCE = 623.3848876953125\n",
      "                              Voc = 64.7593002319336\n",
      "                              Jsc = 98.30833435058594\n",
      "                              FF = 286.5771484375\n",
      "Finished epoch  376\n",
      "On epoch  377\n",
      "Total Epoch Testing MAPE: PCE = 614.4859619140625\n",
      "                              Voc = 65.06275939941406\n",
      "                              Jsc = 98.3652114868164\n",
      "                              FF = 282.1312561035156\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 619.119384765625\n",
      "                              Voc = 65.21412658691406\n",
      "                              Jsc = 98.60140991210938\n",
      "                              FF = 286.2152099609375\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 617.4386596679688\n",
      "                              Voc = 65.225830078125\n",
      "                              Jsc = 98.62783813476562\n",
      "                              FF = 290.2215270996094\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 667.007080078125\n",
      "                              Voc = 64.51748657226562\n",
      "                              Jsc = 98.52012634277344\n",
      "                              FF = 289.2030334472656\n",
      "Finished epoch  380\n",
      "On epoch  381\n",
      "Total Epoch Testing MAPE: PCE = 666.9547729492188\n",
      "                              Voc = 64.94263458251953\n",
      "                              Jsc = 98.65563201904297\n",
      "                              FF = 287.20458984375\n",
      "Finished epoch  381\n",
      "On epoch  382\n",
      "Total Epoch Testing MAPE: PCE = 662.5302124023438\n",
      "                              Voc = 64.64020538330078\n",
      "                              Jsc = 98.99776458740234\n",
      "                              FF = 283.4001159667969\n",
      "Finished epoch  382\n",
      "On epoch  383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 640.649169921875\n",
      "                              Voc = 64.93981170654297\n",
      "                              Jsc = 99.14373016357422\n",
      "                              FF = 282.88360595703125\n",
      "Finished epoch  383\n",
      "On epoch  384\n",
      "Total Epoch Testing MAPE: PCE = 658.364990234375\n",
      "                              Voc = 65.58171844482422\n",
      "                              Jsc = 99.53905487060547\n",
      "                              FF = 285.8542785644531\n",
      "Finished epoch  384\n",
      "On epoch  385\n",
      "Total Epoch Testing MAPE: PCE = 644.3992309570312\n",
      "                              Voc = 65.71952819824219\n",
      "                              Jsc = 99.97355651855469\n",
      "                              FF = 285.2205810546875\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 637.1891479492188\n",
      "                              Voc = 65.21446990966797\n",
      "                              Jsc = 99.7034912109375\n",
      "                              FF = 287.0790100097656\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 607.2870483398438\n",
      "                              Voc = 65.00540161132812\n",
      "                              Jsc = 99.60883331298828\n",
      "                              FF = 285.8126220703125\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 640.6349487304688\n",
      "                              Voc = 64.08639526367188\n",
      "                              Jsc = 99.43878936767578\n",
      "                              FF = 288.3706359863281\n",
      "Finished epoch  388\n",
      "On epoch  389\n",
      "Total Epoch Testing MAPE: PCE = 605.3038330078125\n",
      "                              Voc = 64.14176940917969\n",
      "                              Jsc = 99.51359558105469\n",
      "                              FF = 288.9531555175781\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 637.1973876953125\n",
      "                              Voc = 64.5067138671875\n",
      "                              Jsc = 99.4592056274414\n",
      "                              FF = 292.92803955078125\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 630.500244140625\n",
      "                              Voc = 64.06207275390625\n",
      "                              Jsc = 99.15696716308594\n",
      "                              FF = 295.8533020019531\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 607.1002197265625\n",
      "                              Voc = 63.9775390625\n",
      "                              Jsc = 98.73912048339844\n",
      "                              FF = 291.0756530761719\n",
      "Finished epoch  392\n",
      "On epoch  393\n",
      "Total Epoch Testing MAPE: PCE = 660.8255615234375\n",
      "                              Voc = 63.68299865722656\n",
      "                              Jsc = 98.55116271972656\n",
      "                              FF = 291.7027282714844\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 664.5060424804688\n",
      "                              Voc = 64.1373062133789\n",
      "                              Jsc = 98.31678009033203\n",
      "                              FF = 292.00244140625\n",
      "Finished epoch  394\n",
      "On epoch  395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 593.3850708007812\n",
      "                              Voc = 64.37631225585938\n",
      "                              Jsc = 98.24007415771484\n",
      "                              FF = 299.5312194824219\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 600.4609375\n",
      "                              Voc = 64.23551177978516\n",
      "                              Jsc = 98.26161193847656\n",
      "                              FF = 298.2191467285156\n",
      "Finished epoch  396\n",
      "On epoch  397\n",
      "Total Epoch Testing MAPE: PCE = 599.1038818359375\n",
      "                              Voc = 63.616310119628906\n",
      "                              Jsc = 98.49701690673828\n",
      "                              FF = 302.3050537109375\n",
      "Finished epoch  397\n",
      "On epoch  398\n",
      "Total Epoch Testing MAPE: PCE = 585.9461669921875\n",
      "                              Voc = 63.52473068237305\n",
      "                              Jsc = 98.7357406616211\n",
      "                              FF = 300.6225891113281\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 573.2621459960938\n",
      "                              Voc = 63.41106414794922\n",
      "                              Jsc = 99.06452941894531\n",
      "                              FF = 304.36712646484375\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 585.06787109375\n",
      "                              Voc = 64.25341033935547\n",
      "                              Jsc = 99.23442077636719\n",
      "                              FF = 308.4107971191406\n",
      "Finished epoch  400\n",
      "On epoch  401\n",
      "Total Epoch Testing MAPE: PCE = 632.76904296875\n",
      "                              Voc = 65.2262191772461\n",
      "                              Jsc = 99.0662612915039\n",
      "                              FF = 303.0233459472656\n",
      "Finished epoch  401\n",
      "On epoch  402\n",
      "Total Epoch Testing MAPE: PCE = 675.8580932617188\n",
      "                              Voc = 65.13460540771484\n",
      "                              Jsc = 99.26131439208984\n",
      "                              FF = 302.741943359375\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 680.5594482421875\n",
      "                              Voc = 65.60223388671875\n",
      "                              Jsc = 99.16676330566406\n",
      "                              FF = 303.7801208496094\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 674.0130615234375\n",
      "                              Voc = 64.98137664794922\n",
      "                              Jsc = 98.84632873535156\n",
      "                              FF = 305.00079345703125\n",
      "Finished epoch  404\n",
      "On epoch  405\n",
      "Total Epoch Testing MAPE: PCE = 686.7233276367188\n",
      "                              Voc = 65.14116668701172\n",
      "                              Jsc = 99.09709930419922\n",
      "                              FF = 311.2699890136719\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 698.0353393554688\n",
      "                              Voc = 65.40122985839844\n",
      "                              Jsc = 99.37115478515625\n",
      "                              FF = 304.43939208984375\n",
      "Finished epoch  406\n",
      "On epoch  407\n",
      "Total Epoch Testing MAPE: PCE = 695.1258544921875\n",
      "                              Voc = 65.77128601074219\n",
      "                              Jsc = 99.3785171508789\n",
      "                              FF = 302.9975891113281\n",
      "Finished epoch  407\n",
      "On epoch  408\n",
      "Total Epoch Testing MAPE: PCE = 668.03173828125\n",
      "                              Voc = 66.07605743408203\n",
      "                              Jsc = 99.04946899414062\n",
      "                              FF = 302.3613586425781\n",
      "Finished epoch  408\n",
      "On epoch  409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 682.0476684570312\n",
      "                              Voc = 65.13563537597656\n",
      "                              Jsc = 99.58898162841797\n",
      "                              FF = 296.3470764160156\n",
      "Finished epoch  409\n",
      "On epoch  410\n",
      "Total Epoch Testing MAPE: PCE = 678.521484375\n",
      "                              Voc = 65.08793640136719\n",
      "                              Jsc = 99.51595306396484\n",
      "                              FF = 296.0241394042969\n",
      "Finished epoch  410\n",
      "On epoch  411\n",
      "Total Epoch Testing MAPE: PCE = 663.33740234375\n",
      "                              Voc = 64.8109359741211\n",
      "                              Jsc = 99.51297760009766\n",
      "                              FF = 286.8876037597656\n",
      "Finished epoch  411\n",
      "On epoch  412\n",
      "Total Epoch Testing MAPE: PCE = 715.77880859375\n",
      "                              Voc = 64.9307861328125\n",
      "                              Jsc = 99.23465728759766\n",
      "                              FF = 287.0072021484375\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 744.0245971679688\n",
      "                              Voc = 65.08614349365234\n",
      "                              Jsc = 99.07296752929688\n",
      "                              FF = 286.76348876953125\n",
      "Finished epoch  413\n",
      "On epoch  414\n",
      "Total Epoch Testing MAPE: PCE = 693.1650390625\n",
      "                              Voc = 64.96695709228516\n",
      "                              Jsc = 99.1712417602539\n",
      "                              FF = 287.582763671875\n",
      "Finished epoch  414\n",
      "On epoch  415\n",
      "Total Epoch Testing MAPE: PCE = 722.1805419921875\n",
      "                              Voc = 65.70001220703125\n",
      "                              Jsc = 98.74671936035156\n",
      "                              FF = 286.8333435058594\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 765.5429077148438\n",
      "                              Voc = 66.12325286865234\n",
      "                              Jsc = 98.82714080810547\n",
      "                              FF = 286.05242919921875\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 824.883544921875\n",
      "                              Voc = 66.6690444946289\n",
      "                              Jsc = 99.0250473022461\n",
      "                              FF = 289.6089172363281\n",
      "Finished epoch  417\n",
      "On epoch  418\n",
      "Total Epoch Testing MAPE: PCE = 762.1836547851562\n",
      "                              Voc = 66.56216430664062\n",
      "                              Jsc = 99.42521667480469\n",
      "                              FF = 287.34033203125\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 781.163818359375\n",
      "                              Voc = 65.67240905761719\n",
      "                              Jsc = 99.04920196533203\n",
      "                              FF = 292.4360046386719\n",
      "Finished epoch  419\n",
      "On epoch  420\n",
      "Total Epoch Testing MAPE: PCE = 809.129150390625\n",
      "                              Voc = 65.8524398803711\n",
      "                              Jsc = 99.23656463623047\n",
      "                              FF = 294.7991638183594\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 868.6825561523438\n",
      "                              Voc = 65.89058685302734\n",
      "                              Jsc = 99.36289978027344\n",
      "                              FF = 296.8422546386719\n",
      "Finished epoch  421\n",
      "On epoch  422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 855.4512939453125\n",
      "                              Voc = 65.35681915283203\n",
      "                              Jsc = 99.210205078125\n",
      "                              FF = 295.5019226074219\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 855.983154296875\n",
      "                              Voc = 65.35562896728516\n",
      "                              Jsc = 99.73834991455078\n",
      "                              FF = 291.64093017578125\n",
      "Finished epoch  423\n",
      "On epoch  424\n",
      "Total Epoch Testing MAPE: PCE = 836.8097534179688\n",
      "                              Voc = 64.87728118896484\n",
      "                              Jsc = 99.52102661132812\n",
      "                              FF = 289.6330261230469\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 786.9865112304688\n",
      "                              Voc = 64.51707458496094\n",
      "                              Jsc = 99.50827026367188\n",
      "                              FF = 295.6292419433594\n",
      "Finished epoch  425\n",
      "On epoch  426\n",
      "Total Epoch Testing MAPE: PCE = 742.358642578125\n",
      "                              Voc = 64.04705047607422\n",
      "                              Jsc = 99.6676025390625\n",
      "                              FF = 300.0047302246094\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 742.2268676757812\n",
      "                              Voc = 63.79475402832031\n",
      "                              Jsc = 99.64691925048828\n",
      "                              FF = 295.967041015625\n",
      "Finished epoch  427\n",
      "On epoch  428\n",
      "Total Epoch Testing MAPE: PCE = 755.3287963867188\n",
      "                              Voc = 63.44352340698242\n",
      "                              Jsc = 99.81791687011719\n",
      "                              FF = 297.0678405761719\n",
      "Finished epoch  428\n",
      "On epoch  429\n",
      "Total Epoch Testing MAPE: PCE = 738.7015380859375\n",
      "                              Voc = 63.67396545410156\n",
      "                              Jsc = 99.54055786132812\n",
      "                              FF = 300.1061096191406\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 750.5574951171875\n",
      "                              Voc = 64.07503509521484\n",
      "                              Jsc = 99.8530502319336\n",
      "                              FF = 302.96942138671875\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 713.71923828125\n",
      "                              Voc = 64.82006072998047\n",
      "                              Jsc = 99.94843292236328\n",
      "                              FF = 303.45135498046875\n",
      "Finished epoch  431\n",
      "On epoch  432\n",
      "Total Epoch Testing MAPE: PCE = 747.0054321289062\n",
      "                              Voc = 64.95667266845703\n",
      "                              Jsc = 99.35293579101562\n",
      "                              FF = 303.3403625488281\n",
      "Finished epoch  432\n",
      "On epoch  433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 757.5294189453125\n",
      "                              Voc = 65.40013122558594\n",
      "                              Jsc = 99.32553100585938\n",
      "                              FF = 300.04156494140625\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 807.5731201171875\n",
      "                              Voc = 65.22865295410156\n",
      "                              Jsc = 98.83503723144531\n",
      "                              FF = 297.25653076171875\n",
      "Finished epoch  434\n",
      "On epoch  435\n",
      "Total Epoch Testing MAPE: PCE = 785.2261352539062\n",
      "                              Voc = 65.41978454589844\n",
      "                              Jsc = 98.98100280761719\n",
      "                              FF = 296.6822204589844\n",
      "Finished epoch  435\n",
      "On epoch  436\n",
      "Total Epoch Testing MAPE: PCE = 780.6812744140625\n",
      "                              Voc = 65.09476470947266\n",
      "                              Jsc = 99.18620300292969\n",
      "                              FF = 288.19940185546875\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 802.1070556640625\n",
      "                              Voc = 64.41925811767578\n",
      "                              Jsc = 99.29798889160156\n",
      "                              FF = 282.8484802246094\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 762.0402221679688\n",
      "                              Voc = 64.40930938720703\n",
      "                              Jsc = 99.70997619628906\n",
      "                              FF = 276.587646484375\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 831.1253662109375\n",
      "                              Voc = 64.76412963867188\n",
      "                              Jsc = 99.83612060546875\n",
      "                              FF = 273.48876953125\n",
      "Finished epoch  439\n",
      "On epoch  440\n",
      "Total Epoch Testing MAPE: PCE = 832.5941162109375\n",
      "                              Voc = 64.98201751708984\n",
      "                              Jsc = 99.79679870605469\n",
      "                              FF = 275.2015686035156\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 825.2103881835938\n",
      "                              Voc = 65.42100524902344\n",
      "                              Jsc = 99.45021057128906\n",
      "                              FF = 274.9550476074219\n",
      "Finished epoch  441\n",
      "On epoch  442\n",
      "Total Epoch Testing MAPE: PCE = 825.5474853515625\n",
      "                              Voc = 65.98001861572266\n",
      "                              Jsc = 99.43675994873047\n",
      "                              FF = 282.16851806640625\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 740.860595703125\n",
      "                              Voc = 65.49791717529297\n",
      "                              Jsc = 99.56745147705078\n",
      "                              FF = 279.0084228515625\n",
      "Finished epoch  443\n",
      "On epoch  444\n",
      "Total Epoch Testing MAPE: PCE = 738.9649047851562\n",
      "                              Voc = 65.24169921875\n",
      "                              Jsc = 99.41539764404297\n",
      "                              FF = 282.9230651855469\n",
      "Finished epoch  444\n",
      "On epoch  445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 778.74072265625\n",
      "                              Voc = 65.42779541015625\n",
      "                              Jsc = 99.65789031982422\n",
      "                              FF = 287.1078796386719\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 808.5015258789062\n",
      "                              Voc = 64.77066040039062\n",
      "                              Jsc = 100.0\n",
      "                              FF = 285.7160339355469\n",
      "Finished epoch  446\n",
      "On epoch  447\n",
      "Total Epoch Testing MAPE: PCE = 805.46435546875\n",
      "                              Voc = 64.22027587890625\n",
      "                              Jsc = 100.0\n",
      "                              FF = 293.9796447753906\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 865.6663818359375\n",
      "                              Voc = 64.13927459716797\n",
      "                              Jsc = 100.0\n",
      "                              FF = 291.278076171875\n",
      "Finished epoch  448\n",
      "On epoch  449\n",
      "Total Epoch Testing MAPE: PCE = 855.2844848632812\n",
      "                              Voc = 63.92012405395508\n",
      "                              Jsc = 99.82656860351562\n",
      "                              FF = 286.6512756347656\n",
      "Finished epoch  449\n",
      "On epoch  450\n",
      "Total Epoch Testing MAPE: PCE = 844.1392822265625\n",
      "                              Voc = 64.5343246459961\n",
      "                              Jsc = 99.67491149902344\n",
      "                              FF = 289.33758544921875\n",
      "Finished epoch  450\n",
      "On epoch  451\n",
      "Total Epoch Testing MAPE: PCE = 806.0050048828125\n",
      "                              Voc = 64.80680847167969\n",
      "                              Jsc = 99.46464538574219\n",
      "                              FF = 288.99139404296875\n",
      "Finished epoch  451\n",
      "On epoch  452\n",
      "Total Epoch Testing MAPE: PCE = 804.6068115234375\n",
      "                              Voc = 64.99681091308594\n",
      "                              Jsc = 99.3563003540039\n",
      "                              FF = 290.5735168457031\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 836.8052368164062\n",
      "                              Voc = 64.74378967285156\n",
      "                              Jsc = 99.27957916259766\n",
      "                              FF = 295.3992004394531\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 826.0665893554688\n",
      "                              Voc = 64.501953125\n",
      "                              Jsc = 99.38259887695312\n",
      "                              FF = 299.217529296875\n",
      "Finished epoch  454\n",
      "On epoch  455\n",
      "Total Epoch Testing MAPE: PCE = 801.6654052734375\n",
      "                              Voc = 64.76494598388672\n",
      "                              Jsc = 99.47608947753906\n",
      "                              FF = 301.192626953125\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 752.6683349609375\n",
      "                              Voc = 65.03277587890625\n",
      "                              Jsc = 99.81877136230469\n",
      "                              FF = 297.8299255371094\n",
      "Finished epoch  456\n",
      "On epoch  457\n",
      "Total Epoch Testing MAPE: PCE = 785.0650634765625\n",
      "                              Voc = 65.34043884277344\n",
      "                              Jsc = 99.74124908447266\n",
      "                              FF = 299.2304382324219\n",
      "Finished epoch  457\n",
      "On epoch  458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 822.9860229492188\n",
      "                              Voc = 65.48673248291016\n",
      "                              Jsc = 99.52055358886719\n",
      "                              FF = 297.2998352050781\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 821.2490844726562\n",
      "                              Voc = 65.67994689941406\n",
      "                              Jsc = 99.62799835205078\n",
      "                              FF = 290.8937072753906\n",
      "Finished epoch  459\n",
      "On epoch  460\n",
      "Total Epoch Testing MAPE: PCE = 837.1013793945312\n",
      "                              Voc = 65.35810089111328\n",
      "                              Jsc = 99.72109985351562\n",
      "                              FF = 292.7696838378906\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 855.0206909179688\n",
      "                              Voc = 65.94600677490234\n",
      "                              Jsc = 99.26239013671875\n",
      "                              FF = 298.8254699707031\n",
      "Finished epoch  461\n",
      "On epoch  462\n",
      "Total Epoch Testing MAPE: PCE = 859.4589233398438\n",
      "                              Voc = 65.53189086914062\n",
      "                              Jsc = 99.32445526123047\n",
      "                              FF = 295.6981506347656\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 898.8762817382812\n",
      "                              Voc = 65.23590087890625\n",
      "                              Jsc = 99.42491912841797\n",
      "                              FF = 304.9891052246094\n",
      "Finished epoch  463\n",
      "On epoch  464\n",
      "Total Epoch Testing MAPE: PCE = 876.069091796875\n",
      "                              Voc = 65.22603607177734\n",
      "                              Jsc = 99.08447265625\n",
      "                              FF = 301.89300537109375\n",
      "Finished epoch  464\n",
      "On epoch  465\n",
      "Total Epoch Testing MAPE: PCE = 852.900146484375\n",
      "                              Voc = 64.67841339111328\n",
      "                              Jsc = 98.80980682373047\n",
      "                              FF = 299.31915283203125\n",
      "Finished epoch  465\n",
      "On epoch  466\n",
      "Total Epoch Testing MAPE: PCE = 858.8997802734375\n",
      "                              Voc = 64.46227264404297\n",
      "                              Jsc = 98.86598205566406\n",
      "                              FF = 300.260498046875\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 826.62353515625\n",
      "                              Voc = 64.0252685546875\n",
      "                              Jsc = 99.07366180419922\n",
      "                              FF = 307.21038818359375\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 877.4296875\n",
      "                              Voc = 64.46944427490234\n",
      "                              Jsc = 98.69647979736328\n",
      "                              FF = 309.13397216796875\n",
      "Finished epoch  468\n",
      "On epoch  469\n",
      "Total Epoch Testing MAPE: PCE = 833.6134643554688\n",
      "                              Voc = 64.712158203125\n",
      "                              Jsc = 98.6532211303711\n",
      "                              FF = 309.62579345703125\n",
      "Finished epoch  469\n",
      "On epoch  470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 833.4556884765625\n",
      "                              Voc = 65.11090087890625\n",
      "                              Jsc = 98.43311309814453\n",
      "                              FF = 298.0729675292969\n",
      "Finished epoch  470\n",
      "On epoch  471\n",
      "Total Epoch Testing MAPE: PCE = 781.4606323242188\n",
      "                              Voc = 65.1103515625\n",
      "                              Jsc = 98.65350341796875\n",
      "                              FF = 298.20953369140625\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 771.1537475585938\n",
      "                              Voc = 65.83511352539062\n",
      "                              Jsc = 98.98046875\n",
      "                              FF = 307.610595703125\n",
      "Finished epoch  472\n",
      "On epoch  473\n",
      "Total Epoch Testing MAPE: PCE = 823.4339599609375\n",
      "                              Voc = 66.01514434814453\n",
      "                              Jsc = 98.48545837402344\n",
      "                              FF = 304.8053894042969\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 792.0521240234375\n",
      "                              Voc = 66.0444107055664\n",
      "                              Jsc = 98.43896484375\n",
      "                              FF = 308.03717041015625\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 786.2748413085938\n",
      "                              Voc = 66.05010986328125\n",
      "                              Jsc = 98.82831573486328\n",
      "                              FF = 296.6731872558594\n",
      "Finished epoch  475\n",
      "On epoch  476\n",
      "Total Epoch Testing MAPE: PCE = 755.151123046875\n",
      "                              Voc = 66.26597595214844\n",
      "                              Jsc = 98.92032623291016\n",
      "                              FF = 297.726318359375\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 806.2977905273438\n",
      "                              Voc = 66.07231140136719\n",
      "                              Jsc = 98.99992370605469\n",
      "                              FF = 299.3704528808594\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 786.4867553710938\n",
      "                              Voc = 65.70160675048828\n",
      "                              Jsc = 98.94849395751953\n",
      "                              FF = 303.0416259765625\n",
      "Finished epoch  478\n",
      "On epoch  479\n",
      "Total Epoch Testing MAPE: PCE = 822.6563720703125\n",
      "                              Voc = 65.7179946899414\n",
      "                              Jsc = 99.07786560058594\n",
      "                              FF = 301.3257751464844\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 798.0365600585938\n",
      "                              Voc = 65.51128387451172\n",
      "                              Jsc = 98.96910858154297\n",
      "                              FF = 303.7744445800781\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 792.6630249023438\n",
      "                              Voc = 66.7782974243164\n",
      "                              Jsc = 98.90174865722656\n",
      "                              FF = 301.55914306640625\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 787.46826171875\n",
      "                              Voc = 66.42056274414062\n",
      "                              Jsc = 99.0960693359375\n",
      "                              FF = 304.6217041015625\n",
      "Finished epoch  482\n",
      "On epoch  483\n",
      "Total Epoch Testing MAPE: PCE = 822.9539184570312\n",
      "                              Voc = 66.1731185913086\n",
      "                              Jsc = 99.13990783691406\n",
      "                              FF = 301.5802001953125\n",
      "Finished epoch  483\n",
      "On epoch  484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 856.9674682617188\n",
      "                              Voc = 65.99317932128906\n",
      "                              Jsc = 98.94596862792969\n",
      "                              FF = 303.88629150390625\n",
      "Finished epoch  484\n",
      "On epoch  485\n",
      "Total Epoch Testing MAPE: PCE = 918.3390502929688\n",
      "                              Voc = 65.9308090209961\n",
      "                              Jsc = 98.64315795898438\n",
      "                              FF = 308.8233947753906\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 929.7922973632812\n",
      "                              Voc = 66.00523376464844\n",
      "                              Jsc = 98.63209533691406\n",
      "                              FF = 296.9618225097656\n",
      "Finished epoch  486\n",
      "On epoch  487\n",
      "Total Epoch Testing MAPE: PCE = 900.6907348632812\n",
      "                              Voc = 65.31238555908203\n",
      "                              Jsc = 98.60748291015625\n",
      "                              FF = 296.8065185546875\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 925.0936279296875\n",
      "                              Voc = 64.90654754638672\n",
      "                              Jsc = 98.52619171142578\n",
      "                              FF = 298.2707214355469\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 929.5435180664062\n",
      "                              Voc = 64.98988342285156\n",
      "                              Jsc = 98.61589813232422\n",
      "                              FF = 296.9877624511719\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 990.5015869140625\n",
      "                              Voc = 65.1219711303711\n",
      "                              Jsc = 98.61701202392578\n",
      "                              FF = 293.8297119140625\n",
      "Finished epoch  490\n",
      "On epoch  491\n",
      "Total Epoch Testing MAPE: PCE = 956.1315307617188\n",
      "                              Voc = 65.12361145019531\n",
      "                              Jsc = 98.55376434326172\n",
      "                              FF = 295.2380676269531\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 939.6837158203125\n",
      "                              Voc = 65.4343032836914\n",
      "                              Jsc = 98.45372772216797\n",
      "                              FF = 294.3684997558594\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 958.666015625\n",
      "                              Voc = 65.15457153320312\n",
      "                              Jsc = 98.49662780761719\n",
      "                              FF = 293.83294677734375\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 991.587158203125\n",
      "                              Voc = 64.9892807006836\n",
      "                              Jsc = 98.25800323486328\n",
      "                              FF = 292.639404296875\n",
      "Finished epoch  494\n",
      "On epoch  495\n",
      "Total Epoch Testing MAPE: PCE = 970.4679565429688\n",
      "                              Voc = 64.71041107177734\n",
      "                              Jsc = 98.45699310302734\n",
      "                              FF = 297.0977783203125\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 950.1515502929688\n",
      "                              Voc = 64.44551086425781\n",
      "                              Jsc = 98.25857543945312\n",
      "                              FF = 296.8154602050781\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 921.6098022460938\n",
      "                              Voc = 65.10189056396484\n",
      "                              Jsc = 98.12014770507812\n",
      "                              FF = 298.8307800292969\n",
      "Finished epoch  497\n",
      "On epoch  498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 931.446044921875\n",
      "                              Voc = 65.36537170410156\n",
      "                              Jsc = 98.23303985595703\n",
      "                              FF = 295.0730895996094\n",
      "Finished epoch  498\n",
      "On epoch  499\n",
      "Total Epoch Testing MAPE: PCE = 939.6243286132812\n",
      "                              Voc = 65.1037368774414\n",
      "                              Jsc = 97.9413070678711\n",
      "                              FF = 292.9951477050781\n",
      "Finished epoch  499\n",
      "Fold # 4\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 142.23631286621094\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 118.49625396728516\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 97.64664459228516\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 88.23333740234375\n",
      "Finished epoch  2\n",
      "On epoch  3\n",
      "Total Epoch Testing MAPE: PCE = 79.24691772460938\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 77.66407012939453\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 71.4427490234375\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 65.35981750488281\n",
      "Finished epoch  4\n",
      "On epoch  5\n",
      "Total Epoch Testing MAPE: PCE = 64.33985900878906\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 61.34027862548828\n",
      "Finished epoch  5\n",
      "On epoch  6\n",
      "Total Epoch Testing MAPE: PCE = 59.807491302490234\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 56.0753288269043\n",
      "Finished epoch  6\n",
      "On epoch  7\n",
      "Total Epoch Testing MAPE: PCE = 55.606407165527344\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 98.1895980834961\n",
      "                              FF = 46.289344787597656\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 55.381752014160156\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 96.53627014160156\n",
      "                              FF = 44.06959533691406\n",
      "Finished epoch  8\n",
      "On epoch  9\n",
      "Total Epoch Testing MAPE: PCE = 56.923919677734375\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 95.0376205444336\n",
      "                              FF = 39.1772346496582\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 59.020362854003906\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 93.24343872070312\n",
      "                              FF = 34.433834075927734\n",
      "Finished epoch  10\n",
      "On epoch  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 61.60464859008789\n",
      "                              Voc = 99.699462890625\n",
      "                              Jsc = 92.95987701416016\n",
      "                              FF = 34.355865478515625\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 64.10321044921875\n",
      "                              Voc = 96.29888916015625\n",
      "                              Jsc = 92.48252868652344\n",
      "                              FF = 31.268661499023438\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 62.588375091552734\n",
      "                              Voc = 93.60427856445312\n",
      "                              Jsc = 92.08419799804688\n",
      "                              FF = 30.260601043701172\n",
      "Finished epoch  13\n",
      "On epoch  14\n",
      "Total Epoch Testing MAPE: PCE = 63.46391296386719\n",
      "                              Voc = 91.16828918457031\n",
      "                              Jsc = 91.76582336425781\n",
      "                              FF = 27.707660675048828\n",
      "Finished epoch  14\n",
      "On epoch  15\n",
      "Total Epoch Testing MAPE: PCE = 63.361148834228516\n",
      "                              Voc = 88.92549896240234\n",
      "                              Jsc = 90.36979675292969\n",
      "                              FF = 25.464969635009766\n",
      "Finished epoch  15\n",
      "On epoch  16\n",
      "Total Epoch Testing MAPE: PCE = 63.66730880737305\n",
      "                              Voc = 86.15311431884766\n",
      "                              Jsc = 89.07479858398438\n",
      "                              FF = 24.672325134277344\n",
      "Finished epoch  16\n",
      "On epoch  17\n",
      "Total Epoch Testing MAPE: PCE = 63.0289192199707\n",
      "                              Voc = 85.38127899169922\n",
      "                              Jsc = 88.01812744140625\n",
      "                              FF = 22.219987869262695\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 62.480735778808594\n",
      "                              Voc = 83.85106658935547\n",
      "                              Jsc = 87.66352844238281\n",
      "                              FF = 22.17047119140625\n",
      "Finished epoch  18\n",
      "On epoch  19\n",
      "Total Epoch Testing MAPE: PCE = 62.66706466674805\n",
      "                              Voc = 82.04332733154297\n",
      "                              Jsc = 86.87004089355469\n",
      "                              FF = 20.4455509185791\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 61.37665939331055\n",
      "                              Voc = 80.90647888183594\n",
      "                              Jsc = 85.98648071289062\n",
      "                              FF = 21.308486938476562\n",
      "Finished epoch  20\n",
      "On epoch  21\n",
      "Total Epoch Testing MAPE: PCE = 61.82545471191406\n",
      "                              Voc = 78.89128112792969\n",
      "                              Jsc = 85.10894012451172\n",
      "                              FF = 21.985639572143555\n",
      "Finished epoch  21\n",
      "On epoch  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 63.18488311767578\n",
      "                              Voc = 77.4197006225586\n",
      "                              Jsc = 84.65865325927734\n",
      "                              FF = 21.872041702270508\n",
      "Finished epoch  22\n",
      "On epoch  23\n",
      "Total Epoch Testing MAPE: PCE = 62.463043212890625\n",
      "                              Voc = 75.72199249267578\n",
      "                              Jsc = 83.37378692626953\n",
      "                              FF = 18.371152877807617\n",
      "Finished epoch  23\n",
      "On epoch  24\n",
      "Total Epoch Testing MAPE: PCE = 63.36783218383789\n",
      "                              Voc = 74.24899291992188\n",
      "                              Jsc = 82.64118194580078\n",
      "                              FF = 19.622169494628906\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 63.315887451171875\n",
      "                              Voc = 73.37158966064453\n",
      "                              Jsc = 82.85919189453125\n",
      "                              FF = 18.710071563720703\n",
      "Finished epoch  25\n",
      "On epoch  26\n",
      "Total Epoch Testing MAPE: PCE = 62.757728576660156\n",
      "                              Voc = 71.49385070800781\n",
      "                              Jsc = 82.7919921875\n",
      "                              FF = 18.68062973022461\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 64.48040771484375\n",
      "                              Voc = 72.4966049194336\n",
      "                              Jsc = 82.84452819824219\n",
      "                              FF = 17.994924545288086\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 64.77083587646484\n",
      "                              Voc = 70.73993682861328\n",
      "                              Jsc = 82.64923858642578\n",
      "                              FF = 17.430862426757812\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 64.14875793457031\n",
      "                              Voc = 70.02322387695312\n",
      "                              Jsc = 82.9512939453125\n",
      "                              FF = 16.982744216918945\n",
      "Finished epoch  29\n",
      "On epoch  30\n",
      "Total Epoch Testing MAPE: PCE = 67.2905044555664\n",
      "                              Voc = 68.18030548095703\n",
      "                              Jsc = 82.16690063476562\n",
      "                              FF = 16.051496505737305\n",
      "Finished epoch  30\n",
      "On epoch  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 67.01660919189453\n",
      "                              Voc = 67.09827423095703\n",
      "                              Jsc = 81.77420043945312\n",
      "                              FF = 16.030105590820312\n",
      "Finished epoch  31\n",
      "On epoch  32\n",
      "Total Epoch Testing MAPE: PCE = 64.73899841308594\n",
      "                              Voc = 66.28241729736328\n",
      "                              Jsc = 81.61810302734375\n",
      "                              FF = 16.652427673339844\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 64.82561492919922\n",
      "                              Voc = 66.5128402709961\n",
      "                              Jsc = 81.3441162109375\n",
      "                              FF = 16.53713035583496\n",
      "Finished epoch  33\n",
      "On epoch  34\n",
      "Total Epoch Testing MAPE: PCE = 67.82197570800781\n",
      "                              Voc = 66.81629180908203\n",
      "                              Jsc = 80.45042419433594\n",
      "                              FF = 15.395784378051758\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 67.55298614501953\n",
      "                              Voc = 68.27743530273438\n",
      "                              Jsc = 80.53006744384766\n",
      "                              FF = 14.455856323242188\n",
      "Finished epoch  35\n",
      "On epoch  36\n",
      "Total Epoch Testing MAPE: PCE = 68.66913604736328\n",
      "                              Voc = 68.07433319091797\n",
      "                              Jsc = 80.15250396728516\n",
      "                              FF = 13.49778938293457\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 69.54029083251953\n",
      "                              Voc = 69.1285171508789\n",
      "                              Jsc = 80.76673126220703\n",
      "                              FF = 13.528573989868164\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 67.8029556274414\n",
      "                              Voc = 68.82880401611328\n",
      "                              Jsc = 80.55620574951172\n",
      "                              FF = 15.274622917175293\n",
      "Finished epoch  38\n",
      "On epoch  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 67.80835723876953\n",
      "                              Voc = 69.88142395019531\n",
      "                              Jsc = 79.83138275146484\n",
      "                              FF = 15.87533187866211\n",
      "Finished epoch  39\n",
      "On epoch  40\n",
      "Total Epoch Testing MAPE: PCE = 68.20189666748047\n",
      "                              Voc = 69.9716567993164\n",
      "                              Jsc = 80.16083526611328\n",
      "                              FF = 15.80164623260498\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 68.22633361816406\n",
      "                              Voc = 70.21631622314453\n",
      "                              Jsc = 79.19258117675781\n",
      "                              FF = 15.323038101196289\n",
      "Finished epoch  41\n",
      "On epoch  42\n",
      "Total Epoch Testing MAPE: PCE = 68.1412582397461\n",
      "                              Voc = 71.4358901977539\n",
      "                              Jsc = 79.20915222167969\n",
      "                              FF = 16.034626007080078\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 68.09982299804688\n",
      "                              Voc = 70.6243667602539\n",
      "                              Jsc = 79.31880950927734\n",
      "                              FF = 16.76131248474121\n",
      "Finished epoch  43\n",
      "On epoch  44\n",
      "Total Epoch Testing MAPE: PCE = 68.70951843261719\n",
      "                              Voc = 70.63835906982422\n",
      "                              Jsc = 78.61064910888672\n",
      "                              FF = 16.166643142700195\n",
      "Finished epoch  44\n",
      "On epoch  45\n",
      "Total Epoch Testing MAPE: PCE = 68.75028228759766\n",
      "                              Voc = 71.5492935180664\n",
      "                              Jsc = 78.2178726196289\n",
      "                              FF = 16.347030639648438\n",
      "Finished epoch  45\n",
      "On epoch  46\n",
      "Total Epoch Testing MAPE: PCE = 69.04804229736328\n",
      "                              Voc = 72.21367645263672\n",
      "                              Jsc = 77.28699493408203\n",
      "                              FF = 15.856952667236328\n",
      "Finished epoch  46\n",
      "On epoch  47\n",
      "Total Epoch Testing MAPE: PCE = 68.90676879882812\n",
      "                              Voc = 71.92680358886719\n",
      "                              Jsc = 76.78059387207031\n",
      "                              FF = 15.737936019897461\n",
      "Finished epoch  47\n",
      "On epoch  48\n",
      "Total Epoch Testing MAPE: PCE = 68.05284881591797\n",
      "                              Voc = 71.12728881835938\n",
      "                              Jsc = 76.95321655273438\n",
      "                              FF = 15.81106948852539\n",
      "Finished epoch  48\n",
      "On epoch  49\n",
      "Total Epoch Testing MAPE: PCE = 66.68338775634766\n",
      "                              Voc = 71.0631103515625\n",
      "                              Jsc = 77.67430877685547\n",
      "                              FF = 14.553650856018066\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 68.55186462402344\n",
      "                              Voc = 71.5807876586914\n",
      "                              Jsc = 78.02655029296875\n",
      "                              FF = 13.959161758422852\n",
      "Finished epoch  50\n",
      "On epoch  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 69.18427276611328\n",
      "                              Voc = 71.47520446777344\n",
      "                              Jsc = 78.5219955444336\n",
      "                              FF = 12.998761177062988\n",
      "Finished epoch  51\n",
      "On epoch  52\n",
      "Total Epoch Testing MAPE: PCE = 69.00032806396484\n",
      "                              Voc = 70.55955505371094\n",
      "                              Jsc = 78.18754577636719\n",
      "                              FF = 12.443544387817383\n",
      "Finished epoch  52\n",
      "On epoch  53\n",
      "Total Epoch Testing MAPE: PCE = 69.47541809082031\n",
      "                              Voc = 70.18925476074219\n",
      "                              Jsc = 79.00213623046875\n",
      "                              FF = 13.060179710388184\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 71.04811096191406\n",
      "                              Voc = 69.44380187988281\n",
      "                              Jsc = 78.75479125976562\n",
      "                              FF = 12.485767364501953\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 71.64116668701172\n",
      "                              Voc = 68.65550231933594\n",
      "                              Jsc = 78.59967041015625\n",
      "                              FF = 13.540159225463867\n",
      "Finished epoch  55\n",
      "On epoch  56\n",
      "Total Epoch Testing MAPE: PCE = 72.30036926269531\n",
      "                              Voc = 69.32318878173828\n",
      "                              Jsc = 78.24946594238281\n",
      "                              FF = 13.177397727966309\n",
      "Finished epoch  56\n",
      "On epoch  57\n",
      "Total Epoch Testing MAPE: PCE = 71.90029907226562\n",
      "                              Voc = 69.39830780029297\n",
      "                              Jsc = 78.39386749267578\n",
      "                              FF = 11.856988906860352\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 73.07141876220703\n",
      "                              Voc = 68.81768035888672\n",
      "                              Jsc = 78.84563446044922\n",
      "                              FF = 13.480517387390137\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 72.55455017089844\n",
      "                              Voc = 68.23636627197266\n",
      "                              Jsc = 77.78690338134766\n",
      "                              FF = 12.804283142089844\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 73.2149429321289\n",
      "                              Voc = 68.16002655029297\n",
      "                              Jsc = 77.22224426269531\n",
      "                              FF = 12.607504844665527\n",
      "Finished epoch  60\n",
      "On epoch  61\n",
      "Total Epoch Testing MAPE: PCE = 72.79618072509766\n",
      "                              Voc = 68.09722137451172\n",
      "                              Jsc = 78.0779800415039\n",
      "                              FF = 11.903188705444336\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 73.90094757080078\n",
      "                              Voc = 67.41519927978516\n",
      "                              Jsc = 77.46392822265625\n",
      "                              FF = 11.55602741241455\n",
      "Finished epoch  62\n",
      "On epoch  63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 70.81856536865234\n",
      "                              Voc = 66.207275390625\n",
      "                              Jsc = 78.43631744384766\n",
      "                              FF = 11.934329986572266\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 70.81523895263672\n",
      "                              Voc = 66.35089874267578\n",
      "                              Jsc = 77.55152893066406\n",
      "                              FF = 12.590042114257812\n",
      "Finished epoch  64\n",
      "On epoch  65\n",
      "Total Epoch Testing MAPE: PCE = 72.62743377685547\n",
      "                              Voc = 65.4559326171875\n",
      "                              Jsc = 77.56959533691406\n",
      "                              FF = 12.787860870361328\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 72.52565002441406\n",
      "                              Voc = 64.70496368408203\n",
      "                              Jsc = 77.5008773803711\n",
      "                              FF = 13.120015144348145\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 72.8370590209961\n",
      "                              Voc = 64.8423080444336\n",
      "                              Jsc = 77.73646545410156\n",
      "                              FF = 12.745525360107422\n",
      "Finished epoch  67\n",
      "On epoch  68\n",
      "Total Epoch Testing MAPE: PCE = 70.41939544677734\n",
      "                              Voc = 64.4891357421875\n",
      "                              Jsc = 77.61769104003906\n",
      "                              FF = 14.115972518920898\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 72.31471252441406\n",
      "                              Voc = 65.00102233886719\n",
      "                              Jsc = 77.4683837890625\n",
      "                              FF = 13.857202529907227\n",
      "Finished epoch  69\n",
      "On epoch  70\n",
      "Total Epoch Testing MAPE: PCE = 72.65311431884766\n",
      "                              Voc = 63.502017974853516\n",
      "                              Jsc = 77.37954711914062\n",
      "                              FF = 13.748323440551758\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 73.77208709716797\n",
      "                              Voc = 63.234371185302734\n",
      "                              Jsc = 77.74944305419922\n",
      "                              FF = 14.003613471984863\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 74.74276733398438\n",
      "                              Voc = 62.52918243408203\n",
      "                              Jsc = 78.91852569580078\n",
      "                              FF = 13.796147346496582\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 74.53364562988281\n",
      "                              Voc = 62.79165267944336\n",
      "                              Jsc = 77.35645294189453\n",
      "                              FF = 13.960151672363281\n",
      "Finished epoch  73\n",
      "On epoch  74\n",
      "Total Epoch Testing MAPE: PCE = 73.36312103271484\n",
      "                              Voc = 62.21300506591797\n",
      "                              Jsc = 76.56536102294922\n",
      "                              FF = 12.93906307220459\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 73.50888061523438\n",
      "                              Voc = 62.51286315917969\n",
      "                              Jsc = 77.09725952148438\n",
      "                              FF = 13.129130363464355\n",
      "Finished epoch  75\n",
      "On epoch  76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 73.49906158447266\n",
      "                              Voc = 62.02410888671875\n",
      "                              Jsc = 77.22819519042969\n",
      "                              FF = 13.550551414489746\n",
      "Finished epoch  76\n",
      "On epoch  77\n",
      "Total Epoch Testing MAPE: PCE = 70.20405578613281\n",
      "                              Voc = 59.88508987426758\n",
      "                              Jsc = 76.90676879882812\n",
      "                              FF = 13.599178314208984\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 70.42249298095703\n",
      "                              Voc = 60.51185607910156\n",
      "                              Jsc = 76.25090789794922\n",
      "                              FF = 13.898563385009766\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 72.9127426147461\n",
      "                              Voc = 60.082489013671875\n",
      "                              Jsc = 76.25426483154297\n",
      "                              FF = 14.937214851379395\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 72.2264404296875\n",
      "                              Voc = 61.091739654541016\n",
      "                              Jsc = 76.2847900390625\n",
      "                              FF = 15.430438995361328\n",
      "Finished epoch  80\n",
      "On epoch  81\n",
      "Total Epoch Testing MAPE: PCE = 72.71820068359375\n",
      "                              Voc = 59.12209701538086\n",
      "                              Jsc = 77.62264251708984\n",
      "                              FF = 14.624096870422363\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 71.55689239501953\n",
      "                              Voc = 59.35744094848633\n",
      "                              Jsc = 78.11446380615234\n",
      "                              FF = 15.335972785949707\n",
      "Finished epoch  82\n",
      "On epoch  83\n",
      "Total Epoch Testing MAPE: PCE = 71.61478424072266\n",
      "                              Voc = 58.06481170654297\n",
      "                              Jsc = 78.62672424316406\n",
      "                              FF = 15.979461669921875\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 68.87934875488281\n",
      "                              Voc = 56.54288101196289\n",
      "                              Jsc = 78.3572998046875\n",
      "                              FF = 16.766895294189453\n",
      "Finished epoch  84\n",
      "On epoch  85\n",
      "Total Epoch Testing MAPE: PCE = 69.70179748535156\n",
      "                              Voc = 55.250091552734375\n",
      "                              Jsc = 78.96640014648438\n",
      "                              FF = 14.888998985290527\n",
      "Finished epoch  85\n",
      "On epoch  86\n",
      "Total Epoch Testing MAPE: PCE = 69.7299575805664\n",
      "                              Voc = 54.807342529296875\n",
      "                              Jsc = 78.64567565917969\n",
      "                              FF = 14.496498107910156\n",
      "Finished epoch  86\n",
      "On epoch  87\n",
      "Total Epoch Testing MAPE: PCE = 69.19829559326172\n",
      "                              Voc = 54.28938293457031\n",
      "                              Jsc = 78.62425231933594\n",
      "                              FF = 13.858957290649414\n",
      "Finished epoch  87\n",
      "On epoch  88\n",
      "Total Epoch Testing MAPE: PCE = 67.0975570678711\n",
      "                              Voc = 53.863136291503906\n",
      "                              Jsc = 78.59547424316406\n",
      "                              FF = 15.605165481567383\n",
      "Finished epoch  88\n",
      "On epoch  89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 66.92656707763672\n",
      "                              Voc = 54.36557388305664\n",
      "                              Jsc = 79.33399963378906\n",
      "                              FF = 15.30186653137207\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 64.50180053710938\n",
      "                              Voc = 54.13658142089844\n",
      "                              Jsc = 78.76032257080078\n",
      "                              FF = 15.369626998901367\n",
      "Finished epoch  90\n",
      "On epoch  91\n",
      "Total Epoch Testing MAPE: PCE = 65.46653747558594\n",
      "                              Voc = 53.60371398925781\n",
      "                              Jsc = 78.35011291503906\n",
      "                              FF = 16.106748580932617\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 66.32505798339844\n",
      "                              Voc = 53.0899658203125\n",
      "                              Jsc = 78.16890716552734\n",
      "                              FF = 15.11432933807373\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 65.35292053222656\n",
      "                              Voc = 52.94028091430664\n",
      "                              Jsc = 78.42304992675781\n",
      "                              FF = 16.199323654174805\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 67.20089721679688\n",
      "                              Voc = 53.38176345825195\n",
      "                              Jsc = 78.27925109863281\n",
      "                              FF = 15.796165466308594\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 66.73684692382812\n",
      "                              Voc = 52.9836311340332\n",
      "                              Jsc = 78.92660522460938\n",
      "                              FF = 15.677855491638184\n",
      "Finished epoch  95\n",
      "On epoch  96\n",
      "Total Epoch Testing MAPE: PCE = 68.22672271728516\n",
      "                              Voc = 52.165306091308594\n",
      "                              Jsc = 79.67803192138672\n",
      "                              FF = 13.87485122680664\n",
      "Finished epoch  96\n",
      "On epoch  97\n",
      "Total Epoch Testing MAPE: PCE = 69.09998321533203\n",
      "                              Voc = 51.49845504760742\n",
      "                              Jsc = 79.48111724853516\n",
      "                              FF = 15.35997200012207\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 69.98310852050781\n",
      "                              Voc = 50.9566650390625\n",
      "                              Jsc = 79.4171142578125\n",
      "                              FF = 14.76170825958252\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 68.70287322998047\n",
      "                              Voc = 50.49789047241211\n",
      "                              Jsc = 79.3104019165039\n",
      "                              FF = 16.246749877929688\n",
      "Finished epoch  99\n",
      "On epoch  100\n",
      "Total Epoch Testing MAPE: PCE = 68.84912109375\n",
      "                              Voc = 50.077430725097656\n",
      "                              Jsc = 79.0113296508789\n",
      "                              FF = 16.282535552978516\n",
      "Finished epoch  100\n",
      "On epoch  101\n",
      "Total Epoch Testing MAPE: PCE = 67.63700866699219\n",
      "                              Voc = 49.939544677734375\n",
      "                              Jsc = 78.11726379394531\n",
      "                              FF = 17.661611557006836\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 67.65141296386719\n",
      "                              Voc = 50.359230041503906\n",
      "                              Jsc = 78.8763198852539\n",
      "                              FF = 16.842510223388672\n",
      "Finished epoch  102\n",
      "On epoch  103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 68.4329605102539\n",
      "                              Voc = 50.33030700683594\n",
      "                              Jsc = 80.00016784667969\n",
      "                              FF = 16.697450637817383\n",
      "Finished epoch  103\n",
      "On epoch  104\n",
      "Total Epoch Testing MAPE: PCE = 69.03077697753906\n",
      "                              Voc = 50.704444885253906\n",
      "                              Jsc = 81.16512298583984\n",
      "                              FF = 16.220401763916016\n",
      "Finished epoch  104\n",
      "On epoch  105\n",
      "Total Epoch Testing MAPE: PCE = 67.15702056884766\n",
      "                              Voc = 50.76284408569336\n",
      "                              Jsc = 80.82870483398438\n",
      "                              FF = 17.504777908325195\n",
      "Finished epoch  105\n",
      "On epoch  106\n",
      "Total Epoch Testing MAPE: PCE = 67.9142074584961\n",
      "                              Voc = 50.73261260986328\n",
      "                              Jsc = 81.04073333740234\n",
      "                              FF = 16.112180709838867\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 67.60201263427734\n",
      "                              Voc = 50.743709564208984\n",
      "                              Jsc = 80.1137466430664\n",
      "                              FF = 16.10564422607422\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 68.80794525146484\n",
      "                              Voc = 50.80301284790039\n",
      "                              Jsc = 79.88558959960938\n",
      "                              FF = 16.280014038085938\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 68.90229797363281\n",
      "                              Voc = 50.87847137451172\n",
      "                              Jsc = 80.26822662353516\n",
      "                              FF = 16.16756820678711\n",
      "Finished epoch  109\n",
      "On epoch  110\n",
      "Total Epoch Testing MAPE: PCE = 69.84799194335938\n",
      "                              Voc = 51.31624221801758\n",
      "                              Jsc = 80.9259262084961\n",
      "                              FF = 15.601722717285156\n",
      "Finished epoch  110\n",
      "On epoch  111\n",
      "Total Epoch Testing MAPE: PCE = 70.229736328125\n",
      "                              Voc = 50.88845443725586\n",
      "                              Jsc = 80.47948455810547\n",
      "                              FF = 15.33059024810791\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 69.96768188476562\n",
      "                              Voc = 51.06568145751953\n",
      "                              Jsc = 80.14339447021484\n",
      "                              FF = 15.616217613220215\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 71.59019470214844\n",
      "                              Voc = 49.28544235229492\n",
      "                              Jsc = 79.33557891845703\n",
      "                              FF = 15.101705551147461\n",
      "Finished epoch  113\n",
      "On epoch  114\n",
      "Total Epoch Testing MAPE: PCE = 72.36814880371094\n",
      "                              Voc = 48.856258392333984\n",
      "                              Jsc = 77.97643280029297\n",
      "                              FF = 17.140228271484375\n",
      "Finished epoch  114\n",
      "On epoch  115\n",
      "Total Epoch Testing MAPE: PCE = 72.91499328613281\n",
      "                              Voc = 49.07643508911133\n",
      "                              Jsc = 77.56207275390625\n",
      "                              FF = 17.167890548706055\n",
      "Finished epoch  115\n",
      "On epoch  116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 72.65570831298828\n",
      "                              Voc = 49.5595588684082\n",
      "                              Jsc = 76.93490600585938\n",
      "                              FF = 17.999025344848633\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 72.66195678710938\n",
      "                              Voc = 48.83843231201172\n",
      "                              Jsc = 78.01702880859375\n",
      "                              FF = 17.238147735595703\n",
      "Finished epoch  117\n",
      "On epoch  118\n",
      "Total Epoch Testing MAPE: PCE = 71.8058090209961\n",
      "                              Voc = 49.126705169677734\n",
      "                              Jsc = 77.85057830810547\n",
      "                              FF = 17.73128318786621\n",
      "Finished epoch  118\n",
      "On epoch  119\n",
      "Total Epoch Testing MAPE: PCE = 72.84890747070312\n",
      "                              Voc = 48.1790885925293\n",
      "                              Jsc = 78.47320556640625\n",
      "                              FF = 16.48461151123047\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 71.71353149414062\n",
      "                              Voc = 48.511192321777344\n",
      "                              Jsc = 77.809326171875\n",
      "                              FF = 16.318002700805664\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 71.87089538574219\n",
      "                              Voc = 48.143638610839844\n",
      "                              Jsc = 77.88540649414062\n",
      "                              FF = 16.258773803710938\n",
      "Finished epoch  121\n",
      "On epoch  122\n",
      "Total Epoch Testing MAPE: PCE = 70.8414535522461\n",
      "                              Voc = 47.202659606933594\n",
      "                              Jsc = 78.18244934082031\n",
      "                              FF = 15.5348539352417\n",
      "Finished epoch  122\n",
      "On epoch  123\n",
      "Total Epoch Testing MAPE: PCE = 72.1482162475586\n",
      "                              Voc = 46.56431579589844\n",
      "                              Jsc = 78.56430053710938\n",
      "                              FF = 14.455522537231445\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 71.0853500366211\n",
      "                              Voc = 46.98134994506836\n",
      "                              Jsc = 78.48819732666016\n",
      "                              FF = 15.597089767456055\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 71.70496368408203\n",
      "                              Voc = 46.26292419433594\n",
      "                              Jsc = 78.67728424072266\n",
      "                              FF = 15.495522499084473\n",
      "Finished epoch  125\n",
      "On epoch  126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.38308715820312\n",
      "                              Voc = 45.16782760620117\n",
      "                              Jsc = 78.96319580078125\n",
      "                              FF = 15.259268760681152\n",
      "Finished epoch  126\n",
      "On epoch  127\n",
      "Total Epoch Testing MAPE: PCE = 75.82474517822266\n",
      "                              Voc = 44.704524993896484\n",
      "                              Jsc = 78.52961730957031\n",
      "                              FF = 13.397576332092285\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 75.60009002685547\n",
      "                              Voc = 43.96770095825195\n",
      "                              Jsc = 79.31014251708984\n",
      "                              FF = 13.441024780273438\n",
      "Finished epoch  128\n",
      "On epoch  129\n",
      "Total Epoch Testing MAPE: PCE = 75.89095306396484\n",
      "                              Voc = 43.6478385925293\n",
      "                              Jsc = 79.36968994140625\n",
      "                              FF = 13.989392280578613\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 75.42454528808594\n",
      "                              Voc = 43.29368591308594\n",
      "                              Jsc = 78.28339385986328\n",
      "                              FF = 13.42777156829834\n",
      "Finished epoch  130\n",
      "On epoch  131\n",
      "Total Epoch Testing MAPE: PCE = 74.34986114501953\n",
      "                              Voc = 43.264244079589844\n",
      "                              Jsc = 76.86540222167969\n",
      "                              FF = 13.657853126525879\n",
      "Finished epoch  131\n",
      "On epoch  132\n",
      "Total Epoch Testing MAPE: PCE = 74.27009582519531\n",
      "                              Voc = 43.47004699707031\n",
      "                              Jsc = 76.90491485595703\n",
      "                              FF = 15.047268867492676\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 73.478271484375\n",
      "                              Voc = 42.93149185180664\n",
      "                              Jsc = 77.60282135009766\n",
      "                              FF = 14.518692970275879\n",
      "Finished epoch  133\n",
      "On epoch  134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.48632049560547\n",
      "                              Voc = 45.760135650634766\n",
      "                              Jsc = 78.5142593383789\n",
      "                              FF = 16.19504737854004\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 74.3994140625\n",
      "                              Voc = 44.78361511230469\n",
      "                              Jsc = 79.53510284423828\n",
      "                              FF = 14.552728652954102\n",
      "Finished epoch  135\n",
      "On epoch  136\n",
      "Total Epoch Testing MAPE: PCE = 72.2413101196289\n",
      "                              Voc = 44.87644958496094\n",
      "                              Jsc = 79.89126586914062\n",
      "                              FF = 14.25166130065918\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 74.00151824951172\n",
      "                              Voc = 46.40021514892578\n",
      "                              Jsc = 79.75555419921875\n",
      "                              FF = 13.321402549743652\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 72.43708038330078\n",
      "                              Voc = 43.530818939208984\n",
      "                              Jsc = 78.80615234375\n",
      "                              FF = 13.659015655517578\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 69.09795379638672\n",
      "                              Voc = 42.506919860839844\n",
      "                              Jsc = 77.63391876220703\n",
      "                              FF = 14.629261016845703\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 71.33650207519531\n",
      "                              Voc = 43.26384735107422\n",
      "                              Jsc = 78.4075927734375\n",
      "                              FF = 14.851890563964844\n",
      "Finished epoch  140\n",
      "On epoch  141\n",
      "Total Epoch Testing MAPE: PCE = 70.4367904663086\n",
      "                              Voc = 42.34205627441406\n",
      "                              Jsc = 78.01960754394531\n",
      "                              FF = 15.449155807495117\n",
      "Finished epoch  141\n",
      "On epoch  142\n",
      "Total Epoch Testing MAPE: PCE = 68.52763366699219\n",
      "                              Voc = 42.24845886230469\n",
      "                              Jsc = 79.23152160644531\n",
      "                              FF = 17.38054847717285\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 69.73295593261719\n",
      "                              Voc = 42.69529724121094\n",
      "                              Jsc = 79.37785339355469\n",
      "                              FF = 18.03212547302246\n",
      "Finished epoch  143\n",
      "On epoch  144\n",
      "Total Epoch Testing MAPE: PCE = 70.52277374267578\n",
      "                              Voc = 43.69059371948242\n",
      "                              Jsc = 80.46355438232422\n",
      "                              FF = 17.34077262878418\n",
      "Finished epoch  144\n",
      "On epoch  145\n",
      "Total Epoch Testing MAPE: PCE = 70.49444580078125\n",
      "                              Voc = 44.78044891357422\n",
      "                              Jsc = 79.7372817993164\n",
      "                              FF = 16.8714599609375\n",
      "Finished epoch  145\n",
      "On epoch  146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 71.9314956665039\n",
      "                              Voc = 45.00588607788086\n",
      "                              Jsc = 79.3453598022461\n",
      "                              FF = 18.505590438842773\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 71.1173095703125\n",
      "                              Voc = 47.631927490234375\n",
      "                              Jsc = 79.33119201660156\n",
      "                              FF = 18.242259979248047\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 71.99899291992188\n",
      "                              Voc = 47.50874710083008\n",
      "                              Jsc = 79.86663818359375\n",
      "                              FF = 17.51838493347168\n",
      "Finished epoch  148\n",
      "On epoch  149\n",
      "Total Epoch Testing MAPE: PCE = 70.93626403808594\n",
      "                              Voc = 45.9759521484375\n",
      "                              Jsc = 79.50634002685547\n",
      "                              FF = 16.88628387451172\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 68.63518524169922\n",
      "                              Voc = 49.129859924316406\n",
      "                              Jsc = 79.2128677368164\n",
      "                              FF = 17.03339385986328\n",
      "Finished epoch  150\n",
      "On epoch  151\n",
      "Total Epoch Testing MAPE: PCE = 71.91011047363281\n",
      "                              Voc = 46.58470916748047\n",
      "                              Jsc = 79.64938354492188\n",
      "                              FF = 16.543609619140625\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 70.51715087890625\n",
      "                              Voc = 46.93602752685547\n",
      "                              Jsc = 79.51472473144531\n",
      "                              FF = 16.407094955444336\n",
      "Finished epoch  152\n",
      "On epoch  153\n",
      "Total Epoch Testing MAPE: PCE = 70.92597198486328\n",
      "                              Voc = 47.34162521362305\n",
      "                              Jsc = 79.9336166381836\n",
      "                              FF = 16.798295974731445\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 68.38358306884766\n",
      "                              Voc = 45.90456008911133\n",
      "                              Jsc = 80.04080200195312\n",
      "                              FF = 15.907514572143555\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 69.6238021850586\n",
      "                              Voc = 44.91359329223633\n",
      "                              Jsc = 80.36360931396484\n",
      "                              FF = 14.321764945983887\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 68.08797454833984\n",
      "                              Voc = 44.9842414855957\n",
      "                              Jsc = 80.27997589111328\n",
      "                              FF = 15.175321578979492\n",
      "Finished epoch  156\n",
      "On epoch  157\n",
      "Total Epoch Testing MAPE: PCE = 70.86431121826172\n",
      "                              Voc = 46.416385650634766\n",
      "                              Jsc = 80.2739486694336\n",
      "                              FF = 14.30626106262207\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 70.2364501953125\n",
      "                              Voc = 51.64533233642578\n",
      "                              Jsc = 80.20501708984375\n",
      "                              FF = 16.168325424194336\n",
      "Finished epoch  158\n",
      "On epoch  159\n",
      "Total Epoch Testing MAPE: PCE = 70.17914581298828\n",
      "                              Voc = 46.835227966308594\n",
      "                              Jsc = 79.60295867919922\n",
      "                              FF = 18.098459243774414\n",
      "Finished epoch  159\n",
      "On epoch  160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 68.78924560546875\n",
      "                              Voc = 44.986488342285156\n",
      "                              Jsc = 80.14026641845703\n",
      "                              FF = 16.963382720947266\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 68.53748321533203\n",
      "                              Voc = 44.512184143066406\n",
      "                              Jsc = 79.92326354980469\n",
      "                              FF = 17.190244674682617\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 66.78620147705078\n",
      "                              Voc = 46.98943328857422\n",
      "                              Jsc = 79.46887969970703\n",
      "                              FF = 17.284557342529297\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 68.38036346435547\n",
      "                              Voc = 43.414451599121094\n",
      "                              Jsc = 80.58103942871094\n",
      "                              FF = 19.08807373046875\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 69.55962371826172\n",
      "                              Voc = 43.22350311279297\n",
      "                              Jsc = 78.56163024902344\n",
      "                              FF = 17.97260284423828\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 69.20506286621094\n",
      "                              Voc = 41.64628219604492\n",
      "                              Jsc = 79.11836242675781\n",
      "                              FF = 16.747739791870117\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 70.71134185791016\n",
      "                              Voc = 42.198978424072266\n",
      "                              Jsc = 79.86311340332031\n",
      "                              FF = 15.877182960510254\n",
      "Finished epoch  166\n",
      "On epoch  167\n",
      "Total Epoch Testing MAPE: PCE = 69.67174530029297\n",
      "                              Voc = 42.76704788208008\n",
      "                              Jsc = 79.46475982666016\n",
      "                              FF = 17.078102111816406\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 68.21588897705078\n",
      "                              Voc = 44.51945877075195\n",
      "                              Jsc = 79.16001892089844\n",
      "                              FF = 16.54678726196289\n",
      "Finished epoch  168\n",
      "On epoch  169\n",
      "Total Epoch Testing MAPE: PCE = 68.38480377197266\n",
      "                              Voc = 44.780094146728516\n",
      "                              Jsc = 79.00289916992188\n",
      "                              FF = 15.68490219116211\n",
      "Finished epoch  169\n",
      "On epoch  170\n",
      "Total Epoch Testing MAPE: PCE = 70.97821044921875\n",
      "                              Voc = 44.67603302001953\n",
      "                              Jsc = 78.3973388671875\n",
      "                              FF = 17.514209747314453\n",
      "Finished epoch  170\n",
      "On epoch  171\n",
      "Total Epoch Testing MAPE: PCE = 69.89750671386719\n",
      "                              Voc = 45.79822540283203\n",
      "                              Jsc = 78.85833740234375\n",
      "                              FF = 16.732160568237305\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 71.58675384521484\n",
      "                              Voc = 48.174076080322266\n",
      "                              Jsc = 79.05908203125\n",
      "                              FF = 18.712678909301758\n",
      "Finished epoch  172\n",
      "On epoch  173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 72.5928726196289\n",
      "                              Voc = 49.55222702026367\n",
      "                              Jsc = 79.41890716552734\n",
      "                              FF = 18.504074096679688\n",
      "Finished epoch  173\n",
      "On epoch  174\n",
      "Total Epoch Testing MAPE: PCE = 70.07147979736328\n",
      "                              Voc = 48.73990249633789\n",
      "                              Jsc = 78.7450942993164\n",
      "                              FF = 18.31634521484375\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 71.01899719238281\n",
      "                              Voc = 48.657958984375\n",
      "                              Jsc = 80.04121398925781\n",
      "                              FF = 18.41000747680664\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 70.47351837158203\n",
      "                              Voc = 50.18437957763672\n",
      "                              Jsc = 80.63562774658203\n",
      "                              FF = 16.04956817626953\n",
      "Finished epoch  176\n",
      "On epoch  177\n",
      "Total Epoch Testing MAPE: PCE = 72.6661148071289\n",
      "                              Voc = 50.06877136230469\n",
      "                              Jsc = 80.1803970336914\n",
      "                              FF = 16.196704864501953\n",
      "Finished epoch  177\n",
      "On epoch  178\n",
      "Total Epoch Testing MAPE: PCE = 70.664306640625\n",
      "                              Voc = 53.089927673339844\n",
      "                              Jsc = 79.7920913696289\n",
      "                              FF = 15.788188934326172\n",
      "Finished epoch  178\n",
      "On epoch  179\n",
      "Total Epoch Testing MAPE: PCE = 72.75753784179688\n",
      "                              Voc = 51.342193603515625\n",
      "                              Jsc = 79.82683563232422\n",
      "                              FF = 15.048663139343262\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 71.31626892089844\n",
      "                              Voc = 52.51063537597656\n",
      "                              Jsc = 80.06108093261719\n",
      "                              FF = 13.888883590698242\n",
      "Finished epoch  180\n",
      "On epoch  181\n",
      "Total Epoch Testing MAPE: PCE = 73.47026824951172\n",
      "                              Voc = 51.840675354003906\n",
      "                              Jsc = 79.81026458740234\n",
      "                              FF = 12.898198127746582\n",
      "Finished epoch  181\n",
      "On epoch  182\n",
      "Total Epoch Testing MAPE: PCE = 71.68309020996094\n",
      "                              Voc = 52.02689743041992\n",
      "                              Jsc = 79.83272552490234\n",
      "                              FF = 13.283197402954102\n",
      "Finished epoch  182\n",
      "On epoch  183\n",
      "Total Epoch Testing MAPE: PCE = 67.66778564453125\n",
      "                              Voc = 51.13674545288086\n",
      "                              Jsc = 80.09291076660156\n",
      "                              FF = 13.180615425109863\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 69.14154052734375\n",
      "                              Voc = 51.803035736083984\n",
      "                              Jsc = 81.57844543457031\n",
      "                              FF = 13.80455493927002\n",
      "Finished epoch  184\n",
      "On epoch  185\n",
      "Total Epoch Testing MAPE: PCE = 67.8598403930664\n",
      "                              Voc = 52.84382247924805\n",
      "                              Jsc = 80.92858123779297\n",
      "                              FF = 13.311280250549316\n",
      "Finished epoch  185\n",
      "On epoch  186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 66.81632232666016\n",
      "                              Voc = 52.795284271240234\n",
      "                              Jsc = 80.14346313476562\n",
      "                              FF = 13.002530097961426\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 67.7688217163086\n",
      "                              Voc = 54.283905029296875\n",
      "                              Jsc = 79.50321960449219\n",
      "                              FF = 12.699821472167969\n",
      "Finished epoch  187\n",
      "On epoch  188\n",
      "Total Epoch Testing MAPE: PCE = 66.66816711425781\n",
      "                              Voc = 54.29346466064453\n",
      "                              Jsc = 79.7623062133789\n",
      "                              FF = 12.00882339477539\n",
      "Finished epoch  188\n",
      "On epoch  189\n",
      "Total Epoch Testing MAPE: PCE = 68.07818603515625\n",
      "                              Voc = 53.22209548950195\n",
      "                              Jsc = 79.90275573730469\n",
      "                              FF = 11.983083724975586\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 67.39299774169922\n",
      "                              Voc = 57.436092376708984\n",
      "                              Jsc = 81.45032501220703\n",
      "                              FF = 12.620784759521484\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 68.43230438232422\n",
      "                              Voc = 56.92958068847656\n",
      "                              Jsc = 81.71504211425781\n",
      "                              FF = 11.520249366760254\n",
      "Finished epoch  191\n",
      "On epoch  192\n",
      "Total Epoch Testing MAPE: PCE = 68.23248291015625\n",
      "                              Voc = 59.475196838378906\n",
      "                              Jsc = 82.33290100097656\n",
      "                              FF = 11.513354301452637\n",
      "Finished epoch  192\n",
      "On epoch  193\n",
      "Total Epoch Testing MAPE: PCE = 67.78936767578125\n",
      "                              Voc = 60.62955093383789\n",
      "                              Jsc = 82.5234146118164\n",
      "                              FF = 11.787766456604004\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 70.30907440185547\n",
      "                              Voc = 59.314144134521484\n",
      "                              Jsc = 81.55988311767578\n",
      "                              FF = 11.653579711914062\n",
      "Finished epoch  194\n",
      "On epoch  195\n",
      "Total Epoch Testing MAPE: PCE = 70.10945892333984\n",
      "                              Voc = 58.304405212402344\n",
      "                              Jsc = 81.4052505493164\n",
      "                              FF = 12.151395797729492\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 69.00078582763672\n",
      "                              Voc = 60.8989372253418\n",
      "                              Jsc = 80.79452514648438\n",
      "                              FF = 13.320450782775879\n",
      "Finished epoch  196\n",
      "On epoch  197\n",
      "Total Epoch Testing MAPE: PCE = 69.35335540771484\n",
      "                              Voc = 57.37665939331055\n",
      "                              Jsc = 80.66796875\n",
      "                              FF = 13.230731010437012\n",
      "Finished epoch  197\n",
      "On epoch  198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 72.26795196533203\n",
      "                              Voc = 56.852046966552734\n",
      "                              Jsc = 80.22848510742188\n",
      "                              FF = 12.204894065856934\n",
      "Finished epoch  198\n",
      "On epoch  199\n",
      "Total Epoch Testing MAPE: PCE = 72.25777435302734\n",
      "                              Voc = 59.733028411865234\n",
      "                              Jsc = 81.07644653320312\n",
      "                              FF = 12.377285957336426\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 69.78990173339844\n",
      "                              Voc = 56.68069839477539\n",
      "                              Jsc = 81.35408020019531\n",
      "                              FF = 11.57040786743164\n",
      "Finished epoch  200\n",
      "On epoch  201\n",
      "Total Epoch Testing MAPE: PCE = 71.8993148803711\n",
      "                              Voc = 56.79338455200195\n",
      "                              Jsc = 81.17091369628906\n",
      "                              FF = 11.812830924987793\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 70.27783203125\n",
      "                              Voc = 57.88542175292969\n",
      "                              Jsc = 80.2925796508789\n",
      "                              FF = 11.720986366271973\n",
      "Finished epoch  202\n",
      "On epoch  203\n",
      "Total Epoch Testing MAPE: PCE = 68.48030853271484\n",
      "                              Voc = 54.735355377197266\n",
      "                              Jsc = 80.4303970336914\n",
      "                              FF = 12.538363456726074\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 70.12430572509766\n",
      "                              Voc = 53.8827018737793\n",
      "                              Jsc = 79.89111328125\n",
      "                              FF = 14.597976684570312\n",
      "Finished epoch  204\n",
      "On epoch  205\n",
      "Total Epoch Testing MAPE: PCE = 70.34520721435547\n",
      "                              Voc = 54.21195602416992\n",
      "                              Jsc = 79.22035217285156\n",
      "                              FF = 14.155922889709473\n",
      "Finished epoch  205\n",
      "On epoch  206\n",
      "Total Epoch Testing MAPE: PCE = 70.10350036621094\n",
      "                              Voc = 54.0235595703125\n",
      "                              Jsc = 77.80997467041016\n",
      "                              FF = 13.464700698852539\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 72.181884765625\n",
      "                              Voc = 53.41553497314453\n",
      "                              Jsc = 78.29376220703125\n",
      "                              FF = 12.242330551147461\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 70.86727142333984\n",
      "                              Voc = 53.92552185058594\n",
      "                              Jsc = 76.99364471435547\n",
      "                              FF = 12.202228546142578\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 73.4136962890625\n",
      "                              Voc = 54.955345153808594\n",
      "                              Jsc = 77.31148529052734\n",
      "                              FF = 12.10011100769043\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 74.5180435180664\n",
      "                              Voc = 55.30341339111328\n",
      "                              Jsc = 77.64572143554688\n",
      "                              FF = 12.418771743774414\n",
      "Finished epoch  210\n",
      "On epoch  211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.02957153320312\n",
      "                              Voc = 55.51215362548828\n",
      "                              Jsc = 77.43435668945312\n",
      "                              FF = 12.22237777709961\n",
      "Finished epoch  211\n",
      "On epoch  212\n",
      "Total Epoch Testing MAPE: PCE = 73.49073028564453\n",
      "                              Voc = 56.65963363647461\n",
      "                              Jsc = 78.35746765136719\n",
      "                              FF = 11.56469440460205\n",
      "Finished epoch  212\n",
      "On epoch  213\n",
      "Total Epoch Testing MAPE: PCE = 73.18367004394531\n",
      "                              Voc = 55.045326232910156\n",
      "                              Jsc = 78.6063461303711\n",
      "                              FF = 11.55575942993164\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 71.7907485961914\n",
      "                              Voc = 55.76889419555664\n",
      "                              Jsc = 77.78181457519531\n",
      "                              FF = 12.08277416229248\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 70.28569793701172\n",
      "                              Voc = 56.311458587646484\n",
      "                              Jsc = 76.84761810302734\n",
      "                              FF = 12.2595853805542\n",
      "Finished epoch  215\n",
      "On epoch  216\n",
      "Total Epoch Testing MAPE: PCE = 69.25968170166016\n",
      "                              Voc = 55.52368927001953\n",
      "                              Jsc = 77.54884338378906\n",
      "                              FF = 11.981637954711914\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 69.274658203125\n",
      "                              Voc = 55.30583190917969\n",
      "                              Jsc = 78.83080291748047\n",
      "                              FF = 13.264581680297852\n",
      "Finished epoch  217\n",
      "On epoch  218\n",
      "Total Epoch Testing MAPE: PCE = 68.9361572265625\n",
      "                              Voc = 54.40512466430664\n",
      "                              Jsc = 79.42842864990234\n",
      "                              FF = 13.73263931274414\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 67.030029296875\n",
      "                              Voc = 54.45555877685547\n",
      "                              Jsc = 79.2000732421875\n",
      "                              FF = 15.01164436340332\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 68.09231567382812\n",
      "                              Voc = 54.826473236083984\n",
      "                              Jsc = 78.50999450683594\n",
      "                              FF = 15.507176399230957\n",
      "Finished epoch  220\n",
      "On epoch  221\n",
      "Total Epoch Testing MAPE: PCE = 68.35955047607422\n",
      "                              Voc = 53.19871139526367\n",
      "                              Jsc = 79.4981460571289\n",
      "                              FF = 15.372546195983887\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 69.80279541015625\n",
      "                              Voc = 53.26350784301758\n",
      "                              Jsc = 79.49756622314453\n",
      "                              FF = 14.562542915344238\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 69.65338897705078\n",
      "                              Voc = 53.3438606262207\n",
      "                              Jsc = 79.24661254882812\n",
      "                              FF = 14.025293350219727\n",
      "Finished epoch  223\n",
      "On epoch  224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 69.54444122314453\n",
      "                              Voc = 52.21928787231445\n",
      "                              Jsc = 79.78990936279297\n",
      "                              FF = 13.555747032165527\n",
      "Finished epoch  224\n",
      "On epoch  225\n",
      "Total Epoch Testing MAPE: PCE = 69.98967742919922\n",
      "                              Voc = 51.22081756591797\n",
      "                              Jsc = 79.2237777709961\n",
      "                              FF = 16.36123275756836\n",
      "Finished epoch  225\n",
      "On epoch  226\n",
      "Total Epoch Testing MAPE: PCE = 70.1283187866211\n",
      "                              Voc = 51.63737487792969\n",
      "                              Jsc = 79.53532409667969\n",
      "                              FF = 15.42402458190918\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 71.4906997680664\n",
      "                              Voc = 50.60034942626953\n",
      "                              Jsc = 78.40889739990234\n",
      "                              FF = 16.055702209472656\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 70.9868392944336\n",
      "                              Voc = 50.5638427734375\n",
      "                              Jsc = 79.08802032470703\n",
      "                              FF = 14.86643123626709\n",
      "Finished epoch  228\n",
      "On epoch  229\n",
      "Total Epoch Testing MAPE: PCE = 67.8055191040039\n",
      "                              Voc = 50.79453659057617\n",
      "                              Jsc = 79.0964126586914\n",
      "                              FF = 14.963205337524414\n",
      "Finished epoch  229\n",
      "On epoch  230\n",
      "Total Epoch Testing MAPE: PCE = 68.18682861328125\n",
      "                              Voc = 51.3449592590332\n",
      "                              Jsc = 79.9822006225586\n",
      "                              FF = 14.236286163330078\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 70.08748626708984\n",
      "                              Voc = 51.53433609008789\n",
      "                              Jsc = 80.22784423828125\n",
      "                              FF = 14.425432205200195\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 71.87468719482422\n",
      "                              Voc = 52.6817741394043\n",
      "                              Jsc = 80.27334594726562\n",
      "                              FF = 15.586244583129883\n",
      "Finished epoch  232\n",
      "On epoch  233\n",
      "Total Epoch Testing MAPE: PCE = 70.62251281738281\n",
      "                              Voc = 52.62759780883789\n",
      "                              Jsc = 80.06748962402344\n",
      "                              FF = 15.752789497375488\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 74.04638671875\n",
      "                              Voc = 52.50041961669922\n",
      "                              Jsc = 80.84265899658203\n",
      "                              FF = 17.21272087097168\n",
      "Finished epoch  234\n",
      "On epoch  235\n",
      "Total Epoch Testing MAPE: PCE = 75.38310241699219\n",
      "                              Voc = 52.82109832763672\n",
      "                              Jsc = 80.73418426513672\n",
      "                              FF = 15.236936569213867\n",
      "Finished epoch  235\n",
      "On epoch  236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.74871063232422\n",
      "                              Voc = 52.996009826660156\n",
      "                              Jsc = 80.49956512451172\n",
      "                              FF = 14.263382911682129\n",
      "Finished epoch  236\n",
      "On epoch  237\n",
      "Total Epoch Testing MAPE: PCE = 75.56924438476562\n",
      "                              Voc = 52.72527313232422\n",
      "                              Jsc = 80.50407409667969\n",
      "                              FF = 13.861960411071777\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 75.38272857666016\n",
      "                              Voc = 54.177345275878906\n",
      "                              Jsc = 80.5240707397461\n",
      "                              FF = 14.467742919921875\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 76.72665405273438\n",
      "                              Voc = 53.05447006225586\n",
      "                              Jsc = 79.75212860107422\n",
      "                              FF = 14.74036693572998\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 75.98274230957031\n",
      "                              Voc = 53.43459701538086\n",
      "                              Jsc = 79.12706756591797\n",
      "                              FF = 15.599322319030762\n",
      "Finished epoch  240\n",
      "On epoch  241\n",
      "Total Epoch Testing MAPE: PCE = 76.08030700683594\n",
      "                              Voc = 54.72614288330078\n",
      "                              Jsc = 79.0898208618164\n",
      "                              FF = 15.850107192993164\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 76.20660400390625\n",
      "                              Voc = 54.611915588378906\n",
      "                              Jsc = 78.55208587646484\n",
      "                              FF = 15.226600646972656\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 73.40585327148438\n",
      "                              Voc = 54.34296798706055\n",
      "                              Jsc = 78.39920806884766\n",
      "                              FF = 15.91379451751709\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 73.84031677246094\n",
      "                              Voc = 54.27421188354492\n",
      "                              Jsc = 78.25245666503906\n",
      "                              FF = 16.141267776489258\n",
      "Finished epoch  244\n",
      "On epoch  245\n",
      "Total Epoch Testing MAPE: PCE = 71.12771606445312\n",
      "                              Voc = 54.729347229003906\n",
      "                              Jsc = 79.20349884033203\n",
      "                              FF = 16.611143112182617\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 73.35808563232422\n",
      "                              Voc = 56.28062057495117\n",
      "                              Jsc = 79.44111633300781\n",
      "                              FF = 16.395498275756836\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 74.26661682128906\n",
      "                              Voc = 56.19827651977539\n",
      "                              Jsc = 79.293701171875\n",
      "                              FF = 17.81308364868164\n",
      "Finished epoch  247\n",
      "On epoch  248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 73.58873748779297\n",
      "                              Voc = 58.71974182128906\n",
      "                              Jsc = 80.0660171508789\n",
      "                              FF = 18.858013153076172\n",
      "Finished epoch  248\n",
      "On epoch  249\n",
      "Total Epoch Testing MAPE: PCE = 74.78553771972656\n",
      "                              Voc = 59.33341979980469\n",
      "                              Jsc = 79.62857818603516\n",
      "                              FF = 19.33942222595215\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 75.68206787109375\n",
      "                              Voc = 58.6334228515625\n",
      "                              Jsc = 79.364501953125\n",
      "                              FF = 18.872312545776367\n",
      "Finished epoch  250\n",
      "On epoch  251\n",
      "Total Epoch Testing MAPE: PCE = 76.46442413330078\n",
      "                              Voc = 57.52889633178711\n",
      "                              Jsc = 79.29811096191406\n",
      "                              FF = 18.562477111816406\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 77.31510162353516\n",
      "                              Voc = 58.21065139770508\n",
      "                              Jsc = 79.20394897460938\n",
      "                              FF = 17.98409080505371\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 78.34686279296875\n",
      "                              Voc = 59.108680725097656\n",
      "                              Jsc = 78.97571563720703\n",
      "                              FF = 16.98879051208496\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 77.7702407836914\n",
      "                              Voc = 59.94233322143555\n",
      "                              Jsc = 77.49504089355469\n",
      "                              FF = 18.02471923828125\n",
      "Finished epoch  254\n",
      "On epoch  255\n",
      "Total Epoch Testing MAPE: PCE = 78.16438293457031\n",
      "                              Voc = 60.15992736816406\n",
      "                              Jsc = 77.4119644165039\n",
      "                              FF = 17.37190818786621\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 76.91795349121094\n",
      "                              Voc = 60.575260162353516\n",
      "                              Jsc = 77.15912628173828\n",
      "                              FF = 15.747769355773926\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 77.3211669921875\n",
      "                              Voc = 59.154693603515625\n",
      "                              Jsc = 77.96815490722656\n",
      "                              FF = 16.2821102142334\n",
      "Finished epoch  257\n",
      "On epoch  258\n",
      "Total Epoch Testing MAPE: PCE = 76.620849609375\n",
      "                              Voc = 58.79010009765625\n",
      "                              Jsc = 79.30441284179688\n",
      "                              FF = 15.479768753051758\n",
      "Finished epoch  258\n",
      "On epoch  259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 77.33219909667969\n",
      "                              Voc = 57.374210357666016\n",
      "                              Jsc = 79.44318389892578\n",
      "                              FF = 15.486433029174805\n",
      "Finished epoch  259\n",
      "On epoch  260\n",
      "Total Epoch Testing MAPE: PCE = 77.74261474609375\n",
      "                              Voc = 58.10286331176758\n",
      "                              Jsc = 79.84429931640625\n",
      "                              FF = 15.723225593566895\n",
      "Finished epoch  260\n",
      "On epoch  261\n",
      "Total Epoch Testing MAPE: PCE = 77.21490478515625\n",
      "                              Voc = 58.73904800415039\n",
      "                              Jsc = 79.18519592285156\n",
      "                              FF = 15.994413375854492\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 76.5802230834961\n",
      "                              Voc = 57.08746337890625\n",
      "                              Jsc = 78.84255981445312\n",
      "                              FF = 15.168205261230469\n",
      "Finished epoch  262\n",
      "On epoch  263\n",
      "Total Epoch Testing MAPE: PCE = 75.14832305908203\n",
      "                              Voc = 56.132022857666016\n",
      "                              Jsc = 80.62042236328125\n",
      "                              FF = 13.43694019317627\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 73.56886291503906\n",
      "                              Voc = 55.60356521606445\n",
      "                              Jsc = 80.81987762451172\n",
      "                              FF = 13.227893829345703\n",
      "Finished epoch  264\n",
      "On epoch  265\n",
      "Total Epoch Testing MAPE: PCE = 73.20523071289062\n",
      "                              Voc = 56.70956802368164\n",
      "                              Jsc = 80.06685638427734\n",
      "                              FF = 12.13646125793457\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 71.77871704101562\n",
      "                              Voc = 57.82368850708008\n",
      "                              Jsc = 79.99502563476562\n",
      "                              FF = 15.058636665344238\n",
      "Finished epoch  266\n",
      "On epoch  267\n",
      "Total Epoch Testing MAPE: PCE = 71.08200073242188\n",
      "                              Voc = 56.86643600463867\n",
      "                              Jsc = 80.13579559326172\n",
      "                              FF = 14.162217140197754\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 72.96024322509766\n",
      "                              Voc = 57.097023010253906\n",
      "                              Jsc = 80.83372497558594\n",
      "                              FF = 16.505016326904297\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 72.90904998779297\n",
      "                              Voc = 57.78228759765625\n",
      "                              Jsc = 80.65937042236328\n",
      "                              FF = 16.849021911621094\n",
      "Finished epoch  269\n",
      "On epoch  270\n",
      "Total Epoch Testing MAPE: PCE = 73.04524230957031\n",
      "                              Voc = 58.402950286865234\n",
      "                              Jsc = 79.88935089111328\n",
      "                              FF = 16.869766235351562\n",
      "Finished epoch  270\n",
      "On epoch  271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.98661804199219\n",
      "                              Voc = 58.603878021240234\n",
      "                              Jsc = 79.78304290771484\n",
      "                              FF = 15.62324333190918\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 73.93151092529297\n",
      "                              Voc = 58.13336944580078\n",
      "                              Jsc = 79.38318634033203\n",
      "                              FF = 15.314200401306152\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 74.48836517333984\n",
      "                              Voc = 59.2646484375\n",
      "                              Jsc = 79.16932678222656\n",
      "                              FF = 14.635555267333984\n",
      "Finished epoch  273\n",
      "On epoch  274\n",
      "Total Epoch Testing MAPE: PCE = 76.11223602294922\n",
      "                              Voc = 58.770751953125\n",
      "                              Jsc = 78.25199890136719\n",
      "                              FF = 12.542954444885254\n",
      "Finished epoch  274\n",
      "On epoch  275\n",
      "Total Epoch Testing MAPE: PCE = 75.39891052246094\n",
      "                              Voc = 59.16877365112305\n",
      "                              Jsc = 77.29557037353516\n",
      "                              FF = 12.017374038696289\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 76.14188385009766\n",
      "                              Voc = 60.523681640625\n",
      "                              Jsc = 78.13190460205078\n",
      "                              FF = 12.083990097045898\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 76.29439544677734\n",
      "                              Voc = 61.09326934814453\n",
      "                              Jsc = 77.97319030761719\n",
      "                              FF = 12.47058391571045\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 76.3636245727539\n",
      "                              Voc = 60.595428466796875\n",
      "                              Jsc = 77.523681640625\n",
      "                              FF = 12.554849624633789\n",
      "Finished epoch  278\n",
      "On epoch  279\n",
      "Total Epoch Testing MAPE: PCE = 76.04576873779297\n",
      "                              Voc = 62.61709976196289\n",
      "                              Jsc = 77.36138153076172\n",
      "                              FF = 12.483237266540527\n",
      "Finished epoch  279\n",
      "On epoch  280\n",
      "Total Epoch Testing MAPE: PCE = 75.4735336303711\n",
      "                              Voc = 62.49932098388672\n",
      "                              Jsc = 77.38092803955078\n",
      "                              FF = 12.472818374633789\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 74.7849349975586\n",
      "                              Voc = 61.49005126953125\n",
      "                              Jsc = 76.49433898925781\n",
      "                              FF = 12.503568649291992\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 75.40646362304688\n",
      "                              Voc = 60.099857330322266\n",
      "                              Jsc = 76.67832946777344\n",
      "                              FF = 12.590442657470703\n",
      "Finished epoch  282\n",
      "On epoch  283\n",
      "Total Epoch Testing MAPE: PCE = 73.96160888671875\n",
      "                              Voc = 61.179466247558594\n",
      "                              Jsc = 77.2485122680664\n",
      "                              FF = 12.413396835327148\n",
      "Finished epoch  283\n",
      "On epoch  284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.99915313720703\n",
      "                              Voc = 61.327789306640625\n",
      "                              Jsc = 76.79398345947266\n",
      "                              FF = 12.215866088867188\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 74.55313873291016\n",
      "                              Voc = 61.34958267211914\n",
      "                              Jsc = 76.54585266113281\n",
      "                              FF = 12.187986373901367\n",
      "Finished epoch  285\n",
      "On epoch  286\n",
      "Total Epoch Testing MAPE: PCE = 71.89324188232422\n",
      "                              Voc = 60.982852935791016\n",
      "                              Jsc = 76.87862396240234\n",
      "                              FF = 11.954742431640625\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 71.66393280029297\n",
      "                              Voc = 60.526405334472656\n",
      "                              Jsc = 76.7524185180664\n",
      "                              FF = 11.814361572265625\n",
      "Finished epoch  287\n",
      "On epoch  288\n",
      "Total Epoch Testing MAPE: PCE = 73.87187957763672\n",
      "                              Voc = 59.16566848754883\n",
      "                              Jsc = 77.07360076904297\n",
      "                              FF = 11.667156219482422\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 75.97431182861328\n",
      "                              Voc = 59.19816970825195\n",
      "                              Jsc = 77.39556121826172\n",
      "                              FF = 11.795385360717773\n",
      "Finished epoch  289\n",
      "On epoch  290\n",
      "Total Epoch Testing MAPE: PCE = 74.21430969238281\n",
      "                              Voc = 59.29295349121094\n",
      "                              Jsc = 78.28723907470703\n",
      "                              FF = 11.821718215942383\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 75.4807357788086\n",
      "                              Voc = 58.02824020385742\n",
      "                              Jsc = 78.36722564697266\n",
      "                              FF = 11.632025718688965\n",
      "Finished epoch  291\n",
      "On epoch  292\n",
      "Total Epoch Testing MAPE: PCE = 75.24848937988281\n",
      "                              Voc = 56.79925537109375\n",
      "                              Jsc = 78.321533203125\n",
      "                              FF = 12.38658332824707\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 72.24008178710938\n",
      "                              Voc = 56.840736389160156\n",
      "                              Jsc = 77.62511444091797\n",
      "                              FF = 13.459393501281738\n",
      "Finished epoch  293\n",
      "On epoch  294\n",
      "Total Epoch Testing MAPE: PCE = 73.54283905029297\n",
      "                              Voc = 57.41094207763672\n",
      "                              Jsc = 78.38029479980469\n",
      "                              FF = 12.664151191711426\n",
      "Finished epoch  294\n",
      "On epoch  295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 75.50743103027344\n",
      "                              Voc = 57.478065490722656\n",
      "                              Jsc = 78.03153228759766\n",
      "                              FF = 12.761007308959961\n",
      "Finished epoch  295\n",
      "On epoch  296\n",
      "Total Epoch Testing MAPE: PCE = 74.03193664550781\n",
      "                              Voc = 57.362823486328125\n",
      "                              Jsc = 77.0696792602539\n",
      "                              FF = 12.192307472229004\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 74.3994140625\n",
      "                              Voc = 57.50020217895508\n",
      "                              Jsc = 78.36688995361328\n",
      "                              FF = 12.053857803344727\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 77.02132415771484\n",
      "                              Voc = 57.78953552246094\n",
      "                              Jsc = 78.85897827148438\n",
      "                              FF = 12.59605884552002\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 77.13861083984375\n",
      "                              Voc = 57.894752502441406\n",
      "                              Jsc = 79.51136779785156\n",
      "                              FF = 12.336453437805176\n",
      "Finished epoch  299\n",
      "On epoch  300\n",
      "Total Epoch Testing MAPE: PCE = 76.14342498779297\n",
      "                              Voc = 57.6119499206543\n",
      "                              Jsc = 78.76445007324219\n",
      "                              FF = 12.38980770111084\n",
      "Finished epoch  300\n",
      "On epoch  301\n",
      "Total Epoch Testing MAPE: PCE = 76.5740966796875\n",
      "                              Voc = 57.619136810302734\n",
      "                              Jsc = 79.0456314086914\n",
      "                              FF = 12.209052085876465\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 74.22941589355469\n",
      "                              Voc = 58.14667892456055\n",
      "                              Jsc = 79.0196762084961\n",
      "                              FF = 12.378788948059082\n",
      "Finished epoch  302\n",
      "On epoch  303\n",
      "Total Epoch Testing MAPE: PCE = 75.72726440429688\n",
      "                              Voc = 58.17383575439453\n",
      "                              Jsc = 78.82038116455078\n",
      "                              FF = 12.603646278381348\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 73.62440490722656\n",
      "                              Voc = 58.416324615478516\n",
      "                              Jsc = 78.56446838378906\n",
      "                              FF = 12.569448471069336\n",
      "Finished epoch  304\n",
      "On epoch  305\n",
      "Total Epoch Testing MAPE: PCE = 74.4605484008789\n",
      "                              Voc = 58.054237365722656\n",
      "                              Jsc = 78.00888061523438\n",
      "                              FF = 13.40291976928711\n",
      "Finished epoch  305\n",
      "On epoch  306\n",
      "Total Epoch Testing MAPE: PCE = 74.31202697753906\n",
      "                              Voc = 58.02739715576172\n",
      "                              Jsc = 79.07908630371094\n",
      "                              FF = 12.996459007263184\n",
      "Finished epoch  306\n",
      "On epoch  307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.5123291015625\n",
      "                              Voc = 57.831607818603516\n",
      "                              Jsc = 78.49620056152344\n",
      "                              FF = 12.787970542907715\n",
      "Finished epoch  307\n",
      "On epoch  308\n",
      "Total Epoch Testing MAPE: PCE = 76.4108657836914\n",
      "                              Voc = 57.259273529052734\n",
      "                              Jsc = 77.8100814819336\n",
      "                              FF = 12.868806838989258\n",
      "Finished epoch  308\n",
      "On epoch  309\n",
      "Total Epoch Testing MAPE: PCE = 77.1986083984375\n",
      "                              Voc = 57.89830017089844\n",
      "                              Jsc = 78.9886474609375\n",
      "                              FF = 12.7792329788208\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 77.34650421142578\n",
      "                              Voc = 57.90098190307617\n",
      "                              Jsc = 79.46460723876953\n",
      "                              FF = 12.257166862487793\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 77.17469024658203\n",
      "                              Voc = 58.19623947143555\n",
      "                              Jsc = 78.95339965820312\n",
      "                              FF = 12.738016128540039\n",
      "Finished epoch  311\n",
      "On epoch  312\n",
      "Total Epoch Testing MAPE: PCE = 76.80918884277344\n",
      "                              Voc = 58.23869705200195\n",
      "                              Jsc = 77.85958099365234\n",
      "                              FF = 12.760723114013672\n",
      "Finished epoch  312\n",
      "On epoch  313\n",
      "Total Epoch Testing MAPE: PCE = 76.68243408203125\n",
      "                              Voc = 57.94831085205078\n",
      "                              Jsc = 77.66207122802734\n",
      "                              FF = 14.246834754943848\n",
      "Finished epoch  313\n",
      "On epoch  314\n",
      "Total Epoch Testing MAPE: PCE = 77.00215911865234\n",
      "                              Voc = 57.7664680480957\n",
      "                              Jsc = 77.5394058227539\n",
      "                              FF = 14.60997200012207\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 76.697509765625\n",
      "                              Voc = 58.17316818237305\n",
      "                              Jsc = 76.97419738769531\n",
      "                              FF = 18.182491302490234\n",
      "Finished epoch  315\n",
      "On epoch  316\n",
      "Total Epoch Testing MAPE: PCE = 76.50041961669922\n",
      "                              Voc = 58.100807189941406\n",
      "                              Jsc = 78.10462951660156\n",
      "                              FF = 18.99677848815918\n",
      "Finished epoch  316\n",
      "On epoch  317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.95587158203125\n",
      "                              Voc = 58.04776382446289\n",
      "                              Jsc = 78.86930084228516\n",
      "                              FF = 19.022777557373047\n",
      "Finished epoch  317\n",
      "On epoch  318\n",
      "Total Epoch Testing MAPE: PCE = 73.54804992675781\n",
      "                              Voc = 57.78216552734375\n",
      "                              Jsc = 79.49359893798828\n",
      "                              FF = 17.472885131835938\n",
      "Finished epoch  318\n",
      "On epoch  319\n",
      "Total Epoch Testing MAPE: PCE = 72.71446990966797\n",
      "                              Voc = 58.188377380371094\n",
      "                              Jsc = 78.2739486694336\n",
      "                              FF = 15.731098175048828\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 73.4130859375\n",
      "                              Voc = 57.888065338134766\n",
      "                              Jsc = 78.1598892211914\n",
      "                              FF = 14.79510498046875\n",
      "Finished epoch  320\n",
      "On epoch  321\n",
      "Total Epoch Testing MAPE: PCE = 74.19707489013672\n",
      "                              Voc = 58.05324935913086\n",
      "                              Jsc = 78.59119415283203\n",
      "                              FF = 14.954204559326172\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 72.1115493774414\n",
      "                              Voc = 57.96025848388672\n",
      "                              Jsc = 79.37187957763672\n",
      "                              FF = 12.225321769714355\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 70.76679992675781\n",
      "                              Voc = 58.27167510986328\n",
      "                              Jsc = 79.36602783203125\n",
      "                              FF = 12.02441692352295\n",
      "Finished epoch  323\n",
      "On epoch  324\n",
      "Total Epoch Testing MAPE: PCE = 70.26642608642578\n",
      "                              Voc = 58.54045104980469\n",
      "                              Jsc = 79.75765228271484\n",
      "                              FF = 11.834299087524414\n",
      "Finished epoch  324\n",
      "On epoch  325\n",
      "Total Epoch Testing MAPE: PCE = 73.50375366210938\n",
      "                              Voc = 59.203548431396484\n",
      "                              Jsc = 80.62857818603516\n",
      "                              FF = 11.825050354003906\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 71.48239135742188\n",
      "                              Voc = 58.51531219482422\n",
      "                              Jsc = 80.20421600341797\n",
      "                              FF = 11.965025901794434\n",
      "Finished epoch  326\n",
      "On epoch  327\n",
      "Total Epoch Testing MAPE: PCE = 74.52578735351562\n",
      "                              Voc = 59.16222381591797\n",
      "                              Jsc = 79.40751647949219\n",
      "                              FF = 11.934311866760254\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 74.73455810546875\n",
      "                              Voc = 59.1453742980957\n",
      "                              Jsc = 79.1651840209961\n",
      "                              FF = 12.306900024414062\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 76.48857879638672\n",
      "                              Voc = 59.43600082397461\n",
      "                              Jsc = 78.78810119628906\n",
      "                              FF = 12.449994087219238\n",
      "Finished epoch  329\n",
      "On epoch  330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 77.13052368164062\n",
      "                              Voc = 59.8024787902832\n",
      "                              Jsc = 79.59352111816406\n",
      "                              FF = 12.000472068786621\n",
      "Finished epoch  330\n",
      "On epoch  331\n",
      "Total Epoch Testing MAPE: PCE = 75.9066162109375\n",
      "                              Voc = 60.130821228027344\n",
      "                              Jsc = 79.54324340820312\n",
      "                              FF = 12.274617195129395\n",
      "Finished epoch  331\n",
      "On epoch  332\n",
      "Total Epoch Testing MAPE: PCE = 75.39106750488281\n",
      "                              Voc = 60.638587951660156\n",
      "                              Jsc = 80.06669616699219\n",
      "                              FF = 12.609489440917969\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 73.922119140625\n",
      "                              Voc = 61.667823791503906\n",
      "                              Jsc = 79.68929290771484\n",
      "                              FF = 12.374005317687988\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 72.94942474365234\n",
      "                              Voc = 62.08892059326172\n",
      "                              Jsc = 79.63301849365234\n",
      "                              FF = 11.98007583618164\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 72.37591552734375\n",
      "                              Voc = 60.98054122924805\n",
      "                              Jsc = 78.5865478515625\n",
      "                              FF = 11.888760566711426\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 73.06367492675781\n",
      "                              Voc = 60.71221160888672\n",
      "                              Jsc = 80.04229736328125\n",
      "                              FF = 11.866421699523926\n",
      "Finished epoch  336\n",
      "On epoch  337\n",
      "Total Epoch Testing MAPE: PCE = 72.61138916015625\n",
      "                              Voc = 61.19932556152344\n",
      "                              Jsc = 80.33704376220703\n",
      "                              FF = 11.79173755645752\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 74.04878997802734\n",
      "                              Voc = 61.18901443481445\n",
      "                              Jsc = 80.58219909667969\n",
      "                              FF = 12.206255912780762\n",
      "Finished epoch  338\n",
      "On epoch  339\n",
      "Total Epoch Testing MAPE: PCE = 71.53768920898438\n",
      "                              Voc = 60.713619232177734\n",
      "                              Jsc = 80.88642883300781\n",
      "                              FF = 12.726299285888672\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 71.74873352050781\n",
      "                              Voc = 61.30918884277344\n",
      "                              Jsc = 80.58094787597656\n",
      "                              FF = 12.71850872039795\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 72.74441528320312\n",
      "                              Voc = 60.31166458129883\n",
      "                              Jsc = 80.85432434082031\n",
      "                              FF = 12.688589096069336\n",
      "Finished epoch  341\n",
      "On epoch  342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.7545166015625\n",
      "                              Voc = 60.14894104003906\n",
      "                              Jsc = 81.45875549316406\n",
      "                              FF = 13.212953567504883\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 76.02111053466797\n",
      "                              Voc = 59.67070388793945\n",
      "                              Jsc = 82.71842956542969\n",
      "                              FF = 12.886634826660156\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 73.3223648071289\n",
      "                              Voc = 59.282936096191406\n",
      "                              Jsc = 82.88060760498047\n",
      "                              FF = 13.38836669921875\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 74.42485046386719\n",
      "                              Voc = 59.868370056152344\n",
      "                              Jsc = 83.16030883789062\n",
      "                              FF = 13.277290344238281\n",
      "Finished epoch  345\n",
      "On epoch  346\n",
      "Total Epoch Testing MAPE: PCE = 75.48541259765625\n",
      "                              Voc = 59.641849517822266\n",
      "                              Jsc = 82.00369262695312\n",
      "                              FF = 13.622194290161133\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 72.83161926269531\n",
      "                              Voc = 58.54574966430664\n",
      "                              Jsc = 81.78839874267578\n",
      "                              FF = 13.600927352905273\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 72.597900390625\n",
      "                              Voc = 58.18519592285156\n",
      "                              Jsc = 81.28450775146484\n",
      "                              FF = 13.254281997680664\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 71.76102447509766\n",
      "                              Voc = 58.70686721801758\n",
      "                              Jsc = 80.21660614013672\n",
      "                              FF = 13.387161254882812\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 70.21485900878906\n",
      "                              Voc = 59.03663635253906\n",
      "                              Jsc = 80.22808074951172\n",
      "                              FF = 13.395519256591797\n",
      "Finished epoch  350\n",
      "On epoch  351\n",
      "Total Epoch Testing MAPE: PCE = 73.53239440917969\n",
      "                              Voc = 59.66363525390625\n",
      "                              Jsc = 80.81788635253906\n",
      "                              FF = 13.655363082885742\n",
      "Finished epoch  351\n",
      "On epoch  352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 73.40209197998047\n",
      "                              Voc = 59.65303039550781\n",
      "                              Jsc = 80.5537109375\n",
      "                              FF = 14.073410987854004\n",
      "Finished epoch  352\n",
      "On epoch  353\n",
      "Total Epoch Testing MAPE: PCE = 71.22167205810547\n",
      "                              Voc = 59.65869903564453\n",
      "                              Jsc = 80.44981384277344\n",
      "                              FF = 13.824807167053223\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 72.2055435180664\n",
      "                              Voc = 59.3271598815918\n",
      "                              Jsc = 80.1772689819336\n",
      "                              FF = 13.806594848632812\n",
      "Finished epoch  354\n",
      "On epoch  355\n",
      "Total Epoch Testing MAPE: PCE = 74.19900512695312\n",
      "                              Voc = 59.15655517578125\n",
      "                              Jsc = 80.8171157836914\n",
      "                              FF = 13.94039535522461\n",
      "Finished epoch  355\n",
      "On epoch  356\n",
      "Total Epoch Testing MAPE: PCE = 77.00127410888672\n",
      "                              Voc = 58.363468170166016\n",
      "                              Jsc = 80.33824157714844\n",
      "                              FF = 13.004583358764648\n",
      "Finished epoch  356\n",
      "On epoch  357\n",
      "Total Epoch Testing MAPE: PCE = 77.43327331542969\n",
      "                              Voc = 58.41960906982422\n",
      "                              Jsc = 81.09468841552734\n",
      "                              FF = 12.49128532409668\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 77.81935119628906\n",
      "                              Voc = 60.06375503540039\n",
      "                              Jsc = 82.09232330322266\n",
      "                              FF = 12.263493537902832\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 76.70885467529297\n",
      "                              Voc = 59.45089340209961\n",
      "                              Jsc = 81.38580322265625\n",
      "                              FF = 12.163888931274414\n",
      "Finished epoch  359\n",
      "On epoch  360\n",
      "Total Epoch Testing MAPE: PCE = 76.66578674316406\n",
      "                              Voc = 59.54579162597656\n",
      "                              Jsc = 80.63859558105469\n",
      "                              FF = 12.682092666625977\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 76.32608032226562\n",
      "                              Voc = 59.335411071777344\n",
      "                              Jsc = 79.76800537109375\n",
      "                              FF = 12.273125648498535\n",
      "Finished epoch  361\n",
      "On epoch  362\n",
      "Total Epoch Testing MAPE: PCE = 75.8084487915039\n",
      "                              Voc = 59.01409912109375\n",
      "                              Jsc = 79.34010314941406\n",
      "                              FF = 11.991425514221191\n",
      "Finished epoch  362\n",
      "On epoch  363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 76.75603485107422\n",
      "                              Voc = 59.10820770263672\n",
      "                              Jsc = 79.5882797241211\n",
      "                              FF = 12.033164978027344\n",
      "Finished epoch  363\n",
      "On epoch  364\n",
      "Total Epoch Testing MAPE: PCE = 74.96025848388672\n",
      "                              Voc = 59.30789566040039\n",
      "                              Jsc = 80.01392364501953\n",
      "                              FF = 11.63657283782959\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 75.64610290527344\n",
      "                              Voc = 59.23954391479492\n",
      "                              Jsc = 78.80228424072266\n",
      "                              FF = 11.867433547973633\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 75.90743255615234\n",
      "                              Voc = 59.495704650878906\n",
      "                              Jsc = 78.25236511230469\n",
      "                              FF = 11.547833442687988\n",
      "Finished epoch  366\n",
      "On epoch  367\n",
      "Total Epoch Testing MAPE: PCE = 75.3298110961914\n",
      "                              Voc = 59.491600036621094\n",
      "                              Jsc = 78.25909423828125\n",
      "                              FF = 11.941628456115723\n",
      "Finished epoch  367\n",
      "On epoch  368\n",
      "Total Epoch Testing MAPE: PCE = 76.32377624511719\n",
      "                              Voc = 59.514530181884766\n",
      "                              Jsc = 79.41259765625\n",
      "                              FF = 12.162785530090332\n",
      "Finished epoch  368\n",
      "On epoch  369\n",
      "Total Epoch Testing MAPE: PCE = 77.1524429321289\n",
      "                              Voc = 58.95212173461914\n",
      "                              Jsc = 78.3055419921875\n",
      "                              FF = 12.380305290222168\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 75.81734466552734\n",
      "                              Voc = 58.549644470214844\n",
      "                              Jsc = 77.96637725830078\n",
      "                              FF = 12.233614921569824\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 77.10224914550781\n",
      "                              Voc = 58.47966003417969\n",
      "                              Jsc = 79.06641387939453\n",
      "                              FF = 12.626649856567383\n",
      "Finished epoch  371\n",
      "On epoch  372\n",
      "Total Epoch Testing MAPE: PCE = 75.35323333740234\n",
      "                              Voc = 58.438270568847656\n",
      "                              Jsc = 79.01628875732422\n",
      "                              FF = 12.529439926147461\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 76.11801147460938\n",
      "                              Voc = 58.44807052612305\n",
      "                              Jsc = 78.80899047851562\n",
      "                              FF = 12.627138137817383\n",
      "Finished epoch  373\n",
      "On epoch  374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 76.7811508178711\n",
      "                              Voc = 58.56260681152344\n",
      "                              Jsc = 78.14215087890625\n",
      "                              FF = 12.113471984863281\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 73.23778533935547\n",
      "                              Voc = 58.95553207397461\n",
      "                              Jsc = 77.97755432128906\n",
      "                              FF = 12.300670623779297\n",
      "Finished epoch  375\n",
      "On epoch  376\n",
      "Total Epoch Testing MAPE: PCE = 73.75904083251953\n",
      "                              Voc = 58.79570770263672\n",
      "                              Jsc = 78.10042572021484\n",
      "                              FF = 11.939852714538574\n",
      "Finished epoch  376\n",
      "On epoch  377\n",
      "Total Epoch Testing MAPE: PCE = 73.70311737060547\n",
      "                              Voc = 58.42995071411133\n",
      "                              Jsc = 78.3912124633789\n",
      "                              FF = 11.641148567199707\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 71.58452606201172\n",
      "                              Voc = 57.93803405761719\n",
      "                              Jsc = 79.04344940185547\n",
      "                              FF = 11.277477264404297\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 73.80936431884766\n",
      "                              Voc = 57.95116424560547\n",
      "                              Jsc = 77.08565521240234\n",
      "                              FF = 11.269701957702637\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 72.70927429199219\n",
      "                              Voc = 58.105224609375\n",
      "                              Jsc = 77.03235626220703\n",
      "                              FF = 11.255876541137695\n",
      "Finished epoch  380\n",
      "On epoch  381\n",
      "Total Epoch Testing MAPE: PCE = 72.58068084716797\n",
      "                              Voc = 58.07482147216797\n",
      "                              Jsc = 76.58832550048828\n",
      "                              FF = 11.771472930908203\n",
      "Finished epoch  381\n",
      "On epoch  382\n",
      "Total Epoch Testing MAPE: PCE = 72.8998031616211\n",
      "                              Voc = 57.96214294433594\n",
      "                              Jsc = 76.59228515625\n",
      "                              FF = 11.744277000427246\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 73.17542266845703\n",
      "                              Voc = 58.10371780395508\n",
      "                              Jsc = 76.76670837402344\n",
      "                              FF = 11.400375366210938\n",
      "Finished epoch  383\n",
      "On epoch  384\n",
      "Total Epoch Testing MAPE: PCE = 72.27970123291016\n",
      "                              Voc = 58.32758712768555\n",
      "                              Jsc = 76.0201187133789\n",
      "                              FF = 11.817593574523926\n",
      "Finished epoch  384\n",
      "On epoch  385\n",
      "Total Epoch Testing MAPE: PCE = 69.82781219482422\n",
      "                              Voc = 58.763092041015625\n",
      "                              Jsc = 77.03861236572266\n",
      "                              FF = 12.642701148986816\n",
      "Finished epoch  385\n",
      "On epoch  386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 69.98825073242188\n",
      "                              Voc = 58.070068359375\n",
      "                              Jsc = 77.0681381225586\n",
      "                              FF = 12.625856399536133\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 71.43455505371094\n",
      "                              Voc = 58.03660583496094\n",
      "                              Jsc = 76.70996856689453\n",
      "                              FF = 12.252928733825684\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 70.65070343017578\n",
      "                              Voc = 57.753639221191406\n",
      "                              Jsc = 77.68765258789062\n",
      "                              FF = 12.615105628967285\n",
      "Finished epoch  388\n",
      "On epoch  389\n",
      "Total Epoch Testing MAPE: PCE = 70.24332427978516\n",
      "                              Voc = 57.73280715942383\n",
      "                              Jsc = 77.15438842773438\n",
      "                              FF = 12.515695571899414\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 72.23808288574219\n",
      "                              Voc = 57.663326263427734\n",
      "                              Jsc = 77.2708740234375\n",
      "                              FF = 12.905131340026855\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 70.31498718261719\n",
      "                              Voc = 57.71712112426758\n",
      "                              Jsc = 76.2005386352539\n",
      "                              FF = 12.521794319152832\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 73.01252746582031\n",
      "                              Voc = 57.523250579833984\n",
      "                              Jsc = 76.96635437011719\n",
      "                              FF = 12.087644577026367\n",
      "Finished epoch  392\n",
      "On epoch  393\n",
      "Total Epoch Testing MAPE: PCE = 73.19849395751953\n",
      "                              Voc = 57.547752380371094\n",
      "                              Jsc = 76.62431335449219\n",
      "                              FF = 12.10883617401123\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 69.46662902832031\n",
      "                              Voc = 57.647884368896484\n",
      "                              Jsc = 77.34457397460938\n",
      "                              FF = 12.018866539001465\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 73.43592071533203\n",
      "                              Voc = 57.380592346191406\n",
      "                              Jsc = 77.7861099243164\n",
      "                              FF = 12.549999237060547\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 70.57494354248047\n",
      "                              Voc = 57.51228332519531\n",
      "                              Jsc = 76.99140930175781\n",
      "                              FF = 12.348711013793945\n",
      "Finished epoch  396\n",
      "On epoch  397\n",
      "Total Epoch Testing MAPE: PCE = 68.9048843383789\n",
      "                              Voc = 57.51673126220703\n",
      "                              Jsc = 77.25428771972656\n",
      "                              FF = 11.924628257751465\n",
      "Finished epoch  397\n",
      "On epoch  398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 69.81581115722656\n",
      "                              Voc = 57.82207107543945\n",
      "                              Jsc = 77.04251098632812\n",
      "                              FF = 11.882266998291016\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 69.39374542236328\n",
      "                              Voc = 57.694923400878906\n",
      "                              Jsc = 76.52462768554688\n",
      "                              FF = 11.711289405822754\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 68.98886108398438\n",
      "                              Voc = 57.76618194580078\n",
      "                              Jsc = 76.32652282714844\n",
      "                              FF = 11.768837928771973\n",
      "Finished epoch  400\n",
      "On epoch  401\n",
      "Total Epoch Testing MAPE: PCE = 70.86640930175781\n",
      "                              Voc = 58.58435821533203\n",
      "                              Jsc = 77.7140121459961\n",
      "                              FF = 12.463730812072754\n",
      "Finished epoch  401\n",
      "On epoch  402\n",
      "Total Epoch Testing MAPE: PCE = 71.29820251464844\n",
      "                              Voc = 58.27273178100586\n",
      "                              Jsc = 77.94528198242188\n",
      "                              FF = 12.290762901306152\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 71.67005920410156\n",
      "                              Voc = 58.05131149291992\n",
      "                              Jsc = 77.38368225097656\n",
      "                              FF = 12.334001541137695\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 71.43180847167969\n",
      "                              Voc = 58.10292434692383\n",
      "                              Jsc = 77.06027221679688\n",
      "                              FF = 12.770710945129395\n",
      "Finished epoch  404\n",
      "On epoch  405\n",
      "Total Epoch Testing MAPE: PCE = 73.76485443115234\n",
      "                              Voc = 58.40099334716797\n",
      "                              Jsc = 77.29029846191406\n",
      "                              FF = 13.059881210327148\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 74.22017669677734\n",
      "                              Voc = 58.7181396484375\n",
      "                              Jsc = 76.517822265625\n",
      "                              FF = 12.458905220031738\n",
      "Finished epoch  406\n",
      "On epoch  407\n",
      "Total Epoch Testing MAPE: PCE = 74.89863586425781\n",
      "                              Voc = 58.57294845581055\n",
      "                              Jsc = 76.37661743164062\n",
      "                              FF = 12.531961441040039\n",
      "Finished epoch  407\n",
      "On epoch  408\n",
      "Total Epoch Testing MAPE: PCE = 74.29629516601562\n",
      "                              Voc = 58.33330154418945\n",
      "                              Jsc = 77.08283996582031\n",
      "                              FF = 12.508310317993164\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 75.65999603271484\n",
      "                              Voc = 58.1573371887207\n",
      "                              Jsc = 76.66913604736328\n",
      "                              FF = 12.343998908996582\n",
      "Finished epoch  409\n",
      "On epoch  410\n",
      "Total Epoch Testing MAPE: PCE = 76.03816986083984\n",
      "                              Voc = 58.77610397338867\n",
      "                              Jsc = 77.7563705444336\n",
      "                              FF = 12.164116859436035\n",
      "Finished epoch  410\n",
      "On epoch  411\n",
      "Total Epoch Testing MAPE: PCE = 73.63159942626953\n",
      "                              Voc = 58.355438232421875\n",
      "                              Jsc = 78.1229476928711\n",
      "                              FF = 11.750831604003906\n",
      "Finished epoch  411\n",
      "On epoch  412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 74.04647064208984\n",
      "                              Voc = 58.30091857910156\n",
      "                              Jsc = 77.84620666503906\n",
      "                              FF = 11.439449310302734\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 72.69037628173828\n",
      "                              Voc = 58.29905700683594\n",
      "                              Jsc = 77.3444595336914\n",
      "                              FF = 12.454923629760742\n",
      "Finished epoch  413\n",
      "On epoch  414\n",
      "Total Epoch Testing MAPE: PCE = 73.43334197998047\n",
      "                              Voc = 58.09064483642578\n",
      "                              Jsc = 78.84383392333984\n",
      "                              FF = 12.468756675720215\n",
      "Finished epoch  414\n",
      "On epoch  415\n",
      "Total Epoch Testing MAPE: PCE = 73.64555358886719\n",
      "                              Voc = 57.9964714050293\n",
      "                              Jsc = 79.06159973144531\n",
      "                              FF = 12.641629219055176\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 74.42414093017578\n",
      "                              Voc = 58.06008529663086\n",
      "                              Jsc = 79.98868560791016\n",
      "                              FF = 12.780176162719727\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 76.88617706298828\n",
      "                              Voc = 58.075462341308594\n",
      "                              Jsc = 79.44539642333984\n",
      "                              FF = 12.509526252746582\n",
      "Finished epoch  417\n",
      "On epoch  418\n",
      "Total Epoch Testing MAPE: PCE = 74.87926483154297\n",
      "                              Voc = 57.94071960449219\n",
      "                              Jsc = 80.19932556152344\n",
      "                              FF = 11.995954513549805\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 74.0615005493164\n",
      "                              Voc = 58.94266891479492\n",
      "                              Jsc = 79.75277709960938\n",
      "                              FF = 11.377684593200684\n",
      "Finished epoch  419\n",
      "On epoch  420\n",
      "Total Epoch Testing MAPE: PCE = 73.03392028808594\n",
      "                              Voc = 58.97510528564453\n",
      "                              Jsc = 80.82687377929688\n",
      "                              FF = 11.690046310424805\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 68.35579681396484\n",
      "                              Voc = 58.69318771362305\n",
      "                              Jsc = 81.00682067871094\n",
      "                              FF = 11.363740921020508\n",
      "Finished epoch  421\n",
      "On epoch  422\n",
      "Total Epoch Testing MAPE: PCE = 69.59009552001953\n",
      "                              Voc = 57.992637634277344\n",
      "                              Jsc = 80.68207550048828\n",
      "                              FF = 11.373231887817383\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 68.8084487915039\n",
      "                              Voc = 57.924598693847656\n",
      "                              Jsc = 81.76708221435547\n",
      "                              FF = 11.285818099975586\n",
      "Finished epoch  423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch  424\n",
      "Total Epoch Testing MAPE: PCE = 68.0394515991211\n",
      "                              Voc = 57.43097686767578\n",
      "                              Jsc = 80.27279663085938\n",
      "                              FF = 12.309452056884766\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 68.17417907714844\n",
      "                              Voc = 57.37549591064453\n",
      "                              Jsc = 80.04736328125\n",
      "                              FF = 11.988698959350586\n",
      "Finished epoch  425\n",
      "On epoch  426\n",
      "Total Epoch Testing MAPE: PCE = 68.66571044921875\n",
      "                              Voc = 57.308685302734375\n",
      "                              Jsc = 78.90672302246094\n",
      "                              FF = 11.94589900970459\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 70.50818634033203\n",
      "                              Voc = 56.848777770996094\n",
      "                              Jsc = 79.25487518310547\n",
      "                              FF = 11.853217124938965\n",
      "Finished epoch  427\n",
      "On epoch  428\n",
      "Total Epoch Testing MAPE: PCE = 71.92973327636719\n",
      "                              Voc = 57.37250518798828\n",
      "                              Jsc = 78.10879516601562\n",
      "                              FF = 12.147272109985352\n",
      "Finished epoch  428\n",
      "On epoch  429\n",
      "Total Epoch Testing MAPE: PCE = 70.33502197265625\n",
      "                              Voc = 57.10794448852539\n",
      "                              Jsc = 77.91317749023438\n",
      "                              FF = 11.783097267150879\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 73.3786849975586\n",
      "                              Voc = 56.689613342285156\n",
      "                              Jsc = 77.24815368652344\n",
      "                              FF = 12.596254348754883\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 72.05937194824219\n",
      "                              Voc = 56.634681701660156\n",
      "                              Jsc = 77.72830963134766\n",
      "                              FF = 13.904415130615234\n",
      "Finished epoch  431\n",
      "On epoch  432\n",
      "Total Epoch Testing MAPE: PCE = 69.91410827636719\n",
      "                              Voc = 56.851810455322266\n",
      "                              Jsc = 77.56150817871094\n",
      "                              FF = 13.809005737304688\n",
      "Finished epoch  432\n",
      "On epoch  433\n",
      "Total Epoch Testing MAPE: PCE = 69.3597412109375\n",
      "                              Voc = 57.242435455322266\n",
      "                              Jsc = 77.9754638671875\n",
      "                              FF = 13.57847785949707\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 69.36485290527344\n",
      "                              Voc = 57.02250671386719\n",
      "                              Jsc = 78.81925964355469\n",
      "                              FF = 13.02370834350586\n",
      "Finished epoch  434\n",
      "On epoch  435\n",
      "Total Epoch Testing MAPE: PCE = 71.2131118774414\n",
      "                              Voc = 57.21755599975586\n",
      "                              Jsc = 78.51441192626953\n",
      "                              FF = 13.333441734313965\n",
      "Finished epoch  435\n",
      "On epoch  436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 70.76214599609375\n",
      "                              Voc = 57.70510482788086\n",
      "                              Jsc = 79.72775268554688\n",
      "                              FF = 13.651426315307617\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 69.68314361572266\n",
      "                              Voc = 57.00643539428711\n",
      "                              Jsc = 80.07662963867188\n",
      "                              FF = 13.128293991088867\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 69.38599395751953\n",
      "                              Voc = 56.673370361328125\n",
      "                              Jsc = 79.59113311767578\n",
      "                              FF = 13.004332542419434\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 71.59500885009766\n",
      "                              Voc = 57.253665924072266\n",
      "                              Jsc = 79.6355209350586\n",
      "                              FF = 12.618255615234375\n",
      "Finished epoch  439\n",
      "On epoch  440\n",
      "Total Epoch Testing MAPE: PCE = 71.12804412841797\n",
      "                              Voc = 57.6080207824707\n",
      "                              Jsc = 79.88996887207031\n",
      "                              FF = 12.18763542175293\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 72.4979019165039\n",
      "                              Voc = 57.31402587890625\n",
      "                              Jsc = 79.45603942871094\n",
      "                              FF = 11.652361869812012\n",
      "Finished epoch  441\n",
      "On epoch  442\n",
      "Total Epoch Testing MAPE: PCE = 69.64230346679688\n",
      "                              Voc = 58.074378967285156\n",
      "                              Jsc = 79.11309814453125\n",
      "                              FF = 11.89023208618164\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 68.58855438232422\n",
      "                              Voc = 58.18905258178711\n",
      "                              Jsc = 80.54638671875\n",
      "                              FF = 12.630614280700684\n",
      "Finished epoch  443\n",
      "On epoch  444\n",
      "Total Epoch Testing MAPE: PCE = 70.81896209716797\n",
      "                              Voc = 57.84735107421875\n",
      "                              Jsc = 80.26372528076172\n",
      "                              FF = 12.434171676635742\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 72.42506408691406\n",
      "                              Voc = 58.34650421142578\n",
      "                              Jsc = 80.37203216552734\n",
      "                              FF = 12.464017868041992\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 74.79041290283203\n",
      "                              Voc = 57.97441482543945\n",
      "                              Jsc = 80.12213897705078\n",
      "                              FF = 12.500749588012695\n",
      "Finished epoch  446\n",
      "On epoch  447\n",
      "Total Epoch Testing MAPE: PCE = 73.8857192993164\n",
      "                              Voc = 57.897621154785156\n",
      "                              Jsc = 80.44705963134766\n",
      "                              FF = 12.40446662902832\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 76.84390258789062\n",
      "                              Voc = 57.91862487792969\n",
      "                              Jsc = 79.94652557373047\n",
      "                              FF = 12.889315605163574\n",
      "Finished epoch  448\n",
      "On epoch  449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 76.29728698730469\n",
      "                              Voc = 57.44580078125\n",
      "                              Jsc = 81.000244140625\n",
      "                              FF = 13.063240051269531\n",
      "Finished epoch  449\n",
      "On epoch  450\n",
      "Total Epoch Testing MAPE: PCE = 74.7716064453125\n",
      "                              Voc = 57.663124084472656\n",
      "                              Jsc = 81.00521087646484\n",
      "                              FF = 12.687370300292969\n",
      "Finished epoch  450\n",
      "On epoch  451\n",
      "Total Epoch Testing MAPE: PCE = 75.37054443359375\n",
      "                              Voc = 57.688270568847656\n",
      "                              Jsc = 80.55028533935547\n",
      "                              FF = 13.527339935302734\n",
      "Finished epoch  451\n",
      "On epoch  452\n",
      "Total Epoch Testing MAPE: PCE = 73.13236236572266\n",
      "                              Voc = 57.95044708251953\n",
      "                              Jsc = 81.08328247070312\n",
      "                              FF = 13.036331176757812\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 70.0115966796875\n",
      "                              Voc = 58.400474548339844\n",
      "                              Jsc = 80.0044937133789\n",
      "                              FF = 13.789985656738281\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 71.41138458251953\n",
      "                              Voc = 58.353759765625\n",
      "                              Jsc = 80.4862060546875\n",
      "                              FF = 15.793089866638184\n",
      "Finished epoch  454\n",
      "On epoch  455\n",
      "Total Epoch Testing MAPE: PCE = 72.47599792480469\n",
      "                              Voc = 58.68573760986328\n",
      "                              Jsc = 80.4471435546875\n",
      "                              FF = 16.946426391601562\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 71.29254150390625\n",
      "                              Voc = 58.76498794555664\n",
      "                              Jsc = 80.43247985839844\n",
      "                              FF = 16.816879272460938\n",
      "Finished epoch  456\n",
      "On epoch  457\n",
      "Total Epoch Testing MAPE: PCE = 73.02999877929688\n",
      "                              Voc = 59.23811721801758\n",
      "                              Jsc = 80.2403564453125\n",
      "                              FF = 14.270697593688965\n",
      "Finished epoch  457\n",
      "On epoch  458\n",
      "Total Epoch Testing MAPE: PCE = 75.20565032958984\n",
      "                              Voc = 58.94228744506836\n",
      "                              Jsc = 80.62509155273438\n",
      "                              FF = 13.148183822631836\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 73.40947723388672\n",
      "                              Voc = 58.7695426940918\n",
      "                              Jsc = 80.45994567871094\n",
      "                              FF = 12.406169891357422\n",
      "Finished epoch  459\n",
      "On epoch  460\n",
      "Total Epoch Testing MAPE: PCE = 74.48213958740234\n",
      "                              Voc = 58.42680358886719\n",
      "                              Jsc = 80.59143829345703\n",
      "                              FF = 11.965835571289062\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 75.45502471923828\n",
      "                              Voc = 58.5632209777832\n",
      "                              Jsc = 79.65332794189453\n",
      "                              FF = 11.982479095458984\n",
      "Finished epoch  461\n",
      "On epoch  462\n",
      "Total Epoch Testing MAPE: PCE = 72.58623504638672\n",
      "                              Voc = 58.66455841064453\n",
      "                              Jsc = 79.22827911376953\n",
      "                              FF = 11.889843940734863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 72.98554992675781\n",
      "                              Voc = 58.70781707763672\n",
      "                              Jsc = 79.00830078125\n",
      "                              FF = 11.975610733032227\n",
      "Finished epoch  463\n",
      "On epoch  464\n",
      "Total Epoch Testing MAPE: PCE = 75.47549438476562\n",
      "                              Voc = 57.651580810546875\n",
      "                              Jsc = 78.15106964111328\n",
      "                              FF = 12.466957092285156\n",
      "Finished epoch  464\n",
      "On epoch  465\n",
      "Total Epoch Testing MAPE: PCE = 78.86046600341797\n",
      "                              Voc = 57.836307525634766\n",
      "                              Jsc = 78.38260650634766\n",
      "                              FF = 12.871810913085938\n",
      "Finished epoch  465\n",
      "On epoch  466\n",
      "Total Epoch Testing MAPE: PCE = 80.29856872558594\n",
      "                              Voc = 57.828765869140625\n",
      "                              Jsc = 78.53738403320312\n",
      "                              FF = 13.169363021850586\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 80.20340728759766\n",
      "                              Voc = 57.98540115356445\n",
      "                              Jsc = 79.25648498535156\n",
      "                              FF = 13.160808563232422\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 78.8078384399414\n",
      "                              Voc = 57.87569808959961\n",
      "                              Jsc = 79.70348358154297\n",
      "                              FF = 13.48419189453125\n",
      "Finished epoch  468\n",
      "On epoch  469\n",
      "Total Epoch Testing MAPE: PCE = 79.89364624023438\n",
      "                              Voc = 57.646751403808594\n",
      "                              Jsc = 79.00892639160156\n",
      "                              FF = 14.174863815307617\n",
      "Finished epoch  469\n",
      "On epoch  470\n",
      "Total Epoch Testing MAPE: PCE = 80.37454223632812\n",
      "                              Voc = 58.47895812988281\n",
      "                              Jsc = 78.52030944824219\n",
      "                              FF = 13.322281837463379\n",
      "Finished epoch  470\n",
      "On epoch  471\n",
      "Total Epoch Testing MAPE: PCE = 81.22623443603516\n",
      "                              Voc = 58.43861770629883\n",
      "                              Jsc = 77.40943908691406\n",
      "                              FF = 12.865480422973633\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 80.75241088867188\n",
      "                              Voc = 59.11769104003906\n",
      "                              Jsc = 77.76836395263672\n",
      "                              FF = 13.066521644592285\n",
      "Finished epoch  472\n",
      "On epoch  473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 80.5973892211914\n",
      "                              Voc = 58.51520919799805\n",
      "                              Jsc = 76.94829559326172\n",
      "                              FF = 12.630738258361816\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 79.98570251464844\n",
      "                              Voc = 58.234012603759766\n",
      "                              Jsc = 77.3382797241211\n",
      "                              FF = 12.125539779663086\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 79.5076675415039\n",
      "                              Voc = 58.02823257446289\n",
      "                              Jsc = 78.04893493652344\n",
      "                              FF = 11.752080917358398\n",
      "Finished epoch  475\n",
      "On epoch  476\n",
      "Total Epoch Testing MAPE: PCE = 79.02881622314453\n",
      "                              Voc = 57.897239685058594\n",
      "                              Jsc = 77.45854187011719\n",
      "                              FF = 12.480400085449219\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 78.1530532836914\n",
      "                              Voc = 57.79804992675781\n",
      "                              Jsc = 77.9668960571289\n",
      "                              FF = 12.217459678649902\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 78.7251205444336\n",
      "                              Voc = 57.67323303222656\n",
      "                              Jsc = 79.0838394165039\n",
      "                              FF = 12.441868782043457\n",
      "Finished epoch  478\n",
      "On epoch  479\n",
      "Total Epoch Testing MAPE: PCE = 78.70091247558594\n",
      "                              Voc = 58.20597839355469\n",
      "                              Jsc = 78.55626678466797\n",
      "                              FF = 13.431458473205566\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 78.03880310058594\n",
      "                              Voc = 58.23584747314453\n",
      "                              Jsc = 79.343017578125\n",
      "                              FF = 12.594943046569824\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 79.10093688964844\n",
      "                              Voc = 58.16560363769531\n",
      "                              Jsc = 79.92428588867188\n",
      "                              FF = 12.990802764892578\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 78.15541076660156\n",
      "                              Voc = 58.37127685546875\n",
      "                              Jsc = 79.60748291015625\n",
      "                              FF = 12.900386810302734\n",
      "Finished epoch  482\n",
      "On epoch  483\n",
      "Total Epoch Testing MAPE: PCE = 76.87264251708984\n",
      "                              Voc = 58.12033462524414\n",
      "                              Jsc = 80.48979949951172\n",
      "                              FF = 12.738897323608398\n",
      "Finished epoch  483\n",
      "On epoch  484\n",
      "Total Epoch Testing MAPE: PCE = 75.930908203125\n",
      "                              Voc = 58.249969482421875\n",
      "                              Jsc = 80.66598510742188\n",
      "                              FF = 13.268759727478027\n",
      "Finished epoch  484\n",
      "On epoch  485\n",
      "Total Epoch Testing MAPE: PCE = 75.41854095458984\n",
      "                              Voc = 57.99296569824219\n",
      "                              Jsc = 80.7823486328125\n",
      "                              FF = 13.006505012512207\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 72.74810791015625\n",
      "                              Voc = 58.4277458190918\n",
      "                              Jsc = 80.93090057373047\n",
      "                              FF = 13.372352600097656\n",
      "Finished epoch  486\n",
      "On epoch  487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 73.96167755126953\n",
      "                              Voc = 58.5006103515625\n",
      "                              Jsc = 80.42534637451172\n",
      "                              FF = 13.674203872680664\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 71.15686798095703\n",
      "                              Voc = 58.334266662597656\n",
      "                              Jsc = 79.39139556884766\n",
      "                              FF = 13.920206069946289\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 68.72957611083984\n",
      "                              Voc = 58.65286636352539\n",
      "                              Jsc = 80.25922393798828\n",
      "                              FF = 13.916863441467285\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 70.7703857421875\n",
      "                              Voc = 58.922176361083984\n",
      "                              Jsc = 79.22928619384766\n",
      "                              FF = 13.581748008728027\n",
      "Finished epoch  490\n",
      "On epoch  491\n",
      "Total Epoch Testing MAPE: PCE = 71.64520263671875\n",
      "                              Voc = 58.60945510864258\n",
      "                              Jsc = 79.16847229003906\n",
      "                              FF = 13.98486614227295\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 70.15863037109375\n",
      "                              Voc = 58.64419174194336\n",
      "                              Jsc = 79.7193374633789\n",
      "                              FF = 13.769891738891602\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 69.23455810546875\n",
      "                              Voc = 58.55460739135742\n",
      "                              Jsc = 78.98272705078125\n",
      "                              FF = 13.499983787536621\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 72.88438415527344\n",
      "                              Voc = 59.073387145996094\n",
      "                              Jsc = 80.04725646972656\n",
      "                              FF = 14.234704971313477\n",
      "Finished epoch  494\n",
      "On epoch  495\n",
      "Total Epoch Testing MAPE: PCE = 75.21060180664062\n",
      "                              Voc = 58.96586227416992\n",
      "                              Jsc = 79.734375\n",
      "                              FF = 13.878266334533691\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 72.40348815917969\n",
      "                              Voc = 59.12870788574219\n",
      "                              Jsc = 79.93667602539062\n",
      "                              FF = 13.336464881896973\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 75.64415740966797\n",
      "                              Voc = 59.620635986328125\n",
      "                              Jsc = 78.64629364013672\n",
      "                              FF = 13.171789169311523\n",
      "Finished epoch  497\n",
      "On epoch  498\n",
      "Total Epoch Testing MAPE: PCE = 77.21357727050781\n",
      "                              Voc = 59.282474517822266\n",
      "                              Jsc = 79.33651733398438\n",
      "                              FF = 14.097627639770508\n",
      "Finished epoch  498\n",
      "On epoch  499\n",
      "Total Epoch Testing MAPE: PCE = 76.55695343017578\n",
      "                              Voc = 59.18445587158203\n",
      "                              Jsc = 79.17906951904297\n",
      "                              FF = 14.0747652053833\n",
      "Finished epoch  499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "cv_fits = {}\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "    print(f'Fold # {fold}')\n",
    "    print('-----------------------------')\n",
    "    \n",
    "    train_loader, test_loader = nuts.get_fold_dataloaders_df(x_train, y_train, train_index, test_index)\n",
    "        \n",
    "    model = net.OPV_df_NN(in_dims = in_dims, out_dims = out_dims).to(device)\n",
    "    model.apply(nuts.init_weights)\n",
    "    \n",
    "    cv_fits[fold] = nuts.CV_OPV_fit(model, train_loader, test_loader, lr = learning_rate, epochs = num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'lr': 1e-06,\n",
       "  'best_loss_epoch': 16,\n",
       "  'best_acc_epoch': 67,\n",
       "  'best_r2_epoch': 2,\n",
       "  'pce_loss': [0.4118993580341339,\n",
       "   0.3264749348163605,\n",
       "   0.24494045972824097,\n",
       "   0.21544542908668518,\n",
       "   0.22122803330421448,\n",
       "   0.18884047865867615,\n",
       "   0.1914738565683365,\n",
       "   0.1760891079902649,\n",
       "   0.17497818171977997,\n",
       "   0.17064811289310455,\n",
       "   0.15187039971351624,\n",
       "   0.18127848207950592,\n",
       "   0.16984033584594727,\n",
       "   0.181237131357193,\n",
       "   0.1854371726512909,\n",
       "   0.21338482201099396,\n",
       "   0.22496718168258667,\n",
       "   0.27478817105293274,\n",
       "   0.31798943877220154,\n",
       "   0.3683077096939087,\n",
       "   0.3633786737918854,\n",
       "   0.3638986647129059,\n",
       "   0.3570529818534851,\n",
       "   0.35656318068504333,\n",
       "   0.3650798201560974,\n",
       "   0.4209156334400177,\n",
       "   0.4290125072002411,\n",
       "   0.44594085216522217,\n",
       "   0.43775492906570435,\n",
       "   0.46751412749290466,\n",
       "   0.47730207443237305,\n",
       "   0.4899076819419861,\n",
       "   0.49681875109672546,\n",
       "   0.5026088953018188,\n",
       "   0.5067609548568726,\n",
       "   0.5143938660621643,\n",
       "   0.5114617347717285,\n",
       "   0.5175507068634033,\n",
       "   0.5271695256233215,\n",
       "   0.5275349617004395,\n",
       "   0.5526522994041443,\n",
       "   0.5419723391532898,\n",
       "   0.5297365188598633,\n",
       "   0.5478897094726562,\n",
       "   0.5175111889839172,\n",
       "   0.520563006401062,\n",
       "   0.4969289004802704,\n",
       "   0.503188967704773,\n",
       "   0.5186052322387695,\n",
       "   0.5325254201889038,\n",
       "   0.5341944098472595,\n",
       "   0.5302083492279053,\n",
       "   0.5522377490997314,\n",
       "   0.5471814870834351,\n",
       "   0.5419344305992126,\n",
       "   0.5405877828598022,\n",
       "   0.5283783674240112,\n",
       "   0.5313429236412048,\n",
       "   0.5147972702980042,\n",
       "   0.5205269455909729,\n",
       "   0.5234003067016602,\n",
       "   0.5229362845420837,\n",
       "   0.5340460538864136,\n",
       "   0.5029987096786499,\n",
       "   0.4702646732330322,\n",
       "   0.4746536612510681,\n",
       "   0.4798223078250885,\n",
       "   0.4802103340625763,\n",
       "   0.487676739692688,\n",
       "   0.4940136969089508,\n",
       "   0.514920175075531,\n",
       "   0.5277673602104187,\n",
       "   0.5162520408630371,\n",
       "   0.5034392476081848,\n",
       "   0.49969860911369324,\n",
       "   0.5261749625205994,\n",
       "   0.5314922332763672,\n",
       "   0.5443293452262878,\n",
       "   0.5355465412139893,\n",
       "   0.5441018342971802,\n",
       "   0.5406332015991211,\n",
       "   0.529102087020874,\n",
       "   0.5170137882232666,\n",
       "   0.5125131607055664,\n",
       "   0.5049855709075928,\n",
       "   0.49197110533714294,\n",
       "   0.48952460289001465,\n",
       "   0.4836518466472626,\n",
       "   0.48924148082733154,\n",
       "   0.4989176094532013,\n",
       "   0.521425724029541,\n",
       "   0.5162554383277893,\n",
       "   0.49514177441596985,\n",
       "   0.49439871311187744,\n",
       "   0.4917469620704651,\n",
       "   0.49831944704055786,\n",
       "   0.5062493681907654,\n",
       "   0.5059917569160461,\n",
       "   0.4874895215034485,\n",
       "   0.49468737840652466,\n",
       "   0.4927451014518738,\n",
       "   0.4999059736728668,\n",
       "   0.52410489320755,\n",
       "   0.525033712387085,\n",
       "   0.520821213722229,\n",
       "   0.512812614440918,\n",
       "   0.5130659937858582,\n",
       "   0.5297227501869202,\n",
       "   0.5231218934059143,\n",
       "   0.518802285194397,\n",
       "   0.5375377535820007,\n",
       "   0.5364013910293579,\n",
       "   0.5462095737457275,\n",
       "   0.5593783855438232,\n",
       "   0.5547147393226624,\n",
       "   0.5475854277610779,\n",
       "   0.532523512840271,\n",
       "   0.5318410992622375,\n",
       "   0.5314099788665771,\n",
       "   0.527741014957428,\n",
       "   0.5271309614181519,\n",
       "   0.5422830581665039,\n",
       "   0.5328201651573181,\n",
       "   0.5293140411376953,\n",
       "   0.5092639327049255,\n",
       "   0.47926342487335205,\n",
       "   0.47988176345825195,\n",
       "   0.46646344661712646,\n",
       "   0.462240070104599,\n",
       "   0.4791639745235443,\n",
       "   0.4841647744178772,\n",
       "   0.4973735809326172,\n",
       "   0.5129580497741699,\n",
       "   0.5085336565971375,\n",
       "   0.5113630294799805,\n",
       "   0.5062087178230286,\n",
       "   0.5096350312232971,\n",
       "   0.5337008833885193,\n",
       "   0.5352924466133118,\n",
       "   0.5185006260871887,\n",
       "   0.5272688269615173,\n",
       "   0.526798665523529,\n",
       "   0.5255841016769409,\n",
       "   0.501784086227417,\n",
       "   0.5110312104225159,\n",
       "   0.5002181529998779,\n",
       "   0.5088273286819458,\n",
       "   0.5010513663291931,\n",
       "   0.4954391121864319,\n",
       "   0.5083080530166626,\n",
       "   0.4987426996231079,\n",
       "   0.504828155040741,\n",
       "   0.506992757320404,\n",
       "   0.48775196075439453,\n",
       "   0.5044223666191101,\n",
       "   0.5076850056648254,\n",
       "   0.5211998820304871,\n",
       "   0.5307623744010925,\n",
       "   0.533759593963623,\n",
       "   0.517412543296814,\n",
       "   0.5266354084014893,\n",
       "   0.5574995875358582,\n",
       "   0.5519431829452515,\n",
       "   0.5456830263137817,\n",
       "   0.5420119762420654,\n",
       "   0.5358992218971252,\n",
       "   0.5357098579406738,\n",
       "   0.5155906081199646,\n",
       "   0.5305727124214172,\n",
       "   0.5398916006088257,\n",
       "   0.5377880930900574,\n",
       "   0.5194811820983887,\n",
       "   0.5401900410652161,\n",
       "   0.5371918678283691,\n",
       "   0.5404135584831238,\n",
       "   0.5172909498214722,\n",
       "   0.4964914321899414,\n",
       "   0.5106411576271057,\n",
       "   0.4865471422672272,\n",
       "   0.4996531903743744,\n",
       "   0.5047619342803955,\n",
       "   0.5156837701797485,\n",
       "   0.5072277188301086,\n",
       "   0.5174193382263184,\n",
       "   0.5081799626350403,\n",
       "   0.5128779411315918,\n",
       "   0.518872082233429,\n",
       "   0.5185061097145081,\n",
       "   0.5332266092300415,\n",
       "   0.5368309020996094,\n",
       "   0.5094939470291138,\n",
       "   0.49526557326316833,\n",
       "   0.5088374018669128,\n",
       "   0.5001766085624695,\n",
       "   0.49667567014694214,\n",
       "   0.4841937720775604,\n",
       "   0.4902993142604828,\n",
       "   0.46781933307647705,\n",
       "   0.4767360985279083,\n",
       "   0.4734514355659485,\n",
       "   0.4769423007965088,\n",
       "   0.48193907737731934,\n",
       "   0.4927719235420227,\n",
       "   0.4781259298324585,\n",
       "   0.47852426767349243,\n",
       "   0.4899841547012329,\n",
       "   0.490494042634964,\n",
       "   0.49799495935440063,\n",
       "   0.5048975944519043,\n",
       "   0.499483197927475,\n",
       "   0.5046178698539734,\n",
       "   0.4917590320110321,\n",
       "   0.4793626368045807,\n",
       "   0.4744502305984497,\n",
       "   0.4815289080142975,\n",
       "   0.48014381527900696,\n",
       "   0.49257802963256836,\n",
       "   0.4773408770561218,\n",
       "   0.4893477261066437,\n",
       "   0.48498114943504333,\n",
       "   0.5090107321739197,\n",
       "   0.5120099186897278,\n",
       "   0.517123281955719,\n",
       "   0.5234874486923218,\n",
       "   0.5139400362968445,\n",
       "   0.5239445567131042,\n",
       "   0.5221962332725525,\n",
       "   0.5059520602226257,\n",
       "   0.5213475227355957,\n",
       "   0.5237950682640076,\n",
       "   0.5230506658554077,\n",
       "   0.5272701978683472,\n",
       "   0.522662878036499,\n",
       "   0.5169351100921631,\n",
       "   0.5074099898338318,\n",
       "   0.497313916683197,\n",
       "   0.5054215788841248,\n",
       "   0.5068617463111877,\n",
       "   0.5149129629135132,\n",
       "   0.5238023400306702,\n",
       "   0.5288742780685425,\n",
       "   0.5074403285980225,\n",
       "   0.5149077773094177,\n",
       "   0.5189399719238281,\n",
       "   0.5243805050849915,\n",
       "   0.54078209400177,\n",
       "   0.5259708762168884,\n",
       "   0.508291482925415,\n",
       "   0.525601863861084,\n",
       "   0.5172996520996094,\n",
       "   0.5294393301010132,\n",
       "   0.5424298048019409,\n",
       "   0.5497850775718689,\n",
       "   0.5216377973556519,\n",
       "   0.5196638107299805,\n",
       "   0.5232621431350708,\n",
       "   0.5180315375328064,\n",
       "   0.5229834318161011,\n",
       "   0.4999963641166687,\n",
       "   0.5103738903999329,\n",
       "   0.5119433403015137,\n",
       "   0.4973216652870178,\n",
       "   0.5072619915008545,\n",
       "   0.4958075284957886,\n",
       "   0.5023466944694519,\n",
       "   0.5138964653015137,\n",
       "   0.5159505605697632,\n",
       "   0.5092682838439941,\n",
       "   0.5235652923583984,\n",
       "   0.5347733497619629,\n",
       "   0.5389735698699951,\n",
       "   0.5152709484100342,\n",
       "   0.5049204230308533,\n",
       "   0.5039353966712952,\n",
       "   0.49761059880256653,\n",
       "   0.5137568712234497,\n",
       "   0.510148823261261,\n",
       "   0.5013223886489868,\n",
       "   0.5136333107948303,\n",
       "   0.5180672407150269,\n",
       "   0.508531391620636,\n",
       "   0.5202251672744751,\n",
       "   0.5282265543937683,\n",
       "   0.528946578502655,\n",
       "   0.5291817784309387,\n",
       "   0.5012133121490479,\n",
       "   0.5024616122245789,\n",
       "   0.50797438621521,\n",
       "   0.5018277764320374,\n",
       "   0.5022736191749573,\n",
       "   0.4981667697429657,\n",
       "   0.48254626989364624,\n",
       "   0.48792266845703125,\n",
       "   0.47124820947647095,\n",
       "   0.469950795173645,\n",
       "   0.4814656674861908,\n",
       "   0.48037004470825195,\n",
       "   0.46810460090637207,\n",
       "   0.47406768798828125,\n",
       "   0.4835493564605713,\n",
       "   0.4762047529220581,\n",
       "   0.4723316431045532,\n",
       "   0.46904703974723816,\n",
       "   0.48736056685447693,\n",
       "   0.4854665994644165,\n",
       "   0.4844138026237488,\n",
       "   0.4869445562362671,\n",
       "   0.48344624042510986,\n",
       "   0.4959788918495178,\n",
       "   0.4919089078903198,\n",
       "   0.5105108618736267,\n",
       "   0.5074847340583801,\n",
       "   0.5137647986412048,\n",
       "   0.5121570825576782,\n",
       "   0.5249613523483276,\n",
       "   0.5561444759368896,\n",
       "   0.5613691210746765,\n",
       "   0.5588972568511963,\n",
       "   0.530737042427063,\n",
       "   0.5352240800857544,\n",
       "   0.5251457095146179,\n",
       "   0.5269292593002319,\n",
       "   0.5125384330749512,\n",
       "   0.5195190906524658,\n",
       "   0.5243117809295654,\n",
       "   0.539506733417511,\n",
       "   0.5511046648025513,\n",
       "   0.5152523517608643,\n",
       "   0.5226249098777771,\n",
       "   0.5138688087463379,\n",
       "   0.5241998434066772,\n",
       "   0.5319486260414124,\n",
       "   0.5190160274505615,\n",
       "   0.5137656927108765,\n",
       "   0.5016639828681946,\n",
       "   0.5068941712379456,\n",
       "   0.5045258402824402,\n",
       "   0.5044667720794678,\n",
       "   0.5133805871009827,\n",
       "   0.5149431228637695,\n",
       "   0.5058516263961792,\n",
       "   0.4866265058517456,\n",
       "   0.4915865659713745,\n",
       "   0.4932446777820587,\n",
       "   0.50592041015625,\n",
       "   0.5038929581642151,\n",
       "   0.5090475082397461,\n",
       "   0.49886178970336914,\n",
       "   0.5127419233322144,\n",
       "   0.5147016644477844,\n",
       "   0.5131018757820129,\n",
       "   0.49911099672317505,\n",
       "   0.5039048194885254,\n",
       "   0.5068178176879883,\n",
       "   0.5188890695571899,\n",
       "   0.515428900718689,\n",
       "   0.5254690647125244,\n",
       "   0.5347216129302979,\n",
       "   0.5333608388900757,\n",
       "   0.5310208201408386,\n",
       "   0.5307896137237549,\n",
       "   0.5206250548362732,\n",
       "   0.510697066783905,\n",
       "   0.490852952003479,\n",
       "   0.4915444552898407,\n",
       "   0.4768984317779541,\n",
       "   0.48908746242523193,\n",
       "   0.4689229726791382,\n",
       "   0.46292129158973694,\n",
       "   0.4333496689796448,\n",
       "   0.45550400018692017,\n",
       "   0.4674321115016937,\n",
       "   0.49041932821273804,\n",
       "   0.47725653648376465,\n",
       "   0.49415409564971924,\n",
       "   0.5036609768867493,\n",
       "   0.5130135416984558,\n",
       "   0.5183767676353455,\n",
       "   0.5141509771347046,\n",
       "   0.5123626589775085,\n",
       "   0.4998639225959778,\n",
       "   0.511329710483551,\n",
       "   0.5128189325332642,\n",
       "   0.5141786336898804,\n",
       "   0.5058456063270569,\n",
       "   0.5241091847419739,\n",
       "   0.5234705805778503,\n",
       "   0.5176207423210144,\n",
       "   0.5061631798744202,\n",
       "   0.515552818775177,\n",
       "   0.5197044610977173,\n",
       "   0.513983964920044,\n",
       "   0.519697904586792,\n",
       "   0.5013033747673035,\n",
       "   0.4932699203491211,\n",
       "   0.4962466359138489,\n",
       "   0.4937984347343445,\n",
       "   0.4866825044155121,\n",
       "   0.4925820231437683,\n",
       "   0.4909381866455078,\n",
       "   0.5062073469161987,\n",
       "   0.5003559589385986,\n",
       "   0.5025466680526733,\n",
       "   0.5234416127204895,\n",
       "   0.5235755443572998,\n",
       "   0.5101029872894287,\n",
       "   0.4943634867668152,\n",
       "   0.5051251649856567,\n",
       "   0.49097952246665955,\n",
       "   0.49898195266723633,\n",
       "   0.4908391833305359,\n",
       "   0.48746156692504883,\n",
       "   0.5055646300315857,\n",
       "   0.5009990334510803,\n",
       "   0.5236390829086304,\n",
       "   0.5191487669944763,\n",
       "   0.5191815495491028,\n",
       "   0.5130470395088196,\n",
       "   0.5160237550735474,\n",
       "   0.5159280300140381,\n",
       "   0.522013247013092,\n",
       "   0.5270465016365051,\n",
       "   0.5262250304222107,\n",
       "   0.5375156402587891,\n",
       "   0.525495171546936,\n",
       "   0.5459641218185425,\n",
       "   0.5556149482727051,\n",
       "   0.5467536449432373,\n",
       "   0.5326389074325562,\n",
       "   0.5386032462120056,\n",
       "   0.5395075082778931,\n",
       "   0.5597790479660034,\n",
       "   0.5621326565742493,\n",
       "   0.5543906688690186,\n",
       "   0.565974771976471,\n",
       "   0.5496365427970886,\n",
       "   0.5520408749580383,\n",
       "   0.536902666091919,\n",
       "   0.5178707838058472,\n",
       "   0.5134254693984985,\n",
       "   0.5114311575889587,\n",
       "   0.522735059261322,\n",
       "   0.5354286432266235,\n",
       "   0.5432935357093811,\n",
       "   0.5439402461051941,\n",
       "   0.5443169474601746,\n",
       "   0.5334375500679016,\n",
       "   0.517602801322937,\n",
       "   0.525426983833313,\n",
       "   0.5315071940422058,\n",
       "   0.5013080835342407,\n",
       "   0.5147847533226013,\n",
       "   0.5337873101234436,\n",
       "   0.5205216407775879,\n",
       "   0.5315856337547302,\n",
       "   0.5210955142974854,\n",
       "   0.5143376588821411,\n",
       "   0.5164657235145569,\n",
       "   0.4955284893512726,\n",
       "   0.497405469417572,\n",
       "   0.5047091245651245,\n",
       "   0.48749619722366333,\n",
       "   0.4972153902053833,\n",
       "   0.4917007088661194,\n",
       "   0.4878677725791931,\n",
       "   0.4875444173812866,\n",
       "   0.488636314868927,\n",
       "   0.48405465483665466,\n",
       "   0.4835713505744934,\n",
       "   0.5030112266540527,\n",
       "   0.5072945356369019,\n",
       "   0.5067904591560364,\n",
       "   0.5114248394966125,\n",
       "   0.520697832107544,\n",
       "   0.5101723670959473,\n",
       "   0.5009862184524536,\n",
       "   0.49514737725257874,\n",
       "   0.48846858739852905,\n",
       "   0.49432000517845154,\n",
       "   0.4983907639980316,\n",
       "   0.5068504214286804,\n",
       "   0.5120049715042114,\n",
       "   0.4978826642036438,\n",
       "   0.4865726828575134,\n",
       "   0.48524364829063416,\n",
       "   0.47413742542266846,\n",
       "   0.4928193688392639,\n",
       "   0.4958849549293518,\n",
       "   0.5075070858001709,\n",
       "   0.49374744296073914,\n",
       "   0.4982166290283203,\n",
       "   0.5029211044311523,\n",
       "   0.5064951181411743,\n",
       "   0.5140160918235779,\n",
       "   0.512606680393219,\n",
       "   0.5093886256217957,\n",
       "   0.5060743093490601,\n",
       "   0.5100557804107666,\n",
       "   0.5075708031654358,\n",
       "   0.4929301142692566],\n",
       "  'voc_loss': [0.6515462398529053,\n",
       "   0.6515462398529053,\n",
       "   0.6515462398529053,\n",
       "   0.6515462398529053,\n",
       "   0.6515462398529053,\n",
       "   0.6515462398529053,\n",
       "   0.6515462398529053,\n",
       "   0.6473163962364197,\n",
       "   0.555924117565155,\n",
       "   0.47488975524902344,\n",
       "   0.39244216680526733,\n",
       "   0.32599884271621704,\n",
       "   0.2793034613132477,\n",
       "   0.24569885432720184,\n",
       "   0.22262996435165405,\n",
       "   0.20065507292747498,\n",
       "   0.1740218997001648,\n",
       "   0.13753363490104675,\n",
       "   0.12402113527059555,\n",
       "   0.11220245063304901,\n",
       "   0.10039571672677994,\n",
       "   0.09982176125049591,\n",
       "   0.07853813469409943,\n",
       "   0.07417742908000946,\n",
       "   0.05924514681100845,\n",
       "   0.0529632493853569,\n",
       "   0.05116444081068039,\n",
       "   0.04206226021051407,\n",
       "   0.03465159609913826,\n",
       "   0.03698897361755371,\n",
       "   0.03833360970020294,\n",
       "   0.04023609310388565,\n",
       "   0.03816957771778107,\n",
       "   0.03630394488573074,\n",
       "   0.04047571122646332,\n",
       "   0.034608207643032074,\n",
       "   0.0334075503051281,\n",
       "   0.03299296274781227,\n",
       "   0.03359093517065048,\n",
       "   0.03571425750851631,\n",
       "   0.0321723036468029,\n",
       "   0.02844688482582569,\n",
       "   0.02788136899471283,\n",
       "   0.026914969086647034,\n",
       "   0.02857583947479725,\n",
       "   0.028462540358304977,\n",
       "   0.028402084484696388,\n",
       "   0.02927919290959835,\n",
       "   0.027514001354575157,\n",
       "   0.028574269264936447,\n",
       "   0.028637446463108063,\n",
       "   0.027627751231193542,\n",
       "   0.02790350653231144,\n",
       "   0.028510315343737602,\n",
       "   0.02750144712626934,\n",
       "   0.025552790611982346,\n",
       "   0.027341356500983238,\n",
       "   0.02741936780512333,\n",
       "   0.02835676446557045,\n",
       "   0.028995418921113014,\n",
       "   0.028277786448597908,\n",
       "   0.03283636271953583,\n",
       "   0.031185723841190338,\n",
       "   0.033921290189027786,\n",
       "   0.03305238485336304,\n",
       "   0.0336383655667305,\n",
       "   0.03135969117283821,\n",
       "   0.027834462001919746,\n",
       "   0.027035048231482506,\n",
       "   0.025846440345048904,\n",
       "   0.02604552172124386,\n",
       "   0.026311112567782402,\n",
       "   0.026827014982700348,\n",
       "   0.02822689339518547,\n",
       "   0.025699429214000702,\n",
       "   0.025064704939723015,\n",
       "   0.026982644572854042,\n",
       "   0.025286581367254257,\n",
       "   0.026747817173600197,\n",
       "   0.025447577238082886,\n",
       "   0.02531753107905388,\n",
       "   0.02479439042508602,\n",
       "   0.02565159648656845,\n",
       "   0.02664034627377987,\n",
       "   0.026944341138005257,\n",
       "   0.028309527784585953,\n",
       "   0.026724549010396004,\n",
       "   0.02483402192592621,\n",
       "   0.025185152888298035,\n",
       "   0.02521202526986599,\n",
       "   0.025218423455953598,\n",
       "   0.02554636262357235,\n",
       "   0.02657410502433777,\n",
       "   0.025215333327651024,\n",
       "   0.02535163424909115,\n",
       "   0.025137323886156082,\n",
       "   0.02581951767206192,\n",
       "   0.025373132899403572,\n",
       "   0.025498103350400925,\n",
       "   0.02510552853345871,\n",
       "   0.02497350424528122,\n",
       "   0.025468043982982635,\n",
       "   0.02559218555688858,\n",
       "   0.025317776948213577,\n",
       "   0.024796906858682632,\n",
       "   0.026041541248559952,\n",
       "   0.025261299684643745,\n",
       "   0.02518622577190399,\n",
       "   0.025489479303359985,\n",
       "   0.024818886071443558,\n",
       "   0.025053348392248154,\n",
       "   0.024825122207403183,\n",
       "   0.02473609335720539,\n",
       "   0.024972138926386833,\n",
       "   0.024980761110782623,\n",
       "   0.024655666202306747,\n",
       "   0.024288777261972427,\n",
       "   0.02467077597975731,\n",
       "   0.024373017251491547,\n",
       "   0.023732537403702736,\n",
       "   0.022994419559836388,\n",
       "   0.023267047479748726,\n",
       "   0.022872809320688248,\n",
       "   0.022698460146784782,\n",
       "   0.022672703489661217,\n",
       "   0.023505646735429764,\n",
       "   0.024815967306494713,\n",
       "   0.025243327021598816,\n",
       "   0.02549702860414982,\n",
       "   0.0245169997215271,\n",
       "   0.024474266916513443,\n",
       "   0.023484565317630768,\n",
       "   0.02354886755347252,\n",
       "   0.023686816915869713,\n",
       "   0.023319333791732788,\n",
       "   0.023099876940250397,\n",
       "   0.024282434955239296,\n",
       "   0.022725505754351616,\n",
       "   0.022800803184509277,\n",
       "   0.023798994719982147,\n",
       "   0.024437185376882553,\n",
       "   0.02427736669778824,\n",
       "   0.02342049777507782,\n",
       "   0.023357590660452843,\n",
       "   0.023722046986222267,\n",
       "   0.02281353995203972,\n",
       "   0.022509612143039703,\n",
       "   0.022983824834227562,\n",
       "   0.023194821551442146,\n",
       "   0.023742590099573135,\n",
       "   0.025416094809770584,\n",
       "   0.025894202291965485,\n",
       "   0.02585773915052414,\n",
       "   0.026536067947745323,\n",
       "   0.025537628680467606,\n",
       "   0.026530325412750244,\n",
       "   0.026380755007267,\n",
       "   0.026924826204776764,\n",
       "   0.028629669919610023,\n",
       "   0.028746889904141426,\n",
       "   0.0259413942694664,\n",
       "   0.024697870016098022,\n",
       "   0.025016840547323227,\n",
       "   0.025092866271734238,\n",
       "   0.024643952026963234,\n",
       "   0.023755298927426338,\n",
       "   0.023404153063893318,\n",
       "   0.023779958486557007,\n",
       "   0.023675108328461647,\n",
       "   0.023933084681630135,\n",
       "   0.02452133595943451,\n",
       "   0.02670656144618988,\n",
       "   0.030591707676649094,\n",
       "   0.03047492355108261,\n",
       "   0.03146053105592728,\n",
       "   0.031156226992607117,\n",
       "   0.0272744782269001,\n",
       "   0.028014445677399635,\n",
       "   0.030477210879325867,\n",
       "   0.029004914686083794,\n",
       "   0.025890715420246124,\n",
       "   0.026042094454169273,\n",
       "   0.026073312386870384,\n",
       "   0.024291135370731354,\n",
       "   0.02374342828989029,\n",
       "   0.023545000702142715,\n",
       "   0.022701267153024673,\n",
       "   0.022494662553071976,\n",
       "   0.02239387109875679,\n",
       "   0.022425971925258636,\n",
       "   0.0222407765686512,\n",
       "   0.024121563881635666,\n",
       "   0.023156462237238884,\n",
       "   0.02338433638215065,\n",
       "   0.023145418614149094,\n",
       "   0.023955680429935455,\n",
       "   0.02327793650329113,\n",
       "   0.022144954651594162,\n",
       "   0.020982803776860237,\n",
       "   0.021482937037944794,\n",
       "   0.022560259327292442,\n",
       "   0.023379424586892128,\n",
       "   0.022206582129001617,\n",
       "   0.021644605323672295,\n",
       "   0.02207072451710701,\n",
       "   0.022428222000598907,\n",
       "   0.023027027025818825,\n",
       "   0.02322639897465706,\n",
       "   0.02256998047232628,\n",
       "   0.022364970296621323,\n",
       "   0.022327985614538193,\n",
       "   0.022977713495492935,\n",
       "   0.022232307121157646,\n",
       "   0.02185172587633133,\n",
       "   0.023582633584737778,\n",
       "   0.02468264475464821,\n",
       "   0.02444039285182953,\n",
       "   0.024806112051010132,\n",
       "   0.02560797706246376,\n",
       "   0.02415989711880684,\n",
       "   0.02473752573132515,\n",
       "   0.02462146058678627,\n",
       "   0.023224543780088425,\n",
       "   0.022525519132614136,\n",
       "   0.023691661655902863,\n",
       "   0.024238627403974533,\n",
       "   0.026620855554938316,\n",
       "   0.02959636226296425,\n",
       "   0.026298176497220993,\n",
       "   0.024579819291830063,\n",
       "   0.023456543684005737,\n",
       "   0.024024344980716705,\n",
       "   0.02247447706758976,\n",
       "   0.022801944985985756,\n",
       "   0.022510342299938202,\n",
       "   0.02290111780166626,\n",
       "   0.022833798080682755,\n",
       "   0.023181790485978127,\n",
       "   0.023539818823337555,\n",
       "   0.023562097921967506,\n",
       "   0.023592626675963402,\n",
       "   0.02388237789273262,\n",
       "   0.023592926561832428,\n",
       "   0.023639336228370667,\n",
       "   0.024986375123262405,\n",
       "   0.023870866745710373,\n",
       "   0.02333170175552368,\n",
       "   0.023752152919769287,\n",
       "   0.02434975653886795,\n",
       "   0.023462887853384018,\n",
       "   0.024768434464931488,\n",
       "   0.023479726165533066,\n",
       "   0.02416236698627472,\n",
       "   0.02444957010447979,\n",
       "   0.0247892364859581,\n",
       "   0.024186240509152412,\n",
       "   0.02370777726173401,\n",
       "   0.023482222110033035,\n",
       "   0.023990383371710777,\n",
       "   0.023979900404810905,\n",
       "   0.02713046967983246,\n",
       "   0.026570303365588188,\n",
       "   0.02626177668571472,\n",
       "   0.026180634275078773,\n",
       "   0.024891268461942673,\n",
       "   0.026493901386857033,\n",
       "   0.027238208800554276,\n",
       "   0.026755917817354202,\n",
       "   0.026850396767258644,\n",
       "   0.025347597897052765,\n",
       "   0.026225915178656578,\n",
       "   0.02519514039158821,\n",
       "   0.02501143515110016,\n",
       "   0.0248641986399889,\n",
       "   0.02456994354724884,\n",
       "   0.02468664012849331,\n",
       "   0.02362356334924698,\n",
       "   0.024024000391364098,\n",
       "   0.02396267093718052,\n",
       "   0.024259941652417183,\n",
       "   0.02368667908012867,\n",
       "   0.02436785399913788,\n",
       "   0.024287870153784752,\n",
       "   0.02356799505650997,\n",
       "   0.02364506945014,\n",
       "   0.02387033775448799,\n",
       "   0.023590924218297005,\n",
       "   0.02282583899796009,\n",
       "   0.023072438314557076,\n",
       "   0.02270117774605751,\n",
       "   0.02261771261692047,\n",
       "   0.023335609585046768,\n",
       "   0.023951655253767967,\n",
       "   0.024477511644363403,\n",
       "   0.024625960737466812,\n",
       "   0.024033332243561745,\n",
       "   0.024855654686689377,\n",
       "   0.02509162761271,\n",
       "   0.0255274698138237,\n",
       "   0.025606274604797363,\n",
       "   0.0265949834138155,\n",
       "   0.025803662836551666,\n",
       "   0.025908568874001503,\n",
       "   0.026521269232034683,\n",
       "   0.024179277941584587,\n",
       "   0.024790192022919655,\n",
       "   0.025289108976721764,\n",
       "   0.024124061688780785,\n",
       "   0.023186728358268738,\n",
       "   0.02336074598133564,\n",
       "   0.023479677736759186,\n",
       "   0.02346191555261612,\n",
       "   0.023150509223341942,\n",
       "   0.023593425750732422,\n",
       "   0.023797329515218735,\n",
       "   0.02460903488099575,\n",
       "   0.025248374789953232,\n",
       "   0.023307038471102715,\n",
       "   0.023977141827344894,\n",
       "   0.025353271514177322,\n",
       "   0.02537037432193756,\n",
       "   0.024483492597937584,\n",
       "   0.02370872162282467,\n",
       "   0.02444387599825859,\n",
       "   0.0248585045337677,\n",
       "   0.02595728263258934,\n",
       "   0.026310957968235016,\n",
       "   0.027400076389312744,\n",
       "   0.026826974004507065,\n",
       "   0.025822553783655167,\n",
       "   0.02664627507328987,\n",
       "   0.026659034192562103,\n",
       "   0.026404689997434616,\n",
       "   0.02750927023589611,\n",
       "   0.025410961359739304,\n",
       "   0.024534443393349648,\n",
       "   0.023886200040578842,\n",
       "   0.024585142731666565,\n",
       "   0.0252457857131958,\n",
       "   0.024648018181324005,\n",
       "   0.02639676257967949,\n",
       "   0.02729499340057373,\n",
       "   0.02922079898416996,\n",
       "   0.028056256473064423,\n",
       "   0.02888643555343151,\n",
       "   0.02911163866519928,\n",
       "   0.027096588164567947,\n",
       "   0.025407277047634125,\n",
       "   0.02474706992506981,\n",
       "   0.025308504700660706,\n",
       "   0.02602197602391243,\n",
       "   0.024201268330216408,\n",
       "   0.02586059458553791,\n",
       "   0.025383125990629196,\n",
       "   0.026455387473106384,\n",
       "   0.025998583063483238,\n",
       "   0.026935338973999023,\n",
       "   0.02735041454434395,\n",
       "   0.02730352059006691,\n",
       "   0.027603160589933395,\n",
       "   0.027201898396015167,\n",
       "   0.025494610890746117,\n",
       "   0.026692241430282593,\n",
       "   0.025759749114513397,\n",
       "   0.02491733431816101,\n",
       "   0.02459525689482689,\n",
       "   0.022669624537229538,\n",
       "   0.023286987096071243,\n",
       "   0.023569878190755844,\n",
       "   0.02448202297091484,\n",
       "   0.022558365017175674,\n",
       "   0.0233855452388525,\n",
       "   0.023421617224812508,\n",
       "   0.025114214047789574,\n",
       "   0.02466013841331005,\n",
       "   0.026015525683760643,\n",
       "   0.0253024660050869,\n",
       "   0.023537058383226395,\n",
       "   0.024361319839954376,\n",
       "   0.024387691169977188,\n",
       "   0.02320825308561325,\n",
       "   0.023106053471565247,\n",
       "   0.02322043664753437,\n",
       "   0.022390011698007584,\n",
       "   0.022769376635551453,\n",
       "   0.023301351815462112,\n",
       "   0.023708900436758995,\n",
       "   0.023495955392718315,\n",
       "   0.023312635719776154,\n",
       "   0.02261718176305294,\n",
       "   0.024402646347880363,\n",
       "   0.022952282801270485,\n",
       "   0.02293054386973381,\n",
       "   0.022256091237068176,\n",
       "   0.02261378988623619,\n",
       "   0.023073315620422363,\n",
       "   0.022283155471086502,\n",
       "   0.022916410118341446,\n",
       "   0.0226792860776186,\n",
       "   0.02297794818878174,\n",
       "   0.023446012288331985,\n",
       "   0.024058585986495018,\n",
       "   0.023817896842956543,\n",
       "   0.024122636765241623,\n",
       "   0.0241171233355999,\n",
       "   0.023916106671094894,\n",
       "   0.024358617141842842,\n",
       "   0.024370118975639343,\n",
       "   0.024781925603747368,\n",
       "   0.023905716836452484,\n",
       "   0.024546876549720764,\n",
       "   0.025516197085380554,\n",
       "   0.025308674201369286,\n",
       "   0.025221407413482666,\n",
       "   0.023937467485666275,\n",
       "   0.022933200001716614,\n",
       "   0.02347448654472828,\n",
       "   0.023548943921923637,\n",
       "   0.02358020655810833,\n",
       "   0.02325923554599285,\n",
       "   0.02266697958111763,\n",
       "   0.023095067590475082,\n",
       "   0.022674210369586945,\n",
       "   0.023839231580495834,\n",
       "   0.024307936429977417,\n",
       "   0.026290059089660645,\n",
       "   0.025008082389831543,\n",
       "   0.02439296431839466,\n",
       "   0.02627771534025669,\n",
       "   0.0262659452855587,\n",
       "   0.02561158314347267,\n",
       "   0.026229264214634895,\n",
       "   0.025387383997440338,\n",
       "   0.027359429746866226,\n",
       "   0.026696033775806427,\n",
       "   0.027423586696386337,\n",
       "   0.029762305319309235,\n",
       "   0.028770769014954567,\n",
       "   0.027832001447677612,\n",
       "   0.028879642486572266,\n",
       "   0.028448380529880524,\n",
       "   0.027769748121500015,\n",
       "   0.029427189379930496,\n",
       "   0.029026582837104797,\n",
       "   0.026676801964640617,\n",
       "   0.02612951770424843,\n",
       "   0.025997940450906754,\n",
       "   0.026215720921754837,\n",
       "   0.0251628328114748,\n",
       "   0.026019200682640076,\n",
       "   0.02556920051574707,\n",
       "   0.025897443294525146,\n",
       "   0.027800746262073517,\n",
       "   0.02806995064020157,\n",
       "   0.029522815719246864,\n",
       "   0.03221569582819939,\n",
       "   0.03483016416430473,\n",
       "   0.03068716637790203,\n",
       "   0.029296018183231354,\n",
       "   0.02967003360390663,\n",
       "   0.029736358672380447,\n",
       "   0.029522264376282692,\n",
       "   0.028742168098688126,\n",
       "   0.028249120339751244,\n",
       "   0.029988065361976624,\n",
       "   0.030081868171691895,\n",
       "   0.028312940150499344,\n",
       "   0.026872020214796066,\n",
       "   0.025155164301395416,\n",
       "   0.023355018347501755,\n",
       "   0.023340990766882896,\n",
       "   0.02315801940858364,\n",
       "   0.023396000266075134,\n",
       "   0.023692764341831207,\n",
       "   0.024279143661260605,\n",
       "   0.024850156158208847,\n",
       "   0.025136545300483704,\n",
       "   0.026132363826036453,\n",
       "   0.026903828606009483,\n",
       "   0.026802416890859604,\n",
       "   0.026677288115024567,\n",
       "   0.02715350314974785,\n",
       "   0.02707686275243759,\n",
       "   0.026342837139964104,\n",
       "   0.026967428624629974,\n",
       "   0.02722938545048237,\n",
       "   0.03098297491669655,\n",
       "   0.027480725198984146,\n",
       "   0.026767641305923462,\n",
       "   0.026905827224254608,\n",
       "   0.026515595614910126,\n",
       "   0.0268330629914999,\n",
       "   0.02541816234588623,\n",
       "   0.025979818776249886,\n",
       "   0.028034597635269165,\n",
       "   0.02753441035747528,\n",
       "   0.029254984110593796,\n",
       "   0.028650686144828796,\n",
       "   0.030961528420448303,\n",
       "   0.03107687458395958],\n",
       "  'jsc_loss': [0.8854710459709167,\n",
       "   0.8854710459709167,\n",
       "   0.8854710459709167,\n",
       "   0.8854710459709167,\n",
       "   0.8854710459709167,\n",
       "   0.8854710459709167,\n",
       "   0.8854710459709167,\n",
       "   0.8854710459709167,\n",
       "   0.8854710459709167,\n",
       "   0.8833537697792053,\n",
       "   0.8854710459709167,\n",
       "   0.8824853897094727,\n",
       "   0.8466649651527405,\n",
       "   0.8391003012657166,\n",
       "   0.8142099380493164,\n",
       "   0.792374849319458,\n",
       "   0.7736402750015259,\n",
       "   0.7819221615791321,\n",
       "   0.7944415807723999,\n",
       "   0.7904768586158752,\n",
       "   0.7984641790390015,\n",
       "   0.8109689354896545,\n",
       "   0.8165346384048462,\n",
       "   0.8120885491371155,\n",
       "   0.8165678381919861,\n",
       "   0.8158124089241028,\n",
       "   0.8134832382202148,\n",
       "   0.8143787384033203,\n",
       "   0.8210602402687073,\n",
       "   0.8010910749435425,\n",
       "   0.8174487352371216,\n",
       "   0.8193214535713196,\n",
       "   0.8205847144126892,\n",
       "   0.8231973052024841,\n",
       "   0.8233955502510071,\n",
       "   0.8321965336799622,\n",
       "   0.8363721966743469,\n",
       "   0.8369500041007996,\n",
       "   0.8271673321723938,\n",
       "   0.8300303816795349,\n",
       "   0.8334134221076965,\n",
       "   0.8304183483123779,\n",
       "   0.8243581652641296,\n",
       "   0.8214002251625061,\n",
       "   0.8283213376998901,\n",
       "   0.8273553848266602,\n",
       "   0.8262467384338379,\n",
       "   0.8277153968811035,\n",
       "   0.8246262669563293,\n",
       "   0.8253070116043091,\n",
       "   0.8264293074607849,\n",
       "   0.8322015404701233,\n",
       "   0.8337353467941284,\n",
       "   0.8229770660400391,\n",
       "   0.8251487016677856,\n",
       "   0.8276273608207703,\n",
       "   0.8339004516601562,\n",
       "   0.8408927321434021,\n",
       "   0.8327680826187134,\n",
       "   0.8275399208068848,\n",
       "   0.8331474661827087,\n",
       "   0.8340999484062195,\n",
       "   0.8261274099349976,\n",
       "   0.8246219158172607,\n",
       "   0.8123112320899963,\n",
       "   0.8137267231941223,\n",
       "   0.786946713924408,\n",
       "   0.774813175201416,\n",
       "   0.7885820865631104,\n",
       "   0.7966132164001465,\n",
       "   0.802313506603241,\n",
       "   0.8076080083847046,\n",
       "   0.8026757836341858,\n",
       "   0.7931352257728577,\n",
       "   0.7875269651412964,\n",
       "   0.779426634311676,\n",
       "   0.7900144457817078,\n",
       "   0.7884878516197205,\n",
       "   0.8068828582763672,\n",
       "   0.8082851767539978,\n",
       "   0.8209568858146667,\n",
       "   0.8216550350189209,\n",
       "   0.8302099704742432,\n",
       "   0.8303464651107788,\n",
       "   0.8332672715187073,\n",
       "   0.8432992100715637,\n",
       "   0.8414130806922913,\n",
       "   0.8517382144927979,\n",
       "   0.8476794362068176,\n",
       "   0.8436681032180786,\n",
       "   0.8376619219779968,\n",
       "   0.8321584463119507,\n",
       "   0.816946268081665,\n",
       "   0.8261979222297668,\n",
       "   0.8048110604286194,\n",
       "   0.7971245646476746,\n",
       "   0.7982155084609985,\n",
       "   0.8016455173492432,\n",
       "   0.7976474165916443,\n",
       "   0.8032419681549072,\n",
       "   0.8055592179298401,\n",
       "   0.7998486161231995,\n",
       "   0.7962326407432556,\n",
       "   0.7952763438224792,\n",
       "   0.7933645248413086,\n",
       "   0.8027379512786865,\n",
       "   0.8085805177688599,\n",
       "   0.8125264644622803,\n",
       "   0.8172996044158936,\n",
       "   0.8227440118789673,\n",
       "   0.8191078305244446,\n",
       "   0.8174337148666382,\n",
       "   0.8125505447387695,\n",
       "   0.8202130198478699,\n",
       "   0.823480486869812,\n",
       "   0.8332098722457886,\n",
       "   0.8292827606201172,\n",
       "   0.8232751488685608,\n",
       "   0.8228589296340942,\n",
       "   0.8154623508453369,\n",
       "   0.8098482489585876,\n",
       "   0.8124346137046814,\n",
       "   0.8171543478965759,\n",
       "   0.818234920501709,\n",
       "   0.8262099027633667,\n",
       "   0.8270270228385925,\n",
       "   0.8276563882827759,\n",
       "   0.8295668363571167,\n",
       "   0.8322287797927856,\n",
       "   0.8358446955680847,\n",
       "   0.8430418968200684,\n",
       "   0.8355085849761963,\n",
       "   0.8372768759727478,\n",
       "   0.8406098484992981,\n",
       "   0.8431800603866577,\n",
       "   0.8456277847290039,\n",
       "   0.843220591545105,\n",
       "   0.8327886462211609,\n",
       "   0.8333756923675537,\n",
       "   0.8348751068115234,\n",
       "   0.826637327671051,\n",
       "   0.8353444933891296,\n",
       "   0.8378502130508423,\n",
       "   0.8428214192390442,\n",
       "   0.8468409776687622,\n",
       "   0.8464343547821045,\n",
       "   0.8499051928520203,\n",
       "   0.8463222980499268,\n",
       "   0.8403916954994202,\n",
       "   0.8444316983222961,\n",
       "   0.8424713015556335,\n",
       "   0.8356714248657227,\n",
       "   0.832010805606842,\n",
       "   0.8301936984062195,\n",
       "   0.8323111534118652,\n",
       "   0.8384073972702026,\n",
       "   0.8338192701339722,\n",
       "   0.8371814489364624,\n",
       "   0.834579586982727,\n",
       "   0.8366808891296387,\n",
       "   0.833486795425415,\n",
       "   0.831176221370697,\n",
       "   0.8350902199745178,\n",
       "   0.84063720703125,\n",
       "   0.838018536567688,\n",
       "   0.8387609720230103,\n",
       "   0.8312762379646301,\n",
       "   0.8386841416358948,\n",
       "   0.835627555847168,\n",
       "   0.8308397531509399,\n",
       "   0.8289639949798584,\n",
       "   0.8299209475517273,\n",
       "   0.839688241481781,\n",
       "   0.8369414806365967,\n",
       "   0.833639919757843,\n",
       "   0.8300120830535889,\n",
       "   0.8263788223266602,\n",
       "   0.8231305480003357,\n",
       "   0.8191065788269043,\n",
       "   0.8192511796951294,\n",
       "   0.8258389234542847,\n",
       "   0.82175612449646,\n",
       "   0.8270069360733032,\n",
       "   0.8271228671073914,\n",
       "   0.8299275636672974,\n",
       "   0.8268855214118958,\n",
       "   0.8281438946723938,\n",
       "   0.8297274112701416,\n",
       "   0.8333415985107422,\n",
       "   0.8333296179771423,\n",
       "   0.8359389901161194,\n",
       "   0.8396127223968506,\n",
       "   0.8391799926757812,\n",
       "   0.8444941639900208,\n",
       "   0.8377275466918945,\n",
       "   0.8454314470291138,\n",
       "   0.8455151915550232,\n",
       "   0.8416040539741516,\n",
       "   0.8356610536575317,\n",
       "   0.8344626426696777,\n",
       "   0.8427706360816956,\n",
       "   0.8486785292625427,\n",
       "   0.8506420850753784,\n",
       "   0.8476013541221619,\n",
       "   0.8505985736846924,\n",
       "   0.8518142104148865,\n",
       "   0.8485159277915955,\n",
       "   0.8468133211135864,\n",
       "   0.8462674617767334,\n",
       "   0.8471865653991699,\n",
       "   0.8434101939201355,\n",
       "   0.8416889309883118,\n",
       "   0.8313659429550171,\n",
       "   0.8448522090911865,\n",
       "   0.8366397619247437,\n",
       "   0.8359532356262207,\n",
       "   0.8358163237571716,\n",
       "   0.8426821827888489,\n",
       "   0.8426528573036194,\n",
       "   0.8391628265380859,\n",
       "   0.8430025577545166,\n",
       "   0.8420913219451904,\n",
       "   0.8439585566520691,\n",
       "   0.8449101448059082,\n",
       "   0.8454241156578064,\n",
       "   0.8425561785697937,\n",
       "   0.841823935508728,\n",
       "   0.8410813212394714,\n",
       "   0.8420109152793884,\n",
       "   0.8445209860801697,\n",
       "   0.8368571996688843,\n",
       "   0.8372891545295715,\n",
       "   0.8407717943191528,\n",
       "   0.8380260467529297,\n",
       "   0.8301217555999756,\n",
       "   0.8395506143569946,\n",
       "   0.8373012542724609,\n",
       "   0.8417233228683472,\n",
       "   0.8392816781997681,\n",
       "   0.8364810347557068,\n",
       "   0.8410859107971191,\n",
       "   0.8446630239486694,\n",
       "   0.8355610370635986,\n",
       "   0.8365033864974976,\n",
       "   0.8324634432792664,\n",
       "   0.8265259861946106,\n",
       "   0.8320540189743042,\n",
       "   0.8213587403297424,\n",
       "   0.8323091268539429,\n",
       "   0.8334409594535828,\n",
       "   0.8304873108863831,\n",
       "   0.8272151350975037,\n",
       "   0.8318787217140198,\n",
       "   0.831613302230835,\n",
       "   0.834547221660614,\n",
       "   0.8369271159172058,\n",
       "   0.8424766659736633,\n",
       "   0.8436633348464966,\n",
       "   0.8438469171524048,\n",
       "   0.8414006233215332,\n",
       "   0.8404121398925781,\n",
       "   0.8364554047584534,\n",
       "   0.8407138586044312,\n",
       "   0.845382571220398,\n",
       "   0.8421151638031006,\n",
       "   0.843313455581665,\n",
       "   0.8454445004463196,\n",
       "   0.8491380214691162,\n",
       "   0.8450750708580017,\n",
       "   0.8391197919845581,\n",
       "   0.8403761386871338,\n",
       "   0.8409501314163208,\n",
       "   0.8400548696517944,\n",
       "   0.8371186256408691,\n",
       "   0.8344777226448059,\n",
       "   0.825200080871582,\n",
       "   0.8290669918060303,\n",
       "   0.8286368250846863,\n",
       "   0.8248712420463562,\n",
       "   0.8273178339004517,\n",
       "   0.8277828097343445,\n",
       "   0.8224740028381348,\n",
       "   0.8206927180290222,\n",
       "   0.828926146030426,\n",
       "   0.8230995535850525,\n",
       "   0.835438072681427,\n",
       "   0.8406267762184143,\n",
       "   0.8366597890853882,\n",
       "   0.8414624929428101,\n",
       "   0.8403439521789551,\n",
       "   0.837722897529602,\n",
       "   0.8392730355262756,\n",
       "   0.8404297828674316,\n",
       "   0.8441632390022278,\n",
       "   0.8387526273727417,\n",
       "   0.8418490886688232,\n",
       "   0.8418821692466736,\n",
       "   0.8455976247787476,\n",
       "   0.8449285626411438,\n",
       "   0.8420981764793396,\n",
       "   0.8465422987937927,\n",
       "   0.8436638712882996,\n",
       "   0.8457038998603821,\n",
       "   0.8419179916381836,\n",
       "   0.8393588662147522,\n",
       "   0.8421445488929749,\n",
       "   0.8428025245666504,\n",
       "   0.8451257348060608,\n",
       "   0.8422843217849731,\n",
       "   0.8468062877655029,\n",
       "   0.8460574746131897,\n",
       "   0.8424113988876343,\n",
       "   0.8375234007835388,\n",
       "   0.8376048803329468,\n",
       "   0.8406063318252563,\n",
       "   0.8439679145812988,\n",
       "   0.8373019695281982,\n",
       "   0.8366082906723022,\n",
       "   0.8368149399757385,\n",
       "   0.8405032157897949,\n",
       "   0.8404131531715393,\n",
       "   0.8423445224761963,\n",
       "   0.8458643555641174,\n",
       "   0.8473063707351685,\n",
       "   0.8454786539077759,\n",
       "   0.8451458215713501,\n",
       "   0.8372525572776794,\n",
       "   0.8429962396621704,\n",
       "   0.8447493314743042,\n",
       "   0.8426348567008972,\n",
       "   0.8386948108673096,\n",
       "   0.8357236385345459,\n",
       "   0.837822437286377,\n",
       "   0.840376079082489,\n",
       "   0.8367184400558472,\n",
       "   0.8335493206977844,\n",
       "   0.8324559926986694,\n",
       "   0.8386868238449097,\n",
       "   0.8435290455818176,\n",
       "   0.8426753282546997,\n",
       "   0.8416370749473572,\n",
       "   0.8427085876464844,\n",
       "   0.8422724008560181,\n",
       "   0.8465553522109985,\n",
       "   0.8433962464332581,\n",
       "   0.8400577902793884,\n",
       "   0.8378006219863892,\n",
       "   0.8360651135444641,\n",
       "   0.8378151655197144,\n",
       "   0.8381945490837097,\n",
       "   0.8389981389045715,\n",
       "   0.8377588391304016,\n",
       "   0.8431270718574524,\n",
       "   0.8431112766265869,\n",
       "   0.8454703092575073,\n",
       "   0.8445143103599548,\n",
       "   0.8453710079193115,\n",
       "   0.8451729416847229,\n",
       "   0.846470296382904,\n",
       "   0.8455072641372681,\n",
       "   0.8396285176277161,\n",
       "   0.8448478579521179,\n",
       "   0.8459919095039368,\n",
       "   0.8408207297325134,\n",
       "   0.8407974243164062,\n",
       "   0.8414173126220703,\n",
       "   0.8357873558998108,\n",
       "   0.8364530801773071,\n",
       "   0.833824872970581,\n",
       "   0.8267244696617126,\n",
       "   0.8312124013900757,\n",
       "   0.8389420509338379,\n",
       "   0.842430591583252,\n",
       "   0.8394449353218079,\n",
       "   0.8324691653251648,\n",
       "   0.8378975987434387,\n",
       "   0.8416028022766113,\n",
       "   0.8399421572685242,\n",
       "   0.8452712297439575,\n",
       "   0.8399820923805237,\n",
       "   0.8355164527893066,\n",
       "   0.8324037790298462,\n",
       "   0.8296272158622742,\n",
       "   0.8320924043655396,\n",
       "   0.8201671242713928,\n",
       "   0.8304018378257751,\n",
       "   0.8307787775993347,\n",
       "   0.8328922986984253,\n",
       "   0.8294556140899658,\n",
       "   0.8210577964782715,\n",
       "   0.8264472484588623,\n",
       "   0.8298032283782959,\n",
       "   0.8354122638702393,\n",
       "   0.8345481753349304,\n",
       "   0.8417972326278687,\n",
       "   0.8320627212524414,\n",
       "   0.8272997736930847,\n",
       "   0.8295296430587769,\n",
       "   0.8290675282478333,\n",
       "   0.8289006948471069,\n",
       "   0.8313656449317932,\n",
       "   0.8358350396156311,\n",
       "   0.8368683457374573,\n",
       "   0.8328450918197632,\n",
       "   0.8309710621833801,\n",
       "   0.8241176009178162,\n",
       "   0.8147152662277222,\n",
       "   0.8205108642578125,\n",
       "   0.8285568952560425,\n",
       "   0.8321632146835327,\n",
       "   0.8228269815444946,\n",
       "   0.8193687796592712,\n",
       "   0.8175025582313538,\n",
       "   0.8189059495925903,\n",
       "   0.8178347945213318,\n",
       "   0.8228030800819397,\n",
       "   0.824971079826355,\n",
       "   0.8290671110153198,\n",
       "   0.8219025135040283,\n",
       "   0.8269577026367188,\n",
       "   0.8280895352363586,\n",
       "   0.8381047248840332,\n",
       "   0.8399382829666138,\n",
       "   0.834159255027771,\n",
       "   0.8299852609634399,\n",
       "   0.8300216197967529,\n",
       "   0.8299294114112854,\n",
       "   0.8273875713348389,\n",
       "   0.8317941427230835,\n",
       "   0.8341726064682007,\n",
       "   0.8352769017219543,\n",
       "   0.8309761881828308,\n",
       "   0.8253812789916992,\n",
       "   0.8311569094657898,\n",
       "   0.8376597762107849,\n",
       "   0.833369791507721,\n",
       "   0.8256556987762451,\n",
       "   0.8293169736862183,\n",
       "   0.8304503560066223,\n",
       "   0.8389604687690735,\n",
       "   0.8411385416984558,\n",
       "   0.8437480926513672,\n",
       "   0.8436973690986633,\n",
       "   0.8447034358978271,\n",
       "   0.839740514755249,\n",
       "   0.843117892742157,\n",
       "   0.8410494327545166,\n",
       "   0.8352799415588379,\n",
       "   0.8423275947570801,\n",
       "   0.846010148525238,\n",
       "   0.8466998338699341,\n",
       "   0.8464047908782959,\n",
       "   0.8418624401092529,\n",
       "   0.8419239521026611,\n",
       "   0.8463937044143677,\n",
       "   0.839160680770874,\n",
       "   0.841090738773346,\n",
       "   0.8418940901756287,\n",
       "   0.8390193581581116,\n",
       "   0.8375878930091858,\n",
       "   0.829103946685791,\n",
       "   0.8288599252700806,\n",
       "   0.8340449333190918,\n",
       "   0.837283194065094,\n",
       "   0.8367472290992737,\n",
       "   0.8382452130317688,\n",
       "   0.8354175090789795,\n",
       "   0.8343621492385864,\n",
       "   0.830062747001648,\n",
       "   0.8314740061759949,\n",
       "   0.832224428653717,\n",
       "   0.8238064646720886,\n",
       "   0.8165748119354248,\n",
       "   0.8209946155548096,\n",
       "   0.812507152557373,\n",
       "   0.8139089941978455,\n",
       "   0.8158855438232422,\n",
       "   0.8185197114944458,\n",
       "   0.8230924606323242,\n",
       "   0.8203673958778381,\n",
       "   0.8266208171844482,\n",
       "   0.8266031742095947,\n",
       "   0.8291869759559631,\n",
       "   0.8323819637298584,\n",
       "   0.8308753967285156,\n",
       "   0.8339582681655884,\n",
       "   0.832915723323822,\n",
       "   0.8312440514564514,\n",
       "   0.8300208449363708,\n",
       "   0.834686815738678,\n",
       "   0.828170895576477,\n",
       "   0.8360175490379333,\n",
       "   0.8366395235061646,\n",
       "   0.8358689546585083,\n",
       "   0.8414767384529114,\n",
       "   0.8474235534667969,\n",
       "   0.8412935733795166,\n",
       "   0.8294194936752319,\n",
       "   0.8268134593963623,\n",
       "   0.8267204761505127],\n",
       "  'ff_loss': [0.7417651414871216,\n",
       "   0.7417651414871216,\n",
       "   0.7417651414871216,\n",
       "   0.7417651414871216,\n",
       "   0.7417651414871216,\n",
       "   0.7344274520874023,\n",
       "   0.6300538778305054,\n",
       "   0.513581395149231,\n",
       "   0.43692746758461,\n",
       "   0.31583622097969055,\n",
       "   0.23920422792434692,\n",
       "   0.20175963640213013,\n",
       "   0.16598498821258545,\n",
       "   0.11845745146274567,\n",
       "   0.11359983682632446,\n",
       "   0.1094389259815216,\n",
       "   0.08767484873533249,\n",
       "   0.07827030122280121,\n",
       "   0.07375318557024002,\n",
       "   0.0646606907248497,\n",
       "   0.05801857262849808,\n",
       "   0.051946453750133514,\n",
       "   0.04869948700070381,\n",
       "   0.04887765645980835,\n",
       "   0.044353410601615906,\n",
       "   0.043151870369911194,\n",
       "   0.04025290533900261,\n",
       "   0.03700178861618042,\n",
       "   0.03395839408040047,\n",
       "   0.03209838271141052,\n",
       "   0.03191033750772476,\n",
       "   0.03071330487728119,\n",
       "   0.027976036071777344,\n",
       "   0.02652146853506565,\n",
       "   0.027460500597953796,\n",
       "   0.02602367475628853,\n",
       "   0.026356279850006104,\n",
       "   0.026727229356765747,\n",
       "   0.029331866651773453,\n",
       "   0.02932111918926239,\n",
       "   0.030889712274074554,\n",
       "   0.028641480952501297,\n",
       "   0.029154568910598755,\n",
       "   0.031231049448251724,\n",
       "   0.03333679959177971,\n",
       "   0.03144488111138344,\n",
       "   0.03294530510902405,\n",
       "   0.031030043959617615,\n",
       "   0.02921433188021183,\n",
       "   0.02880326472222805,\n",
       "   0.028537098318338394,\n",
       "   0.02747032232582569,\n",
       "   0.029131565243005753,\n",
       "   0.029188551008701324,\n",
       "   0.028615932911634445,\n",
       "   0.029274793341755867,\n",
       "   0.027767794206738472,\n",
       "   0.02827281877398491,\n",
       "   0.027467690408229828,\n",
       "   0.02817583456635475,\n",
       "   0.02878400683403015,\n",
       "   0.031082814559340477,\n",
       "   0.03180394321680069,\n",
       "   0.03196735680103302,\n",
       "   0.031003985553979874,\n",
       "   0.031015682965517044,\n",
       "   0.029280927032232285,\n",
       "   0.02798306941986084,\n",
       "   0.029444456100463867,\n",
       "   0.030011693015694618,\n",
       "   0.030568020418286324,\n",
       "   0.030812229961156845,\n",
       "   0.03156616538763046,\n",
       "   0.030669864267110825,\n",
       "   0.027266012504696846,\n",
       "   0.03033783845603466,\n",
       "   0.029554076492786407,\n",
       "   0.027971066534519196,\n",
       "   0.028786778450012207,\n",
       "   0.02840501442551613,\n",
       "   0.02712980844080448,\n",
       "   0.025548167526721954,\n",
       "   0.026789944618940353,\n",
       "   0.027116863057017326,\n",
       "   0.02901969663798809,\n",
       "   0.028033237904310226,\n",
       "   0.02982180565595627,\n",
       "   0.02815081924200058,\n",
       "   0.02957986108958721,\n",
       "   0.029495500028133392,\n",
       "   0.02865404263138771,\n",
       "   0.028743643313646317,\n",
       "   0.028118303045630455,\n",
       "   0.02600002847611904,\n",
       "   0.022867783904075623,\n",
       "   0.024636160582304,\n",
       "   0.023964636027812958,\n",
       "   0.024310708045959473,\n",
       "   0.022239066660404205,\n",
       "   0.024577558040618896,\n",
       "   0.02562088891863823,\n",
       "   0.027141574770212173,\n",
       "   0.026049453765153885,\n",
       "   0.02480093389749527,\n",
       "   0.025536591187119484,\n",
       "   0.028578218072652817,\n",
       "   0.027702558785676956,\n",
       "   0.027918562293052673,\n",
       "   0.028496086597442627,\n",
       "   0.029209628701210022,\n",
       "   0.02849385142326355,\n",
       "   0.02824491634964943,\n",
       "   0.0308529082685709,\n",
       "   0.03174683451652527,\n",
       "   0.03144725039601326,\n",
       "   0.03189985454082489,\n",
       "   0.03197804093360901,\n",
       "   0.030542626976966858,\n",
       "   0.033035874366760254,\n",
       "   0.03200852870941162,\n",
       "   0.031516797840595245,\n",
       "   0.03176960349082947,\n",
       "   0.03259700536727905,\n",
       "   0.030665798112750053,\n",
       "   0.03037683293223381,\n",
       "   0.02819538116455078,\n",
       "   0.030814241617918015,\n",
       "   0.032488863915205,\n",
       "   0.031428657472133636,\n",
       "   0.03035806119441986,\n",
       "   0.031480107456445694,\n",
       "   0.02860822156071663,\n",
       "   0.028970595449209213,\n",
       "   0.030651666224002838,\n",
       "   0.03030313178896904,\n",
       "   0.033115021884441376,\n",
       "   0.03200998529791832,\n",
       "   0.029012300074100494,\n",
       "   0.029582440853118896,\n",
       "   0.028749587014317513,\n",
       "   0.027685482054948807,\n",
       "   0.02638709545135498,\n",
       "   0.02574007585644722,\n",
       "   0.024182133376598358,\n",
       "   0.022070815786719322,\n",
       "   0.02057161182165146,\n",
       "   0.02003059908747673,\n",
       "   0.018186086788773537,\n",
       "   0.02060633897781372,\n",
       "   0.021566536277532578,\n",
       "   0.023192690685391426,\n",
       "   0.02402004599571228,\n",
       "   0.023781035095453262,\n",
       "   0.028321728110313416,\n",
       "   0.02907562628388405,\n",
       "   0.028272077441215515,\n",
       "   0.023760264739394188,\n",
       "   0.022781968116760254,\n",
       "   0.023428088054060936,\n",
       "   0.023811060935258865,\n",
       "   0.021321838721632957,\n",
       "   0.021084286272525787,\n",
       "   0.021352143958210945,\n",
       "   0.021579552441835403,\n",
       "   0.024469498544931412,\n",
       "   0.023080002516508102,\n",
       "   0.02413301169872284,\n",
       "   0.024031158536672592,\n",
       "   0.02312006801366806,\n",
       "   0.02248014509677887,\n",
       "   0.023913279175758362,\n",
       "   0.024056842550635338,\n",
       "   0.024605315178632736,\n",
       "   0.02696303464472294,\n",
       "   0.02533572167158127,\n",
       "   0.02525912970304489,\n",
       "   0.02504856511950493,\n",
       "   0.0256805382668972,\n",
       "   0.02345319464802742,\n",
       "   0.022530701011419296,\n",
       "   0.026054121553897858,\n",
       "   0.026555143296718597,\n",
       "   0.026296865195035934,\n",
       "   0.025726106017827988,\n",
       "   0.027705103158950806,\n",
       "   0.028503593057394028,\n",
       "   0.027505945414304733,\n",
       "   0.028533656150102615,\n",
       "   0.02876298874616623,\n",
       "   0.02774849534034729,\n",
       "   0.027616728097200394,\n",
       "   0.028200699016451836,\n",
       "   0.02851090207695961,\n",
       "   0.02757280319929123,\n",
       "   0.026224002242088318,\n",
       "   0.026921546086668968,\n",
       "   0.025278359651565552,\n",
       "   0.024949638172984123,\n",
       "   0.024756932631134987,\n",
       "   0.025431498885154724,\n",
       "   0.02286442369222641,\n",
       "   0.023235268890857697,\n",
       "   0.02454998716711998,\n",
       "   0.024253811687231064,\n",
       "   0.024455726146697998,\n",
       "   0.0251384936273098,\n",
       "   0.023221731185913086,\n",
       "   0.024126999080181122,\n",
       "   0.025335077196359634,\n",
       "   0.026270119473338127,\n",
       "   0.02610437199473381,\n",
       "   0.0273284874856472,\n",
       "   0.02797909453511238,\n",
       "   0.027367334812879562,\n",
       "   0.028816841542720795,\n",
       "   0.02883409522473812,\n",
       "   0.02908957004547119,\n",
       "   0.028624966740608215,\n",
       "   0.029627278447151184,\n",
       "   0.027629345655441284,\n",
       "   0.030476903542876244,\n",
       "   0.031072907149791718,\n",
       "   0.03199334815144539,\n",
       "   0.031438253819942474,\n",
       "   0.030834317207336426,\n",
       "   0.029272867366671562,\n",
       "   0.027699198573827744,\n",
       "   0.02639598771929741,\n",
       "   0.028323769569396973,\n",
       "   0.026967935264110565,\n",
       "   0.0265959482640028,\n",
       "   0.026109077036380768,\n",
       "   0.02772001549601555,\n",
       "   0.030520794913172722,\n",
       "   0.03039882890880108,\n",
       "   0.027664538472890854,\n",
       "   0.027742689475417137,\n",
       "   0.02795618399977684,\n",
       "   0.02465943619608879,\n",
       "   0.025123629719018936,\n",
       "   0.025663428008556366,\n",
       "   0.025529352948069572,\n",
       "   0.029159165918827057,\n",
       "   0.02862708643078804,\n",
       "   0.029821209609508514,\n",
       "   0.030522601678967476,\n",
       "   0.03009386733174324,\n",
       "   0.031211288645863533,\n",
       "   0.029609743505716324,\n",
       "   0.02912796102464199,\n",
       "   0.028703052550554276,\n",
       "   0.028041770681738853,\n",
       "   0.02710954286158085,\n",
       "   0.024731963872909546,\n",
       "   0.023760979995131493,\n",
       "   0.024490706622600555,\n",
       "   0.02506309375166893,\n",
       "   0.02378471940755844,\n",
       "   0.02407989650964737,\n",
       "   0.02446010150015354,\n",
       "   0.02366993948817253,\n",
       "   0.023877738043665886,\n",
       "   0.02477356232702732,\n",
       "   0.023050488904118538,\n",
       "   0.024534184485673904,\n",
       "   0.02621280774474144,\n",
       "   0.026256665587425232,\n",
       "   0.026428673416376114,\n",
       "   0.025355855002999306,\n",
       "   0.026134571060538292,\n",
       "   0.025598058477044106,\n",
       "   0.026986807584762573,\n",
       "   0.02828039973974228,\n",
       "   0.0277877077460289,\n",
       "   0.02970118075609207,\n",
       "   0.02668040245771408,\n",
       "   0.027311701327562332,\n",
       "   0.02751927450299263,\n",
       "   0.025892924517393112,\n",
       "   0.02452678605914116,\n",
       "   0.02647240087389946,\n",
       "   0.02526463195681572,\n",
       "   0.022020064294338226,\n",
       "   0.022061364725232124,\n",
       "   0.021238841116428375,\n",
       "   0.022246701642870903,\n",
       "   0.023514173924922943,\n",
       "   0.021616242825984955,\n",
       "   0.020864158868789673,\n",
       "   0.02090565860271454,\n",
       "   0.019282780587673187,\n",
       "   0.020547131076455116,\n",
       "   0.019407138228416443,\n",
       "   0.020048916339874268,\n",
       "   0.022251248359680176,\n",
       "   0.02401614375412464,\n",
       "   0.022853203117847443,\n",
       "   0.024858225136995316,\n",
       "   0.0252832118421793,\n",
       "   0.024100221693515778,\n",
       "   0.023586254566907883,\n",
       "   0.020974332466721535,\n",
       "   0.02024201862514019,\n",
       "   0.021386103704571724,\n",
       "   0.018750006332993507,\n",
       "   0.02110747992992401,\n",
       "   0.020286455750465393,\n",
       "   0.020910929888486862,\n",
       "   0.022093437612056732,\n",
       "   0.024574562907218933,\n",
       "   0.02374999225139618,\n",
       "   0.02402356266975403,\n",
       "   0.02757204696536064,\n",
       "   0.029648110270500183,\n",
       "   0.028681419789791107,\n",
       "   0.029102027416229248,\n",
       "   0.026175472885370255,\n",
       "   0.026751291006803513,\n",
       "   0.028101474046707153,\n",
       "   0.027750734239816666,\n",
       "   0.027925308793783188,\n",
       "   0.027744300663471222,\n",
       "   0.027352698147296906,\n",
       "   0.03090292401611805,\n",
       "   0.03012629598379135,\n",
       "   0.029865674674510956,\n",
       "   0.027533579617738724,\n",
       "   0.027771584689617157,\n",
       "   0.025297090411186218,\n",
       "   0.027483096346259117,\n",
       "   0.02747415006160736,\n",
       "   0.02562187984585762,\n",
       "   0.024982869625091553,\n",
       "   0.02361215464770794,\n",
       "   0.02635325863957405,\n",
       "   0.02378310263156891,\n",
       "   0.02385224774479866,\n",
       "   0.02346472069621086,\n",
       "   0.024187428876757622,\n",
       "   0.026383981108665466,\n",
       "   0.02726825512945652,\n",
       "   0.025961576029658318,\n",
       "   0.027380190789699554,\n",
       "   0.027008576318621635,\n",
       "   0.02664347179234028,\n",
       "   0.02859921008348465,\n",
       "   0.02998727560043335,\n",
       "   0.029393913224339485,\n",
       "   0.029366929084062576,\n",
       "   0.029984889551997185,\n",
       "   0.02902861498296261,\n",
       "   0.029504388570785522,\n",
       "   0.02835315652191639,\n",
       "   0.029500216245651245,\n",
       "   0.030485913157463074,\n",
       "   0.02913494035601616,\n",
       "   0.03087073564529419,\n",
       "   0.02987237088382244,\n",
       "   0.031178709119558334,\n",
       "   0.030430857092142105,\n",
       "   0.02834075316786766,\n",
       "   0.027871889993548393,\n",
       "   0.027180790901184082,\n",
       "   0.027584630995988846,\n",
       "   0.027585474774241447,\n",
       "   0.027969717979431152,\n",
       "   0.027597829699516296,\n",
       "   0.02837337553501129,\n",
       "   0.029992740601301193,\n",
       "   0.027805708348751068,\n",
       "   0.027004854753613472,\n",
       "   0.024783344939351082,\n",
       "   0.02112535387277603,\n",
       "   0.022790778428316116,\n",
       "   0.02124192751944065,\n",
       "   0.02304813638329506,\n",
       "   0.022825203835964203,\n",
       "   0.023790722712874413,\n",
       "   0.0251214187592268,\n",
       "   0.02460302598774433,\n",
       "   0.022818708792328835,\n",
       "   0.022252656519412994,\n",
       "   0.022404439747333527,\n",
       "   0.025900833308696747,\n",
       "   0.024629320949316025,\n",
       "   0.02636907435953617,\n",
       "   0.02773917466402054,\n",
       "   0.024186085909605026,\n",
       "   0.025000108405947685,\n",
       "   0.02504109963774681,\n",
       "   0.025131579488515854,\n",
       "   0.02529437094926834,\n",
       "   0.026331285014748573,\n",
       "   0.026013702154159546,\n",
       "   0.025417432188987732,\n",
       "   0.02507069520652294,\n",
       "   0.025519095361232758,\n",
       "   0.02377713844180107,\n",
       "   0.025993872433900833,\n",
       "   0.026445776224136353,\n",
       "   0.025353819131851196,\n",
       "   0.024803170934319496,\n",
       "   0.024017879739403725,\n",
       "   0.02390839159488678,\n",
       "   0.023127209395170212,\n",
       "   0.023592732846736908,\n",
       "   0.021838350221514702,\n",
       "   0.020384835079312325,\n",
       "   0.02159041166305542,\n",
       "   0.02458827570080757,\n",
       "   0.023602565750479698,\n",
       "   0.023876510560512543,\n",
       "   0.02302766777575016,\n",
       "   0.023134345188736916,\n",
       "   0.024767115712165833,\n",
       "   0.025076311081647873,\n",
       "   0.027041947469115257,\n",
       "   0.027091220021247864,\n",
       "   0.025153810158371925,\n",
       "   0.024752208963036537,\n",
       "   0.024739209562540054,\n",
       "   0.02552850730717182,\n",
       "   0.024483028799295425,\n",
       "   0.025260046124458313,\n",
       "   0.023708369582891464,\n",
       "   0.026963356882333755,\n",
       "   0.0264553464949131,\n",
       "   0.02629208378493786,\n",
       "   0.026988480240106583,\n",
       "   0.025930453091859818,\n",
       "   0.026921022683382034,\n",
       "   0.027261584997177124,\n",
       "   0.02783086523413658,\n",
       "   0.03182283788919449,\n",
       "   0.030285293236374855,\n",
       "   0.0282638780772686,\n",
       "   0.030325748026371002,\n",
       "   0.03154076635837555,\n",
       "   0.03287341818213463,\n",
       "   0.03484633192420006,\n",
       "   0.03289312496781349,\n",
       "   0.029683049768209457,\n",
       "   0.029258251190185547,\n",
       "   0.028890335932374,\n",
       "   0.029676971957087517,\n",
       "   0.02905961126089096,\n",
       "   0.03170943260192871,\n",
       "   0.031751785427331924,\n",
       "   0.033290471881628036,\n",
       "   0.030017059296369553,\n",
       "   0.02820505201816559,\n",
       "   0.0296340212225914,\n",
       "   0.028213493525981903,\n",
       "   0.026898708194494247,\n",
       "   0.02729899436235428,\n",
       "   0.026516355574131012,\n",
       "   0.0239766426384449,\n",
       "   0.02607710286974907,\n",
       "   0.025121480226516724,\n",
       "   0.02637556567788124,\n",
       "   0.02593492716550827,\n",
       "   0.023758795112371445,\n",
       "   0.025357119739055634,\n",
       "   0.027554642409086227,\n",
       "   0.025727417320013046,\n",
       "   0.025514476001262665,\n",
       "   0.023676052689552307,\n",
       "   0.022737521678209305,\n",
       "   0.022868048399686813,\n",
       "   0.023181671276688576,\n",
       "   0.025788187980651855,\n",
       "   0.026182442903518677,\n",
       "   0.024711940437555313,\n",
       "   0.02424287050962448,\n",
       "   0.023319989442825317,\n",
       "   0.021595923230051994,\n",
       "   0.02125658094882965,\n",
       "   0.019861094653606415,\n",
       "   0.02229919284582138,\n",
       "   0.02497592568397522,\n",
       "   0.025300737470388412,\n",
       "   0.027066674083471298,\n",
       "   0.027829144150018692,\n",
       "   0.0284174382686615,\n",
       "   0.028815291821956635,\n",
       "   0.030965592712163925,\n",
       "   0.02933332324028015,\n",
       "   0.02747158333659172,\n",
       "   0.026849281042814255,\n",
       "   0.024778937920928,\n",
       "   0.02349555306136608,\n",
       "   0.023876111954450607,\n",
       "   0.024575011804699898,\n",
       "   0.02480713650584221,\n",
       "   0.022243164479732513,\n",
       "   0.02052404172718525,\n",
       "   0.02111341618001461,\n",
       "   0.020892418920993805,\n",
       "   0.020129984244704247,\n",
       "   0.017984066158533096],\n",
       "  'test_losses': [2.6906817853450775,\n",
       "   2.605257362127304,\n",
       "   2.5237228870391846,\n",
       "   2.494227856397629,\n",
       "   2.500010460615158,\n",
       "   2.4602852165699005,\n",
       "   2.358545020222664,\n",
       "   2.2224579453468323,\n",
       "   2.0533008128404617,\n",
       "   1.8447278589010239,\n",
       "   1.6689878404140472,\n",
       "   1.5915223509073257,\n",
       "   1.4617937505245209,\n",
       "   1.384493738412857,\n",
       "   1.3358769118785858,\n",
       "   1.3158536702394485,\n",
       "   1.2603042051196098,\n",
       "   1.2725142687559128,\n",
       "   1.310205340385437,\n",
       "   1.3356477096676826,\n",
       "   1.3202571421861649,\n",
       "   1.3266358152031898,\n",
       "   1.3008252419531345,\n",
       "   1.2917068153619766,\n",
       "   1.2852462157607079,\n",
       "   1.3328431621193886,\n",
       "   1.333913091570139,\n",
       "   1.339383639395237,\n",
       "   1.3274251595139503,\n",
       "   1.3376925587654114,\n",
       "   1.3649947568774223,\n",
       "   1.3801785334944725,\n",
       "   1.383549079298973,\n",
       "   1.3886316139250994,\n",
       "   1.3980927169322968,\n",
       "   1.407222282141447,\n",
       "   1.4075977616012096,\n",
       "   1.414220903068781,\n",
       "   1.4172596596181393,\n",
       "   1.422600720077753,\n",
       "   1.4491277374327183,\n",
       "   1.4294790532439947,\n",
       "   1.4111306220293045,\n",
       "   1.4274359531700611,\n",
       "   1.4077451657503843,\n",
       "   1.4078258126974106,\n",
       "   1.3845230285078287,\n",
       "   1.3912136014550924,\n",
       "   1.3999598324298859,\n",
       "   1.4152099657803774,\n",
       "   1.417798262089491,\n",
       "   1.4175079632550478,\n",
       "   1.443008167669177,\n",
       "   1.427857419475913,\n",
       "   1.423200512304902,\n",
       "   1.4230427276343107,\n",
       "   1.4173879697918892,\n",
       "   1.4279278423637152,\n",
       "   1.4033898077905178,\n",
       "   1.4052381198853254,\n",
       "   1.413609566166997,\n",
       "   1.4209554102271795,\n",
       "   1.4231631308794022,\n",
       "   1.3935092724859715,\n",
       "   1.3466322757303715,\n",
       "   1.353034432977438,\n",
       "   1.327409639954567,\n",
       "   1.310841040685773,\n",
       "   1.3327383305877447,\n",
       "   1.3464850466698408,\n",
       "   1.3738472238183022,\n",
       "   1.3924987111240625,\n",
       "   1.3773210048675537,\n",
       "   1.3554712310433388,\n",
       "   1.3401910159736872,\n",
       "   1.361004140228033,\n",
       "   1.3780434001237154,\n",
       "   1.3860748447477818,\n",
       "   1.3979639951139688,\n",
       "   1.406239602714777,\n",
       "   1.4140374269336462,\n",
       "   1.401099679991603,\n",
       "   1.3996652998030186,\n",
       "   1.3966168351471424,\n",
       "   1.3942168802022934,\n",
       "   1.3916130810976028,\n",
       "   1.3874840382486582,\n",
       "   1.3883749023079872,\n",
       "   1.3916859310120344,\n",
       "   1.3972932379692793,\n",
       "   1.4129601120948792,\n",
       "   1.4027038905769587,\n",
       "   1.3667804505676031,\n",
       "   1.3718119971454144,\n",
       "   1.3447774406522512,\n",
       "   1.3452174961566925,\n",
       "   1.3542490303516388,\n",
       "   1.3573211152106524,\n",
       "   1.332874108105898,\n",
       "   1.3476124331355095,\n",
       "   1.3488987125456333,\n",
       "   1.352364208549261,\n",
       "   1.3719791732728481,\n",
       "   1.370428767055273,\n",
       "   1.3645192366093397,\n",
       "   1.3701703250408173,\n",
       "   1.3746103700250387,\n",
       "   1.395354002714157,\n",
       "   1.3944070637226105,\n",
       "   1.3955748118460178,\n",
       "   1.410192783921957,\n",
       "   1.4069051444530487,\n",
       "   1.4143491201102734,\n",
       "   1.4363103788346052,\n",
       "   1.4346232376992702,\n",
       "   1.437350820749998,\n",
       "   1.4180730916559696,\n",
       "   1.4103296510875225,\n",
       "   1.4116778001189232,\n",
       "   1.3989444319158792,\n",
       "   1.3914904277771711,\n",
       "   1.4097543228417635,\n",
       "   1.4054443277418613,\n",
       "   1.4009132198989391,\n",
       "   1.3885233718901873,\n",
       "   1.3579914756119251,\n",
       "   1.3631683606654406,\n",
       "   1.353762473911047,\n",
       "   1.351394535973668,\n",
       "   1.369883731007576,\n",
       "   1.3831610456109047,\n",
       "   1.3849749527871609,\n",
       "   1.4027543887495995,\n",
       "   1.403481988236308,\n",
       "   1.40816555544734,\n",
       "   1.4080514013767242,\n",
       "   1.4091480430215597,\n",
       "   1.4182273354381323,\n",
       "   1.4210513830184937,\n",
       "   1.4059243146330118,\n",
       "   1.4060288220643997,\n",
       "   1.412807621061802,\n",
       "   1.4125948883593082,\n",
       "   1.3921452295035124,\n",
       "   1.4036650508642197,\n",
       "   1.3900376595556736,\n",
       "   1.4012727327644825,\n",
       "   1.388543576002121,\n",
       "   1.379631968215108,\n",
       "   1.3980488777160645,\n",
       "   1.3898227866739035,\n",
       "   1.3904138281941414,\n",
       "   1.3886423371732235,\n",
       "   1.3728034552186728,\n",
       "   1.391346774995327,\n",
       "   1.4008948057889938,\n",
       "   1.4051601719111204,\n",
       "   1.417650617659092,\n",
       "   1.420396938920021,\n",
       "   1.406651383265853,\n",
       "   1.4073854368180037,\n",
       "   1.434457965195179,\n",
       "   1.4334023874253035,\n",
       "   1.4329926520586014,\n",
       "   1.429143963381648,\n",
       "   1.42149549536407,\n",
       "   1.4145232606679201,\n",
       "   1.402085866779089,\n",
       "   1.412995444610715,\n",
       "   1.4171445835381746,\n",
       "   1.4151867032051086,\n",
       "   1.4001655336469412,\n",
       "   1.435075305402279,\n",
       "   1.4315713066607714,\n",
       "   1.4308497309684753,\n",
       "   1.403718389570713,\n",
       "   1.3751932978630066,\n",
       "   1.3874666895717382,\n",
       "   1.3595841266214848,\n",
       "   1.3704399857670069,\n",
       "   1.3825456947088242,\n",
       "   1.3900371324270964,\n",
       "   1.3866048324853182,\n",
       "   1.394559446722269,\n",
       "   1.3895560577511787,\n",
       "   1.3918120563030243,\n",
       "   1.3972231894731522,\n",
       "   1.3992618396878242,\n",
       "   1.4177250675857067,\n",
       "   1.4203349873423576,\n",
       "   1.3952904418110847,\n",
       "   1.3872005585581064,\n",
       "   1.3996847588568926,\n",
       "   1.3956279121339321,\n",
       "   1.383772637695074,\n",
       "   1.3805024456232786,\n",
       "   1.3843708019703627,\n",
       "   1.356517979875207,\n",
       "   1.3581368885934353,\n",
       "   1.3548285141587257,\n",
       "   1.3651376198977232,\n",
       "   1.3772323001176119,\n",
       "   1.3901705779135227,\n",
       "   1.3716257009655237,\n",
       "   1.3756492920219898,\n",
       "   1.389365080744028,\n",
       "   1.3852587286382914,\n",
       "   1.3921616785228252,\n",
       "   1.3990701138973236,\n",
       "   1.3953048530966043,\n",
       "   1.396460421383381,\n",
       "   1.383754163980484,\n",
       "   1.3609399814158678,\n",
       "   1.3685215003788471,\n",
       "   1.3705681450664997,\n",
       "   1.369613790884614,\n",
       "   1.3819243162870407,\n",
       "   1.373454138636589,\n",
       "   1.387235838919878,\n",
       "   1.3759332187473774,\n",
       "   1.4072277192026377,\n",
       "   1.4097956083714962,\n",
       "   1.416299730539322,\n",
       "   1.4223613664507866,\n",
       "   1.4138901308178902,\n",
       "   1.420012230053544,\n",
       "   1.4183402229100466,\n",
       "   1.4030257314443588,\n",
       "   1.417980384081602,\n",
       "   1.4198638089001179,\n",
       "   1.4099603574723005,\n",
       "   1.4146927744150162,\n",
       "   1.4136291649192572,\n",
       "   1.4082838967442513,\n",
       "   1.3904409166425467,\n",
       "   1.3874301873147488,\n",
       "   1.3932993207126856,\n",
       "   1.3997230436652899,\n",
       "   1.4023938961327076,\n",
       "   1.4089691024273634,\n",
       "   1.4192162435501814,\n",
       "   1.401515083387494,\n",
       "   1.4032209068536758,\n",
       "   1.4077097810804844,\n",
       "   1.4116515330970287,\n",
       "   1.4217015486210585,\n",
       "   1.4114504642784595,\n",
       "   1.3846136648207903,\n",
       "   1.4118704907596111,\n",
       "   1.4033314604312181,\n",
       "   1.413398128002882,\n",
       "   1.4211664367467165,\n",
       "   1.4329357091337442,\n",
       "   1.4024326335638762,\n",
       "   1.402761248871684,\n",
       "   1.4088662061840296,\n",
       "   1.4092790745198727,\n",
       "   1.4139137081801891,\n",
       "   1.3919135611504316,\n",
       "   1.4002145156264305,\n",
       "   1.4031558893620968,\n",
       "   1.3842251114547253,\n",
       "   1.3990111891180277,\n",
       "   1.3904212228953838,\n",
       "   1.393887311220169,\n",
       "   1.4099166300147772,\n",
       "   1.4148899354040623,\n",
       "   1.4115908965468407,\n",
       "   1.420846614986658,\n",
       "   1.425375310704112,\n",
       "   1.4311736822128296,\n",
       "   1.4084030278027058,\n",
       "   1.3982671275734901,\n",
       "   1.393705928698182,\n",
       "   1.3863594457507133,\n",
       "   1.3903239946812391,\n",
       "   1.3901510797441006,\n",
       "   1.3815024886280298,\n",
       "   1.3883601482957602,\n",
       "   1.3941718023270369,\n",
       "   1.3864732813090086,\n",
       "   1.3923316560685635,\n",
       "   1.3952272068709135,\n",
       "   1.4035020843148232,\n",
       "   1.3971652425825596,\n",
       "   1.3827684242278337,\n",
       "   1.3901934865862131,\n",
       "   1.3890762571245432,\n",
       "   1.3872268665581942,\n",
       "   1.3862244077026844,\n",
       "   1.3777901604771614,\n",
       "   1.3657020460814238,\n",
       "   1.3717112448066473,\n",
       "   1.3599378764629364,\n",
       "   1.3555806316435337,\n",
       "   1.3713642321527004,\n",
       "   1.3699610717594624,\n",
       "   1.363652078434825,\n",
       "   1.369806932285428,\n",
       "   1.375354029238224,\n",
       "   1.3729282896965742,\n",
       "   1.362773509696126,\n",
       "   1.360901527106762,\n",
       "   1.377185931429267,\n",
       "   1.3677547499537468,\n",
       "   1.3724560234695673,\n",
       "   1.3753226455301046,\n",
       "   1.3736069668084383,\n",
       "   1.3835433796048164,\n",
       "   1.3866505045443773,\n",
       "   1.4037980064749718,\n",
       "   1.3973816111683846,\n",
       "   1.4020107556134462,\n",
       "   1.4030034989118576,\n",
       "   1.4180464334785938,\n",
       "   1.4538234528154135,\n",
       "   1.4500949382781982,\n",
       "   1.4455638770014048,\n",
       "   1.4196305982768536,\n",
       "   1.4288313016295433,\n",
       "   1.418854545801878,\n",
       "   1.421501575037837,\n",
       "   1.4094642084091902,\n",
       "   1.422172261402011,\n",
       "   1.4247752353549004,\n",
       "   1.4404755122959614,\n",
       "   1.4422017596662045,\n",
       "   1.4134202525019646,\n",
       "   1.4194983057677746,\n",
       "   1.4098093155771494,\n",
       "   1.417015079408884,\n",
       "   1.419953178614378,\n",
       "   1.4082260243594646,\n",
       "   1.4052631966769695,\n",
       "   1.390146642923355,\n",
       "   1.3887610379606485,\n",
       "   1.3847202807664871,\n",
       "   1.3912034593522549,\n",
       "   1.4063428472727537,\n",
       "   1.4086504504084587,\n",
       "   1.4011537190526724,\n",
       "   1.382591662928462,\n",
       "   1.390459956601262,\n",
       "   1.3948648627847433,\n",
       "   1.4048465639352798,\n",
       "   1.4016615971922874,\n",
       "   1.4039319939911366,\n",
       "   1.3897280935198069,\n",
       "   1.404671087861061,\n",
       "   1.408189607784152,\n",
       "   1.4071506056934595,\n",
       "   1.3905754927545786,\n",
       "   1.401245642453432,\n",
       "   1.4048124365508556,\n",
       "   1.4213006794452667,\n",
       "   1.4150767344981432,\n",
       "   1.4286461472511292,\n",
       "   1.4371173400431871,\n",
       "   1.438313364982605,\n",
       "   1.4345621019601822,\n",
       "   1.4259607829153538,\n",
       "   1.4188394136726856,\n",
       "   1.4105620086193085,\n",
       "   1.3850180618464947,\n",
       "   1.3848446886986494,\n",
       "   1.3708807192742825,\n",
       "   1.3751422725617886,\n",
       "   1.3570364154875278,\n",
       "   1.350308783352375,\n",
       "   1.3123618699610233,\n",
       "   1.336279621347785,\n",
       "   1.3545430526137352,\n",
       "   1.3773968908935785,\n",
       "   1.3646064642816782,\n",
       "   1.3725253269076347,\n",
       "   1.3906222376972437,\n",
       "   1.4027440138161182,\n",
       "   1.4056467059999704,\n",
       "   1.4089049454778433,\n",
       "   1.4013354685157537,\n",
       "   1.3814073372632265,\n",
       "   1.3890921995043755,\n",
       "   1.3880710247904062,\n",
       "   1.3945618830621243,\n",
       "   1.3734114281833172,\n",
       "   1.4041814487427473,\n",
       "   1.4056974332779646,\n",
       "   1.398195082321763,\n",
       "   1.3839315380901098,\n",
       "   1.3842688966542482,\n",
       "   1.3956859353929758,\n",
       "   1.3920338470488787,\n",
       "   1.4043719973415136,\n",
       "   1.3841213434934616,\n",
       "   1.3830983750522137,\n",
       "   1.3764533679932356,\n",
       "   1.3689004592597485,\n",
       "   1.3629056960344315,\n",
       "   1.370322709903121,\n",
       "   1.3692626059055328,\n",
       "   1.3863728232681751,\n",
       "   1.3850527554750443,\n",
       "   1.3872507903724909,\n",
       "   1.404317732900381,\n",
       "   1.40179093927145,\n",
       "   1.3817294277250767,\n",
       "   1.355275720357895,\n",
       "   1.370390983298421,\n",
       "   1.3659087549895048,\n",
       "   1.379639159888029,\n",
       "   1.361815607175231,\n",
       "   1.3562230542302132,\n",
       "   1.371403530240059,\n",
       "   1.3682607356458902,\n",
       "   1.3901784606277943,\n",
       "   1.3899613581597805,\n",
       "   1.3946690633893013,\n",
       "   1.392754314467311,\n",
       "   1.386660285294056,\n",
       "   1.3908971771597862,\n",
       "   1.3975089713931084,\n",
       "   1.4137748014181852,\n",
       "   1.4133205525577068,\n",
       "   1.4207741729915142,\n",
       "   1.4034967385232449,\n",
       "   1.4292391575872898,\n",
       "   1.4370077885687351,\n",
       "   1.4248262643814087,\n",
       "   1.417699245736003,\n",
       "   1.4249722510576248,\n",
       "   1.4273170158267021,\n",
       "   1.4442460853606462,\n",
       "   1.4407321847975254,\n",
       "   1.444729845970869,\n",
       "   1.4606158751994371,\n",
       "   1.4386937990784645,\n",
       "   1.4377846270799637,\n",
       "   1.4265311751514673,\n",
       "   1.4090265594422817,\n",
       "   1.4161119125783443,\n",
       "   1.4139112047851086,\n",
       "   1.4239359498023987,\n",
       "   1.437811452895403,\n",
       "   1.445913890376687,\n",
       "   1.4400345347821712,\n",
       "   1.442623969167471,\n",
       "   1.4321943558752537,\n",
       "   1.4108502492308617,\n",
       "   1.426207883283496,\n",
       "   1.4335536025464535,\n",
       "   1.4017821699380875,\n",
       "   1.4167210087180138,\n",
       "   1.431663990020752,\n",
       "   1.4174142517149448,\n",
       "   1.434801148250699,\n",
       "   1.4189882464706898,\n",
       "   1.4142352044582367,\n",
       "   1.4151240829378366,\n",
       "   1.3889653459191322,\n",
       "   1.3910389617085457,\n",
       "   1.3894843570888042,\n",
       "   1.369637181982398,\n",
       "   1.3853596113622189,\n",
       "   1.3847876656800508,\n",
       "   1.3803304843604565,\n",
       "   1.38138597458601,\n",
       "   1.3760428167879581,\n",
       "   1.3680263459682465,\n",
       "   1.3616573102772236,\n",
       "   1.381021922454238,\n",
       "   1.3886481430381536,\n",
       "   1.3799373861402273,\n",
       "   1.3761075921356678,\n",
       "   1.3896280825138092,\n",
       "   1.3702786527574062,\n",
       "   1.36134129203856,\n",
       "   1.3574260473251343,\n",
       "   1.3529817573726177,\n",
       "   1.3666154872626066,\n",
       "   1.3705365024507046,\n",
       "   1.3854492641985416,\n",
       "   1.3928283229470253,\n",
       "   1.3819756470620632,\n",
       "   1.3737149219959974,\n",
       "   1.3719017654657364,\n",
       "   1.3662906717509031,\n",
       "   1.3860513903200626,\n",
       "   1.382081314921379,\n",
       "   1.3911448530852795,\n",
       "   1.3801190238445997,\n",
       "   1.3763986732810736,\n",
       "   1.3896478284150362,\n",
       "   1.393127815797925,\n",
       "   1.4006720017641783,\n",
       "   1.404361180961132,\n",
       "   1.404870631173253,\n",
       "   1.397736283019185,\n",
       "   1.3890183791518211,\n",
       "   1.3854757752269506,\n",
       "   1.368711531162262],\n",
       "  'pce_acc': [66.416259765625,\n",
       "   55.396156311035156,\n",
       "   47.86339569091797,\n",
       "   45.55832290649414,\n",
       "   46.72878646850586,\n",
       "   43.337581634521484,\n",
       "   43.62934112548828,\n",
       "   41.898929595947266,\n",
       "   41.57978439331055,\n",
       "   41.303688049316406,\n",
       "   39.160709381103516,\n",
       "   44.03520202636719,\n",
       "   43.03180694580078,\n",
       "   44.626888275146484,\n",
       "   45.27760314941406,\n",
       "   50.20248794555664,\n",
       "   52.41787338256836,\n",
       "   59.67013168334961,\n",
       "   64.48544311523438,\n",
       "   69.15489196777344,\n",
       "   69.02069091796875,\n",
       "   69.21772003173828,\n",
       "   68.98683166503906,\n",
       "   68.79444885253906,\n",
       "   69.45986938476562,\n",
       "   74.66808319091797,\n",
       "   75.90562438964844,\n",
       "   77.26820373535156,\n",
       "   76.8387680053711,\n",
       "   79.39427185058594,\n",
       "   79.94857788085938,\n",
       "   81.21063995361328,\n",
       "   81.95722961425781,\n",
       "   82.58503723144531,\n",
       "   83.03215789794922,\n",
       "   83.73262023925781,\n",
       "   83.40967559814453,\n",
       "   84.04743957519531,\n",
       "   84.94068145751953,\n",
       "   84.9511489868164,\n",
       "   87.27222442626953,\n",
       "   86.3096923828125,\n",
       "   85.22382354736328,\n",
       "   86.95897674560547,\n",
       "   84.09278106689453,\n",
       "   84.4097900390625,\n",
       "   82.12826538085938,\n",
       "   82.7439956665039,\n",
       "   84.23697662353516,\n",
       "   85.49856567382812,\n",
       "   85.65507507324219,\n",
       "   85.29208374023438,\n",
       "   87.25371551513672,\n",
       "   86.78783416748047,\n",
       "   86.30696868896484,\n",
       "   86.21162414550781,\n",
       "   85.14733123779297,\n",
       "   85.44239044189453,\n",
       "   83.90975952148438,\n",
       "   84.39495086669922,\n",
       "   84.67070007324219,\n",
       "   84.6239242553711,\n",
       "   85.68131256103516,\n",
       "   82.84262084960938,\n",
       "   79.81737518310547,\n",
       "   80.21707153320312,\n",
       "   80.68021392822266,\n",
       "   80.5426254272461,\n",
       "   81.26769256591797,\n",
       "   81.80935668945312,\n",
       "   83.87682342529297,\n",
       "   85.14688873291016,\n",
       "   84.0225601196289,\n",
       "   82.78540802001953,\n",
       "   82.40190887451172,\n",
       "   84.98030090332031,\n",
       "   85.47595977783203,\n",
       "   86.566650390625,\n",
       "   85.73894500732422,\n",
       "   86.51823425292969,\n",
       "   86.19892120361328,\n",
       "   85.2229232788086,\n",
       "   84.10247039794922,\n",
       "   83.66376495361328,\n",
       "   82.93403625488281,\n",
       "   81.75457000732422,\n",
       "   81.47176361083984,\n",
       "   80.92042541503906,\n",
       "   81.31246948242188,\n",
       "   82.2950668334961,\n",
       "   84.54139709472656,\n",
       "   84.02275085449219,\n",
       "   81.93033599853516,\n",
       "   81.84349060058594,\n",
       "   81.56634521484375,\n",
       "   82.20878601074219,\n",
       "   83.01307678222656,\n",
       "   83.00831604003906,\n",
       "   81.14598846435547,\n",
       "   81.86347198486328,\n",
       "   81.70867919921875,\n",
       "   82.44473266601562,\n",
       "   84.79803466796875,\n",
       "   84.90776062011719,\n",
       "   84.4715576171875,\n",
       "   83.70130920410156,\n",
       "   83.72515106201172,\n",
       "   85.31768035888672,\n",
       "   84.70935821533203,\n",
       "   84.28436279296875,\n",
       "   86.0746078491211,\n",
       "   85.94361877441406,\n",
       "   86.87425231933594,\n",
       "   88.07475280761719,\n",
       "   87.64088439941406,\n",
       "   86.97594451904297,\n",
       "   85.57733154296875,\n",
       "   85.50823974609375,\n",
       "   85.45390319824219,\n",
       "   85.08859252929688,\n",
       "   85.0671615600586,\n",
       "   86.484619140625,\n",
       "   85.63128662109375,\n",
       "   85.29023742675781,\n",
       "   83.3199691772461,\n",
       "   80.25923156738281,\n",
       "   80.37165069580078,\n",
       "   78.989013671875,\n",
       "   78.606201171875,\n",
       "   80.26712799072266,\n",
       "   80.78874969482422,\n",
       "   82.21267700195312,\n",
       "   83.75616455078125,\n",
       "   83.28997802734375,\n",
       "   83.55401611328125,\n",
       "   82.91246032714844,\n",
       "   83.21920776367188,\n",
       "   85.68497467041016,\n",
       "   85.854248046875,\n",
       "   84.24240112304688,\n",
       "   85.12734985351562,\n",
       "   85.08293914794922,\n",
       "   84.99580383300781,\n",
       "   82.68302154541016,\n",
       "   83.50263977050781,\n",
       "   82.41241455078125,\n",
       "   83.27820587158203,\n",
       "   82.5009536743164,\n",
       "   81.9808578491211,\n",
       "   83.22329711914062,\n",
       "   82.27477264404297,\n",
       "   82.88484191894531,\n",
       "   83.10073852539062,\n",
       "   81.15790557861328,\n",
       "   82.83841705322266,\n",
       "   83.159912109375,\n",
       "   84.47354125976562,\n",
       "   85.38094329833984,\n",
       "   85.67485809326172,\n",
       "   84.11022186279297,\n",
       "   84.98477172851562,\n",
       "   87.84285736083984,\n",
       "   87.34510803222656,\n",
       "   86.75816345214844,\n",
       "   86.41804504394531,\n",
       "   85.83470916748047,\n",
       "   85.83475494384766,\n",
       "   83.9276123046875,\n",
       "   85.3792724609375,\n",
       "   86.2625503540039,\n",
       "   86.09716033935547,\n",
       "   84.34352111816406,\n",
       "   86.28955841064453,\n",
       "   85.99116516113281,\n",
       "   86.34813690185547,\n",
       "   84.12286376953125,\n",
       "   82.0652084350586,\n",
       "   83.50517272949219,\n",
       "   81.04486083984375,\n",
       "   82.38243103027344,\n",
       "   82.88069152832031,\n",
       "   83.94969177246094,\n",
       "   83.12361145019531,\n",
       "   84.12855529785156,\n",
       "   83.25975799560547,\n",
       "   83.72112274169922,\n",
       "   84.26520538330078,\n",
       "   84.23445892333984,\n",
       "   85.6649398803711,\n",
       "   85.98857116699219,\n",
       "   83.3985595703125,\n",
       "   81.97673034667969,\n",
       "   83.29606628417969,\n",
       "   82.43388366699219,\n",
       "   82.10803985595703,\n",
       "   80.82071685791016,\n",
       "   81.47804260253906,\n",
       "   79.04512023925781,\n",
       "   80.00096893310547,\n",
       "   79.64252471923828,\n",
       "   80.022705078125,\n",
       "   80.53358459472656,\n",
       "   81.67092895507812,\n",
       "   80.11437225341797,\n",
       "   80.16407775878906,\n",
       "   81.34674072265625,\n",
       "   81.4212417602539,\n",
       "   82.13549041748047,\n",
       "   82.89390563964844,\n",
       "   82.3536376953125,\n",
       "   82.85509490966797,\n",
       "   81.5553207397461,\n",
       "   80.27381134033203,\n",
       "   79.73403930664062,\n",
       "   80.48030853271484,\n",
       "   80.35276794433594,\n",
       "   81.61946105957031,\n",
       "   80.0605697631836,\n",
       "   81.30817413330078,\n",
       "   80.85865783691406,\n",
       "   83.29296112060547,\n",
       "   83.62834930419922,\n",
       "   84.09684753417969,\n",
       "   84.65324401855469,\n",
       "   83.74712371826172,\n",
       "   84.7083969116211,\n",
       "   84.60820770263672,\n",
       "   82.9966812133789,\n",
       "   84.50419616699219,\n",
       "   84.68926239013672,\n",
       "   84.64013671875,\n",
       "   85.07911682128906,\n",
       "   84.6149673461914,\n",
       "   84.11071014404297,\n",
       "   83.17684173583984,\n",
       "   82.18958282470703,\n",
       "   82.96273040771484,\n",
       "   83.08687591552734,\n",
       "   83.87735748291016,\n",
       "   84.75985717773438,\n",
       "   85.17120361328125,\n",
       "   83.12480163574219,\n",
       "   83.88417053222656,\n",
       "   84.23841094970703,\n",
       "   84.67315673828125,\n",
       "   86.21343994140625,\n",
       "   84.88213348388672,\n",
       "   83.21978759765625,\n",
       "   84.90383911132812,\n",
       "   84.02566528320312,\n",
       "   85.28002166748047,\n",
       "   86.52777862548828,\n",
       "   87.19363403320312,\n",
       "   84.57256317138672,\n",
       "   84.36315155029297,\n",
       "   84.7177505493164,\n",
       "   84.22298431396484,\n",
       "   84.71363067626953,\n",
       "   82.47723388671875,\n",
       "   83.46392822265625,\n",
       "   83.6347427368164,\n",
       "   82.2191390991211,\n",
       "   83.26579284667969,\n",
       "   82.30967712402344,\n",
       "   82.90760803222656,\n",
       "   83.9197006225586,\n",
       "   83.97367095947266,\n",
       "   83.30126190185547,\n",
       "   84.66165161132812,\n",
       "   85.79566192626953,\n",
       "   86.17172241210938,\n",
       "   83.94174194335938,\n",
       "   82.89460754394531,\n",
       "   82.78559112548828,\n",
       "   82.21863555908203,\n",
       "   83.83929443359375,\n",
       "   83.41254425048828,\n",
       "   82.53506469726562,\n",
       "   83.72899627685547,\n",
       "   84.15782165527344,\n",
       "   83.23946380615234,\n",
       "   84.38848114013672,\n",
       "   85.15167999267578,\n",
       "   85.21170043945312,\n",
       "   85.2296371459961,\n",
       "   82.51859283447266,\n",
       "   82.64701080322266,\n",
       "   83.18313598632812,\n",
       "   82.56594848632812,\n",
       "   82.59967041015625,\n",
       "   82.21836853027344,\n",
       "   80.67735290527344,\n",
       "   81.24363708496094,\n",
       "   79.82110595703125,\n",
       "   79.50021362304688,\n",
       "   80.52848815917969,\n",
       "   80.4277114868164,\n",
       "   79.31816101074219,\n",
       "   79.81156921386719,\n",
       "   80.71025085449219,\n",
       "   79.96643829345703,\n",
       "   79.55012512207031,\n",
       "   79.1650390625,\n",
       "   81.13688659667969,\n",
       "   80.90196228027344,\n",
       "   80.77455139160156,\n",
       "   81.11609649658203,\n",
       "   80.71129608154297,\n",
       "   81.98966979980469,\n",
       "   81.61339569091797,\n",
       "   83.46206665039062,\n",
       "   83.14352416992188,\n",
       "   83.76983642578125,\n",
       "   83.61238098144531,\n",
       "   84.8427963256836,\n",
       "   87.66963958740234,\n",
       "   88.09967041015625,\n",
       "   87.81908416748047,\n",
       "   85.27310943603516,\n",
       "   85.66920471191406,\n",
       "   84.76158905029297,\n",
       "   85.02256774902344,\n",
       "   83.6776351928711,\n",
       "   84.34293365478516,\n",
       "   84.80962371826172,\n",
       "   86.232421875,\n",
       "   87.253662109375,\n",
       "   83.83563995361328,\n",
       "   84.58454895019531,\n",
       "   83.713623046875,\n",
       "   84.72904968261719,\n",
       "   85.47924041748047,\n",
       "   84.24806213378906,\n",
       "   83.74983215332031,\n",
       "   82.656005859375,\n",
       "   83.02173614501953,\n",
       "   82.99683380126953,\n",
       "   82.95449829101562,\n",
       "   83.6410140991211,\n",
       "   83.81596374511719,\n",
       "   82.92820739746094,\n",
       "   81.28499603271484,\n",
       "   81.6240005493164,\n",
       "   81.74690246582031,\n",
       "   82.9333267211914,\n",
       "   82.75611877441406,\n",
       "   83.29749298095703,\n",
       "   82.28186798095703,\n",
       "   83.66834259033203,\n",
       "   83.85601806640625,\n",
       "   83.69351196289062,\n",
       "   82.3122787475586,\n",
       "   82.7795181274414,\n",
       "   83.04179382324219,\n",
       "   84.2279281616211,\n",
       "   83.92527770996094,\n",
       "   84.88839721679688,\n",
       "   85.75640106201172,\n",
       "   85.62783813476562,\n",
       "   85.40021514892578,\n",
       "   85.41310119628906,\n",
       "   84.42383575439453,\n",
       "   83.48454284667969,\n",
       "   81.51708221435547,\n",
       "   81.53128814697266,\n",
       "   80.05830383300781,\n",
       "   81.33644104003906,\n",
       "   79.44368743896484,\n",
       "   78.97005462646484,\n",
       "   76.17586517333984,\n",
       "   78.32243347167969,\n",
       "   79.37625122070312,\n",
       "   81.66043090820312,\n",
       "   80.51749420166016,\n",
       "   82.06869506835938,\n",
       "   82.84766387939453,\n",
       "   83.74182891845703,\n",
       "   84.24203491210938,\n",
       "   83.8588638305664,\n",
       "   83.64030456542969,\n",
       "   82.44949340820312,\n",
       "   83.5347900390625,\n",
       "   83.66998291015625,\n",
       "   83.8009262084961,\n",
       "   82.97229766845703,\n",
       "   84.756103515625,\n",
       "   84.70420837402344,\n",
       "   84.13359832763672,\n",
       "   82.99954223632812,\n",
       "   83.92860412597656,\n",
       "   84.33985137939453,\n",
       "   83.78485107421875,\n",
       "   84.34471130371094,\n",
       "   82.5762939453125,\n",
       "   81.78842163085938,\n",
       "   82.08429718017578,\n",
       "   81.84020233154297,\n",
       "   81.04579162597656,\n",
       "   81.67581939697266,\n",
       "   81.51438903808594,\n",
       "   83.06512451171875,\n",
       "   82.49414825439453,\n",
       "   82.67198944091797,\n",
       "   84.75331115722656,\n",
       "   84.739013671875,\n",
       "   83.42478942871094,\n",
       "   81.845703125,\n",
       "   82.91253662109375,\n",
       "   81.50540924072266,\n",
       "   82.31128692626953,\n",
       "   81.49304962158203,\n",
       "   81.10191345214844,\n",
       "   82.94591522216797,\n",
       "   82.50894165039062,\n",
       "   84.68680572509766,\n",
       "   84.25006866455078,\n",
       "   84.2584228515625,\n",
       "   83.67901611328125,\n",
       "   83.97151947021484,\n",
       "   83.99237823486328,\n",
       "   84.523681640625,\n",
       "   84.9717788696289,\n",
       "   84.92647552490234,\n",
       "   86.03089904785156,\n",
       "   84.90636444091797,\n",
       "   86.77851867675781,\n",
       "   87.58704376220703,\n",
       "   86.78253936767578,\n",
       "   85.46165466308594,\n",
       "   86.06033325195312,\n",
       "   86.15866088867188,\n",
       "   87.99700927734375,\n",
       "   88.22362518310547,\n",
       "   87.5904769897461,\n",
       "   88.59200286865234,\n",
       "   87.11212921142578,\n",
       "   87.33268737792969,\n",
       "   85.96080780029297,\n",
       "   84.15106201171875,\n",
       "   83.71398162841797,\n",
       "   83.52021789550781,\n",
       "   84.61712646484375,\n",
       "   85.818359375,\n",
       "   86.55184173583984,\n",
       "   86.603271484375,\n",
       "   86.62379455566406,\n",
       "   85.64540100097656,\n",
       "   84.1159439086914,\n",
       "   84.88142395019531,\n",
       "   85.44642639160156,\n",
       "   82.51304626464844,\n",
       "   83.84526062011719,\n",
       "   85.63584899902344,\n",
       "   84.3936767578125,\n",
       "   85.43499755859375,\n",
       "   84.45928192138672,\n",
       "   83.79995727539062,\n",
       "   84.02216339111328,\n",
       "   81.95468139648438,\n",
       "   82.13357543945312,\n",
       "   82.86434173583984,\n",
       "   81.1343994140625,\n",
       "   82.1184310913086,\n",
       "   81.55549621582031,\n",
       "   81.15287780761719,\n",
       "   81.12503051757812,\n",
       "   81.25770568847656,\n",
       "   80.76197052001953,\n",
       "   80.70614624023438,\n",
       "   82.6558837890625,\n",
       "   83.12220001220703,\n",
       "   83.05191040039062,\n",
       "   83.49322509765625,\n",
       "   84.35733032226562,\n",
       "   83.37849426269531,\n",
       "   82.4787368774414,\n",
       "   81.9053726196289,\n",
       "   81.28422546386719,\n",
       "   81.8525161743164,\n",
       "   82.26607513427734,\n",
       "   83.08733367919922,\n",
       "   83.59690856933594,\n",
       "   82.22400665283203,\n",
       "   81.06490325927734,\n",
       "   80.94579315185547,\n",
       "   79.77854919433594,\n",
       "   81.73667907714844,\n",
       "   82.0388412475586,\n",
       "   83.15684509277344,\n",
       "   81.79119110107422,\n",
       "   82.20279693603516,\n",
       "   82.74700927734375,\n",
       "   83.1026611328125,\n",
       "   83.8041763305664,\n",
       "   83.69393920898438,\n",
       "   83.38430786132812,\n",
       "   83.02915954589844,\n",
       "   83.43090057373047,\n",
       "   83.14654541015625,\n",
       "   81.72634887695312],\n",
       "  'voc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.68607330322266,\n",
       "   92.26150512695312,\n",
       "   84.9187240600586,\n",
       "   76.74407958984375,\n",
       "   69.38874816894531,\n",
       "   63.56038284301758,\n",
       "   58.94400405883789,\n",
       "   55.49647903442383,\n",
       "   52.4069709777832,\n",
       "   48.33323669433594,\n",
       "   42.20133590698242,\n",
       "   39.598297119140625,\n",
       "   37.31461715698242,\n",
       "   34.74326705932617,\n",
       "   34.62430953979492,\n",
       "   29.560302734375,\n",
       "   28.346168518066406,\n",
       "   24.096782684326172,\n",
       "   21.999656677246094,\n",
       "   21.1434268951416,\n",
       "   17.812543869018555,\n",
       "   14.567737579345703,\n",
       "   15.392213821411133,\n",
       "   15.805191040039062,\n",
       "   16.294235229492188,\n",
       "   15.57193374633789,\n",
       "   14.60576057434082,\n",
       "   16.329530715942383,\n",
       "   14.034798622131348,\n",
       "   13.51773738861084,\n",
       "   12.962677955627441,\n",
       "   13.549320220947266,\n",
       "   14.328058242797852,\n",
       "   12.768793106079102,\n",
       "   11.7489652633667,\n",
       "   11.752686500549316,\n",
       "   12.506973266601562,\n",
       "   11.91316032409668,\n",
       "   11.763596534729004,\n",
       "   11.806909561157227,\n",
       "   11.882686614990234,\n",
       "   12.354463577270508,\n",
       "   12.276531219482422,\n",
       "   12.467501640319824,\n",
       "   12.637153625488281,\n",
       "   12.203054428100586,\n",
       "   12.125198364257812,\n",
       "   12.269383430480957,\n",
       "   12.94596004486084,\n",
       "   12.358808517456055,\n",
       "   12.467856407165527,\n",
       "   12.779614448547363,\n",
       "   12.618371963500977,\n",
       "   12.685625076293945,\n",
       "   12.589159965515137,\n",
       "   12.480932235717773,\n",
       "   12.751115798950195,\n",
       "   12.607627868652344,\n",
       "   12.65969467163086,\n",
       "   12.180316925048828,\n",
       "   12.061630249023438,\n",
       "   12.179025650024414,\n",
       "   12.226739883422852,\n",
       "   12.2949857711792,\n",
       "   11.973175048828125,\n",
       "   12.091226577758789,\n",
       "   11.451552391052246,\n",
       "   11.506014823913574,\n",
       "   11.991720199584961,\n",
       "   11.57446575164795,\n",
       "   12.090337753295898,\n",
       "   11.826976776123047,\n",
       "   12.039925575256348,\n",
       "   12.278711318969727,\n",
       "   12.320516586303711,\n",
       "   12.364174842834473,\n",
       "   11.973370552062988,\n",
       "   12.210798263549805,\n",
       "   11.944458961486816,\n",
       "   11.966458320617676,\n",
       "   12.308910369873047,\n",
       "   11.76816463470459,\n",
       "   11.934389114379883,\n",
       "   11.819377899169922,\n",
       "   11.973798751831055,\n",
       "   11.668191909790039,\n",
       "   13.302996635437012,\n",
       "   12.54533863067627,\n",
       "   13.338846206665039,\n",
       "   13.567778587341309,\n",
       "   12.971284866333008,\n",
       "   12.806519508361816,\n",
       "   12.39427375793457,\n",
       "   12.760810852050781,\n",
       "   13.454299926757812,\n",
       "   14.073875427246094,\n",
       "   14.170024871826172,\n",
       "   14.200155258178711,\n",
       "   15.96354866027832,\n",
       "   14.488252639770508,\n",
       "   14.734664916992188,\n",
       "   15.277901649475098,\n",
       "   13.502740859985352,\n",
       "   13.431389808654785,\n",
       "   13.347940444946289,\n",
       "   12.223538398742676,\n",
       "   12.970614433288574,\n",
       "   13.14046859741211,\n",
       "   13.36335277557373,\n",
       "   12.929132461547852,\n",
       "   13.967233657836914,\n",
       "   14.17357349395752,\n",
       "   12.85453987121582,\n",
       "   11.401205062866211,\n",
       "   11.444616317749023,\n",
       "   10.985596656799316,\n",
       "   11.017410278320312,\n",
       "   11.672367095947266,\n",
       "   11.012409210205078,\n",
       "   10.899829864501953,\n",
       "   10.475092887878418,\n",
       "   10.425104141235352,\n",
       "   11.525874137878418,\n",
       "   11.681365966796875,\n",
       "   11.70694351196289,\n",
       "   13.086870193481445,\n",
       "   12.405519485473633,\n",
       "   13.41385555267334,\n",
       "   13.737581253051758,\n",
       "   15.546777725219727,\n",
       "   13.149359703063965,\n",
       "   13.379101753234863,\n",
       "   14.459609985351562,\n",
       "   15.76370906829834,\n",
       "   15.341120719909668,\n",
       "   14.707742691040039,\n",
       "   14.377776145935059,\n",
       "   15.083083152770996,\n",
       "   13.117106437683105,\n",
       "   12.838491439819336,\n",
       "   14.550142288208008,\n",
       "   14.437058448791504,\n",
       "   14.881070137023926,\n",
       "   16.56065559387207,\n",
       "   16.4891357421875,\n",
       "   16.303709030151367,\n",
       "   17.076343536376953,\n",
       "   16.530109405517578,\n",
       "   17.276994705200195,\n",
       "   17.343172073364258,\n",
       "   17.980478286743164,\n",
       "   19.242586135864258,\n",
       "   19.321184158325195,\n",
       "   17.460790634155273,\n",
       "   16.275218963623047,\n",
       "   16.72930335998535,\n",
       "   16.41557502746582,\n",
       "   15.934608459472656,\n",
       "   15.054207801818848,\n",
       "   15.364286422729492,\n",
       "   16.102527618408203,\n",
       "   15.762585639953613,\n",
       "   16.062225341796875,\n",
       "   16.931631088256836,\n",
       "   18.431787490844727,\n",
       "   20.708675384521484,\n",
       "   20.6907958984375,\n",
       "   21.26238441467285,\n",
       "   21.03019142150879,\n",
       "   18.86571502685547,\n",
       "   19.357742309570312,\n",
       "   20.54876708984375,\n",
       "   19.966930389404297,\n",
       "   18.16181182861328,\n",
       "   18.059406280517578,\n",
       "   18.06134605407715,\n",
       "   16.4370174407959,\n",
       "   15.803665161132812,\n",
       "   15.80928897857666,\n",
       "   14.867886543273926,\n",
       "   14.872686386108398,\n",
       "   14.292510032653809,\n",
       "   14.213520050048828,\n",
       "   13.018501281738281,\n",
       "   16.041824340820312,\n",
       "   14.577512741088867,\n",
       "   13.987104415893555,\n",
       "   13.672775268554688,\n",
       "   15.283785820007324,\n",
       "   14.990385055541992,\n",
       "   12.958441734313965,\n",
       "   12.45976448059082,\n",
       "   12.716447830200195,\n",
       "   14.1631441116333,\n",
       "   15.192108154296875,\n",
       "   14.099637985229492,\n",
       "   13.572019577026367,\n",
       "   13.82747745513916,\n",
       "   13.814324378967285,\n",
       "   14.091489791870117,\n",
       "   15.273432731628418,\n",
       "   14.217403411865234,\n",
       "   14.177038192749023,\n",
       "   14.843036651611328,\n",
       "   15.626363754272461,\n",
       "   13.748699188232422,\n",
       "   13.141663551330566,\n",
       "   15.549057960510254,\n",
       "   16.475004196166992,\n",
       "   16.20306396484375,\n",
       "   16.55310821533203,\n",
       "   17.087047576904297,\n",
       "   15.82909107208252,\n",
       "   16.146343231201172,\n",
       "   15.842448234558105,\n",
       "   14.339455604553223,\n",
       "   13.636941909790039,\n",
       "   15.499259948730469,\n",
       "   16.1129093170166,\n",
       "   18.150775909423828,\n",
       "   19.93022918701172,\n",
       "   18.048032760620117,\n",
       "   17.031145095825195,\n",
       "   16.227935791015625,\n",
       "   16.071720123291016,\n",
       "   13.708385467529297,\n",
       "   13.844532012939453,\n",
       "   12.1605806350708,\n",
       "   11.328327178955078,\n",
       "   11.652152061462402,\n",
       "   11.33139419555664,\n",
       "   11.207164764404297,\n",
       "   11.385507583618164,\n",
       "   11.569649696350098,\n",
       "   11.658397674560547,\n",
       "   12.958964347839355,\n",
       "   13.720780372619629,\n",
       "   14.550664901733398,\n",
       "   14.4649076461792,\n",
       "   13.846336364746094,\n",
       "   13.544082641601562,\n",
       "   14.168211936950684,\n",
       "   13.978385925292969,\n",
       "   14.445517539978027,\n",
       "   13.921058654785156,\n",
       "   14.354125022888184,\n",
       "   14.171266555786133,\n",
       "   14.117613792419434,\n",
       "   12.894251823425293,\n",
       "   12.728485107421875,\n",
       "   12.332265853881836,\n",
       "   11.029609680175781,\n",
       "   11.80302619934082,\n",
       "   11.226484298706055,\n",
       "   11.3566255569458,\n",
       "   10.938589096069336,\n",
       "   11.182228088378906,\n",
       "   11.663724899291992,\n",
       "   11.494667053222656,\n",
       "   11.934089660644531,\n",
       "   12.15600872039795,\n",
       "   12.002405166625977,\n",
       "   11.789016723632812,\n",
       "   12.090290069580078,\n",
       "   12.437206268310547,\n",
       "   12.807149887084961,\n",
       "   12.690576553344727,\n",
       "   13.101970672607422,\n",
       "   12.345917701721191,\n",
       "   12.334712982177734,\n",
       "   12.500596046447754,\n",
       "   13.035667419433594,\n",
       "   13.056905746459961,\n",
       "   12.712541580200195,\n",
       "   12.890450477600098,\n",
       "   12.719193458557129,\n",
       "   12.661920547485352,\n",
       "   12.609766006469727,\n",
       "   12.562280654907227,\n",
       "   13.274389266967773,\n",
       "   12.118640899658203,\n",
       "   11.971659660339355,\n",
       "   12.008834838867188,\n",
       "   12.504837989807129,\n",
       "   12.32556438446045,\n",
       "   12.298232078552246,\n",
       "   12.638163566589355,\n",
       "   12.886187553405762,\n",
       "   12.828664779663086,\n",
       "   13.12157917022705,\n",
       "   13.214315414428711,\n",
       "   13.411224365234375,\n",
       "   12.975422859191895,\n",
       "   13.101187705993652,\n",
       "   12.603534698486328,\n",
       "   12.902003288269043,\n",
       "   13.60744857788086,\n",
       "   12.460693359375,\n",
       "   12.739227294921875,\n",
       "   12.721880912780762,\n",
       "   12.900334358215332,\n",
       "   12.724018096923828,\n",
       "   12.6435546875,\n",
       "   12.47127914428711,\n",
       "   12.429088592529297,\n",
       "   12.437164306640625,\n",
       "   12.523433685302734,\n",
       "   12.021666526794434,\n",
       "   12.092581748962402,\n",
       "   12.4498929977417,\n",
       "   12.471988677978516,\n",
       "   12.61763858795166,\n",
       "   13.970084190368652,\n",
       "   14.05193042755127,\n",
       "   12.775641441345215,\n",
       "   12.399371147155762,\n",
       "   12.230203628540039,\n",
       "   12.173760414123535,\n",
       "   12.31935977935791,\n",
       "   11.912031173706055,\n",
       "   12.183812141418457,\n",
       "   11.88947868347168,\n",
       "   12.25473690032959,\n",
       "   11.935233116149902,\n",
       "   11.941621780395508,\n",
       "   11.983026504516602,\n",
       "   12.181396484375,\n",
       "   12.625604629516602,\n",
       "   12.669290542602539,\n",
       "   12.472806930541992,\n",
       "   12.763818740844727,\n",
       "   13.533841133117676,\n",
       "   13.9647855758667,\n",
       "   15.765836715698242,\n",
       "   16.01289176940918,\n",
       "   17.009632110595703,\n",
       "   15.938459396362305,\n",
       "   15.802190780639648,\n",
       "   16.106718063354492,\n",
       "   14.65019416809082,\n",
       "   13.999795913696289,\n",
       "   13.456744194030762,\n",
       "   13.349061965942383,\n",
       "   13.247262954711914,\n",
       "   12.636322021484375,\n",
       "   12.29489803314209,\n",
       "   12.453767776489258,\n",
       "   13.147086143493652,\n",
       "   12.539895057678223,\n",
       "   12.52188491821289,\n",
       "   13.0418119430542,\n",
       "   12.480758666992188,\n",
       "   12.183478355407715,\n",
       "   12.169118881225586,\n",
       "   11.911325454711914,\n",
       "   12.184311866760254,\n",
       "   12.386350631713867,\n",
       "   12.862003326416016,\n",
       "   13.282086372375488,\n",
       "   12.569597244262695,\n",
       "   12.704041481018066,\n",
       "   13.191503524780273,\n",
       "   13.382599830627441,\n",
       "   12.318536758422852,\n",
       "   12.61005973815918,\n",
       "   12.868660926818848,\n",
       "   14.27268123626709,\n",
       "   14.232322692871094,\n",
       "   15.112214088439941,\n",
       "   14.631295204162598,\n",
       "   14.299978256225586,\n",
       "   14.699116706848145,\n",
       "   14.620134353637695,\n",
       "   14.784442901611328,\n",
       "   14.514495849609375,\n",
       "   14.190755844116211,\n",
       "   13.335577964782715,\n",
       "   12.691707611083984,\n",
       "   12.355706214904785,\n",
       "   12.917876243591309,\n",
       "   13.541605949401855,\n",
       "   14.044992446899414,\n",
       "   13.344051361083984,\n",
       "   15.006491661071777,\n",
       "   13.047896385192871,\n",
       "   12.330395698547363,\n",
       "   12.260354995727539,\n",
       "   12.265397071838379,\n",
       "   13.289773941040039,\n",
       "   13.062949180603027,\n",
       "   12.99394416809082,\n",
       "   12.155251502990723,\n",
       "   12.343892097473145,\n",
       "   11.98324966430664,\n",
       "   11.757901191711426,\n",
       "   11.505109786987305,\n",
       "   11.571693420410156,\n",
       "   11.420960426330566,\n",
       "   11.732702255249023,\n",
       "   11.880526542663574,\n",
       "   12.091806411743164,\n",
       "   12.28358268737793,\n",
       "   13.02634334564209,\n",
       "   11.845356941223145,\n",
       "   13.491111755371094,\n",
       "   12.018111228942871,\n",
       "   12.240602493286133,\n",
       "   12.045523643493652,\n",
       "   12.381331443786621,\n",
       "   13.485713005065918,\n",
       "   13.103538513183594,\n",
       "   14.108914375305176,\n",
       "   12.35382080078125,\n",
       "   13.062919616699219,\n",
       "   14.698561668395996,\n",
       "   13.851216316223145,\n",
       "   15.645120620727539,\n",
       "   15.544137001037598,\n",
       "   17.375537872314453,\n",
       "   16.56731605529785,\n",
       "   15.988579750061035,\n",
       "   17.326919555664062,\n",
       "   17.203304290771484,\n",
       "   16.54793357849121,\n",
       "   16.366636276245117,\n",
       "   15.55639362335205,\n",
       "   17.457971572875977,\n",
       "   17.246273040771484,\n",
       "   17.724035263061523,\n",
       "   18.813156127929688,\n",
       "   17.983583450317383,\n",
       "   17.254295349121094,\n",
       "   17.553977966308594,\n",
       "   16.5317325592041,\n",
       "   16.74887466430664,\n",
       "   18.488950729370117,\n",
       "   18.148113250732422,\n",
       "   16.458337783813477,\n",
       "   15.67264461517334,\n",
       "   14.853480339050293,\n",
       "   15.827581405639648,\n",
       "   14.404709815979004,\n",
       "   14.741908073425293,\n",
       "   15.254727363586426,\n",
       "   15.660402297973633,\n",
       "   17.45371437072754,\n",
       "   17.359657287597656,\n",
       "   18.17107582092285,\n",
       "   19.882034301757812,\n",
       "   21.04738998413086,\n",
       "   18.715484619140625,\n",
       "   17.53444480895996,\n",
       "   18.44743537902832,\n",
       "   18.712312698364258,\n",
       "   19.146759033203125,\n",
       "   18.823766708374023,\n",
       "   18.077072143554688,\n",
       "   18.996564865112305,\n",
       "   19.151153564453125,\n",
       "   17.50278091430664,\n",
       "   16.717239379882812,\n",
       "   15.585556983947754,\n",
       "   14.003020286560059,\n",
       "   10.949824333190918,\n",
       "   10.3723783493042,\n",
       "   10.72513484954834,\n",
       "   11.22711181640625,\n",
       "   12.622603416442871,\n",
       "   13.547572135925293,\n",
       "   13.376334190368652,\n",
       "   13.977355003356934,\n",
       "   14.088079452514648,\n",
       "   14.2916259765625,\n",
       "   12.527972221374512,\n",
       "   13.251291275024414,\n",
       "   13.922673225402832,\n",
       "   13.26625919342041,\n",
       "   14.718219757080078,\n",
       "   15.345928192138672,\n",
       "   17.983346939086914,\n",
       "   15.792054176330566,\n",
       "   15.005147933959961,\n",
       "   14.46657943725586,\n",
       "   14.520771980285645,\n",
       "   15.500922203063965,\n",
       "   14.311549186706543,\n",
       "   14.745672225952148,\n",
       "   16.885395050048828,\n",
       "   16.620708465576172,\n",
       "   17.91602325439453,\n",
       "   17.27887725830078,\n",
       "   18.59975242614746,\n",
       "   18.748394012451172],\n",
       "  'jsc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.89290618896484,\n",
       "   100.0,\n",
       "   99.83577728271484,\n",
       "   97.77171325683594,\n",
       "   97.35261535644531,\n",
       "   95.8457260131836,\n",
       "   94.51205444335938,\n",
       "   93.4320297241211,\n",
       "   94.00799560546875,\n",
       "   94.82213592529297,\n",
       "   94.58658599853516,\n",
       "   95.0678482055664,\n",
       "   95.79029083251953,\n",
       "   96.1020736694336,\n",
       "   95.82203674316406,\n",
       "   96.06864929199219,\n",
       "   96.01461029052734,\n",
       "   95.87841033935547,\n",
       "   95.91107940673828,\n",
       "   96.29608154296875,\n",
       "   95.07640838623047,\n",
       "   96.05316925048828,\n",
       "   96.17449188232422,\n",
       "   96.2470932006836,\n",
       "   96.39765930175781,\n",
       "   96.3918228149414,\n",
       "   96.91229248046875,\n",
       "   97.16829681396484,\n",
       "   97.23064422607422,\n",
       "   96.63933563232422,\n",
       "   96.8127670288086,\n",
       "   97.03800964355469,\n",
       "   96.85089874267578,\n",
       "   96.48868560791016,\n",
       "   96.30848693847656,\n",
       "   96.73087310791016,\n",
       "   96.67642974853516,\n",
       "   96.60874938964844,\n",
       "   96.69452667236328,\n",
       "   96.5133056640625,\n",
       "   96.55931854248047,\n",
       "   96.60399627685547,\n",
       "   96.96380615234375,\n",
       "   97.0694351196289,\n",
       "   96.38789367675781,\n",
       "   96.53133392333984,\n",
       "   96.6846923828125,\n",
       "   97.0645523071289,\n",
       "   97.46965789794922,\n",
       "   96.98908233642578,\n",
       "   96.67350769042969,\n",
       "   97.02278900146484,\n",
       "   97.08882904052734,\n",
       "   96.60452270507812,\n",
       "   96.52542114257812,\n",
       "   95.793212890625,\n",
       "   95.87613677978516,\n",
       "   94.22484588623047,\n",
       "   93.4646224975586,\n",
       "   94.32528686523438,\n",
       "   94.81652069091797,\n",
       "   95.16048431396484,\n",
       "   95.48096466064453,\n",
       "   95.17829132080078,\n",
       "   94.59467315673828,\n",
       "   94.25677490234375,\n",
       "   93.76295471191406,\n",
       "   94.41744232177734,\n",
       "   94.33242797851562,\n",
       "   95.45942687988281,\n",
       "   95.52595520019531,\n",
       "   96.28936767578125,\n",
       "   96.35186004638672,\n",
       "   96.86402130126953,\n",
       "   96.8638687133789,\n",
       "   97.03556060791016,\n",
       "   97.65304565429688,\n",
       "   97.55455017089844,\n",
       "   98.15304565429688,\n",
       "   97.92117309570312,\n",
       "   97.68140411376953,\n",
       "   97.31521606445312,\n",
       "   96.96727752685547,\n",
       "   96.06317901611328,\n",
       "   96.60899353027344,\n",
       "   95.29159545898438,\n",
       "   94.81153106689453,\n",
       "   94.85990142822266,\n",
       "   95.06082916259766,\n",
       "   94.80230712890625,\n",
       "   95.1490249633789,\n",
       "   95.30221557617188,\n",
       "   94.96278381347656,\n",
       "   94.74518585205078,\n",
       "   94.6725082397461,\n",
       "   94.5435562133789,\n",
       "   95.11837005615234,\n",
       "   95.48546600341797,\n",
       "   95.73599243164062,\n",
       "   96.0489273071289,\n",
       "   96.39521789550781,\n",
       "   96.15618133544922,\n",
       "   96.01776885986328,\n",
       "   95.71247863769531,\n",
       "   96.18824768066406,\n",
       "   96.39620208740234,\n",
       "   97.01380920410156,\n",
       "   96.78980255126953,\n",
       "   96.41429901123047,\n",
       "   96.39051818847656,\n",
       "   95.93643188476562,\n",
       "   95.5729751586914,\n",
       "   95.7384262084961,\n",
       "   96.03370666503906,\n",
       "   96.10408782958984,\n",
       "   96.60279846191406,\n",
       "   96.65789031982422,\n",
       "   96.69892883300781,\n",
       "   96.81292724609375,\n",
       "   96.97378540039062,\n",
       "   97.18869018554688,\n",
       "   97.59683990478516,\n",
       "   97.16958618164062,\n",
       "   97.27951049804688,\n",
       "   97.4868392944336,\n",
       "   97.63555145263672,\n",
       "   97.76582336425781,\n",
       "   97.64031982421875,\n",
       "   97.01725769042969,\n",
       "   97.04859924316406,\n",
       "   97.1361312866211,\n",
       "   96.61820220947266,\n",
       "   97.163818359375,\n",
       "   97.32591247558594,\n",
       "   97.61185455322266,\n",
       "   97.854248046875,\n",
       "   97.81334686279297,\n",
       "   98.01329040527344,\n",
       "   97.77833557128906,\n",
       "   97.44361877441406,\n",
       "   97.67424774169922,\n",
       "   97.5735855102539,\n",
       "   97.146240234375,\n",
       "   96.93643188476562,\n",
       "   96.82305145263672,\n",
       "   96.96434783935547,\n",
       "   97.34404754638672,\n",
       "   97.0743408203125,\n",
       "   97.27371978759766,\n",
       "   97.11978149414062,\n",
       "   97.2450180053711,\n",
       "   97.03820037841797,\n",
       "   96.90164947509766,\n",
       "   97.13538360595703,\n",
       "   97.4659652709961,\n",
       "   97.30960845947266,\n",
       "   97.3494873046875,\n",
       "   96.921142578125,\n",
       "   97.36186981201172,\n",
       "   97.18484497070312,\n",
       "   96.90501403808594,\n",
       "   96.79043579101562,\n",
       "   96.84166717529297,\n",
       "   97.4284439086914,\n",
       "   97.25262451171875,\n",
       "   97.0518569946289,\n",
       "   96.82835388183594,\n",
       "   96.59237670898438,\n",
       "   96.37869262695312,\n",
       "   96.10382843017578,\n",
       "   96.1455078125,\n",
       "   96.55602264404297,\n",
       "   96.29875946044922,\n",
       "   96.62518310546875,\n",
       "   96.65283203125,\n",
       "   96.80540466308594,\n",
       "   96.63045501708984,\n",
       "   96.71263122558594,\n",
       "   96.81580352783203,\n",
       "   97.03009796142578,\n",
       "   97.029052734375,\n",
       "   97.19487762451172,\n",
       "   97.4273910522461,\n",
       "   97.38758087158203,\n",
       "   97.70149993896484,\n",
       "   97.30201721191406,\n",
       "   97.75386047363281,\n",
       "   97.75645446777344,\n",
       "   97.53396606445312,\n",
       "   97.18534851074219,\n",
       "   97.10423278808594,\n",
       "   97.59258270263672,\n",
       "   97.91571807861328,\n",
       "   98.03822326660156,\n",
       "   97.88079071044922,\n",
       "   98.05143737792969,\n",
       "   98.11448669433594,\n",
       "   97.91319274902344,\n",
       "   97.81502532958984,\n",
       "   97.78952026367188,\n",
       "   97.84688568115234,\n",
       "   97.63632202148438,\n",
       "   97.5419921875,\n",
       "   96.92547607421875,\n",
       "   97.72947692871094,\n",
       "   97.25643157958984,\n",
       "   97.21141052246094,\n",
       "   97.19575500488281,\n",
       "   97.60780334472656,\n",
       "   97.61001586914062,\n",
       "   97.37555694580078,\n",
       "   97.61563873291016,\n",
       "   97.55558776855469,\n",
       "   97.65803527832031,\n",
       "   97.71591186523438,\n",
       "   97.74296569824219,\n",
       "   97.59576416015625,\n",
       "   97.5499038696289,\n",
       "   97.50482177734375,\n",
       "   97.5599365234375,\n",
       "   97.7025375366211,\n",
       "   97.24201965332031,\n",
       "   97.26533508300781,\n",
       "   97.48131561279297,\n",
       "   97.32784271240234,\n",
       "   96.84695434570312,\n",
       "   97.41927337646484,\n",
       "   97.28430938720703,\n",
       "   97.54008483886719,\n",
       "   97.38496398925781,\n",
       "   97.20997619628906,\n",
       "   97.4763412475586,\n",
       "   97.70124816894531,\n",
       "   97.18013763427734,\n",
       "   97.22093200683594,\n",
       "   96.95325469970703,\n",
       "   96.58155822753906,\n",
       "   96.91260528564453,\n",
       "   96.25814819335938,\n",
       "   96.93787384033203,\n",
       "   97.0135726928711,\n",
       "   96.85152435302734,\n",
       "   96.65637969970703,\n",
       "   96.96892547607422,\n",
       "   96.94839477539062,\n",
       "   97.12103271484375,\n",
       "   97.27307891845703,\n",
       "   97.60359191894531,\n",
       "   97.6585693359375,\n",
       "   97.66413879394531,\n",
       "   97.53413391113281,\n",
       "   97.46439361572266,\n",
       "   97.23548889160156,\n",
       "   97.49556732177734,\n",
       "   97.76744079589844,\n",
       "   97.56787109375,\n",
       "   97.63545989990234,\n",
       "   97.77257537841797,\n",
       "   97.95511627197266,\n",
       "   97.71944427490234,\n",
       "   97.34566497802734,\n",
       "   97.42471313476562,\n",
       "   97.45356750488281,\n",
       "   97.39034271240234,\n",
       "   97.19525909423828,\n",
       "   97.05607604980469,\n",
       "   96.48650360107422,\n",
       "   96.74784851074219,\n",
       "   96.75027465820312,\n",
       "   96.50616455078125,\n",
       "   96.64837646484375,\n",
       "   96.67835235595703,\n",
       "   96.36808776855469,\n",
       "   96.2624282836914,\n",
       "   96.78043365478516,\n",
       "   96.42167663574219,\n",
       "   97.17719268798828,\n",
       "   97.48136138916016,\n",
       "   97.23370361328125,\n",
       "   97.51133728027344,\n",
       "   97.45275115966797,\n",
       "   97.30171966552734,\n",
       "   97.40853881835938,\n",
       "   97.46273040771484,\n",
       "   97.68746185302734,\n",
       "   97.37089538574219,\n",
       "   97.53376007080078,\n",
       "   97.54348754882812,\n",
       "   97.77503967285156,\n",
       "   97.7341537475586,\n",
       "   97.56566619873047,\n",
       "   97.8429183959961,\n",
       "   97.67961883544922,\n",
       "   97.78218841552734,\n",
       "   97.57431030273438,\n",
       "   97.39614868164062,\n",
       "   97.58126068115234,\n",
       "   97.59308624267578,\n",
       "   97.74998474121094,\n",
       "   97.57304382324219,\n",
       "   97.82948303222656,\n",
       "   97.78529357910156,\n",
       "   97.57244873046875,\n",
       "   97.28145599365234,\n",
       "   97.29496765136719,\n",
       "   97.4806900024414,\n",
       "   97.6765365600586,\n",
       "   97.28276062011719,\n",
       "   97.2471694946289,\n",
       "   97.27046966552734,\n",
       "   97.49454498291016,\n",
       "   97.4812240600586,\n",
       "   97.59378051757812,\n",
       "   97.79652404785156,\n",
       "   97.88850402832031,\n",
       "   97.7761001586914,\n",
       "   97.75845336914062,\n",
       "   97.28926086425781,\n",
       "   97.63093566894531,\n",
       "   97.74442291259766,\n",
       "   97.61309814453125,\n",
       "   97.38309478759766,\n",
       "   97.2044677734375,\n",
       "   97.33245849609375,\n",
       "   97.48841857910156,\n",
       "   97.26300048828125,\n",
       "   97.06709289550781,\n",
       "   96.97520446777344,\n",
       "   97.35724639892578,\n",
       "   97.62559509277344,\n",
       "   97.580810546875,\n",
       "   97.50574493408203,\n",
       "   97.5850830078125,\n",
       "   97.55390930175781,\n",
       "   97.80718231201172,\n",
       "   97.62516784667969,\n",
       "   97.41899871826172,\n",
       "   97.28038024902344,\n",
       "   97.17974853515625,\n",
       "   97.28378295898438,\n",
       "   97.30442810058594,\n",
       "   97.37113952636719,\n",
       "   97.30059814453125,\n",
       "   97.61243438720703,\n",
       "   97.60236358642578,\n",
       "   97.7281265258789,\n",
       "   97.67403411865234,\n",
       "   97.73786926269531,\n",
       "   97.73004913330078,\n",
       "   97.815185546875,\n",
       "   97.75515747070312,\n",
       "   97.41693878173828,\n",
       "   97.7266616821289,\n",
       "   97.7785415649414,\n",
       "   97.47882843017578,\n",
       "   97.4838638305664,\n",
       "   97.52391815185547,\n",
       "   97.17927551269531,\n",
       "   97.20863342285156,\n",
       "   97.04299926757812,\n",
       "   96.61837768554688,\n",
       "   96.92859649658203,\n",
       "   97.39433288574219,\n",
       "   97.59358978271484,\n",
       "   97.42090606689453,\n",
       "   96.9930648803711,\n",
       "   97.3194808959961,\n",
       "   97.53517150878906,\n",
       "   97.43309783935547,\n",
       "   97.74227142333984,\n",
       "   97.40618896484375,\n",
       "   97.18163299560547,\n",
       "   97.00382232666016,\n",
       "   96.82563781738281,\n",
       "   96.97503662109375,\n",
       "   96.24369049072266,\n",
       "   96.87459564208984,\n",
       "   96.88672637939453,\n",
       "   97.0190200805664,\n",
       "   96.81351470947266,\n",
       "   96.27876281738281,\n",
       "   96.62040710449219,\n",
       "   96.8246078491211,\n",
       "   97.16637420654297,\n",
       "   97.12841796875,\n",
       "   97.55965423583984,\n",
       "   96.97372436523438,\n",
       "   96.67617797851562,\n",
       "   96.81681060791016,\n",
       "   96.77102661132812,\n",
       "   96.75957489013672,\n",
       "   96.91638946533203,\n",
       "   97.19024658203125,\n",
       "   97.2518539428711,\n",
       "   97.00817108154297,\n",
       "   96.89129638671875,\n",
       "   96.46060180664062,\n",
       "   95.86112213134766,\n",
       "   96.2638931274414,\n",
       "   96.76307678222656,\n",
       "   96.98053741455078,\n",
       "   96.40904235839844,\n",
       "   96.18106842041016,\n",
       "   96.06587982177734,\n",
       "   96.15711212158203,\n",
       "   96.09161376953125,\n",
       "   96.39849853515625,\n",
       "   96.53560638427734,\n",
       "   96.77749633789062,\n",
       "   96.33367919921875,\n",
       "   96.64159393310547,\n",
       "   96.71761322021484,\n",
       "   97.33164978027344,\n",
       "   97.4354248046875,\n",
       "   97.10044860839844,\n",
       "   96.85240173339844,\n",
       "   96.85733032226562,\n",
       "   96.8492660522461,\n",
       "   96.68658447265625,\n",
       "   96.94793701171875,\n",
       "   97.09173583984375,\n",
       "   97.147216796875,\n",
       "   96.87263488769531,\n",
       "   96.53942108154297,\n",
       "   96.89041900634766,\n",
       "   97.3070297241211,\n",
       "   97.0499496459961,\n",
       "   96.58656311035156,\n",
       "   96.81095123291016,\n",
       "   96.86915588378906,\n",
       "   97.37614440917969,\n",
       "   97.4899673461914,\n",
       "   97.64879608154297,\n",
       "   97.64189910888672,\n",
       "   97.70658874511719,\n",
       "   97.4223403930664,\n",
       "   97.61182403564453,\n",
       "   97.49207305908203,\n",
       "   97.13056182861328,\n",
       "   97.55425262451172,\n",
       "   97.75255584716797,\n",
       "   97.8014144897461,\n",
       "   97.80176544189453,\n",
       "   97.49554443359375,\n",
       "   97.51750183105469,\n",
       "   97.78825378417969,\n",
       "   97.33773803710938,\n",
       "   97.47145080566406,\n",
       "   97.53845977783203,\n",
       "   97.38431549072266,\n",
       "   97.298828125,\n",
       "   96.77928924560547,\n",
       "   96.75503540039062,\n",
       "   97.08283233642578,\n",
       "   97.27937316894531,\n",
       "   97.25591278076172,\n",
       "   97.31900024414062,\n",
       "   97.17646026611328,\n",
       "   97.12586975097656,\n",
       "   96.8675765991211,\n",
       "   96.93896484375,\n",
       "   96.99176025390625,\n",
       "   96.45858764648438,\n",
       "   95.99790954589844,\n",
       "   96.26797485351562,\n",
       "   95.72573852539062,\n",
       "   95.79306030273438,\n",
       "   95.93763732910156,\n",
       "   96.11441802978516,\n",
       "   96.39341735839844,\n",
       "   96.22344207763672,\n",
       "   96.6353530883789,\n",
       "   96.61966705322266,\n",
       "   96.78266906738281,\n",
       "   96.97981262207031,\n",
       "   96.89948272705078,\n",
       "   97.0881118774414,\n",
       "   97.02041625976562,\n",
       "   96.92155456542969,\n",
       "   96.83808898925781,\n",
       "   97.11563873291016,\n",
       "   96.71990966796875,\n",
       "   97.2059097290039,\n",
       "   97.24171447753906,\n",
       "   97.19656372070312,\n",
       "   97.54254913330078,\n",
       "   97.87757873535156,\n",
       "   97.5295181274414,\n",
       "   96.79798126220703,\n",
       "   96.64668273925781,\n",
       "   96.64713287353516],\n",
       "  'ff_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.49494934082031,\n",
       "   91.71634674072266,\n",
       "   81.25731658935547,\n",
       "   73.39857482910156,\n",
       "   60.775875091552734,\n",
       "   51.66710662841797,\n",
       "   46.63259506225586,\n",
       "   41.002742767333984,\n",
       "   33.365753173828125,\n",
       "   32.784645080566406,\n",
       "   31.785202026367188,\n",
       "   27.54138946533203,\n",
       "   24.328454971313477,\n",
       "   22.887828826904297,\n",
       "   20.462974548339844,\n",
       "   19.284757614135742,\n",
       "   17.69417953491211,\n",
       "   17.625764846801758,\n",
       "   18.104047775268555,\n",
       "   17.661357879638672,\n",
       "   17.481130599975586,\n",
       "   17.120540618896484,\n",
       "   16.35757064819336,\n",
       "   16.253522872924805,\n",
       "   15.5335111618042,\n",
       "   15.527534484863281,\n",
       "   15.752763748168945,\n",
       "   14.834968566894531,\n",
       "   14.3801908493042,\n",
       "   14.918686866760254,\n",
       "   14.898877143859863,\n",
       "   15.041353225708008,\n",
       "   15.277835845947266,\n",
       "   16.27971839904785,\n",
       "   16.35540199279785,\n",
       "   16.69770050048828,\n",
       "   15.933680534362793,\n",
       "   16.064390182495117,\n",
       "   16.68940544128418,\n",
       "   17.489864349365234,\n",
       "   16.74712371826172,\n",
       "   17.27316665649414,\n",
       "   16.728836059570312,\n",
       "   15.989279747009277,\n",
       "   15.78177261352539,\n",
       "   15.542028427124023,\n",
       "   15.139697074890137,\n",
       "   15.332990646362305,\n",
       "   15.51479434967041,\n",
       "   15.601195335388184,\n",
       "   15.635207176208496,\n",
       "   15.203722953796387,\n",
       "   15.013996124267578,\n",
       "   15.03124713897705,\n",
       "   14.920687675476074,\n",
       "   14.845085144042969,\n",
       "   15.572731018066406,\n",
       "   15.516517639160156,\n",
       "   15.576887130737305,\n",
       "   15.380549430847168,\n",
       "   15.718277931213379,\n",
       "   15.16689395904541,\n",
       "   14.829599380493164,\n",
       "   15.609830856323242,\n",
       "   15.940962791442871,\n",
       "   16.496252059936523,\n",
       "   16.500530242919922,\n",
       "   16.96906089782715,\n",
       "   16.55607032775879,\n",
       "   15.726316452026367,\n",
       "   16.399059295654297,\n",
       "   16.131942749023438,\n",
       "   15.293410301208496,\n",
       "   15.531126022338867,\n",
       "   15.33662223815918,\n",
       "   14.847171783447266,\n",
       "   14.422674179077148,\n",
       "   14.838427543640137,\n",
       "   15.342068672180176,\n",
       "   15.767439842224121,\n",
       "   15.625140190124512,\n",
       "   16.280292510986328,\n",
       "   16.01709747314453,\n",
       "   16.138822555541992,\n",
       "   16.234926223754883,\n",
       "   15.947857856750488,\n",
       "   15.874752044677734,\n",
       "   15.79250431060791,\n",
       "   14.807378768920898,\n",
       "   13.76469898223877,\n",
       "   14.638155937194824,\n",
       "   14.27504825592041,\n",
       "   14.548534393310547,\n",
       "   13.93563175201416,\n",
       "   15.030680656433105,\n",
       "   15.647747039794922,\n",
       "   16.289865493774414,\n",
       "   15.909777641296387,\n",
       "   15.469947814941406,\n",
       "   15.65372085571289,\n",
       "   16.64745330810547,\n",
       "   16.37737274169922,\n",
       "   16.533405303955078,\n",
       "   16.71613311767578,\n",
       "   17.023893356323242,\n",
       "   16.639545440673828,\n",
       "   16.44523811340332,\n",
       "   17.295671463012695,\n",
       "   17.420406341552734,\n",
       "   17.099693298339844,\n",
       "   17.154890060424805,\n",
       "   16.943296432495117,\n",
       "   16.581270217895508,\n",
       "   17.236909866333008,\n",
       "   17.046161651611328,\n",
       "   16.97589111328125,\n",
       "   16.64093589782715,\n",
       "   16.985523223876953,\n",
       "   16.19916343688965,\n",
       "   16.175827026367188,\n",
       "   15.50730037689209,\n",
       "   16.19186019897461,\n",
       "   16.841232299804688,\n",
       "   16.655244827270508,\n",
       "   15.692120552062988,\n",
       "   16.040613174438477,\n",
       "   15.557321548461914,\n",
       "   15.693285942077637,\n",
       "   15.951648712158203,\n",
       "   15.938896179199219,\n",
       "   16.664379119873047,\n",
       "   16.540332794189453,\n",
       "   15.867006301879883,\n",
       "   15.73355484008789,\n",
       "   15.578311920166016,\n",
       "   15.352612495422363,\n",
       "   14.937835693359375,\n",
       "   14.75342845916748,\n",
       "   14.026151657104492,\n",
       "   13.262128829956055,\n",
       "   13.074117660522461,\n",
       "   12.738607406616211,\n",
       "   11.930017471313477,\n",
       "   13.070456504821777,\n",
       "   13.249567031860352,\n",
       "   13.821511268615723,\n",
       "   14.058820724487305,\n",
       "   13.83236312866211,\n",
       "   15.196832656860352,\n",
       "   15.15820598602295,\n",
       "   15.116456031799316,\n",
       "   14.201667785644531,\n",
       "   13.99652099609375,\n",
       "   14.226862907409668,\n",
       "   14.354083061218262,\n",
       "   13.737380981445312,\n",
       "   13.668164253234863,\n",
       "   13.653712272644043,\n",
       "   13.896356582641602,\n",
       "   15.068467140197754,\n",
       "   14.429224967956543,\n",
       "   14.790042877197266,\n",
       "   14.548386573791504,\n",
       "   14.08257007598877,\n",
       "   13.701531410217285,\n",
       "   14.259044647216797,\n",
       "   14.111627578735352,\n",
       "   14.451873779296875,\n",
       "   15.536676406860352,\n",
       "   15.261590957641602,\n",
       "   15.257051467895508,\n",
       "   15.182039260864258,\n",
       "   15.432601928710938,\n",
       "   14.61602783203125,\n",
       "   14.248353004455566,\n",
       "   15.363868713378906,\n",
       "   15.443218231201172,\n",
       "   15.597007751464844,\n",
       "   15.529552459716797,\n",
       "   16.05765151977539,\n",
       "   16.355457305908203,\n",
       "   16.341093063354492,\n",
       "   16.7088680267334,\n",
       "   16.582386016845703,\n",
       "   16.222814559936523,\n",
       "   16.164649963378906,\n",
       "   16.473913192749023,\n",
       "   16.5380859375,\n",
       "   16.189945220947266,\n",
       "   15.608498573303223,\n",
       "   16.001468658447266,\n",
       "   15.516814231872559,\n",
       "   15.279478073120117,\n",
       "   15.229853630065918,\n",
       "   15.516406059265137,\n",
       "   14.783808708190918,\n",
       "   14.999204635620117,\n",
       "   15.509661674499512,\n",
       "   15.271684646606445,\n",
       "   15.364679336547852,\n",
       "   15.506765365600586,\n",
       "   14.82398796081543,\n",
       "   15.086889266967773,\n",
       "   15.427106857299805,\n",
       "   15.650062561035156,\n",
       "   15.555328369140625,\n",
       "   16.03007698059082,\n",
       "   16.008432388305664,\n",
       "   15.88304615020752,\n",
       "   16.189804077148438,\n",
       "   16.369529724121094,\n",
       "   16.51735496520996,\n",
       "   16.331783294677734,\n",
       "   16.645652770996094,\n",
       "   16.1807918548584,\n",
       "   17.201345443725586,\n",
       "   17.2627010345459,\n",
       "   17.642684936523438,\n",
       "   17.43402862548828,\n",
       "   17.15115737915039,\n",
       "   16.734731674194336,\n",
       "   16.20591926574707,\n",
       "   15.704605102539062,\n",
       "   16.36571502685547,\n",
       "   15.836530685424805,\n",
       "   15.761087417602539,\n",
       "   15.55760383605957,\n",
       "   16.118610382080078,\n",
       "   16.975461959838867,\n",
       "   16.906742095947266,\n",
       "   16.042396545410156,\n",
       "   16.271976470947266,\n",
       "   16.283802032470703,\n",
       "   15.130940437316895,\n",
       "   15.151039123535156,\n",
       "   15.446064949035645,\n",
       "   15.406438827514648,\n",
       "   16.67493438720703,\n",
       "   16.50370979309082,\n",
       "   16.845687866210938,\n",
       "   16.899385452270508,\n",
       "   16.94460105895996,\n",
       "   17.037841796875,\n",
       "   16.571609497070312,\n",
       "   16.508995056152344,\n",
       "   16.358430862426758,\n",
       "   16.2808895111084,\n",
       "   16.01483154296875,\n",
       "   15.143205642700195,\n",
       "   14.718877792358398,\n",
       "   15.262118339538574,\n",
       "   15.3591890335083,\n",
       "   14.701947212219238,\n",
       "   14.943700790405273,\n",
       "   15.151812553405762,\n",
       "   14.717531204223633,\n",
       "   14.693376541137695,\n",
       "   14.862422943115234,\n",
       "   14.17215633392334,\n",
       "   14.616474151611328,\n",
       "   15.40405559539795,\n",
       "   15.286734580993652,\n",
       "   15.387776374816895,\n",
       "   15.061744689941406,\n",
       "   15.593537330627441,\n",
       "   15.372074127197266,\n",
       "   15.912466049194336,\n",
       "   16.170724868774414,\n",
       "   15.867167472839355,\n",
       "   16.20187759399414,\n",
       "   15.123804092407227,\n",
       "   15.42381477355957,\n",
       "   15.713292121887207,\n",
       "   15.046117782592773,\n",
       "   14.790834426879883,\n",
       "   15.50864028930664,\n",
       "   14.914751052856445,\n",
       "   13.900591850280762,\n",
       "   13.826367378234863,\n",
       "   13.533682823181152,\n",
       "   13.773423194885254,\n",
       "   14.424936294555664,\n",
       "   13.83242416381836,\n",
       "   13.766956329345703,\n",
       "   13.847233772277832,\n",
       "   13.105876922607422,\n",
       "   13.523444175720215,\n",
       "   13.089763641357422,\n",
       "   13.316588401794434,\n",
       "   14.2181978225708,\n",
       "   14.970715522766113,\n",
       "   14.431962013244629,\n",
       "   15.341764450073242,\n",
       "   15.384225845336914,\n",
       "   14.995277404785156,\n",
       "   14.776084899902344,\n",
       "   13.689501762390137,\n",
       "   13.308103561401367,\n",
       "   13.887736320495605,\n",
       "   12.7028169631958,\n",
       "   13.80339527130127,\n",
       "   13.636019706726074,\n",
       "   13.824978828430176,\n",
       "   14.34047794342041,\n",
       "   15.024561882019043,\n",
       "   14.92428207397461,\n",
       "   14.733335494995117,\n",
       "   15.627164840698242,\n",
       "   16.356996536254883,\n",
       "   15.6857328414917,\n",
       "   16.05121421813965,\n",
       "   14.984016418457031,\n",
       "   15.044135093688965,\n",
       "   15.730303764343262,\n",
       "   15.587313652038574,\n",
       "   15.525467872619629,\n",
       "   15.482352256774902,\n",
       "   15.491840362548828,\n",
       "   16.629859924316406,\n",
       "   16.438095092773438,\n",
       "   16.305557250976562,\n",
       "   15.546292304992676,\n",
       "   15.738248825073242,\n",
       "   15.091863632202148,\n",
       "   15.869714736938477,\n",
       "   15.927555084228516,\n",
       "   15.222765922546387,\n",
       "   14.801817893981934,\n",
       "   14.452261924743652,\n",
       "   15.463602066040039,\n",
       "   14.638884544372559,\n",
       "   14.374235153198242,\n",
       "   14.541228294372559,\n",
       "   14.603188514709473,\n",
       "   15.438851356506348,\n",
       "   15.766853332519531,\n",
       "   15.237578392028809,\n",
       "   15.727252960205078,\n",
       "   15.411565780639648,\n",
       "   15.189212799072266,\n",
       "   15.935598373413086,\n",
       "   16.424110412597656,\n",
       "   16.1724853515625,\n",
       "   16.102357864379883,\n",
       "   16.524993896484375,\n",
       "   16.136442184448242,\n",
       "   16.27296257019043,\n",
       "   15.738720893859863,\n",
       "   15.867132186889648,\n",
       "   16.176408767700195,\n",
       "   15.682201385498047,\n",
       "   16.228023529052734,\n",
       "   16.15360450744629,\n",
       "   16.77464485168457,\n",
       "   16.452224731445312,\n",
       "   15.866494178771973,\n",
       "   15.86031723022461,\n",
       "   15.734436988830566,\n",
       "   15.896248817443848,\n",
       "   16.100688934326172,\n",
       "   16.28573226928711,\n",
       "   16.046289443969727,\n",
       "   16.160297393798828,\n",
       "   16.543004989624023,\n",
       "   15.784194946289062,\n",
       "   15.661531448364258,\n",
       "   14.90065860748291,\n",
       "   13.506144523620605,\n",
       "   14.210997581481934,\n",
       "   13.853466033935547,\n",
       "   14.471175193786621,\n",
       "   14.354151725769043,\n",
       "   14.892777442932129,\n",
       "   15.149295806884766,\n",
       "   15.179567337036133,\n",
       "   14.477354049682617,\n",
       "   14.042804718017578,\n",
       "   14.159175872802734,\n",
       "   15.153543472290039,\n",
       "   14.496207237243652,\n",
       "   15.283266067504883,\n",
       "   15.900167465209961,\n",
       "   14.940511703491211,\n",
       "   15.227301597595215,\n",
       "   14.943634033203125,\n",
       "   14.972116470336914,\n",
       "   15.11109447479248,\n",
       "   15.301937103271484,\n",
       "   15.145709037780762,\n",
       "   14.85015869140625,\n",
       "   15.21943187713623,\n",
       "   15.302217483520508,\n",
       "   14.397811889648438,\n",
       "   15.177353858947754,\n",
       "   15.228409767150879,\n",
       "   14.820104598999023,\n",
       "   14.94714641571045,\n",
       "   14.448129653930664,\n",
       "   14.430244445800781,\n",
       "   14.21467399597168,\n",
       "   14.505204200744629,\n",
       "   13.722655296325684,\n",
       "   13.471841812133789,\n",
       "   13.805474281311035,\n",
       "   14.825998306274414,\n",
       "   14.427663803100586,\n",
       "   14.697305679321289,\n",
       "   14.423589706420898,\n",
       "   14.34634017944336,\n",
       "   14.934857368469238,\n",
       "   15.033056259155273,\n",
       "   15.740182876586914,\n",
       "   15.814946174621582,\n",
       "   15.334847450256348,\n",
       "   15.183008193969727,\n",
       "   15.095928192138672,\n",
       "   15.404558181762695,\n",
       "   15.06042194366455,\n",
       "   15.298138618469238,\n",
       "   14.73110294342041,\n",
       "   15.662701606750488,\n",
       "   15.422415733337402,\n",
       "   15.357354164123535,\n",
       "   15.847912788391113,\n",
       "   15.407669067382812,\n",
       "   15.725713729858398,\n",
       "   15.809524536132812,\n",
       "   16.097553253173828,\n",
       "   17.317138671875,\n",
       "   16.89059066772461,\n",
       "   16.123899459838867,\n",
       "   16.737239837646484,\n",
       "   16.98699378967285,\n",
       "   17.477890014648438,\n",
       "   17.99761199951172,\n",
       "   17.640052795410156,\n",
       "   16.679277420043945,\n",
       "   16.358686447143555,\n",
       "   15.947288513183594,\n",
       "   16.10162925720215,\n",
       "   16.082826614379883,\n",
       "   16.944984436035156,\n",
       "   16.970720291137695,\n",
       "   17.26704216003418,\n",
       "   16.146459579467773,\n",
       "   15.822129249572754,\n",
       "   16.304115295410156,\n",
       "   16.05457305908203,\n",
       "   15.471256256103516,\n",
       "   15.58664321899414,\n",
       "   15.413681030273438,\n",
       "   14.59915828704834,\n",
       "   15.246784210205078,\n",
       "   14.65191650390625,\n",
       "   15.40865707397461,\n",
       "   15.428845405578613,\n",
       "   14.382007598876953,\n",
       "   15.263748168945312,\n",
       "   15.89034652709961,\n",
       "   15.380154609680176,\n",
       "   15.323623657226562,\n",
       "   14.543994903564453,\n",
       "   14.151904106140137,\n",
       "   14.182683944702148,\n",
       "   14.293699264526367,\n",
       "   15.335074424743652,\n",
       "   15.445905685424805,\n",
       "   15.140785217285156,\n",
       "   14.931386947631836,\n",
       "   14.54005241394043,\n",
       "   14.04610824584961,\n",
       "   13.906227111816406,\n",
       "   13.176705360412598,\n",
       "   14.037163734436035,\n",
       "   14.951333045959473,\n",
       "   15.152931213378906,\n",
       "   15.663662910461426,\n",
       "   15.823592185974121,\n",
       "   15.743239402770996,\n",
       "   16.018720626831055,\n",
       "   16.786893844604492,\n",
       "   16.30748748779297,\n",
       "   15.722441673278809,\n",
       "   15.506924629211426,\n",
       "   15.0758056640625,\n",
       "   14.739215850830078,\n",
       "   14.930194854736328,\n",
       "   15.247854232788086,\n",
       "   15.393349647521973,\n",
       "   14.510194778442383,\n",
       "   13.920622825622559,\n",
       "   13.994728088378906,\n",
       "   13.683950424194336,\n",
       "   13.477418899536133,\n",
       "   12.738127708435059],\n",
       "  'test_accs': [366.416259765625,\n",
       "   355.39615631103516,\n",
       "   347.86339569091797,\n",
       "   345.55832290649414,\n",
       "   346.72878646850586,\n",
       "   342.8325309753418,\n",
       "   335.34568786621094,\n",
       "   322.8423194885254,\n",
       "   307.23986434936523,\n",
       "   286.8911933898926,\n",
       "   267.57189559936523,\n",
       "   259.8923225402832,\n",
       "   245.36664581298828,\n",
       "   234.2892608642578,\n",
       "   229.4044532775879,\n",
       "   228.9067153930664,\n",
       "   221.72452926635742,\n",
       "   220.20791816711426,\n",
       "   221.79370498657227,\n",
       "   221.51906967163086,\n",
       "   218.11656379699707,\n",
       "   217.32649993896484,\n",
       "   212.2749729156494,\n",
       "   211.0667018890381,\n",
       "   207.28665924072266,\n",
       "   210.163480758667,\n",
       "   210.048002243042,\n",
       "   207.34939765930176,\n",
       "   203.95611000061035,\n",
       "   205.39640522003174,\n",
       "   207.33447265625,\n",
       "   209.43213081359863,\n",
       "   208.61122512817383,\n",
       "   207.96864795684814,\n",
       "   210.67219829559326,\n",
       "   209.57858848571777,\n",
       "   209.13706302642822,\n",
       "   209.51859760284424,\n",
       "   211.40905570983887,\n",
       "   212.4473762512207,\n",
       "   213.7767276763916,\n",
       "   210.84323692321777,\n",
       "   209.52958583831787,\n",
       "   212.46384239196777,\n",
       "   210.2266788482666,\n",
       "   209.59694004058838,\n",
       "   207.81709098815918,\n",
       "   208.05004501342773,\n",
       "   209.09402561187744,\n",
       "   210.1161880493164,\n",
       "   210.2686014175415,\n",
       "   210.03274059295654,\n",
       "   211.85919570922852,\n",
       "   210.8157205581665,\n",
       "   210.70888137817383,\n",
       "   211.47748374938965,\n",
       "   209.77441501617432,\n",
       "   210.39390087127686,\n",
       "   208.70970344543457,\n",
       "   208.60751819610596,\n",
       "   209.22419929504395,\n",
       "   209.87464427947998,\n",
       "   210.2832851409912,\n",
       "   207.696044921875,\n",
       "   203.59876537322998,\n",
       "   204.47118091583252,\n",
       "   202.25227069854736,\n",
       "   200.8984775543213,\n",
       "   203.3818359375,\n",
       "   204.79358005523682,\n",
       "   207.82854557037354,\n",
       "   209.10155868530273,\n",
       "   208.26113891601562,\n",
       "   205.38770389556885,\n",
       "   203.8910150527954,\n",
       "   207.13403511047363,\n",
       "   207.59981060028076,\n",
       "   208.28282642364502,\n",
       "   208.55647468566895,\n",
       "   209.42073726654053,\n",
       "   209.61417198181152,\n",
       "   208.31797409057617,\n",
       "   208.16909408569336,\n",
       "   207.84307289123535,\n",
       "   207.9478349685669,\n",
       "   206.97721481323242,\n",
       "   207.27306461334229,\n",
       "   207.39947891235352,\n",
       "   207.14062976837158,\n",
       "   208.1457862854004,\n",
       "   209.6238489151001,\n",
       "   208.83857917785645,\n",
       "   205.4542112350464,\n",
       "   206.56285953521729,\n",
       "   203.16797828674316,\n",
       "   204.99731922149658,\n",
       "   205.71580505371094,\n",
       "   205.58896446228027,\n",
       "   202.6904468536377,\n",
       "   204.43745136260986,\n",
       "   205.41945266723633,\n",
       "   207.1516819000244,\n",
       "   209.526873588562,\n",
       "   209.22024154663086,\n",
       "   208.868989944458,\n",
       "   211.4306812286377,\n",
       "   210.0762424468994,\n",
       "   212.3217430114746,\n",
       "   212.75232028961182,\n",
       "   211.20621490478516,\n",
       "   212.30172443389893,\n",
       "   211.75456619262695,\n",
       "   212.10594081878662,\n",
       "   214.65402126312256,\n",
       "   214.27724838256836,\n",
       "   214.50799655914307,\n",
       "   212.23956298828125,\n",
       "   212.47104263305664,\n",
       "   213.25490474700928,\n",
       "   210.92572593688965,\n",
       "   209.01723289489746,\n",
       "   210.30859756469727,\n",
       "   209.63611316680908,\n",
       "   208.61089897155762,\n",
       "   207.7709617614746,\n",
       "   203.4368314743042,\n",
       "   204.16226959228516,\n",
       "   203.11826610565186,\n",
       "   202.66033554077148,\n",
       "   204.67381286621094,\n",
       "   206.10756874084473,\n",
       "   206.64652824401855,\n",
       "   209.8158311843872,\n",
       "   209.13398551940918,\n",
       "   210.54231929779053,\n",
       "   211.08024406433105,\n",
       "   212.9466381072998,\n",
       "   211.7185983657837,\n",
       "   212.01550388336182,\n",
       "   211.41645431518555,\n",
       "   212.86187362670898,\n",
       "   212.52571392059326,\n",
       "   211.78288745880127,\n",
       "   208.69880390167236,\n",
       "   209.70209980010986,\n",
       "   206.41698551177979,\n",
       "   206.86859512329102,\n",
       "   206.75944900512695,\n",
       "   206.93199157714844,\n",
       "   209.02818202972412,\n",
       "   210.23052501678467,\n",
       "   210.57903861999512,\n",
       "   210.17324256896973,\n",
       "   210.2541332244873,\n",
       "   211.49108028411865,\n",
       "   212.89741039276123,\n",
       "   213.0927219390869,\n",
       "   214.6316623687744,\n",
       "   216.26408863067627,\n",
       "   215.03050708770752,\n",
       "   213.22114372253418,\n",
       "   214.6878900527954,\n",
       "   214.863507270813,\n",
       "   214.53606033325195,\n",
       "   214.73072910308838,\n",
       "   212.66762924194336,\n",
       "   212.9102268218994,\n",
       "   211.94039630889893,\n",
       "   212.409273147583,\n",
       "   212.931321144104,\n",
       "   214.07827186584473,\n",
       "   213.7286033630371,\n",
       "   218.8785514831543,\n",
       "   219.4712619781494,\n",
       "   219.92396926879883,\n",
       "   217.23846054077148,\n",
       "   212.7053394317627,\n",
       "   214.67420959472656,\n",
       "   212.31348419189453,\n",
       "   212.7432222366333,\n",
       "   212.96239471435547,\n",
       "   213.7510757446289,\n",
       "   213.40714836120605,\n",
       "   212.74795722961426,\n",
       "   211.9264793395996,\n",
       "   212.51632404327393,\n",
       "   212.18681621551514,\n",
       "   212.63181686401367,\n",
       "   213.5699338912964,\n",
       "   213.45395851135254,\n",
       "   209.7765884399414,\n",
       "   211.91985893249512,\n",
       "   211.7992458343506,\n",
       "   210.31243324279785,\n",
       "   208.691330909729,\n",
       "   209.85983180999756,\n",
       "   209.74169635772705,\n",
       "   204.81700611114502,\n",
       "   204.8759355545044,\n",
       "   204.97961139678955,\n",
       "   206.56224060058594,\n",
       "   208.64061546325684,\n",
       "   209.3184518814087,\n",
       "   206.8388671875,\n",
       "   207.40767192840576,\n",
       "   208.78231716156006,\n",
       "   208.2499122619629,\n",
       "   210.3108377456665,\n",
       "   210.32793617248535,\n",
       "   210.02762413024902,\n",
       "   210.8897819519043,\n",
       "   210.75375366210938,\n",
       "   206.95641899108887,\n",
       "   206.48822593688965,\n",
       "   209.47560214996338,\n",
       "   210.40871238708496,\n",
       "   211.53563499450684,\n",
       "   210.55326461791992,\n",
       "   212.6508903503418,\n",
       "   210.24409770965576,\n",
       "   214.25628852844238,\n",
       "   214.2890863418579,\n",
       "   213.73702335357666,\n",
       "   213.44012641906738,\n",
       "   214.14050674438477,\n",
       "   215.15180206298828,\n",
       "   216.51480674743652,\n",
       "   216.13633728027344,\n",
       "   216.47788047790527,\n",
       "   215.2594757080078,\n",
       "   213.87117958068848,\n",
       "   213.97377586364746,\n",
       "   211.92327880859375,\n",
       "   212.25854682922363,\n",
       "   209.09111881256104,\n",
       "   206.9795799255371,\n",
       "   208.17116832733154,\n",
       "   208.24215698242188,\n",
       "   207.60042667388916,\n",
       "   208.50638008117676,\n",
       "   209.6632595062256,\n",
       "   207.8908863067627,\n",
       "   210.6982069015503,\n",
       "   211.68383312225342,\n",
       "   213.02276420593262,\n",
       "   214.15929126739502,\n",
       "   212.5856761932373,\n",
       "   210.0598602294922,\n",
       "   212.58153438568115,\n",
       "   211.52661895751953,\n",
       "   212.9354944229126,\n",
       "   213.38610649108887,\n",
       "   214.53151607513428,\n",
       "   210.83543014526367,\n",
       "   210.32067584991455,\n",
       "   210.1471996307373,\n",
       "   209.91425037384033,\n",
       "   209.4064130783081,\n",
       "   206.11468315124512,\n",
       "   207.95290088653564,\n",
       "   207.04315185546875,\n",
       "   205.50463008880615,\n",
       "   206.5623722076416,\n",
       "   205.43150234222412,\n",
       "   206.75567817687988,\n",
       "   208.45388317108154,\n",
       "   208.9670705795288,\n",
       "   208.80016326904297,\n",
       "   209.44524574279785,\n",
       "   210.52388095855713,\n",
       "   211.05879974365234,\n",
       "   209.74498176574707,\n",
       "   209.26282501220703,\n",
       "   208.53859424591064,\n",
       "   208.57855987548828,\n",
       "   207.7955198287964,\n",
       "   207.91892051696777,\n",
       "   207.4992275238037,\n",
       "   208.3169460296631,\n",
       "   208.65393829345703,\n",
       "   208.1389980316162,\n",
       "   208.56177043914795,\n",
       "   208.03389358520508,\n",
       "   208.4804220199585,\n",
       "   207.79476261138916,\n",
       "   206.03148937225342,\n",
       "   207.82769775390625,\n",
       "   206.36790466308594,\n",
       "   205.81590175628662,\n",
       "   205.90849018096924,\n",
       "   205.13080310821533,\n",
       "   203.93490028381348,\n",
       "   204.09436321258545,\n",
       "   203.46331977844238,\n",
       "   203.97549438476562,\n",
       "   205.86162853240967,\n",
       "   205.5247402191162,\n",
       "   205.6492805480957,\n",
       "   206.34117317199707,\n",
       "   206.2466173171997,\n",
       "   205.68662929534912,\n",
       "   203.522780418396,\n",
       "   203.15733432769775,\n",
       "   206.20638179779053,\n",
       "   203.46162128448486,\n",
       "   204.89843463897705,\n",
       "   205.06708335876465,\n",
       "   205.1865940093994,\n",
       "   206.6272096633911,\n",
       "   207.11099529266357,\n",
       "   208.6429214477539,\n",
       "   207.87839698791504,\n",
       "   209.11562156677246,\n",
       "   209.78777885437012,\n",
       "   210.03088569641113,\n",
       "   213.489972114563,\n",
       "   212.81634044647217,\n",
       "   212.58237743377686,\n",
       "   210.89152145385742,\n",
       "   212.72114753723145,\n",
       "   211.82021141052246,\n",
       "   210.87434196472168,\n",
       "   209.36537075042725,\n",
       "   211.0915012359619,\n",
       "   211.1975793838501,\n",
       "   212.6157922744751,\n",
       "   212.00124645233154,\n",
       "   209.3886365890503,\n",
       "   209.3103141784668,\n",
       "   209.45117282867432,\n",
       "   209.97493267059326,\n",
       "   209.84809589385986,\n",
       "   208.36536502838135,\n",
       "   207.87190914154053,\n",
       "   208.0082130432129,\n",
       "   207.39700412750244,\n",
       "   206.8190803527832,\n",
       "   207.6167917251587,\n",
       "   209.40363883972168,\n",
       "   210.80041122436523,\n",
       "   211.96664237976074,\n",
       "   210.12054920196533,\n",
       "   211.914794921875,\n",
       "   210.90410995483398,\n",
       "   211.549898147583,\n",
       "   212.21743392944336,\n",
       "   211.65217781066895,\n",
       "   209.63389778137207,\n",
       "   210.51122760772705,\n",
       "   211.03450202941895,\n",
       "   210.44835662841797,\n",
       "   208.52216148376465,\n",
       "   208.4255714416504,\n",
       "   208.96505737304688,\n",
       "   211.27954959869385,\n",
       "   209.82140827178955,\n",
       "   211.3761749267578,\n",
       "   212.681866645813,\n",
       "   212.69842720031738,\n",
       "   211.79107570648193,\n",
       "   210.8656530380249,\n",
       "   209.92214012145996,\n",
       "   209.1818332672119,\n",
       "   207.27851009368896,\n",
       "   207.97784423828125,\n",
       "   207.15004062652588,\n",
       "   207.1316032409668,\n",
       "   205.5166597366333,\n",
       "   205.74756240844727,\n",
       "   201.96103763580322,\n",
       "   203.23109817504883,\n",
       "   204.2813024520874,\n",
       "   205.62882614135742,\n",
       "   206.4220790863037,\n",
       "   207.1475486755371,\n",
       "   209.7505340576172,\n",
       "   210.26244735717773,\n",
       "   210.86788845062256,\n",
       "   211.44954776763916,\n",
       "   210.84619522094727,\n",
       "   208.89292335510254,\n",
       "   209.0959129333496,\n",
       "   208.845552444458,\n",
       "   209.2650842666626,\n",
       "   206.40390300750732,\n",
       "   209.2696714401245,\n",
       "   210.40897846221924,\n",
       "   209.6347360610962,\n",
       "   209.0853509902954,\n",
       "   208.49505233764648,\n",
       "   210.9388666152954,\n",
       "   208.7684497833252,\n",
       "   209.14341831207275,\n",
       "   207.1107759475708,\n",
       "   206.46363162994385,\n",
       "   207.56722736358643,\n",
       "   206.88154697418213,\n",
       "   205.25435829162598,\n",
       "   205.77945137023926,\n",
       "   205.84626579284668,\n",
       "   206.78486824035645,\n",
       "   206.38944244384766,\n",
       "   205.87708282470703,\n",
       "   207.76342010498047,\n",
       "   207.265944480896,\n",
       "   206.12329769134521,\n",
       "   203.3100070953369,\n",
       "   204.7400779724121,\n",
       "   204.35754299163818,\n",
       "   207.14416599273682,\n",
       "   204.1751127243042,\n",
       "   205.47139930725098,\n",
       "   205.45349597930908,\n",
       "   205.25299644470215,\n",
       "   207.7588005065918,\n",
       "   208.06295490264893,\n",
       "   210.01992511749268,\n",
       "   209.37499713897705,\n",
       "   209.74896049499512,\n",
       "   208.17080116271973,\n",
       "   209.40014266967773,\n",
       "   212.40654850006104,\n",
       "   211.27353858947754,\n",
       "   214.07460689544678,\n",
       "   212.0340061187744,\n",
       "   216.67408847808838,\n",
       "   216.42604160308838,\n",
       "   214.8150577545166,\n",
       "   215.58442401885986,\n",
       "   215.76304244995117,\n",
       "   215.57952499389648,\n",
       "   217.045804977417,\n",
       "   216.41699314117432,\n",
       "   219.25600624084473,\n",
       "   220.03589630126953,\n",
       "   218.01001358032227,\n",
       "   219.46964645385742,\n",
       "   217.74233627319336,\n",
       "   215.75240325927734,\n",
       "   216.64171600341797,\n",
       "   215.18197059631348,\n",
       "   215.6940746307373,\n",
       "   218.3078956604004,\n",
       "   218.35383224487305,\n",
       "   216.58557891845703,\n",
       "   215.99108982086182,\n",
       "   214.93593883514404,\n",
       "   214.04480743408203,\n",
       "   214.10742855072021,\n",
       "   214.0873498916626,\n",
       "   211.3913173675537,\n",
       "   213.6115436553955,\n",
       "   216.63968086242676,\n",
       "   214.74209213256836,\n",
       "   216.98097038269043,\n",
       "   217.09273529052734,\n",
       "   216.9179563522339,\n",
       "   215.52289199829102,\n",
       "   211.52535820007324,\n",
       "   213.28849601745605,\n",
       "   213.78478908538818,\n",
       "   211.4182014465332,\n",
       "   213.2887783050537,\n",
       "   212.80228805541992,\n",
       "   212.7855100631714,\n",
       "   212.91880798339844,\n",
       "   210.48094177246094,\n",
       "   208.75698375701904,\n",
       "   207.34196376800537,\n",
       "   207.89156818389893,\n",
       "   206.39885902404785,\n",
       "   205.328782081604,\n",
       "   205.35705471038818,\n",
       "   206.78380393981934,\n",
       "   206.26688861846924,\n",
       "   205.86547756195068,\n",
       "   205.12557125091553,\n",
       "   204.55270385742188,\n",
       "   206.37117671966553,\n",
       "   207.73247623443604,\n",
       "   207.40359020233154,\n",
       "   209.13152980804443,\n",
       "   208.7529411315918,\n",
       "   207.05421447753906,\n",
       "   208.58221626281738,\n",
       "   208.9994831085205,\n",
       "   213.04792976379395,\n",
       "   210.47489166259766,\n",
       "   210.50700664520264,\n",
       "   208.44921493530273,\n",
       "   208.18269443511963,\n",
       "   210.38403606414795,\n",
       "   209.9037790298462,\n",
       "   211.13976192474365,\n",
       "   212.63207817077637,\n",
       "   211.80321788787842,\n",
       "   212.46942901611328,\n",
       "   211.19170951843262,\n",
       "   211.87039947509766,\n",
       "   209.8600034713745],\n",
       "  'pce_r2': [-0.6567223081281532,\n",
       "   -0.3070636536993492,\n",
       "   -0.1648042088362378,\n",
       "   -0.21156806859976074,\n",
       "   -0.4452437112119765,\n",
       "   -0.38821853969661424,\n",
       "   -0.5288743800070672,\n",
       "   -0.6331235938791893,\n",
       "   -0.8670764327105929,\n",
       "   -1.1248669398343378,\n",
       "   -1.1246303456901803,\n",
       "   -1.9424527886280623,\n",
       "   -1.992822408583102,\n",
       "   -2.3006725667296983,\n",
       "   -2.8869951339622504,\n",
       "   -5.178027143125591,\n",
       "   -6.271535895024499,\n",
       "   -9.565760788643885,\n",
       "   -10.17129663504289,\n",
       "   -9.476680359150835,\n",
       "   -10.504151521836285,\n",
       "   -10.845716774260879,\n",
       "   -12.48267632413034,\n",
       "   -11.71613280927126,\n",
       "   -11.13780917090879,\n",
       "   -11.46291675015529,\n",
       "   -13.223132963725545,\n",
       "   -12.968826673618429,\n",
       "   -15.421129020565715,\n",
       "   -15.18105078103478,\n",
       "   -14.45826882070832,\n",
       "   -16.16091453058625,\n",
       "   -16.727401029156866,\n",
       "   -18.008633400668618,\n",
       "   -18.478456702561957,\n",
       "   -18.797096415942594,\n",
       "   -19.05270805829197,\n",
       "   -17.592249152366588,\n",
       "   -17.78823577353116,\n",
       "   -16.960547457455483,\n",
       "   -18.400120622465423,\n",
       "   -17.671117729365104,\n",
       "   -16.370268186979295,\n",
       "   -17.539025172758517,\n",
       "   -15.011561019170685,\n",
       "   -15.561358011797655,\n",
       "   -13.819289391410992,\n",
       "   -15.670972322530844,\n",
       "   -16.14976649658622,\n",
       "   -18.43130756911774,\n",
       "   -18.230338139343843,\n",
       "   -17.3291897103069,\n",
       "   -19.779194850044107,\n",
       "   -18.56048261551283,\n",
       "   -18.47470569631075,\n",
       "   -18.307923093050654,\n",
       "   -19.016391522872166,\n",
       "   -18.957190828005967,\n",
       "   -16.975954105606295,\n",
       "   -19.41944620632164,\n",
       "   -19.885820386261997,\n",
       "   -19.546925588353254,\n",
       "   -20.514698253788072,\n",
       "   -20.0499961098003,\n",
       "   -18.218050398078116,\n",
       "   -18.693645858727017,\n",
       "   -18.773516034796113,\n",
       "   -16.281678185683933,\n",
       "   -16.9148937806671,\n",
       "   -16.701099943479736,\n",
       "   -19.01081349312723,\n",
       "   -19.114417438627555,\n",
       "   -17.152353416293316,\n",
       "   -16.62550654238603,\n",
       "   -15.713535410504555,\n",
       "   -18.065250784795175,\n",
       "   -17.871121824226027,\n",
       "   -17.13610276114599,\n",
       "   -15.876662916549186,\n",
       "   -16.129176849371024,\n",
       "   -15.25262776036799,\n",
       "   -15.521400993798995,\n",
       "   -15.321505725924407,\n",
       "   -14.82787116930232,\n",
       "   -15.651730853310884,\n",
       "   -16.37150815575789,\n",
       "   -15.387641564380633,\n",
       "   -14.411961690773582,\n",
       "   -13.978875262559884,\n",
       "   -14.607969886564167,\n",
       "   -17.370937389708583,\n",
       "   -16.116486431410998,\n",
       "   -14.132100626080316,\n",
       "   -13.582188899543391,\n",
       "   -13.969211770983897,\n",
       "   -16.59842754403145,\n",
       "   -14.506641060714259,\n",
       "   -15.735262283039702,\n",
       "   -13.791077973658641,\n",
       "   -13.890870797306526,\n",
       "   -13.991381343586731,\n",
       "   -15.042722649101087,\n",
       "   -18.259400441734304,\n",
       "   -18.60762957189697,\n",
       "   -17.754066330663587,\n",
       "   -16.614618122132672,\n",
       "   -16.15914932955265,\n",
       "   -17.365005028801402,\n",
       "   -17.563997009380397,\n",
       "   -18.282971869893906,\n",
       "   -19.16053458248995,\n",
       "   -19.348025456050777,\n",
       "   -18.835696432771215,\n",
       "   -20.79340215553011,\n",
       "   -19.600612145727993,\n",
       "   -18.491442298169463,\n",
       "   -16.234396503147302,\n",
       "   -16.07880050038132,\n",
       "   -17.1637236082641,\n",
       "   -16.806867485250965,\n",
       "   -16.322988374681763,\n",
       "   -19.524295217007733,\n",
       "   -19.273886059368063,\n",
       "   -19.298592232483383,\n",
       "   -18.12528648212763,\n",
       "   -14.64722970232499,\n",
       "   -13.196605377823586,\n",
       "   -12.078576270737932,\n",
       "   -12.536433782564714,\n",
       "   -13.556320749542525,\n",
       "   -14.14150871588054,\n",
       "   -14.833329586462545,\n",
       "   -15.58797737900969,\n",
       "   -15.86859925758467,\n",
       "   -15.093893339700813,\n",
       "   -13.918050844741098,\n",
       "   -13.856387793562552,\n",
       "   -18.148643647984382,\n",
       "   -18.69727583190494,\n",
       "   -17.628612106339336,\n",
       "   -20.10263178414877,\n",
       "   -17.560977676536815,\n",
       "   -17.45882393113826,\n",
       "   -17.777437041959942,\n",
       "   -17.58943831330798,\n",
       "   -16.539042747217007,\n",
       "   -17.07719220580552,\n",
       "   -17.14664667413714,\n",
       "   -15.475301590345541,\n",
       "   -16.501449809490694,\n",
       "   -14.184650595666849,\n",
       "   -15.703603682246442,\n",
       "   -15.508118188388732,\n",
       "   -13.745461466687974,\n",
       "   -14.667750457127331,\n",
       "   -14.951315730221978,\n",
       "   -15.362614153032993,\n",
       "   -17.99975826039155,\n",
       "   -18.900176551901247,\n",
       "   -17.903758067250962,\n",
       "   -18.501539674502624,\n",
       "   -21.98753450638486,\n",
       "   -21.71490828589022,\n",
       "   -23.15928388776416,\n",
       "   -22.752335678546476,\n",
       "   -23.19712838155055,\n",
       "   -21.700088423392415,\n",
       "   -20.18639381507537,\n",
       "   -20.647667029602122,\n",
       "   -21.138752020474435,\n",
       "   -21.913660231221893,\n",
       "   -19.224258103513932,\n",
       "   -20.951840968691723,\n",
       "   -19.295099384537867,\n",
       "   -19.911004148984926,\n",
       "   -17.302741508742937,\n",
       "   -14.106469123171719,\n",
       "   -15.99078325199931,\n",
       "   -14.503331206409712,\n",
       "   -15.733543276879772,\n",
       "   -16.396363642345452,\n",
       "   -17.442883523192936,\n",
       "   -15.609984475834938,\n",
       "   -15.613345070654443,\n",
       "   -15.562490602699775,\n",
       "   -16.54451181272224,\n",
       "   -17.17348316239002,\n",
       "   -16.567526394618213,\n",
       "   -17.557195685400114,\n",
       "   -18.58541147678298,\n",
       "   -17.386887678002854,\n",
       "   -17.14069476346763,\n",
       "   -18.820170737128752,\n",
       "   -16.387930590673,\n",
       "   -15.84883519037206,\n",
       "   -14.177235359774683,\n",
       "   -14.581420084061113,\n",
       "   -12.807569444950557,\n",
       "   -13.25410039398307,\n",
       "   -13.394070572442962,\n",
       "   -14.667131860290652,\n",
       "   -15.049019168621076,\n",
       "   -14.851108926654218,\n",
       "   -14.132387838550944,\n",
       "   -14.614952438696745,\n",
       "   -16.751034760106855,\n",
       "   -16.21959487501126,\n",
       "   -17.35743104838963,\n",
       "   -17.80624535123355,\n",
       "   -17.246979669342164,\n",
       "   -17.72837124807404,\n",
       "   -14.542187027801255,\n",
       "   -12.897879583413168,\n",
       "   -13.734561061793066,\n",
       "   -14.936472464416164,\n",
       "   -14.762347921328093,\n",
       "   -17.103599621595038,\n",
       "   -14.277376795367749,\n",
       "   -15.236609701216459,\n",
       "   -14.841827885260365,\n",
       "   -17.614627310790148,\n",
       "   -18.59399115185656,\n",
       "   -20.3216520167334,\n",
       "   -22.385680156667846,\n",
       "   -20.353226260618044,\n",
       "   -20.92685464805265,\n",
       "   -19.890764364643662,\n",
       "   -17.721439155988495,\n",
       "   -19.77701177072638,\n",
       "   -20.291897468422214,\n",
       "   -19.55709658857736,\n",
       "   -20.270963254906697,\n",
       "   -18.770246957654443,\n",
       "   -19.969488280168832,\n",
       "   -17.68958000767045,\n",
       "   -16.16382668794719,\n",
       "   -16.874016474860436,\n",
       "   -16.656215239830132,\n",
       "   -17.829804592266576,\n",
       "   -18.764752916043644,\n",
       "   -19.415558764460755,\n",
       "   -17.800593343118976,\n",
       "   -18.92310921150255,\n",
       "   -21.14443508025387,\n",
       "   -20.279268399737735,\n",
       "   -22.82634257378315,\n",
       "   -22.45265249654631,\n",
       "   -19.099282445743995,\n",
       "   -21.559399088939717,\n",
       "   -20.941011417006536,\n",
       "   -22.302348658828855,\n",
       "   -22.83909676596191,\n",
       "   -23.330424805092846,\n",
       "   -19.514856294552853,\n",
       "   -19.528219006726815,\n",
       "   -19.85134641537848,\n",
       "   -19.042898588241734,\n",
       "   -19.045286620432716,\n",
       "   -15.913331211763172,\n",
       "   -16.979153216718082,\n",
       "   -16.111078262201275,\n",
       "   -15.534308475929727,\n",
       "   -18.594854610785866,\n",
       "   -21.422505107722728,\n",
       "   -19.829019113745638,\n",
       "   -20.225138075544002,\n",
       "   -20.657071170491133,\n",
       "   -20.3795118964433,\n",
       "   -21.685871498880267,\n",
       "   -19.71561269940376,\n",
       "   -19.60360043387313,\n",
       "   -17.025501911678496,\n",
       "   -16.45984883198209,\n",
       "   -17.21892313008004,\n",
       "   -16.020036684070348,\n",
       "   -17.90524746139488,\n",
       "   -16.018026543232914,\n",
       "   -14.710090214161335,\n",
       "   -14.816099610674758,\n",
       "   -14.897760687485016,\n",
       "   -14.956284160730206,\n",
       "   -16.72110178506565,\n",
       "   -17.72520921424866,\n",
       "   -18.88288256089182,\n",
       "   -18.347710253042404,\n",
       "   -15.527814342296846,\n",
       "   -15.235464735941129,\n",
       "   -15.213564192749566,\n",
       "   -14.299435771606115,\n",
       "   -13.23204586746846,\n",
       "   -14.553158750893887,\n",
       "   -14.39128760977495,\n",
       "   -15.765115596372699,\n",
       "   -16.930116526611812,\n",
       "   -15.058251352807257,\n",
       "   -14.870543895505593,\n",
       "   -14.478657338714848,\n",
       "   -14.703730702601506,\n",
       "   -15.299645286182304,\n",
       "   -15.578010666294059,\n",
       "   -13.890634265855326,\n",
       "   -14.056110795180684,\n",
       "   -13.045007524515265,\n",
       "   -16.123228364957107,\n",
       "   -16.721438386809123,\n",
       "   -16.621402521370428,\n",
       "   -16.001283560634185,\n",
       "   -15.24191020538871,\n",
       "   -16.833845944712273,\n",
       "   -17.052258177795974,\n",
       "   -19.12900855360463,\n",
       "   -18.79776690641468,\n",
       "   -18.105230174190005,\n",
       "   -18.045914936957928,\n",
       "   -18.599458445155555,\n",
       "   -19.25095068826177,\n",
       "   -19.795232040632165,\n",
       "   -20.11946750605126,\n",
       "   -17.672651178979034,\n",
       "   -17.637694670284883,\n",
       "   -16.607496022789764,\n",
       "   -17.52167242864597,\n",
       "   -16.081707456320157,\n",
       "   -17.176195386213497,\n",
       "   -17.651898451266888,\n",
       "   -20.669034961649448,\n",
       "   -23.354857085075345,\n",
       "   -21.88579650585553,\n",
       "   -22.94461890207135,\n",
       "   -23.207984885308914,\n",
       "   -22.992543751351647,\n",
       "   -21.788194987400498,\n",
       "   -19.904684378594585,\n",
       "   -20.03306804352138,\n",
       "   -20.9709512708603,\n",
       "   -20.61673461246175,\n",
       "   -23.447013815860384,\n",
       "   -23.065027374749892,\n",
       "   -22.247588356591237,\n",
       "   -21.709558261510285,\n",
       "   -20.569981944425763,\n",
       "   -21.940658236231112,\n",
       "   -19.43113534131981,\n",
       "   -18.26092120251371,\n",
       "   -19.840148354327884,\n",
       "   -17.997719485422984,\n",
       "   -18.427354924615017,\n",
       "   -16.244481916092518,\n",
       "   -17.270109311131097,\n",
       "   -17.642646499727533,\n",
       "   -17.55336345559893,\n",
       "   -15.567455182853955,\n",
       "   -17.429536448209184,\n",
       "   -19.238328053464897,\n",
       "   -20.272498338747955,\n",
       "   -19.231621420700158,\n",
       "   -19.42616668984991,\n",
       "   -20.633535193802615,\n",
       "   -21.501579508330686,\n",
       "   -21.23376608842368,\n",
       "   -20.619527761584617,\n",
       "   -19.73521305646335,\n",
       "   -18.46306447859442,\n",
       "   -16.260253202120413,\n",
       "   -17.199974520825688,\n",
       "   -14.69598216621841,\n",
       "   -15.741194023614447,\n",
       "   -16.160225480378145,\n",
       "   -16.32986686744638,\n",
       "   -14.864533559896877,\n",
       "   -15.386850698213689,\n",
       "   -14.538213733327293,\n",
       "   -16.295262288765123,\n",
       "   -15.694897597610911,\n",
       "   -17.378888726454356,\n",
       "   -16.531675023812955,\n",
       "   -16.579728091447613,\n",
       "   -16.582644497781445,\n",
       "   -17.233112376850197,\n",
       "   -17.668140279254455,\n",
       "   -16.59019220803941,\n",
       "   -17.414889246185133,\n",
       "   -16.824649457503913,\n",
       "   -17.19537425140998,\n",
       "   -15.909625348361168,\n",
       "   -18.538559491767195,\n",
       "   -18.934619734773808,\n",
       "   -18.813632855430175,\n",
       "   -17.652354705062304,\n",
       "   -17.95125809927639,\n",
       "   -19.49031623011644,\n",
       "   -18.720207642212436,\n",
       "   -18.67344710600289,\n",
       "   -15.925466107544032,\n",
       "   -14.532543421151889,\n",
       "   -15.01185503290942,\n",
       "   -15.099738608337475,\n",
       "   -15.333211715569252,\n",
       "   -14.734033390499421,\n",
       "   -15.216834231599982,\n",
       "   -16.207642850157168,\n",
       "   -15.473656308187913,\n",
       "   -15.95144848159467,\n",
       "   -17.39256641139039,\n",
       "   -17.10286511782567,\n",
       "   -14.94249525380086,\n",
       "   -14.503915247343057,\n",
       "   -15.14409526632323,\n",
       "   -13.301900016923492,\n",
       "   -14.38069104371143,\n",
       "   -13.308702220567772,\n",
       "   -12.536005363942952,\n",
       "   -15.870417395267523,\n",
       "   -14.560164048036658,\n",
       "   -16.98499523327925,\n",
       "   -16.846052729028525,\n",
       "   -17.990072694702864,\n",
       "   -16.979153649311296,\n",
       "   -18.770612941853514,\n",
       "   -19.0013987997089,\n",
       "   -21.392328723759306,\n",
       "   -23.328162856235398,\n",
       "   -21.73801279484209,\n",
       "   -22.53791206866946,\n",
       "   -20.44256519978769,\n",
       "   -22.695850215808033,\n",
       "   -27.112740395259213,\n",
       "   -27.765543695001618,\n",
       "   -25.393435851829793,\n",
       "   -24.77807705109126,\n",
       "   -23.831088641666362,\n",
       "   -26.76725244465298,\n",
       "   -25.917212807970177,\n",
       "   -23.362621538005047,\n",
       "   -22.77513624437301,\n",
       "   -21.26458386950718,\n",
       "   -23.434331945082832,\n",
       "   -21.267659906124496,\n",
       "   -19.39024280490726,\n",
       "   -18.071605312241715,\n",
       "   -18.030998231513816,\n",
       "   -19.248938426130184,\n",
       "   -20.303834808020472,\n",
       "   -20.11417903110488,\n",
       "   -22.959431734525214,\n",
       "   -24.825823629008404,\n",
       "   -20.204654232530658,\n",
       "   -19.810536032112353,\n",
       "   -20.049454520772944,\n",
       "   -21.447839198463708,\n",
       "   -18.048852666011058,\n",
       "   -19.724459200037906,\n",
       "   -23.210108711901693,\n",
       "   -20.414048147097393,\n",
       "   -22.24606902749735,\n",
       "   -19.269394482596475,\n",
       "   -19.09965729450115,\n",
       "   -18.12207721467018,\n",
       "   -15.429042363243557,\n",
       "   -16.963504321650802,\n",
       "   -17.862403257066134,\n",
       "   -14.94387628389907,\n",
       "   -15.666408109543159,\n",
       "   -16.072950035714044,\n",
       "   -15.899704169304883,\n",
       "   -16.063076437134928,\n",
       "   -15.161712413829218,\n",
       "   -15.969051155342793,\n",
       "   -15.706976467007252,\n",
       "   -19.392332285725775,\n",
       "   -18.149482376399,\n",
       "   -18.933019648306203,\n",
       "   -21.609089142841864,\n",
       "   -23.24089579162264,\n",
       "   -21.17974253157511,\n",
       "   -19.26084039121116,\n",
       "   -15.801787747400759,\n",
       "   -15.081776156759215,\n",
       "   -15.831634388891167,\n",
       "   -16.7737234030331,\n",
       "   -17.99114158062885,\n",
       "   -18.649160466242563,\n",
       "   -16.208304709053596,\n",
       "   -15.771442211869772,\n",
       "   -13.831023129350891,\n",
       "   -12.57262534299073,\n",
       "   -14.611331143990496,\n",
       "   -16.000215091258216,\n",
       "   -17.487347908402068,\n",
       "   -15.379602320281585,\n",
       "   -15.706950226846274,\n",
       "   -15.286048136871017,\n",
       "   -14.626580120352854,\n",
       "   -15.169158669712147,\n",
       "   -15.304015412972596,\n",
       "   -15.777866071742782,\n",
       "   -15.875425390063032,\n",
       "   -16.752193913277452,\n",
       "   -17.854980019390815,\n",
       "   -17.326003726508848],\n",
       "  'voc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -30118.875392416056,\n",
       "   -332.6780766840559,\n",
       "   -88.47595109091571,\n",
       "   -43.16959314972822,\n",
       "   -25.158000305804702,\n",
       "   -16.095438569860374,\n",
       "   -11.594013153494092,\n",
       "   -9.024989286135053,\n",
       "   -8.165180880572166,\n",
       "   -6.9579549516186585,\n",
       "   -5.409403643909004,\n",
       "   -4.695469356718428,\n",
       "   -4.250733721604158,\n",
       "   -3.636554094644028,\n",
       "   -3.6033734378682043,\n",
       "   -2.638794001180156,\n",
       "   -2.4057855451644907,\n",
       "   -1.7769793823560587,\n",
       "   -1.4776690689981504,\n",
       "   -1.3290353901310672,\n",
       "   -0.9787710110788612,\n",
       "   -0.6007927246853522,\n",
       "   -0.6897770352099131,\n",
       "   -0.7303146395773314,\n",
       "   -0.7575382482938062,\n",
       "   -0.6826496965655118,\n",
       "   -0.5473342810580006,\n",
       "   -0.7509610168847909,\n",
       "   -0.5001899989869769,\n",
       "   -0.4438643170854213,\n",
       "   -0.37159245155741205,\n",
       "   -0.44303829353460866,\n",
       "   -0.5096039377087513,\n",
       "   -0.3638499524473169,\n",
       "   -0.21311169506676308,\n",
       "   -0.17401957139571933,\n",
       "   -0.09351476866749375,\n",
       "   -0.16645192835494904,\n",
       "   -0.1782741126555114,\n",
       "   -0.17545584777600043,\n",
       "   -0.18706105213427615,\n",
       "   -0.11089972819432425,\n",
       "   -0.13855928071106027,\n",
       "   -0.12253813584499818,\n",
       "   -0.09762776285466912,\n",
       "   -0.13089877145685147,\n",
       "   -0.15011503081409683,\n",
       "   -0.12005600549045559,\n",
       "   -0.035107480120764256,\n",
       "   -0.0993703297281543,\n",
       "   -0.0890706004010553,\n",
       "   -0.08543853923250833,\n",
       "   -0.11078668271850889,\n",
       "   -0.08746825119517565,\n",
       "   -0.24559250989294434,\n",
       "   -0.1848944953449252,\n",
       "   -0.33038403794559823,\n",
       "   -0.28412922274038177,\n",
       "   -0.3047457055161926,\n",
       "   -0.22892680699607149,\n",
       "   -0.10699420871486942,\n",
       "   -0.08132876941611977,\n",
       "   -0.0527209818618104,\n",
       "   -0.048274007082170334,\n",
       "   -0.06537350475240156,\n",
       "   -0.07389049454163366,\n",
       "   -0.159984504160952,\n",
       "   -0.09301213790651541,\n",
       "   -0.05832184535090823,\n",
       "   -0.13353626203926483,\n",
       "   -0.06373513693329147,\n",
       "   -0.0988457097951203,\n",
       "   -0.04901275359390023,\n",
       "   -0.02962280131395656,\n",
       "   -0.024742432264585723,\n",
       "   -0.041514201139705165,\n",
       "   -0.08126615627268818,\n",
       "   -0.07070565553189412,\n",
       "   -0.12698563144191888,\n",
       "   -0.07881245374290247,\n",
       "   -0.015215874920994077,\n",
       "   -0.04661945854357086,\n",
       "   -0.03324710553309118,\n",
       "   -0.029915443654335627,\n",
       "   -0.03576523055589953,\n",
       "   -0.06161189096104591,\n",
       "   0.007356822832568222,\n",
       "   0.002796317330331366,\n",
       "   0.006638821488517022,\n",
       "   0.006057783232773173,\n",
       "   0.006874474132287922,\n",
       "   0.005923405165208617,\n",
       "   0.005500680266431135,\n",
       "   0.006449149294804113,\n",
       "   0.007270775374164984,\n",
       "   -0.0008427248701567969,\n",
       "   -0.004043739841073313,\n",
       "   -0.007663342112345317,\n",
       "   -0.04528021685420347,\n",
       "   -0.008915450634405353,\n",
       "   -0.013235575907430075,\n",
       "   -0.025193040225829355,\n",
       "   0.0036969427256319243,\n",
       "   0.004657272871258611,\n",
       "   0.004961680251038492,\n",
       "   0.0066393550609711305,\n",
       "   0.00720693046352483,\n",
       "   0.006800793125203319,\n",
       "   0.005310220162259349,\n",
       "   0.007510609466436002,\n",
       "   -0.000371851134540524,\n",
       "   -0.005518449566820127,\n",
       "   0.006719235803326118,\n",
       "   0.003232760831770376,\n",
       "   0.004880008115568768,\n",
       "   0.0013261658799889053,\n",
       "   0.0004140153436541505,\n",
       "   0.012039822109580212,\n",
       "   -0.0015262043490638089,\n",
       "   -0.016522289992541017,\n",
       "   -0.04601065123145087,\n",
       "   -0.04779548407673517,\n",
       "   0.0016812370726794201,\n",
       "   0.006208982125169182,\n",
       "   0.011324958031562748,\n",
       "   0.01090307656150813,\n",
       "   0.012143414350187953,\n",
       "   0.005031834213836461,\n",
       "   -0.0010270672957790605,\n",
       "   -0.04384394571528705,\n",
       "   0.004620057308439396,\n",
       "   0.001989926767404171,\n",
       "   -0.012442152545634633,\n",
       "   -0.05039946581748822,\n",
       "   -0.03495610988729614,\n",
       "   -0.02472032265503432,\n",
       "   -0.015453111055481195,\n",
       "   -0.03259613523252236,\n",
       "   0.002624710162903665,\n",
       "   0.003936116862325756,\n",
       "   -0.02516168853666456,\n",
       "   -0.019948217564107695,\n",
       "   -0.02707461949104384,\n",
       "   -0.07799132274317055,\n",
       "   -0.06493131294017318,\n",
       "   -0.05511727905139585,\n",
       "   -0.08452420206941169,\n",
       "   -0.07190173532081201,\n",
       "   -0.09929851578018734,\n",
       "   -0.1076826756842888,\n",
       "   -0.14525967567656672,\n",
       "   -0.22899067139478668,\n",
       "   -0.23386867287525637,\n",
       "   -0.12914608712676623,\n",
       "   -0.07292240438256492,\n",
       "   -0.09518204894572713,\n",
       "   -0.07351854863472163,\n",
       "   -0.05655847384415802,\n",
       "   -0.030819633928539814,\n",
       "   -0.048966246182688966,\n",
       "   -0.08363154242013371,\n",
       "   -0.06507231342263231,\n",
       "   -0.07598884660311311,\n",
       "   -0.12536235213529978,\n",
       "   -0.2077010997506048,\n",
       "   -0.39456337409500875,\n",
       "   -0.4042843610746103,\n",
       "   -0.4858104412031341,\n",
       "   -0.4415863701911169,\n",
       "   -0.2415368979749528,\n",
       "   -0.2818277265719331,\n",
       "   -0.3620547532687346,\n",
       "   -0.33986365077223835,\n",
       "   -0.20548796755620025,\n",
       "   -0.1853161849744509,\n",
       "   -0.1849011620395784,\n",
       "   -0.08827116672511637,\n",
       "   -0.05949347549521211,\n",
       "   -0.06366973111509266,\n",
       "   -0.03114196295631899,\n",
       "   -0.03220638950431498,\n",
       "   -0.011799241513845926,\n",
       "   -0.009822110115247806,\n",
       "   0.011453979667802594,\n",
       "   -0.06599495499873553,\n",
       "   -0.017565563537573547,\n",
       "   -0.0011260006734850059,\n",
       "   0.0019294154454402301,\n",
       "   -0.03228241692788991,\n",
       "   -0.03390051858980536,\n",
       "   0.004768537806822848,\n",
       "   0.007209619392182898,\n",
       "   0.006539217922695739,\n",
       "   -0.013131736352631096,\n",
       "   -0.03835414818426841,\n",
       "   -0.014733279822211509,\n",
       "   -0.0033251489999155392,\n",
       "   -0.005754417513615273,\n",
       "   -0.0034733218224844986,\n",
       "   -0.005203722179677905,\n",
       "   -0.04409333738204535,\n",
       "   -0.013281978325159649,\n",
       "   -0.015123246226126374,\n",
       "   -0.0412406561485863,\n",
       "   -0.06949660261481361,\n",
       "   -0.005899019571102393,\n",
       "   0.0026223001591675343,\n",
       "   -0.05487136915512725,\n",
       "   -0.09228754778092063,\n",
       "   -0.08449258121262893,\n",
       "   -0.09843981568267446,\n",
       "   -0.1185075616397342,\n",
       "   -0.06102926724929736,\n",
       "   -0.06937021001763788,\n",
       "   -0.056427135265153305,\n",
       "   -0.015041650918911742,\n",
       "   -0.0013101488147595486,\n",
       "   -0.051197779295511836,\n",
       "   -0.07638425224528067,\n",
       "   -0.18124153116851138,\n",
       "   -0.3138015845657067,\n",
       "   -0.18286394207207834,\n",
       "   -0.13544341262269866,\n",
       "   -0.09613410644782205,\n",
       "   -0.07284233327476208,\n",
       "   0.0034242239521949935,\n",
       "   0.002654168969840609,\n",
       "   0.01810101488416327,\n",
       "   0.005200744341840258,\n",
       "   0.012038933110157024,\n",
       "   0.0027052229488279123,\n",
       "   -0.00805297776736924,\n",
       "   2.930402462508752e-05,\n",
       "   0.00247814618738329,\n",
       "   0.002359020397333045,\n",
       "   0.015289333016215267,\n",
       "   0.0039104185605153274,\n",
       "   -0.009810442449882384,\n",
       "   -0.012184369838396991,\n",
       "   7.136654019834765e-05,\n",
       "   0.013011228562655686,\n",
       "   -0.0033738676275187895,\n",
       "   -0.0022209085488946734,\n",
       "   -0.006111905023084896,\n",
       "   -0.0011045596566365834,\n",
       "   -0.006568889971223735,\n",
       "   -0.002372063167433769,\n",
       "   -0.00014173982345955238,\n",
       "   0.015245131234879472,\n",
       "   0.012509775303454207,\n",
       "   0.01190965172221592,\n",
       "   -0.03910385350395873,\n",
       "   -0.0010175255626769797,\n",
       "   -0.11019692845225171,\n",
       "   -0.07434233648380584,\n",
       "   -0.11486061189382957,\n",
       "   -0.0805335875787585,\n",
       "   -0.018919256658715966,\n",
       "   -0.0701740650421514,\n",
       "   -0.037942926823789414,\n",
       "   -0.016495220649765274,\n",
       "   -0.02452061918641868,\n",
       "   -0.01994909330870609,\n",
       "   -0.015530134586645428,\n",
       "   0.007311658637610741,\n",
       "   0.015340333770570092,\n",
       "   0.013761199863766227,\n",
       "   0.014605449182377717,\n",
       "   0.007168732099425257,\n",
       "   0.014544794866450772,\n",
       "   0.015709342694984807,\n",
       "   0.018980103999579723,\n",
       "   0.01910875753602359,\n",
       "   0.021039830625358857,\n",
       "   0.018037670026936237,\n",
       "   0.01806001415559877,\n",
       "   0.01879107365002386,\n",
       "   0.017816315472109046,\n",
       "   0.01627894874283642,\n",
       "   0.01409169435101798,\n",
       "   0.014242082644688181,\n",
       "   0.007001075597654283,\n",
       "   0.011736315335061254,\n",
       "   0.019903091291446295,\n",
       "   0.014746815721934037,\n",
       "   0.009519408169729182,\n",
       "   0.014545042385977203,\n",
       "   0.014980825929635011,\n",
       "   0.016126345420941934,\n",
       "   0.0164558092291619,\n",
       "   0.017365161025928333,\n",
       "   0.016352957027793713,\n",
       "   0.015095787615736156,\n",
       "   0.013492545906516384,\n",
       "   0.005565999567031632,\n",
       "   0.013374707478109094,\n",
       "   0.013345183810910322,\n",
       "   0.013054277352928523,\n",
       "   0.016093699611516965,\n",
       "   0.012760631292197622,\n",
       "   0.020287707369395447,\n",
       "   0.019946964400562206,\n",
       "   0.019694676795143806,\n",
       "   0.01648474944501699,\n",
       "   0.016696065301575302,\n",
       "   0.018736756603556448,\n",
       "   0.017736256910014347,\n",
       "   6.684387704536388e-05,\n",
       "   -0.006818018084128852,\n",
       "   0.002662081195527799,\n",
       "   0.018726636030939825,\n",
       "   0.018112417092747912,\n",
       "   0.007952417896292752,\n",
       "   0.00657736178400703,\n",
       "   0.01856430163875289,\n",
       "   0.013615499120161023,\n",
       "   0.0002267111369705921,\n",
       "   -0.008430850832002568,\n",
       "   -0.014945405824213553,\n",
       "   -0.05013930447331716,\n",
       "   -0.05269846261332756,\n",
       "   -0.06547000598042785,\n",
       "   -0.016676795703615355,\n",
       "   -0.04796027111270762,\n",
       "   -0.053714483903760435,\n",
       "   -0.04281783402894157,\n",
       "   -0.05082475918459628,\n",
       "   0.006353712767750319,\n",
       "   0.015251677908939554,\n",
       "   0.014886465007270977,\n",
       "   0.017519416735293003,\n",
       "   0.021654228536001074,\n",
       "   0.010238808659398635,\n",
       "   -0.04877481069367673,\n",
       "   -0.052651557899727086,\n",
       "   -0.08355915634986033,\n",
       "   -0.04676318920645439,\n",
       "   -0.043336386739982524,\n",
       "   -0.053314544322117685,\n",
       "   0.0013462921221961688,\n",
       "   0.009410267821280294,\n",
       "   0.018032026506052734,\n",
       "   0.019154590624990142,\n",
       "   0.018445655132163452,\n",
       "   0.015689616151790653,\n",
       "   -0.00821541883172161,\n",
       "   0.002336937667584249,\n",
       "   0.014981007235889243,\n",
       "   0.0006839898970647518,\n",
       "   -0.007907426926608396,\n",
       "   0.008463373997016022,\n",
       "   -0.019688263726387323,\n",
       "   -0.07395634880732471,\n",
       "   -0.05832001882646187,\n",
       "   -0.028853519367852964,\n",
       "   -0.030184226298265004,\n",
       "   -0.008651785393224198,\n",
       "   0.017136526730375357,\n",
       "   0.02556174444764392,\n",
       "   0.02435240034273689,\n",
       "   0.023235745343266112,\n",
       "   0.01897820476563805,\n",
       "   0.016478954499598242,\n",
       "   0.02049771713523052,\n",
       "   0.021378808236273472,\n",
       "   0.019521805066503695,\n",
       "   -0.005146478105913843,\n",
       "   -0.011109243949826375,\n",
       "   -0.04301958967115227,\n",
       "   -0.02000970455682438,\n",
       "   -0.014562134183683684,\n",
       "   -0.031127499457783703,\n",
       "   -0.02675704009157487,\n",
       "   -0.045121013666111986,\n",
       "   -0.032754633209656436,\n",
       "   -0.011173663305867443,\n",
       "   0.011077174593758343,\n",
       "   0.017784868250727892,\n",
       "   0.012792406622640584,\n",
       "   0.01590700306843329,\n",
       "   0.008887166190646223,\n",
       "   -0.0043791237011836515,\n",
       "   0.010960150112265032,\n",
       "   -0.04232709161396686,\n",
       "   0.013559565554680852,\n",
       "   0.01100322930530595,\n",
       "   0.014666404717317616,\n",
       "   0.012681980715254282,\n",
       "   0.009444297329444407,\n",
       "   0.00818178321394325,\n",
       "   0.009641998528407192,\n",
       "   0.0067066057598905715,\n",
       "   0.008070661331567308,\n",
       "   0.0004820957170476037,\n",
       "   -0.013847275539941695,\n",
       "   -0.024791656061936562,\n",
       "   -0.023859402416802178,\n",
       "   -0.029629781222292317,\n",
       "   -0.010590060260769807,\n",
       "   -0.006960528442931713,\n",
       "   -0.003913074289283669,\n",
       "   -0.002335479113649397,\n",
       "   0.0006351160331401484,\n",
       "   -0.009704710771212044,\n",
       "   -0.0018836948276486076,\n",
       "   -0.010277363657039018,\n",
       "   -0.007577031839008264,\n",
       "   -0.007353569753807632,\n",
       "   -0.0022464236325987486,\n",
       "   -0.011906909277298539,\n",
       "   -0.006089658944394616,\n",
       "   -0.027891341434955308,\n",
       "   -0.003494581156216814,\n",
       "   -0.01103674523885867,\n",
       "   -0.04955022994127156,\n",
       "   -0.025925202891843302,\n",
       "   -0.08949647493752755,\n",
       "   -0.0771555202050016,\n",
       "   -0.16221775319522425,\n",
       "   -0.13093166319264893,\n",
       "   -0.10339731658346518,\n",
       "   -0.162523951687779,\n",
       "   -0.15016610965422528,\n",
       "   -0.11384622883736939,\n",
       "   -0.09470253876238033,\n",
       "   -0.06474980962268506,\n",
       "   -0.14542160899235546,\n",
       "   -0.1380513026419583,\n",
       "   -0.1575512113325821,\n",
       "   -0.19150223898962548,\n",
       "   -0.1530769285776632,\n",
       "   -0.12475456051416423,\n",
       "   -0.1240964519687584,\n",
       "   -0.07946113596965065,\n",
       "   -0.09724381392660342,\n",
       "   -0.18429104103194693,\n",
       "   -0.16478012717166357,\n",
       "   -0.08816975581247788,\n",
       "   -0.0631241824912443,\n",
       "   -0.03871385769875113,\n",
       "   -0.06928499265942634,\n",
       "   -0.03974345768980525,\n",
       "   -0.043638132917085404,\n",
       "   -0.05989594730270564,\n",
       "   -0.07281032813958377,\n",
       "   -0.13822075275014867,\n",
       "   -0.13203109775600286,\n",
       "   -0.16554221816251702,\n",
       "   -0.260575678585387,\n",
       "   -0.3240377776206549,\n",
       "   -0.18551085419250501,\n",
       "   -0.12532839076884894,\n",
       "   -0.17529563469734577,\n",
       "   -0.19204734788118816,\n",
       "   -0.2364832529926344,\n",
       "   -0.2238829835504943,\n",
       "   -0.16416987025988683,\n",
       "   -0.21338948041917982,\n",
       "   -0.22556667577477763,\n",
       "   -0.13020723264977208,\n",
       "   -0.11089918490009953,\n",
       "   -0.07725606604406621,\n",
       "   -0.04295291274926938,\n",
       "   -0.035010841154823336,\n",
       "   -0.04583284071738292,\n",
       "   -0.04412627855957063,\n",
       "   -0.0392816110131986,\n",
       "   -0.0308652287125466,\n",
       "   -0.03614249805179792,\n",
       "   -0.03497791887310586,\n",
       "   -0.03958200103956511,\n",
       "   -0.03916803094850407,\n",
       "   -0.04283925301574598,\n",
       "   -0.03759692985544638,\n",
       "   -0.036213452426559156,\n",
       "   -0.03921291934078308,\n",
       "   -0.03568258278856962,\n",
       "   -0.053507755830924486,\n",
       "   -0.06965481688361064,\n",
       "   -0.15278602368140848,\n",
       "   -0.08723079805228529,\n",
       "   -0.06786208013580408,\n",
       "   -0.05747949540842412,\n",
       "   -0.058004427149388915,\n",
       "   -0.07764427683506647,\n",
       "   -0.05480025912184616,\n",
       "   -0.062053194314017235,\n",
       "   -0.12391298747232171,\n",
       "   -0.11578565986199929,\n",
       "   -0.1654756861282778,\n",
       "   -0.13684822960462406,\n",
       "   -0.19700963779337033,\n",
       "   -0.2069225833132391],\n",
       "  'jsc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -110832.4846435255,\n",
       "   0.0,\n",
       "   -115094.94818562579,\n",
       "   -2468.2171932252963,\n",
       "   -795.263168730822,\n",
       "   -383.8076075539731,\n",
       "   -273.86394410089105,\n",
       "   -257.8903964046628,\n",
       "   -329.29123681722376,\n",
       "   -481.5791704928638,\n",
       "   -399.2658777741512,\n",
       "   -412.0982101287393,\n",
       "   -404.02818922856903,\n",
       "   -415.1105552853745,\n",
       "   -351.9622966787864,\n",
       "   -357.1663545153326,\n",
       "   -317.2668364288149,\n",
       "   -308.7813729363048,\n",
       "   -295.84248700023,\n",
       "   -304.67310746588504,\n",
       "   -182.92757602248486,\n",
       "   -242.057677635814,\n",
       "   -251.64670947583116,\n",
       "   -241.39018425612656,\n",
       "   -239.46763052591717,\n",
       "   -241.04943993603518,\n",
       "   -289.17107726152517,\n",
       "   -324.11987028479246,\n",
       "   -360.32576490493267,\n",
       "   -276.2223496753033,\n",
       "   -273.4172143260083,\n",
       "   -327.65977409067375,\n",
       "   -286.9594146247257,\n",
       "   -232.53421590074586,\n",
       "   -210.38651497660805,\n",
       "   -268.785788174499,\n",
       "   -261.16780819201665,\n",
       "   -250.65479165744406,\n",
       "   -263.00222318203816,\n",
       "   -237.57815377638258,\n",
       "   -244.8527569842414,\n",
       "   -245.31546604703007,\n",
       "   -311.16304904800546,\n",
       "   -340.0430223658701,\n",
       "   -215.7288769486104,\n",
       "   -236.84640042068906,\n",
       "   -260.0248012050239,\n",
       "   -332.19997806545337,\n",
       "   -434.2618750475197,\n",
       "   -311.8326258234925,\n",
       "   -256.2622025283595,\n",
       "   -324.67406474241864,\n",
       "   -343.76274128665,\n",
       "   -250.69401222714822,\n",
       "   -240.7077750603154,\n",
       "   -192.06484385838402,\n",
       "   -201.17241727077024,\n",
       "   -131.17609104159337,\n",
       "   -114.06498247219648,\n",
       "   -136.95951564022164,\n",
       "   -157.05484541800354,\n",
       "   -164.60195542505747,\n",
       "   -179.52686758056706,\n",
       "   -168.3407902923907,\n",
       "   -144.02373446999357,\n",
       "   -132.0415066114914,\n",
       "   -120.08109782848923,\n",
       "   -132.68905119721143,\n",
       "   -136.72282027832995,\n",
       "   -176.7338165522068,\n",
       "   -167.89641577976198,\n",
       "   -209.32371453738267,\n",
       "   -240.65450613844257,\n",
       "   -297.37498567526217,\n",
       "   -295.6477779953874,\n",
       "   -330.01532261020157,\n",
       "   -540.7331601838534,\n",
       "   -487.9495021323053,\n",
       "   -859.6579167108838,\n",
       "   -673.5584152573566,\n",
       "   -550.5353880850015,\n",
       "   -410.18558456829925,\n",
       "   -314.50394974092734,\n",
       "   -218.54812921556197,\n",
       "   -251.40657758608228,\n",
       "   -161.1924083679937,\n",
       "   -135.71637094302292,\n",
       "   -136.975730775634,\n",
       "   -136.75294272799215,\n",
       "   -122.48138896006525,\n",
       "   -134.95874672931345,\n",
       "   -143.46548914883016,\n",
       "   -128.35053639163405,\n",
       "   -125.44598605007462,\n",
       "   -121.33256416159712,\n",
       "   -115.62649448177078,\n",
       "   -128.5999697175684,\n",
       "   -137.80330176093443,\n",
       "   -154.70697612150462,\n",
       "   -182.4678264967215,\n",
       "   -221.5152246424097,\n",
       "   -192.07183623353419,\n",
       "   -172.19693847464225,\n",
       "   -153.76332877739318,\n",
       "   -186.65488929668717,\n",
       "   -209.72123867564727,\n",
       "   -315.87102332713636,\n",
       "   -279.0441347156809,\n",
       "   -221.04847677109746,\n",
       "   -218.56595241816382,\n",
       "   -172.58324749495966,\n",
       "   -144.02859220667747,\n",
       "   -155.82907656716085,\n",
       "   -180.14482185641378,\n",
       "   -187.2833029107226,\n",
       "   -248.84000813052464,\n",
       "   -258.59242199605546,\n",
       "   -265.83272116968465,\n",
       "   -285.0555416861711,\n",
       "   -316.7494330226753,\n",
       "   -366.79395682896757,\n",
       "   -480.37008560864734,\n",
       "   -362.32199652411185,\n",
       "   -395.018037929318,\n",
       "   -469.15032410523867,\n",
       "   -530.4170891775449,\n",
       "   -581.5814079388715,\n",
       "   -533.7645932137581,\n",
       "   -329.25060310671216,\n",
       "   -335.39420019729783,\n",
       "   -355.77925610391003,\n",
       "   -247.88100331347826,\n",
       "   -362.72958183963055,\n",
       "   -413.5044510580583,\n",
       "   -518.1342424444994,\n",
       "   -649.4920247902623,\n",
       "   -607.8360351565715,\n",
       "   -732.7185867002579,\n",
       "   -537.1920531766691,\n",
       "   -429.7240309212014,\n",
       "   -504.7047755253488,\n",
       "   -484.8434082859694,\n",
       "   -333.74284939591564,\n",
       "   -297.51316010940775,\n",
       "   -275.3350093804908,\n",
       "   -308.2972269156925,\n",
       "   -413.2144799503071,\n",
       "   -341.1607409260807,\n",
       "   -393.266142926296,\n",
       "   -352.1959926058014,\n",
       "   -385.49664533438784,\n",
       "   -325.50558578648315,\n",
       "   -298.6212724331279,\n",
       "   -348.5259583085239,\n",
       "   -445.4584683793803,\n",
       "   -394.5712884847623,\n",
       "   -402.7240529545011,\n",
       "   -307.3077769738519,\n",
       "   -419.76114098776054,\n",
       "   -369.9173926583269,\n",
       "   -305.87343569913173,\n",
       "   -283.66891517228686,\n",
       "   -292.2390033570039,\n",
       "   -446.10664450112034,\n",
       "   -383.3337291217807,\n",
       "   -330.8852463103078,\n",
       "   -283.7563057117793,\n",
       "   -240.64271456924456,\n",
       "   -209.08705722422215,\n",
       "   -174.9682537088306,\n",
       "   -186.92247009278148,\n",
       "   -234.478012329888,\n",
       "   -201.84837044109702,\n",
       "   -243.07018456189758,\n",
       "   -254.711834407113,\n",
       "   -271.6331735597465,\n",
       "   -248.6709586112256,\n",
       "   -263.2549996265765,\n",
       "   -283.32331222978286,\n",
       "   -324.0621235256421,\n",
       "   -323.66504089530713,\n",
       "   -368.73132887448594,\n",
       "   -447.022121634607,\n",
       "   -425.67345761159925,\n",
       "   -551.8265477870156,\n",
       "   -399.27708334295176,\n",
       "   -574.7015443157646,\n",
       "   -572.7055900866554,\n",
       "   -480.83534644074194,\n",
       "   -369.49577477110546,\n",
       "   -344.6111218448586,\n",
       "   -494.1446836722677,\n",
       "   -604.2501819165083,\n",
       "   -701.8036632268739,\n",
       "   -646.3337777645772,\n",
       "   -756.6530209542415,\n",
       "   -785.3321014253082,\n",
       "   -620.4029556448656,\n",
       "   -571.0160415961109,\n",
       "   -570.9528652644302,\n",
       "   -608.3387686647108,\n",
       "   -519.8613653050633,\n",
       "   -486.39060694233666,\n",
       "   -307.8888849672477,\n",
       "   -573.1397142288674,\n",
       "   -392.43753568714374,\n",
       "   -379.1049189311834,\n",
       "   -372.7378542846072,\n",
       "   -518.6313990928595,\n",
       "   -520.8933061528205,\n",
       "   -412.7069098820467,\n",
       "   -514.160381462053,\n",
       "   -482.6855606634689,\n",
       "   -515.8791548264262,\n",
       "   -545.0731237683489,\n",
       "   -553.0441460297762,\n",
       "   -510.74959624604656,\n",
       "   -489.5499755906645,\n",
       "   -470.80316695458885,\n",
       "   -492.8995082869035,\n",
       "   -551.70319958506,\n",
       "   -376.68281122488037,\n",
       "   -381.30739085319135,\n",
       "   -457.84563935428594,\n",
       "   -411.5685296599739,\n",
       "   -291.4824300907476,\n",
       "   -442.4028247630401,\n",
       "   -398.016928419335,\n",
       "   -482.5027951036694,\n",
       "   -417.7855349176946,\n",
       "   -361.10143597692957,\n",
       "   -430.7917401379299,\n",
       "   -537.9805564165109,\n",
       "   -368.3938006614994,\n",
       "   -371.00320262497104,\n",
       "   -294.4374008769494,\n",
       "   -231.35119484828488,\n",
       "   -277.29939326654363,\n",
       "   -212.46659682579408,\n",
       "   -287.8527809237131,\n",
       "   -306.8702479381269,\n",
       "   -285.4759778027071,\n",
       "   -254.56792107341548,\n",
       "   -319.42716090772467,\n",
       "   -314.50288098028756,\n",
       "   -353.58782578019805,\n",
       "   -397.3696937205014,\n",
       "   -518.2573321172898,\n",
       "   -537.3018053951384,\n",
       "   -534.8430344805689,\n",
       "   -487.9911185452236,\n",
       "   -455.06220598826667,\n",
       "   -384.36505594258296,\n",
       "   -473.1519771302355,\n",
       "   -598.1355351711143,\n",
       "   -497.54260465964705,\n",
       "   -524.426473719136,\n",
       "   -601.5677275380558,\n",
       "   -659.6872399036447,\n",
       "   -536.8544799751543,\n",
       "   -376.1063472072149,\n",
       "   -403.14023813845785,\n",
       "   -405.2644182488977,\n",
       "   -375.4941609297329,\n",
       "   -311.3677996376955,\n",
       "   -300.83754708961965,\n",
       "   -216.02105368755943,\n",
       "   -259.8502079213289,\n",
       "   -272.12004313018167,\n",
       "   -231.0546909818226,\n",
       "   -248.07086794876352,\n",
       "   -253.14921205421697,\n",
       "   -216.13786329075208,\n",
       "   -204.74972815969738,\n",
       "   -280.7600742006113,\n",
       "   -225.58162377875874,\n",
       "   -368.9895157857019,\n",
       "   -464.12030827970295,\n",
       "   -376.7324486782381,\n",
       "   -458.03114503680087,\n",
       "   -444.7514290978581,\n",
       "   -399.18166373268076,\n",
       "   -440.712546395235,\n",
       "   -452.4286875343797,\n",
       "   -550.6706229540748,\n",
       "   -425.41845168387187,\n",
       "   -465.8061782084342,\n",
       "   -478.17525277405065,\n",
       "   -599.0715237385124,\n",
       "   -575.7346158730468,\n",
       "   -495.7194486132762,\n",
       "   -642.5140914590155,\n",
       "   -550.998683569591,\n",
       "   -603.7820703460525,\n",
       "   -504.7056999742996,\n",
       "   -426.96166765297176,\n",
       "   -508.4404914764405,\n",
       "   -492.6715278405164,\n",
       "   -587.3534546912254,\n",
       "   -495.7589266114302,\n",
       "   -607.7039312080176,\n",
       "   -583.0863810554534,\n",
       "   -487.19016722741003,\n",
       "   -387.4762102002446,\n",
       "   -397.33691242168135,\n",
       "   -464.17526492879165,\n",
       "   -545.8896810708852,\n",
       "   -396.84330916738105,\n",
       "   -388.53698149256206,\n",
       "   -396.4109443213943,\n",
       "   -470.63580457258166,\n",
       "   -467.91502144467535,\n",
       "   -513.954479586262,\n",
       "   -614.8331757861815,\n",
       "   -670.5867062405265,\n",
       "   -603.9281291756089,\n",
       "   -594.4058177441879,\n",
       "   -401.91275283355736,\n",
       "   -530.511061666334,\n",
       "   -582.4662966515014,\n",
       "   -522.4822573224827,\n",
       "   -432.1495258065464,\n",
       "   -377.4664836691577,\n",
       "   -415.18388323945857,\n",
       "   -467.3614493072269,\n",
       "   -394.35171675766367,\n",
       "   -341.63423090364034,\n",
       "   -311.60728237957585,\n",
       "   -415.10488438687315,\n",
       "   -491.7981403070459,\n",
       "   -481.6985904797114,\n",
       "   -436.43993088194003,\n",
       "   -486.3490054327464,\n",
       "   -467.33394640461063,\n",
       "   -581.5186668361241,\n",
       "   -502.1595948650617,\n",
       "   -416.5486629846386,\n",
       "   -372.24246833687647,\n",
       "   -349.4245984578207,\n",
       "   -375.47423167623936,\n",
       "   -379.2180933103777,\n",
       "   -415.9852060469999,\n",
       "   -396.7107872463147,\n",
       "   -500.93240547924626,\n",
       "   -484.54017595531064,\n",
       "   -514.7181146985623,\n",
       "   -496.0748212367733,\n",
       "   -547.2582238141938,\n",
       "   -549.8199470717288,\n",
       "   -608.4945953459945,\n",
       "   -570.811823132412,\n",
       "   -437.5123315289919,\n",
       "   -569.2398682125357,\n",
       "   -574.6566985785696,\n",
       "   -452.008356177759,\n",
       "   -459.6603134796998,\n",
       "   -477.6539476837184,\n",
       "   -360.8352924363827,\n",
       "   -361.02046829756785,\n",
       "   -317.0562602172442,\n",
       "   -246.11064340954027,\n",
       "   -310.81696270106346,\n",
       "   -436.39637980641186,\n",
       "   -512.619679469065,\n",
       "   -445.4941133869358,\n",
       "   -322.5361134352121,\n",
       "   -408.6222642841977,\n",
       "   -482.34166867764776,\n",
       "   -441.3280955250672,\n",
       "   -565.6842312462056,\n",
       "   -403.33982706499677,\n",
       "   -370.1193662278536,\n",
       "   -327.22671434350656,\n",
       "   -289.54832639416765,\n",
       "   -319.81042220003553,\n",
       "   -204.39789669669153,\n",
       "   -299.30526445663253,\n",
       "   -299.12467549240716,\n",
       "   -328.45956423043367,\n",
       "   -286.96721047607844,\n",
       "   -205.4010641453043,\n",
       "   -252.27505666321372,\n",
       "   -286.37312672285935,\n",
       "   -362.72271993413887,\n",
       "   -357.0251951631156,\n",
       "   -499.0649183100169,\n",
       "   -319.622394011104,\n",
       "   -261.89479961479066,\n",
       "   -287.3358002785442,\n",
       "   -273.7284460782018,\n",
       "   -271.29904225163807,\n",
       "   -302.881027374841,\n",
       "   -368.3211700667472,\n",
       "   -385.2415097574538,\n",
       "   -323.082629474659,\n",
       "   -297.4655227686259,\n",
       "   -225.3762069729737,\n",
       "   -162.263661765975,\n",
       "   -206.59773756651356,\n",
       "   -278.47797618217857,\n",
       "   -321.25807303848995,\n",
       "   -224.42490214067203,\n",
       "   -196.06646712296455,\n",
       "   -184.63587128515903,\n",
       "   -194.212935211701,\n",
       "   -187.7059349129862,\n",
       "   -221.86688910656719,\n",
       "   -240.92784318458163,\n",
       "   -277.1514191478888,\n",
       "   -212.23271774862602,\n",
       "   -252.6281239424462,\n",
       "   -266.8575025156941,\n",
       "   -412.3727006574432,\n",
       "   -444.03103047262755,\n",
       "   -349.1602567095753,\n",
       "   -295.33218906440374,\n",
       "   -296.4581497971727,\n",
       "   -294.7478343416052,\n",
       "   -264.6148990307045,\n",
       "   -311.43891246580307,\n",
       "   -343.87499657900923,\n",
       "   -351.8307640898801,\n",
       "   -285.0680900353374,\n",
       "   -236.1263443472007,\n",
       "   -291.8139709701779,\n",
       "   -405.4876691743871,\n",
       "   -336.21629632673233,\n",
       "   -249.5862241501489,\n",
       "   -287.3690725549003,\n",
       "   -296.3934813071571,\n",
       "   -423.0331024143251,\n",
       "   -447.86221156162867,\n",
       "   -516.2304258199423,\n",
       "   -507.8355748109699,\n",
       "   -544.8490673386417,\n",
       "   -438.4995986725566,\n",
       "   -500.59824217903366,\n",
       "   -456.51559267881635,\n",
       "   -336.24980978432677,\n",
       "   -463.64911346799977,\n",
       "   -510.99102039538474,\n",
       "   -549.8218638381597,\n",
       "   -585.0465264574269,\n",
       "   -401.6304692055073,\n",
       "   -433.37887957373147,\n",
       "   -553.4180068239185,\n",
       "   -362.82504497791393,\n",
       "   -422.7857664891826,\n",
       "   -469.9574826318781,\n",
       "   -428.63343883228947,\n",
       "   -401.21397338043215,\n",
       "   -277.31666115762437,\n",
       "   -269.73867157852067,\n",
       "   -341.2128691051538,\n",
       "   -394.71666207397027,\n",
       "   -391.20717347210075,\n",
       "   -393.88505072323125,\n",
       "   -368.91722787642084,\n",
       "   -356.09287842166543,\n",
       "   -297.97343351431715,\n",
       "   -312.3431014572795,\n",
       "   -324.5286316400812,\n",
       "   -229.32755445458116,\n",
       "   -176.95939719458224,\n",
       "   -202.72201471910608,\n",
       "   -158.22148477078863,\n",
       "   -154.15672973332954,\n",
       "   -168.92830212808954,\n",
       "   -186.9891384399193,\n",
       "   -215.94828384279313,\n",
       "   -196.6842426923662,\n",
       "   -255.5633322508628,\n",
       "   -249.26093451079248,\n",
       "   -277.3375263669127,\n",
       "   -316.77116738358836,\n",
       "   -303.5559841183922,\n",
       "   -346.0611351230969,\n",
       "   -328.77146856415976,\n",
       "   -307.9962396816708,\n",
       "   -288.95865226840016,\n",
       "   -346.2926045207647,\n",
       "   -266.3946652852273,\n",
       "   -374.7685322863586,\n",
       "   -384.1712439777955,\n",
       "   -372.0271775860513,\n",
       "   -492.1854701383487,\n",
       "   -655.1662343286215,\n",
       "   -486.5590267310768,\n",
       "   -280.44498590796763,\n",
       "   -257.2441285764113,\n",
       "   -258.44905426563224],\n",
       "  'ff_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -13044.676362712187,\n",
       "   -164.74208683956098,\n",
       "   -47.148652276232,\n",
       "   -24.416725755807242,\n",
       "   -14.00976559585654,\n",
       "   -10.317010518066786,\n",
       "   -8.530357534078576,\n",
       "   -6.477918205669648,\n",
       "   -5.510161335477908,\n",
       "   -5.507892699748045,\n",
       "   -5.0765225935600915,\n",
       "   -4.410175225909102,\n",
       "   -3.2660086483874746,\n",
       "   -3.256450332595514,\n",
       "   -3.1666297081950106,\n",
       "   -3.3454701136431257,\n",
       "   -2.8722471375941994,\n",
       "   -3.4131655558275016,\n",
       "   -3.8057809911238625,\n",
       "   -4.458693159706648,\n",
       "   -4.65446320492067,\n",
       "   -5.238705932071534,\n",
       "   -5.17112831235585,\n",
       "   -7.274078527698489,\n",
       "   -5.972125352241297,\n",
       "   -6.267574816511665,\n",
       "   -8.9222514110481,\n",
       "   -7.581431352437599,\n",
       "   -6.979044910508291,\n",
       "   -8.775492978345964,\n",
       "   -12.011936194804656,\n",
       "   -12.080079649618224,\n",
       "   -14.02110065199374,\n",
       "   -19.075244841143142,\n",
       "   -18.28242467244018,\n",
       "   -15.598525903912478,\n",
       "   -14.314618720597851,\n",
       "   -14.865177094859215,\n",
       "   -15.329630101329723,\n",
       "   -19.75268898046776,\n",
       "   -15.38327721477414,\n",
       "   -17.259993418855572,\n",
       "   -16.766825892797034,\n",
       "   -12.793946448152916,\n",
       "   -12.190019233610712,\n",
       "   -10.077869276117555,\n",
       "   -9.26544644599225,\n",
       "   -7.269970297784765,\n",
       "   -8.31407159448335,\n",
       "   -9.702859378068688,\n",
       "   -8.448895818256569,\n",
       "   -8.879033232063133,\n",
       "   -6.94192608212739,\n",
       "   -8.090962579601188,\n",
       "   -6.574178513015033,\n",
       "   -5.566206644901632,\n",
       "   -6.05248171630192,\n",
       "   -5.088846199941393,\n",
       "   -5.252978664698625,\n",
       "   -5.288911846736303,\n",
       "   -6.603325699628636,\n",
       "   -6.351764365661998,\n",
       "   -6.024504611398905,\n",
       "   -7.501068298156902,\n",
       "   -8.661372551548808,\n",
       "   -11.701916386215153,\n",
       "   -11.07055516301363,\n",
       "   -14.644388369866043,\n",
       "   -11.603695588562232,\n",
       "   -15.816459818469614,\n",
       "   -12.98944621724245,\n",
       "   -12.755615662779677,\n",
       "   -9.072746675581943,\n",
       "   -9.137879107596357,\n",
       "   -7.830033313859287,\n",
       "   -7.396848854026102,\n",
       "   -7.739263374008317,\n",
       "   -8.023927275905558,\n",
       "   -10.726826343467005,\n",
       "   -9.304991766985937,\n",
       "   -10.400920862693438,\n",
       "   -11.683991387397038,\n",
       "   -15.363564925895695,\n",
       "   -11.072832145358774,\n",
       "   -12.351239370343603,\n",
       "   -11.145008632476934,\n",
       "   -10.338199518172427,\n",
       "   -11.162871307511718,\n",
       "   -7.727528453581442,\n",
       "   -7.094861626240167,\n",
       "   -10.057317479385304,\n",
       "   -8.768470351654313,\n",
       "   -9.694626410729086,\n",
       "   -10.540108876224147,\n",
       "   -15.481916606632094,\n",
       "   -27.891381356257785,\n",
       "   -42.849320911521325,\n",
       "   -43.13180863557477,\n",
       "   -35.71766199855322,\n",
       "   -30.539159585045017,\n",
       "   -30.675502178977275,\n",
       "   -29.354411152985556,\n",
       "   -33.84597326772497,\n",
       "   -38.70774403176012,\n",
       "   -47.617548701197,\n",
       "   -34.902098026005966,\n",
       "   -28.451549674420527,\n",
       "   -30.261092440463045,\n",
       "   -22.605733029831182,\n",
       "   -14.537146012841518,\n",
       "   -14.952622515220687,\n",
       "   -11.194788551313012,\n",
       "   -12.020342401253577,\n",
       "   -11.78062227197163,\n",
       "   -13.412331456797395,\n",
       "   -14.750143989868794,\n",
       "   -9.506728589771837,\n",
       "   -10.613231685083102,\n",
       "   -8.754298218630158,\n",
       "   -8.732451766656302,\n",
       "   -8.317844408588224,\n",
       "   -8.138716871895019,\n",
       "   -8.395279482324973,\n",
       "   -9.067837274662239,\n",
       "   -6.089355712976705,\n",
       "   -6.228510784207952,\n",
       "   -7.544429617328891,\n",
       "   -7.937762365507812,\n",
       "   -6.863406105010348,\n",
       "   -6.976717530314051,\n",
       "   -6.896586642435792,\n",
       "   -7.6959413315333105,\n",
       "   -8.486850138106533,\n",
       "   -7.196309230504829,\n",
       "   -7.8167343353230425,\n",
       "   -8.440442209589504,\n",
       "   -9.289650759397402,\n",
       "   -8.982191147959977,\n",
       "   -7.925426331305985,\n",
       "   -8.265818678285177,\n",
       "   -10.063369785571597,\n",
       "   -8.51291567178764,\n",
       "   -7.0502409979584435,\n",
       "   -9.216281311870649,\n",
       "   -8.112350608477742,\n",
       "   -8.806233451757775,\n",
       "   -8.322988555987116,\n",
       "   -7.43980804627391,\n",
       "   -8.160193258168777,\n",
       "   -6.8543379807934715,\n",
       "   -7.559433345985919,\n",
       "   -10.640996525064175,\n",
       "   -11.283659905191445,\n",
       "   -11.469559783302893,\n",
       "   -11.069545327757275,\n",
       "   -14.14263337356256,\n",
       "   -13.418305323471671,\n",
       "   -11.755308726127883,\n",
       "   -13.387729478279814,\n",
       "   -19.951473759665518,\n",
       "   -15.584505260196117,\n",
       "   -14.847507107418195,\n",
       "   -11.76588893872819,\n",
       "   -9.67642558605454,\n",
       "   -8.982266667352905,\n",
       "   -9.7543614937424,\n",
       "   -7.919642897115073,\n",
       "   -9.542399388814783,\n",
       "   -14.799184458098518,\n",
       "   -19.913553422116973,\n",
       "   -20.363830923253463,\n",
       "   -19.16608938095809,\n",
       "   -18.481526268036394,\n",
       "   -17.002262564756247,\n",
       "   -17.050541634301677,\n",
       "   -15.986499166416397,\n",
       "   -13.792181884649196,\n",
       "   -17.72949755352623,\n",
       "   -20.373882488587366,\n",
       "   -19.022682029886486,\n",
       "   -20.061821157219967,\n",
       "   -30.152577173840598,\n",
       "   -34.44975476775135,\n",
       "   -22.643613450158348,\n",
       "   -20.28253572661622,\n",
       "   -21.49113712676457,\n",
       "   -27.535290324941364,\n",
       "   -27.077582671938547,\n",
       "   -24.37288043479968,\n",
       "   -16.46169113893422,\n",
       "   -22.90085394327972,\n",
       "   -23.03072173921697,\n",
       "   -16.831196605248138,\n",
       "   -19.659334014042194,\n",
       "   -22.79334853028547,\n",
       "   -27.556815525599262,\n",
       "   -27.84467851822031,\n",
       "   -27.893745355647457,\n",
       "   -22.59595903832271,\n",
       "   -24.103521556976077,\n",
       "   -20.89301953429678,\n",
       "   -18.23824799573628,\n",
       "   -16.87477101226606,\n",
       "   -14.713632556195517,\n",
       "   -14.65423403028531,\n",
       "   -14.906595996030312,\n",
       "   -16.508536943116564,\n",
       "   -13.776723838109756,\n",
       "   -15.49876460739305,\n",
       "   -13.93198470708292,\n",
       "   -16.265528277792335,\n",
       "   -17.44549775075637,\n",
       "   -19.570088395240717,\n",
       "   -18.865473841908834,\n",
       "   -23.464971863476716,\n",
       "   -30.88131770855136,\n",
       "   -25.056653735655694,\n",
       "   -33.42471277441738,\n",
       "   -28.27489538605524,\n",
       "   -24.954699658398177,\n",
       "   -25.8989193954915,\n",
       "   -25.02646300952617,\n",
       "   -20.402462735375465,\n",
       "   -28.89607356782573,\n",
       "   -23.431325775804122,\n",
       "   -23.59665267970918,\n",
       "   -23.717863585865903,\n",
       "   -25.561974873099754,\n",
       "   -23.95092337565131,\n",
       "   -22.046715989567172,\n",
       "   -22.16448584932794,\n",
       "   -28.745121993972305,\n",
       "   -27.034955955427648,\n",
       "   -20.78096274287167,\n",
       "   -17.593927981605763,\n",
       "   -18.804402372287107,\n",
       "   -17.157649303076035,\n",
       "   -20.02457291857332,\n",
       "   -16.43508277507697,\n",
       "   -16.067936704144813,\n",
       "   -14.741776316004044,\n",
       "   -18.85363598646257,\n",
       "   -15.27545354748775,\n",
       "   -15.238627604035528,\n",
       "   -16.55990951672288,\n",
       "   -17.645662880414786,\n",
       "   -22.827663714121996,\n",
       "   -24.37880951389563,\n",
       "   -21.93781808719953,\n",
       "   -15.84648037858436,\n",
       "   -22.305795498649328,\n",
       "   -16.98932472633981,\n",
       "   -12.245577846861572,\n",
       "   -14.766107980717438,\n",
       "   -17.135045042966514,\n",
       "   -14.40795097302093,\n",
       "   -13.326283386074735,\n",
       "   -11.080201975398744,\n",
       "   -10.435715157783648,\n",
       "   -10.217326843585962,\n",
       "   -14.063914887267627,\n",
       "   -11.680553357662957,\n",
       "   -12.053719374579254,\n",
       "   -12.275384660930643,\n",
       "   -17.764574036864005,\n",
       "   -18.110608140841023,\n",
       "   -21.61051077597732,\n",
       "   -16.61097426636921,\n",
       "   -14.072762210930279,\n",
       "   -11.43843453873117,\n",
       "   -9.181052818507352,\n",
       "   -10.690028598101827,\n",
       "   -12.1647530118376,\n",
       "   -9.601096742539088,\n",
       "   -11.233769410957711,\n",
       "   -13.646152503616939,\n",
       "   -10.437127439107885,\n",
       "   -9.805504669254184,\n",
       "   -9.164771324951275,\n",
       "   -8.525333982176765,\n",
       "   -7.812603618760068,\n",
       "   -10.538405820125924,\n",
       "   -9.724227798736544,\n",
       "   -11.491447386860068,\n",
       "   -13.094179067776041,\n",
       "   -10.019895314848602,\n",
       "   -10.202841972704674,\n",
       "   -10.207642024950479,\n",
       "   -10.331589077341805,\n",
       "   -12.707641573064253,\n",
       "   -18.383710520217882,\n",
       "   -12.980340884761048,\n",
       "   -19.990187261972693,\n",
       "   -19.573373358224657,\n",
       "   -19.104982776624645,\n",
       "   -15.937172001984258,\n",
       "   -11.4307612167453,\n",
       "   -10.000508168278525,\n",
       "   -11.787592622934069,\n",
       "   -9.216123303983528,\n",
       "   -12.904489542550747,\n",
       "   -14.420841605405728,\n",
       "   -12.924686671180163,\n",
       "   -15.10436805658179,\n",
       "   -12.431502580682054,\n",
       "   -15.25027019719683,\n",
       "   -11.650539098753608,\n",
       "   -10.049576475620556,\n",
       "   -10.999540632671891,\n",
       "   -7.5831369977599525,\n",
       "   -9.888875120868555,\n",
       "   -7.979235800384037,\n",
       "   -7.790883476949272,\n",
       "   -10.493002638369102,\n",
       "   -10.806410445961724,\n",
       "   -9.406165961417452,\n",
       "   -10.59615116082893,\n",
       "   -11.306069016399215,\n",
       "   -13.771541717914369,\n",
       "   -14.009938636908121,\n",
       "   -12.762194352083615,\n",
       "   -11.318236251394161,\n",
       "   -13.489319268888332,\n",
       "   -13.487030544645952,\n",
       "   -14.879239908222843,\n",
       "   -14.450608781878984,\n",
       "   -12.534508911699954,\n",
       "   -9.247121701081191,\n",
       "   -10.092225459695195,\n",
       "   -12.437664826343887,\n",
       "   -11.894634069897938,\n",
       "   -9.36948492930152,\n",
       "   -11.593364650236497,\n",
       "   -9.979060593018298,\n",
       "   -12.1371684622882,\n",
       "   -13.569725214506255,\n",
       "   -11.815051005433913,\n",
       "   -12.011290370079353,\n",
       "   -10.41436139040888,\n",
       "   -9.524701438318226,\n",
       "   -11.147172098939429,\n",
       "   -11.611556706029647,\n",
       "   -11.408974811805216,\n",
       "   -10.852855056657779,\n",
       "   -12.547027779990366,\n",
       "   -10.204514766699347,\n",
       "   -10.401004583228236,\n",
       "   -8.977284492885177,\n",
       "   -7.489183626017512,\n",
       "   -7.779293758427237,\n",
       "   -7.250311092486049,\n",
       "   -7.405490640761441,\n",
       "   -8.083098025046649,\n",
       "   -11.240826278171719,\n",
       "   -10.844410471378662,\n",
       "   -10.2967212708105,\n",
       "   -12.150948605402245,\n",
       "   -13.979964050006629,\n",
       "   -13.505446738184514,\n",
       "   -19.762009245140735,\n",
       "   -20.2204113624935,\n",
       "   -17.855643111945188,\n",
       "   -14.90554933838022,\n",
       "   -13.057418357284332,\n",
       "   -11.309271921796896,\n",
       "   -11.733308944559607,\n",
       "   -11.027651433534755,\n",
       "   -9.629330475770708,\n",
       "   -10.422436052992875,\n",
       "   -12.440426228590546,\n",
       "   -13.298536653474162,\n",
       "   -13.609526839405651,\n",
       "   -17.153910301705217,\n",
       "   -13.726877406628077,\n",
       "   -16.12902724207972,\n",
       "   -14.537632164935056,\n",
       "   -10.480009235861996,\n",
       "   -11.606463661772317,\n",
       "   -9.52467866846472,\n",
       "   -7.690425485927221,\n",
       "   -9.702003842038483,\n",
       "   -11.5841753245512,\n",
       "   -13.247789611931628,\n",
       "   -13.305698027977412,\n",
       "   -9.602625252966153,\n",
       "   -9.922334568785978,\n",
       "   -10.590334658424204,\n",
       "   -9.458348849631525,\n",
       "   -9.18583825627304,\n",
       "   -8.41613654402444,\n",
       "   -14.422582430838107,\n",
       "   -15.152135353215979,\n",
       "   -10.48954071825014,\n",
       "   -10.956061872193574,\n",
       "   -10.063929357317248,\n",
       "   -8.864836644824333,\n",
       "   -13.648327698344222,\n",
       "   -10.483351354847315,\n",
       "   -10.491148460940044,\n",
       "   -11.080127850282727,\n",
       "   -14.089489697755713,\n",
       "   -11.25949419467632,\n",
       "   -14.811814410247191,\n",
       "   -14.73156994227567,\n",
       "   -15.761884694066495,\n",
       "   -13.718090645474048,\n",
       "   -16.65439289601403,\n",
       "   -15.492214362262054,\n",
       "   -13.679922099684118,\n",
       "   -14.009025443054146,\n",
       "   -14.52724783113952,\n",
       "   -17.13779046591761,\n",
       "   -18.61877438521917,\n",
       "   -19.016072784715874,\n",
       "   -20.247576693846877,\n",
       "   -15.964706068752069,\n",
       "   -17.42114385827416,\n",
       "   -19.683104045009006,\n",
       "   -19.212224949243115,\n",
       "   -16.18790468941894,\n",
       "   -17.05679029388418,\n",
       "   -15.078955532664814,\n",
       "   -12.863938318183777,\n",
       "   -19.486833410994755,\n",
       "   -15.376335275708186,\n",
       "   -18.312800810068214,\n",
       "   -17.93102520770278,\n",
       "   -20.014843417236175,\n",
       "   -25.187649811578538,\n",
       "   -29.569697934691533,\n",
       "   -21.66958233684148,\n",
       "   -21.00545795737874,\n",
       "   -18.74060868801062,\n",
       "   -21.984208316576566,\n",
       "   -19.481166067625104,\n",
       "   -21.782461981639685,\n",
       "   -20.811665133062363,\n",
       "   -17.535024986440014,\n",
       "   -11.59810818470897,\n",
       "   -10.286004887841232,\n",
       "   -11.14423074724433,\n",
       "   -12.551510136470496,\n",
       "   -12.260712331342615,\n",
       "   -10.723657342960113,\n",
       "   -8.901668699838291,\n",
       "   -10.621320335241013,\n",
       "   -11.080582007077801,\n",
       "   -14.090154453145015,\n",
       "   -12.280089783145796,\n",
       "   -12.404927433926739,\n",
       "   -13.054333163168561,\n",
       "   -13.022023987270803,\n",
       "   -12.601878165421827,\n",
       "   -9.000679189050645,\n",
       "   -12.980247185341142,\n",
       "   -14.845096151868,\n",
       "   -10.165654507681923,\n",
       "   -14.647031225300061,\n",
       "   -13.737609577614537,\n",
       "   -14.855719787408596,\n",
       "   -14.22498352197996,\n",
       "   -10.865676311320954,\n",
       "   -9.718639279517404,\n",
       "   -9.58431007497899,\n",
       "   -9.90827667561012,\n",
       "   -12.73434191151875,\n",
       "   -12.446596466641006,\n",
       "   -14.866384429813104,\n",
       "   -15.57737504439891,\n",
       "   -13.23607612363611,\n",
       "   -14.536165732724513,\n",
       "   -12.854121011422441,\n",
       "   -9.558808197608123,\n",
       "   -10.19743134146477,\n",
       "   -10.618885008412963,\n",
       "   -11.376336111962441,\n",
       "   -10.309657113107004,\n",
       "   -9.494676643368772,\n",
       "   -7.530886123867338,\n",
       "   -8.643523396667709,\n",
       "   -10.243670955327989,\n",
       "   -11.135611071239364,\n",
       "   -10.349773027604915,\n",
       "   -10.170992811846498,\n",
       "   -12.096224476244235,\n",
       "   -12.893896905852312,\n",
       "   -13.98923051224355,\n",
       "   -16.844151943138254,\n",
       "   -17.535090127582,\n",
       "   -15.462371068163279,\n",
       "   -16.886293319736517,\n",
       "   -16.023520979046793,\n",
       "   -12.051400088686846,\n",
       "   -13.775495489028096,\n",
       "   -14.62840550933118],\n",
       "  'test_r2s': [-0.6567223081281532,\n",
       "   -0.3070636536993492,\n",
       "   -0.1648042088362378,\n",
       "   -0.21156806859976074,\n",
       "   -0.4452437112119765,\n",
       "   -13045.064581251883,\n",
       "   -165.27096121956805,\n",
       "   -30166.657168286165,\n",
       "   -357.9618788725737,\n",
       "   -110936.09522715211,\n",
       "   -54.61123401348519,\n",
       "   -115130.5789962543,\n",
       "   -2492.7833724094094,\n",
       "   -814.6680157865238,\n",
       "   -401.2274846738185,\n",
       "   -292.28367471814886,\n",
       "   -275.53006247721504,\n",
       "   -347.53240989816413,\n",
       "   -499.70238681722066,\n",
       "   -416.1599215631012,\n",
       "   -429.58438585886273,\n",
       "   -421.3495265782923,\n",
       "   -433.6451911665125,\n",
       "   -369.88999602434603,\n",
       "   -374.53983622830407,\n",
       "   -334.861885452889,\n",
       "   -328.57224722223293,\n",
       "   -314.96121299728316,\n",
       "   -327.96910773883457,\n",
       "   -204.77052919097085,\n",
       "   -263.51383591261134,\n",
       "   -277.4874136657593,\n",
       "   -266.38166633428654,\n",
       "   -265.00264311815204,\n",
       "   -269.0543506338279,\n",
       "   -320.4802998712594,\n",
       "   -355.69652230978807,\n",
       "   -392.31070716085037,\n",
       "   -313.52886858351224,\n",
       "   -309.16979039361274,\n",
       "   -362.02227056949897,\n",
       "   -319.1582627697554,\n",
       "   -263.9436807539801,\n",
       "   -243.34868501936378,\n",
       "   -303.7164901024924,\n",
       "   -292.29071753124396,\n",
       "   -281.9095303154866,\n",
       "   -295.62708244950034,\n",
       "   -266.632766449316,\n",
       "   -275.61264306768095,\n",
       "   -273.74621159833646,\n",
       "   -337.8553129671593,\n",
       "   -367.22308628515583,\n",
       "   -242.7535461894207,\n",
       "   -265.14402150055895,\n",
       "   -286.81672759645187,\n",
       "   -360.1947731501168,\n",
       "   -460.25006255805414,\n",
       "   -336.9849810479325,\n",
       "   -282.36661393041476,\n",
       "   -350.21356002477745,\n",
       "   -369.60774110119814,\n",
       "   -276.4824511762226,\n",
       "   -266.34113387275994,\n",
       "   -215.85593532593884,\n",
       "   -226.7741345346421,\n",
       "   -156.53029824904755,\n",
       "   -136.47815947799418,\n",
       "   -161.45680648846175,\n",
       "   -182.4700388948939,\n",
       "   -195.36295931148203,\n",
       "   -209.77721368696064,\n",
       "   -200.21142257309168,\n",
       "   -172.41292110510278,\n",
       "   -163.66451397837207,\n",
       "   -151.19411667587778,\n",
       "   -163.4493249462564,\n",
       "   -162.9954048519912,\n",
       "   -201.84720428614747,\n",
       "   -191.90463869658618,\n",
       "   -232.0028139530907,\n",
       "   -263.93991293851445,\n",
       "   -320.76193287823185,\n",
       "   -321.2837416644294,\n",
       "   -355.0427508860303,\n",
       "   -567.6325748337466,\n",
       "   -515.0999475378259,\n",
       "   -889.4486592024741,\n",
       "   -698.6567421238188,\n",
       "   -577.5278444474424,\n",
       "   -438.73144603413914,\n",
       "   -340.99440092106664,\n",
       "   -243.90471304011507,\n",
       "   -272.70893811637455,\n",
       "   -182.25368544788742,\n",
       "   -162.36547714495117,\n",
       "   -160.2447844047698,\n",
       "   -162.17595694762866,\n",
       "   -146.80665240478282,\n",
       "   -164.32603345298563,\n",
       "   -185.34180269937988,\n",
       "   -186.23530917688228,\n",
       "   -186.83803785225385,\n",
       "   -175.66189947188838,\n",
       "   -163.92738373959173,\n",
       "   -175.93537023553253,\n",
       "   -183.32577769410705,\n",
       "   -205.9311899939384,\n",
       "   -238.76476057808785,\n",
       "   -287.41204827077496,\n",
       "   -246.12981156915885,\n",
       "   -219.99155192486253,\n",
       "   -202.85347829556648,\n",
       "   -230.04681755158495,\n",
       "   -243.8521960410916,\n",
       "   -349.30977792036424,\n",
       "   -306.4658091606748,\n",
       "   -249.1479915238669,\n",
       "   -247.51581674796637,\n",
       "   -202.7957272012047,\n",
       "   -175.09849181039627,\n",
       "   -184.85522036582483,\n",
       "   -210.03061343498496,\n",
       "   -215.3357793464925,\n",
       "   -275.685706557199,\n",
       "   -281.55902231131773,\n",
       "   -287.1845657093958,\n",
       "   -305.5754080904655,\n",
       "   -338.401499563979,\n",
       "   -386.43795205441415,\n",
       "   -500.73389612661066,\n",
       "   -384.68843076987173,\n",
       "   -418.532874597274,\n",
       "   -491.8701860534835,\n",
       "   -552.482668213346,\n",
       "   -602.3970724933441,\n",
       "   -555.3607662845692,\n",
       "   -355.88147683549465,\n",
       "   -361.2857953329402,\n",
       "   -381.23704469811804,\n",
       "   -276.474476773034,\n",
       "   -389.615166385452,\n",
       "   -439.97018645981154,\n",
       "   -543.8525589288207,\n",
       "   -675.379877917088,\n",
       "   -634.4358229791972,\n",
       "   -758.3047584609886,\n",
       "   -561.4141025373013,\n",
       "   -454.4355620409817,\n",
       "   -529.3456505628083,\n",
       "   -507.91228365613716,\n",
       "   -357.8343729470894,\n",
       "   -320.5162036231218,\n",
       "   -297.32518830741697,\n",
       "   -329.89121708893407,\n",
       "   -435.8245275422952,\n",
       "   -367.2720342798621,\n",
       "   -422.69482076755554,\n",
       "   -382.7947196124003,\n",
       "   -414.70381740227134,\n",
       "   -358.27890492167506,\n",
       "   -334.10003466736697,\n",
       "   -382.09135736948775,\n",
       "   -482.07900029405897,\n",
       "   -437.33165639681846,\n",
       "   -441.5365062301763,\n",
       "   -343.9043387508452,\n",
       "   -451.79705528398426,\n",
       "   -400.3065575874062,\n",
       "   -336.0704432335622,\n",
       "   -315.46229924938643,\n",
       "   -319.5906054573835,\n",
       "   -476.99544823272186,\n",
       "   -417.8322973254917,\n",
       "   -371.19561432261287,\n",
       "   -321.8644645139668,\n",
       "   -274.15680997134933,\n",
       "   -243.84119447082978,\n",
       "   -206.83590223326527,\n",
       "   -220.04641865473516,\n",
       "   -267.06636310620604,\n",
       "   -233.2687520339136,\n",
       "   -276.59456775329835,\n",
       "   -290.7873331330799,\n",
       "   -306.277839667828,\n",
       "   -285.3409613122829,\n",
       "   -310.61220192576343,\n",
       "   -334.37279978165674,\n",
       "   -364.2747319027144,\n",
       "   -362.5428102088216,\n",
       "   -407.5978996995855,\n",
       "   -491.7641016780147,\n",
       "   -471.58877658420414,\n",
       "   -592.5884848131617,\n",
       "   -431.58568025681257,\n",
       "   -611.8119160357469,\n",
       "   -610.3516324285233,\n",
       "   -510.46934395313383,\n",
       "   -402.40199955973856,\n",
       "   -380.7920017296643,\n",
       "   -536.3817627945102,\n",
       "   -647.182233751534,\n",
       "   -744.5632507889978,\n",
       "   -683.0654497904508,\n",
       "   -795.3772493674279,\n",
       "   -822.9796290415343,\n",
       "   -654.8660022377928,\n",
       "   -605.2923369941486,\n",
       "   -603.4860251501844,\n",
       "   -640.2551056105644,\n",
       "   -552.5375732053162,\n",
       "   -517.5108275158693,\n",
       "   -334.56938740834175,\n",
       "   -602.3704175978944,\n",
       "   -421.36086422779795,\n",
       "   -410.22508267808473,\n",
       "   -407.37144423817125,\n",
       "   -552.5773040991506,\n",
       "   -555.1138972575856,\n",
       "   -451.0747388980331,\n",
       "   -562.7256966914122,\n",
       "   -526.3926326862463,\n",
       "   -569.6405612684958,\n",
       "   -595.7350094598868,\n",
       "   -598.403269728088,\n",
       "   -557.651754541836,\n",
       "   -534.6484444960029,\n",
       "   -509.2408704305185,\n",
       "   -541.7554575675276,\n",
       "   -595.561866241909,\n",
       "   -419.9326945996147,\n",
       "   -425.36906002723873,\n",
       "   -502.17443696108796,\n",
       "   -455.48628714682417,\n",
       "   -331.20062507310104,\n",
       "   -480.72593655597336,\n",
       "   -443.62402795505756,\n",
       "   -526.1912610759783,\n",
       "   -456.40435523060023,\n",
       "   -397.4600875705543,\n",
       "   -469.00922312849036,\n",
       "   -572.9364400423086,\n",
       "   -407.3261934585591,\n",
       "   -408.57881006174136,\n",
       "   -330.79441642328186,\n",
       "   -268.9314981079105,\n",
       "   -318.60561038301233,\n",
       "   -246.8283215904632,\n",
       "   -324.65418148431587,\n",
       "   -344.3733897804052,\n",
       "   -325.43010124697383,\n",
       "   -300.235786113156,\n",
       "   -367.14296411668437,\n",
       "   -355.95792742520734,\n",
       "   -388.9626669053327,\n",
       "   -439.5115905032943,\n",
       "   -554.2770456565679,\n",
       "   -568.5807602107105,\n",
       "   -565.5615775265535,\n",
       "   -522.1063343304709,\n",
       "   -485.6914321519411,\n",
       "   -413.29999014107125,\n",
       "   -502.9418943283139,\n",
       "   -630.0742890241994,\n",
       "   -527.6078698736374,\n",
       "   -558.7857007469897,\n",
       "   -633.9432949930336,\n",
       "   -692.136966395317,\n",
       "   -570.8402567541516,\n",
       "   -413.6064830367914,\n",
       "   -440.8699768477586,\n",
       "   -443.8931192779159,\n",
       "   -408.54964369431366,\n",
       "   -342.645723778842,\n",
       "   -328.28141286323876,\n",
       "   -243.10018523536223,\n",
       "   -286.5437182677972,\n",
       "   -298.97917701348564,\n",
       "   -255.45290723103687,\n",
       "   -274.18328928967026,\n",
       "   -281.73060888793873,\n",
       "   -243.2780548448987,\n",
       "   -232.26238202904463,\n",
       "   -308.7889370128044,\n",
       "   -252.4368516985058,\n",
       "   -392.313654798016,\n",
       "   -489.880087141419,\n",
       "   -401.6559985870795,\n",
       "   -483.81502711966937,\n",
       "   -471.06591771776755,\n",
       "   -423.7348147071318,\n",
       "   -465.2919291619927,\n",
       "   -478.39192574753315,\n",
       "   -577.9177835156426,\n",
       "   -453.16936378381376,\n",
       "   -499.0443062787367,\n",
       "   -505.6177951882974,\n",
       "   -633.7480765420606,\n",
       "   -610.591281560426,\n",
       "   -530.3873462685792,\n",
       "   -672.3284051809485,\n",
       "   -576.4799895819499,\n",
       "   -626.8142113313681,\n",
       "   -532.6031757783799,\n",
       "   -452.88617506641145,\n",
       "   -537.9502898407502,\n",
       "   -523.080892375264,\n",
       "   -615.4997638604249,\n",
       "   -527.6771936483237,\n",
       "   -637.1679972897005,\n",
       "   -617.44917505681,\n",
       "   -517.6217771672767,\n",
       "   -415.6122800934516,\n",
       "   -426.36463173440114,\n",
       "   -490.3577935278301,\n",
       "   -575.0363248980997,\n",
       "   -424.6151149272018,\n",
       "   -416.42860583953166,\n",
       "   -424.5584857216497,\n",
       "   -499.071957270932,\n",
       "   -493.9221060670986,\n",
       "   -542.0537388740981,\n",
       "   -642.2073367597807,\n",
       "   -701.5342166335174,\n",
       "   -635.598397114616,\n",
       "   -627.8519924637451,\n",
       "   -436.6359854745002,\n",
       "   -565.9388759036912,\n",
       "   -618.963416104199,\n",
       "   -560.5861589117181,\n",
       "   -469.64063861088977,\n",
       "   -411.8429020521619,\n",
       "   -444.3785071531633,\n",
       "   -497.5375675696281,\n",
       "   -427.75397914210015,\n",
       "   -374.1303479080911,\n",
       "   -344.4088946597305,\n",
       "   -449.7457569951242,\n",
       "   -524.0031350281195,\n",
       "   -515.5350783948505,\n",
       "   -470.6284128515657,\n",
       "   -520.1573662323111,\n",
       "   -498.85993127235963,\n",
       "   -610.2407126182532,\n",
       "   -531.5677810444478,\n",
       "   -445.74686911332316,\n",
       "   -402.28003367539895,\n",
       "   -377.06864491789713,\n",
       "   -403.5791640175222,\n",
       "   -409.3886129994706,\n",
       "   -443.724638614166,\n",
       "   -422.6635573962451,\n",
       "   -527.3474418391723,\n",
       "   -511.2653506971255,\n",
       "   -542.7549257885016,\n",
       "   -522.5560697600624,\n",
       "   -574.0977885717317,\n",
       "   -578.5281169165811,\n",
       "   -641.2566893962234,\n",
       "   -602.9639560410216,\n",
       "   -468.4869005802135,\n",
       "   -601.1548833937692,\n",
       "   -607.129911333469,\n",
       "   -481.78270790345715,\n",
       "   -496.6051607189359,\n",
       "   -512.5447794679826,\n",
       "   -394.4077771715996,\n",
       "   -392.063007370983,\n",
       "   -346.42456723720926,\n",
       "   -272.26796993673446,\n",
       "   -337.91662462670155,\n",
       "   -461.9408661650376,\n",
       "   -538.5247504285344,\n",
       "   -471.6165935156455,\n",
       "   -352.3665376342068,\n",
       "   -438.49549555115595,\n",
       "   -512.5509333130578,\n",
       "   -475.0792124587376,\n",
       "   -596.6753485291417,\n",
       "   -437.1637516264225,\n",
       "   -401.2923116144942,\n",
       "   -355.1543674587633,\n",
       "   -317.9906131767497,\n",
       "   -346.5193979453165,\n",
       "   -227.98016266272919,\n",
       "   -327.53303538381556,\n",
       "   -329.6275635486638,\n",
       "   -360.51209953160486,\n",
       "   -317.92964233281936,\n",
       "   -232.9439873474346,\n",
       "   -281.7300345537301,\n",
       "   -315.6701094579413,\n",
       "   -390.843512660468,\n",
       "   -382.1218331222153,\n",
       "   -522.000916294478,\n",
       "   -349.0473871775221,\n",
       "   -292.13849179313013,\n",
       "   -313.1489107138352,\n",
       "   -299.4118347351349,\n",
       "   -296.5717351792237,\n",
       "   -327.9530247741054,\n",
       "   -397.4570013488193,\n",
       "   -411.7011012499577,\n",
       "   -350.9902037494062,\n",
       "   -325.6781455179566,\n",
       "   -254.41878198479105,\n",
       "   -188.0340317364373,\n",
       "   -236.55756031737326,\n",
       "   -306.5137816204914,\n",
       "   -351.4000136602348,\n",
       "   -251.46139971748505,\n",
       "   -225.25874907774917,\n",
       "   -216.00878040634564,\n",
       "   -222.4605983912608,\n",
       "   -218.70730915907342,\n",
       "   -253.24243609036785,\n",
       "   -276.0676132544794,\n",
       "   -312.7554368413637,\n",
       "   -250.04729481663037,\n",
       "   -291.8805940171582,\n",
       "   -304.22557405344435,\n",
       "   -453.17155760189405,\n",
       "   -485.4780725153705,\n",
       "   -390.99989020242543,\n",
       "   -332.03981447381534,\n",
       "   -336.3730080600602,\n",
       "   -337.07046193272186,\n",
       "   -305.3477783604734,\n",
       "   -356.4817056803154,\n",
       "   -384.1795750154629,\n",
       "   -394.0884997704521,\n",
       "   -329.86107022645547,\n",
       "   -282.12315038202973,\n",
       "   -340.5096639287538,\n",
       "   -457.9705546560936,\n",
       "   -379.3080137444136,\n",
       "   -294.2175162916001,\n",
       "   -327.5304180776131,\n",
       "   -337.8926869891551,\n",
       "   -460.7099702461607,\n",
       "   -487.75513291075185,\n",
       "   -556.3882731930614,\n",
       "   -545.8587256464623,\n",
       "   -576.7261346816272,\n",
       "   -471.8332050507355,\n",
       "   -536.6314207377777,\n",
       "   -489.31047090551624,\n",
       "   -368.3903431404412,\n",
       "   -494.4619687894226,\n",
       "   -541.3841664266039,\n",
       "   -578.5519327867145,\n",
       "   -615.9243779926823,\n",
       "   -439.0689531233042,\n",
       "   -466.20504860173065,\n",
       "   -588.2345455035052,\n",
       "   -395.40934830226433,\n",
       "   -455.2314855485752,\n",
       "   -500.86694886616266,\n",
       "   -453.1884887753525,\n",
       "   -431.3330205221214,\n",
       "   -310.2162079144397,\n",
       "   -295.08468562309434,\n",
       "   -371.7501914235475,\n",
       "   -424.69139155755875,\n",
       "   -422.1759869092334,\n",
       "   -424.3986773581209,\n",
       "   -395.0748238342208,\n",
       "   -381.8914680414257,\n",
       "   -323.34197612234743,\n",
       "   -341.6866633313646,\n",
       "   -355.4474667691538,\n",
       "   -260.75300341024575,\n",
       "   -213.47899704579677,\n",
       "   -241.57956716614083,\n",
       "   -192.6681686547124,\n",
       "   -187.989878355317,\n",
       "   -197.61918880578585,\n",
       "   -211.66930479532618,\n",
       "   -242.01651760409757,\n",
       "   -224.11969035682802,\n",
       "   -284.9684068733095,\n",
       "   -278.2559655425686,\n",
       "   -303.0797206386759,\n",
       "   -340.10917830211406,\n",
       "   -326.0840384002417,\n",
       "   -368.9470862382993,\n",
       "   -354.671196803071,\n",
       "   -334.4334585985862,\n",
       "   -316.6848550687845,\n",
       "   -373.82591081269896,\n",
       "   -295.0535168450753,\n",
       "   -404.1214552123082,\n",
       "   -415.69677630040843,\n",
       "   -404.7934795776595,\n",
       "   -523.0757696069569,\n",
       "   -687.9461793799628,\n",
       "   -518.6234487863148,\n",
       "   -309.38542813953654,\n",
       "   -289.0716137226236,\n",
       "   -290.6103860847855],\n",
       "  'train_pce_loss': [0.5472414493560791,\n",
       "   0.5972881317138672,\n",
       "   0.6824448108673096,\n",
       "   0.5627231001853943,\n",
       "   0.5072490572929382,\n",
       "   0.6644290089607239,\n",
       "   0.5566200613975525,\n",
       "   0.5274447798728943,\n",
       "   0.5970311164855957,\n",
       "   0.5136229991912842,\n",
       "   0.6107292771339417,\n",
       "   0.6466804146766663,\n",
       "   0.3879219889640808,\n",
       "   0.7011987566947937,\n",
       "   0.4971733093261719,\n",
       "   0.6221417188644409,\n",
       "   0.6326499581336975,\n",
       "   0.496301531791687,\n",
       "   0.38635194301605225,\n",
       "   0.4796072840690613,\n",
       "   0.5470399856567383,\n",
       "   0.5451346635818481,\n",
       "   0.4984380900859833,\n",
       "   0.44132906198501587,\n",
       "   0.6629830002784729,\n",
       "   0.6586707830429077,\n",
       "   0.48485511541366577,\n",
       "   0.5472026467323303,\n",
       "   0.5884944200515747,\n",
       "   0.8003556132316589,\n",
       "   0.5526166558265686,\n",
       "   0.5090090036392212,\n",
       "   0.4547724425792694,\n",
       "   0.4124903380870819,\n",
       "   0.5632073283195496,\n",
       "   0.5013020038604736,\n",
       "   0.5081431269645691,\n",
       "   0.5746269822120667,\n",
       "   0.5143038034439087,\n",
       "   0.42912763357162476,\n",
       "   0.494789719581604,\n",
       "   0.49492552876472473,\n",
       "   0.5582360029220581,\n",
       "   0.5472602844238281,\n",
       "   0.5126894116401672,\n",
       "   0.6075958609580994,\n",
       "   0.4445839822292328,\n",
       "   0.4842660427093506,\n",
       "   0.5325034260749817,\n",
       "   0.49967411160469055,\n",
       "   0.4653342366218567,\n",
       "   0.5370359420776367,\n",
       "   0.5136153697967529,\n",
       "   0.5886011719703674,\n",
       "   0.3381350040435791,\n",
       "   0.5936468243598938,\n",
       "   0.6980845332145691,\n",
       "   0.488757848739624,\n",
       "   0.6398667693138123,\n",
       "   0.46698763966560364,\n",
       "   0.5321677327156067,\n",
       "   0.7410483360290527,\n",
       "   0.4821131229400635,\n",
       "   0.5325578451156616,\n",
       "   0.6906511783599854,\n",
       "   0.47728249430656433,\n",
       "   0.5411444306373596,\n",
       "   0.49107182025909424,\n",
       "   0.5975425243377686,\n",
       "   0.4553390145301819,\n",
       "   0.4494827687740326,\n",
       "   0.39711248874664307,\n",
       "   0.4806223213672638,\n",
       "   0.547166109085083,\n",
       "   0.5303013920783997,\n",
       "   0.7184227108955383,\n",
       "   0.5609338283538818,\n",
       "   0.7189249992370605,\n",
       "   0.6618272662162781,\n",
       "   0.5255582928657532,\n",
       "   0.71357262134552,\n",
       "   0.43237507343292236,\n",
       "   0.3991537392139435,\n",
       "   0.5169525742530823,\n",
       "   0.7495150566101074,\n",
       "   0.5332184433937073,\n",
       "   0.6814844608306885,\n",
       "   0.4065507650375366,\n",
       "   0.4056340157985687,\n",
       "   0.4653140604496002,\n",
       "   0.5891404151916504,\n",
       "   0.6491560935974121,\n",
       "   0.675395667552948,\n",
       "   0.48506954312324524,\n",
       "   0.7453027963638306,\n",
       "   0.601929247379303,\n",
       "   0.6419180631637573,\n",
       "   0.4419446289539337,\n",
       "   0.6061520576477051,\n",
       "   0.5960171222686768,\n",
       "   0.7209951281547546,\n",
       "   0.5578224658966064,\n",
       "   0.5753297209739685,\n",
       "   0.582831621170044,\n",
       "   0.5626593232154846,\n",
       "   0.6355476975440979,\n",
       "   0.8024658560752869,\n",
       "   0.6172499656677246,\n",
       "   0.5557147264480591,\n",
       "   0.7248621582984924,\n",
       "   0.6189205050468445,\n",
       "   0.6082600355148315,\n",
       "   0.6704208850860596,\n",
       "   0.5070440173149109,\n",
       "   0.39276906847953796,\n",
       "   0.4657663106918335,\n",
       "   0.4256771206855774,\n",
       "   0.48296013474464417,\n",
       "   0.5562502145767212,\n",
       "   0.361136257648468,\n",
       "   0.4771505892276764,\n",
       "   0.4919497072696686,\n",
       "   0.6431284546852112,\n",
       "   0.4970466196537018,\n",
       "   0.4565558433532715,\n",
       "   0.44375360012054443,\n",
       "   0.48529052734375,\n",
       "   0.5789986848831177,\n",
       "   0.4478515684604645,\n",
       "   0.48574769496917725,\n",
       "   0.6202910542488098,\n",
       "   0.5112573504447937,\n",
       "   0.5282022953033447,\n",
       "   0.6537190675735474,\n",
       "   0.6306173801422119,\n",
       "   0.5622043013572693,\n",
       "   0.6249890923500061,\n",
       "   0.5602396130561829,\n",
       "   0.7265071272850037,\n",
       "   0.46383383870124817,\n",
       "   0.593355119228363,\n",
       "   0.7864614129066467,\n",
       "   0.5030177235603333,\n",
       "   0.4720844030380249,\n",
       "   0.5536904335021973,\n",
       "   0.467916339635849,\n",
       "   0.5044846534729004,\n",
       "   0.5960111618041992,\n",
       "   0.5563556551933289,\n",
       "   0.4439084529876709,\n",
       "   0.6041019558906555,\n",
       "   0.4579800069332123,\n",
       "   0.5821260809898376,\n",
       "   0.5661802291870117,\n",
       "   0.46431681513786316,\n",
       "   0.5585255026817322,\n",
       "   0.4649083614349365,\n",
       "   0.49040162563323975,\n",
       "   0.4996037483215332,\n",
       "   0.4200453758239746,\n",
       "   0.6776633262634277,\n",
       "   0.5463745594024658,\n",
       "   0.6137832403182983,\n",
       "   0.5060800313949585,\n",
       "   0.49876534938812256,\n",
       "   0.6181480884552002,\n",
       "   0.4804748296737671,\n",
       "   0.5371545553207397,\n",
       "   0.5287664532661438,\n",
       "   0.5662639141082764,\n",
       "   0.5205097794532776,\n",
       "   0.4392237365245819,\n",
       "   0.681738018989563,\n",
       "   0.5119760632514954,\n",
       "   0.5430861711502075,\n",
       "   0.39506298303604126,\n",
       "   0.5983654260635376,\n",
       "   0.7752416133880615,\n",
       "   0.5817903280258179,\n",
       "   0.5393487811088562,\n",
       "   0.48188167810440063,\n",
       "   0.569948673248291,\n",
       "   0.5426077842712402,\n",
       "   0.5338871479034424,\n",
       "   0.5223701000213623,\n",
       "   0.5340991616249084,\n",
       "   0.5350359678268433,\n",
       "   0.4481082558631897,\n",
       "   0.45996206998825073,\n",
       "   0.6286540627479553,\n",
       "   0.501685380935669,\n",
       "   0.4199312627315521,\n",
       "   0.5872820019721985,\n",
       "   0.4812000095844269,\n",
       "   0.46940749883651733,\n",
       "   0.41678929328918457,\n",
       "   0.43233683705329895,\n",
       "   0.603776216506958,\n",
       "   0.5357359051704407,\n",
       "   0.6259092092514038,\n",
       "   0.46905291080474854,\n",
       "   0.5470923185348511,\n",
       "   0.4229976236820221,\n",
       "   0.5452064275741577,\n",
       "   0.464240163564682,\n",
       "   0.5070261359214783,\n",
       "   0.6748135685920715,\n",
       "   0.629776120185852,\n",
       "   0.49984243512153625,\n",
       "   0.6557531356811523,\n",
       "   0.4993141293525696,\n",
       "   0.48154520988464355,\n",
       "   0.6413210034370422,\n",
       "   0.44426825642585754,\n",
       "   0.44660934805870056,\n",
       "   0.5118323564529419,\n",
       "   0.4716445505619049,\n",
       "   0.4309372007846832,\n",
       "   0.46498948335647583,\n",
       "   0.461147665977478,\n",
       "   0.5662919282913208,\n",
       "   0.583297848701477,\n",
       "   0.7231675982475281,\n",
       "   0.6257791519165039,\n",
       "   0.4664399325847626,\n",
       "   0.49398839473724365,\n",
       "   0.41522863507270813,\n",
       "   0.7149975895881653,\n",
       "   0.5932593941688538,\n",
       "   0.7695441842079163,\n",
       "   0.47894906997680664,\n",
       "   0.4735262095928192,\n",
       "   0.4499569237232208,\n",
       "   0.513547956943512,\n",
       "   0.6185034513473511,\n",
       "   0.5197755694389343,\n",
       "   0.5591362714767456,\n",
       "   0.42831283807754517,\n",
       "   0.5061185956001282,\n",
       "   0.5068968534469604,\n",
       "   0.5042456388473511,\n",
       "   0.47604304552078247,\n",
       "   0.5235899090766907,\n",
       "   0.45856764912605286,\n",
       "   0.7154949903488159,\n",
       "   0.5695172548294067,\n",
       "   0.36389443278312683,\n",
       "   0.4797523617744446,\n",
       "   0.5364722013473511,\n",
       "   0.6631730794906616,\n",
       "   0.7510430216789246,\n",
       "   0.4704582393169403,\n",
       "   0.5571296215057373,\n",
       "   0.3952501714229584,\n",
       "   0.5657253265380859,\n",
       "   0.554408848285675,\n",
       "   0.601118803024292,\n",
       "   0.5958077907562256,\n",
       "   0.4595315754413605,\n",
       "   0.4401783049106598,\n",
       "   0.5414987206459045,\n",
       "   0.49495330452919006,\n",
       "   0.5764082074165344,\n",
       "   0.47742533683776855,\n",
       "   0.5969136953353882,\n",
       "   0.5783422589302063,\n",
       "   0.5449678301811218,\n",
       "   0.45725730061531067,\n",
       "   0.4663330018520355,\n",
       "   0.5807485580444336,\n",
       "   0.5339030623435974,\n",
       "   0.597499430179596,\n",
       "   0.4825514554977417,\n",
       "   0.4955648183822632,\n",
       "   0.5633826851844788,\n",
       "   0.6285952925682068,\n",
       "   0.492669939994812,\n",
       "   0.41809946298599243,\n",
       "   0.615329384803772,\n",
       "   0.4767702519893646,\n",
       "   0.6691008806228638,\n",
       "   0.3882909119129181,\n",
       "   0.48724502325057983,\n",
       "   0.49325698614120483,\n",
       "   0.5446730852127075,\n",
       "   0.51981520652771,\n",
       "   0.5012427568435669,\n",
       "   0.6838449835777283,\n",
       "   0.4319974482059479,\n",
       "   0.5774136185646057,\n",
       "   0.4834153950214386,\n",
       "   0.4597477316856384,\n",
       "   0.4965265095233917,\n",
       "   0.4640626311302185,\n",
       "   0.7916422486305237,\n",
       "   0.3894350230693817,\n",
       "   0.4572351276874542,\n",
       "   0.4335230588912964,\n",
       "   0.504530131816864,\n",
       "   0.5788892507553101,\n",
       "   0.5089466571807861,\n",
       "   0.5287023186683655,\n",
       "   0.41027671098709106,\n",
       "   0.48022279143333435,\n",
       "   0.556398868560791,\n",
       "   0.47136345505714417,\n",
       "   0.48598822951316833,\n",
       "   0.6565163135528564,\n",
       "   0.5802669525146484,\n",
       "   0.5103914141654968,\n",
       "   0.5189226865768433,\n",
       "   0.5177962779998779,\n",
       "   0.8387622237205505,\n",
       "   0.48048651218414307,\n",
       "   0.4636327028274536,\n",
       "   0.4233221709728241,\n",
       "   0.5825160145759583,\n",
       "   0.4691658914089203,\n",
       "   0.395364373922348,\n",
       "   0.4526480436325073,\n",
       "   0.601161539554596,\n",
       "   0.34509047865867615,\n",
       "   0.40095600485801697,\n",
       "   0.6902133226394653,\n",
       "   0.4956052601337433,\n",
       "   0.4979550540447235,\n",
       "   0.45641008019447327,\n",
       "   0.6033174991607666,\n",
       "   0.7022050619125366,\n",
       "   0.5724241137504578,\n",
       "   0.4927324056625366,\n",
       "   0.6561297178268433,\n",
       "   0.5455236434936523,\n",
       "   0.574194610118866,\n",
       "   0.510267436504364,\n",
       "   0.6596676707267761,\n",
       "   0.4801521301269531,\n",
       "   0.42266103625297546,\n",
       "   0.5991345047950745,\n",
       "   0.5911785364151001,\n",
       "   0.7560029029846191,\n",
       "   0.43796730041503906,\n",
       "   0.5701500177383423,\n",
       "   0.5366730690002441,\n",
       "   0.6664984822273254,\n",
       "   0.513190746307373,\n",
       "   0.39039963483810425,\n",
       "   0.49791058897972107,\n",
       "   0.6897695660591125,\n",
       "   0.5339102745056152,\n",
       "   0.5181307196617126,\n",
       "   0.7057507634162903,\n",
       "   0.4581490755081177,\n",
       "   0.5456541180610657,\n",
       "   0.44497543573379517,\n",
       "   0.5215203166007996,\n",
       "   0.49234122037887573,\n",
       "   0.42641139030456543,\n",
       "   0.4699007570743561,\n",
       "   0.4450746774673462,\n",
       "   0.4811546206474304,\n",
       "   0.5180264711380005,\n",
       "   0.5641005039215088,\n",
       "   0.502103865146637,\n",
       "   0.49958163499832153,\n",
       "   0.47934651374816895,\n",
       "   0.494640052318573,\n",
       "   0.5018125772476196,\n",
       "   0.5587443709373474,\n",
       "   0.5223219394683838,\n",
       "   0.5728528499603271,\n",
       "   0.39174380898475647,\n",
       "   0.47708842158317566,\n",
       "   0.5645620226860046,\n",
       "   0.5148317217826843,\n",
       "   0.5584886074066162,\n",
       "   0.5165888071060181,\n",
       "   0.4588221609592438,\n",
       "   0.4335004985332489,\n",
       "   0.4317331612110138,\n",
       "   0.48849165439605713,\n",
       "   0.4442867338657379,\n",
       "   0.5388767123222351,\n",
       "   0.3241608142852783,\n",
       "   0.6120964288711548,\n",
       "   0.47151538729667664,\n",
       "   0.47326597571372986,\n",
       "   0.4995284080505371,\n",
       "   0.5316511988639832,\n",
       "   0.44789907336235046,\n",
       "   0.5266556143760681,\n",
       "   0.530261754989624,\n",
       "   0.5033231377601624,\n",
       "   0.7931004166603088,\n",
       "   0.5240955948829651,\n",
       "   0.5992159247398376,\n",
       "   0.4780429005622864,\n",
       "   0.6337765455245972,\n",
       "   0.4269544780254364,\n",
       "   0.4175264537334442,\n",
       "   0.6011954545974731,\n",
       "   0.4019750952720642,\n",
       "   0.5959144830703735,\n",
       "   0.5534785985946655,\n",
       "   0.5589139461517334,\n",
       "   0.619694173336029,\n",
       "   0.6101344227790833,\n",
       "   0.542233943939209,\n",
       "   0.5343668460845947,\n",
       "   0.5981289744377136,\n",
       "   0.4068320393562317,\n",
       "   0.4716344475746155,\n",
       "   0.43126943707466125,\n",
       "   0.5397936105728149,\n",
       "   0.5435054302215576,\n",
       "   0.526421308517456,\n",
       "   0.4215403199195862,\n",
       "   0.42592430114746094,\n",
       "   0.5829143524169922,\n",
       "   0.5277187824249268,\n",
       "   0.4994162321090698,\n",
       "   0.4070667028427124,\n",
       "   0.6688482165336609,\n",
       "   0.5907912850379944,\n",
       "   0.4171919524669647,\n",
       "   0.4389415681362152,\n",
       "   0.49172359704971313,\n",
       "   0.5579389929771423,\n",
       "   0.5386661887168884,\n",
       "   0.6192634105682373,\n",
       "   0.5563650727272034,\n",
       "   0.4371185302734375,\n",
       "   0.5696565508842468,\n",
       "   0.5658446550369263,\n",
       "   0.5686613321304321,\n",
       "   0.461408406496048,\n",
       "   0.4065350890159607,\n",
       "   0.5000517964363098,\n",
       "   0.3896198570728302,\n",
       "   0.5683364272117615,\n",
       "   0.48624035716056824,\n",
       "   0.4104115962982178,\n",
       "   0.4192943572998047,\n",
       "   0.6498435735702515,\n",
       "   0.4663833677768707,\n",
       "   0.4241887032985687,\n",
       "   0.6878117322921753,\n",
       "   0.5455045104026794,\n",
       "   0.5523099899291992,\n",
       "   0.5581764578819275,\n",
       "   0.6112410426139832,\n",
       "   0.48937490582466125,\n",
       "   0.6064413189888,\n",
       "   0.5844379663467407,\n",
       "   0.466024786233902,\n",
       "   0.4347268044948578,\n",
       "   0.5195916295051575,\n",
       "   0.5680505633354187,\n",
       "   0.44259998202323914,\n",
       "   0.5664685368537903,\n",
       "   0.7471351623535156,\n",
       "   0.4470589756965637,\n",
       "   0.4987437129020691,\n",
       "   0.5302962064743042,\n",
       "   0.4896008372306824,\n",
       "   0.43134331703186035,\n",
       "   0.5247602462768555,\n",
       "   0.4935615658760071,\n",
       "   0.5427507758140564,\n",
       "   0.5515078902244568,\n",
       "   0.40261906385421753,\n",
       "   0.4786636531352997,\n",
       "   0.5457549095153809,\n",
       "   0.37485143542289734,\n",
       "   0.5211858153343201,\n",
       "   0.42334669828414917,\n",
       "   0.5924404263496399,\n",
       "   0.6222485899925232,\n",
       "   0.4392162263393402,\n",
       "   0.6019743084907532,\n",
       "   0.4824194610118866,\n",
       "   0.5399823188781738,\n",
       "   0.44152796268463135,\n",
       "   0.4391043782234192,\n",
       "   0.39225122332572937,\n",
       "   0.4363223612308502,\n",
       "   0.48746582865715027,\n",
       "   0.44858843088150024,\n",
       "   0.4225430488586426,\n",
       "   0.5370061993598938,\n",
       "   0.4250073730945587,\n",
       "   0.4804760217666626,\n",
       "   0.5185847878456116,\n",
       "   0.5177003145217896,\n",
       "   0.5919705033302307,\n",
       "   0.48824718594551086,\n",
       "   0.4377079904079437,\n",
       "   0.47387734055519104,\n",
       "   0.4759449362754822,\n",
       "   0.5289060473442078],\n",
       "  'train_voc_loss': [1.6143884658813477,\n",
       "   0.9708929061889648,\n",
       "   0.9312894344329834,\n",
       "   0.9335161447525024,\n",
       "   1.035402774810791,\n",
       "   0.8007535934448242,\n",
       "   1.1843881607055664,\n",
       "   0.8353737592697144,\n",
       "   0.7779126167297363,\n",
       "   0.839728057384491,\n",
       "   0.8691160082817078,\n",
       "   1.2398160696029663,\n",
       "   0.987999677658081,\n",
       "   1.065769076347351,\n",
       "   0.9628243446350098,\n",
       "   0.9268867373466492,\n",
       "   0.669861912727356,\n",
       "   0.6903830766677856,\n",
       "   0.8940909504890442,\n",
       "   1.0386536121368408,\n",
       "   1.1201255321502686,\n",
       "   0.6986714005470276,\n",
       "   1.223137617111206,\n",
       "   0.8267581462860107,\n",
       "   0.8484890460968018,\n",
       "   0.9242057204246521,\n",
       "   0.8900299668312073,\n",
       "   1.2151309251785278,\n",
       "   1.4024652242660522,\n",
       "   1.1663124561309814,\n",
       "   0.9559712409973145,\n",
       "   0.7431989312171936,\n",
       "   1.0497729778289795,\n",
       "   1.0456762313842773,\n",
       "   0.5829839110374451,\n",
       "   0.6244081854820251,\n",
       "   1.2930796146392822,\n",
       "   0.7132350206375122,\n",
       "   0.6944087147712708,\n",
       "   0.8445035815238953,\n",
       "   0.8516198992729187,\n",
       "   0.7759788036346436,\n",
       "   0.5926662087440491,\n",
       "   0.9687052965164185,\n",
       "   0.819313108921051,\n",
       "   0.6400216221809387,\n",
       "   1.1752043962478638,\n",
       "   0.8477631211280823,\n",
       "   1.239427924156189,\n",
       "   0.9078615307807922,\n",
       "   0.8869829177856445,\n",
       "   0.9846767783164978,\n",
       "   0.7043983936309814,\n",
       "   0.7217719554901123,\n",
       "   0.5011012554168701,\n",
       "   0.6720845699310303,\n",
       "   0.6512386202812195,\n",
       "   0.6209529638290405,\n",
       "   0.9110752940177917,\n",
       "   0.5279788374900818,\n",
       "   0.6775025129318237,\n",
       "   1.0245180130004883,\n",
       "   0.43831485509872437,\n",
       "   0.8438941836357117,\n",
       "   0.691558837890625,\n",
       "   0.7730746269226074,\n",
       "   1.2478030920028687,\n",
       "   0.6742986440658569,\n",
       "   0.5211105942726135,\n",
       "   0.5552350878715515,\n",
       "   0.5810101628303528,\n",
       "   0.5179445743560791,\n",
       "   0.7075910568237305,\n",
       "   0.75095134973526,\n",
       "   1.1261215209960938,\n",
       "   0.8668246269226074,\n",
       "   1.0249276161193848,\n",
       "   0.6157307624816895,\n",
       "   0.6686314344406128,\n",
       "   0.8554644584655762,\n",
       "   0.6763597726821899,\n",
       "   0.9065107107162476,\n",
       "   1.1363855600357056,\n",
       "   0.8622022271156311,\n",
       "   0.5137103796005249,\n",
       "   0.8230353593826294,\n",
       "   0.7905219197273254,\n",
       "   0.7999712824821472,\n",
       "   0.7698620557785034,\n",
       "   0.8881990313529968,\n",
       "   0.7644745707511902,\n",
       "   0.7097862958908081,\n",
       "   0.7867820262908936,\n",
       "   0.7731841802597046,\n",
       "   0.6036666035652161,\n",
       "   0.7464711666107178,\n",
       "   1.0235501527786255,\n",
       "   0.7547831535339355,\n",
       "   1.0112453699111938,\n",
       "   0.55424964427948,\n",
       "   0.542698860168457,\n",
       "   0.7466438412666321,\n",
       "   1.087734580039978,\n",
       "   0.5454769134521484,\n",
       "   0.5236075520515442,\n",
       "   0.480510950088501,\n",
       "   0.5842671990394592,\n",
       "   0.6626996994018555,\n",
       "   0.9515129327774048,\n",
       "   0.5530462265014648,\n",
       "   0.5138013958930969,\n",
       "   0.4966138005256653,\n",
       "   0.7419673204421997,\n",
       "   0.5006019473075867,\n",
       "   0.5301978588104248,\n",
       "   0.668430745601654,\n",
       "   0.909996747970581,\n",
       "   0.7132961750030518,\n",
       "   0.4505825340747833,\n",
       "   0.728576123714447,\n",
       "   0.5157383680343628,\n",
       "   0.6648640632629395,\n",
       "   0.6372054219245911,\n",
       "   0.4450205862522125,\n",
       "   0.6857229471206665,\n",
       "   0.9693320393562317,\n",
       "   0.7840396165847778,\n",
       "   0.47250521183013916,\n",
       "   0.7938675284385681,\n",
       "   0.9712826609611511,\n",
       "   0.7780641913414001,\n",
       "   0.7434037923812866,\n",
       "   0.8793502449989319,\n",
       "   0.8376930952072144,\n",
       "   0.6298834085464478,\n",
       "   0.6077761650085449,\n",
       "   0.48311319947242737,\n",
       "   0.4247451424598694,\n",
       "   0.6731188297271729,\n",
       "   0.7972875237464905,\n",
       "   0.5374761819839478,\n",
       "   0.6040102243423462,\n",
       "   0.5224210619926453,\n",
       "   0.6980468034744263,\n",
       "   0.49873265624046326,\n",
       "   0.5705451369285583,\n",
       "   0.5969489216804504,\n",
       "   0.6272468566894531,\n",
       "   0.6406545639038086,\n",
       "   0.7307289242744446,\n",
       "   0.5152629613876343,\n",
       "   0.969978928565979,\n",
       "   0.598076581954956,\n",
       "   0.8085190653800964,\n",
       "   0.4969698488712311,\n",
       "   0.9634543657302856,\n",
       "   0.48872870206832886,\n",
       "   0.6466980576515198,\n",
       "   0.6959479451179504,\n",
       "   0.6671152710914612,\n",
       "   0.8278374671936035,\n",
       "   0.6275355815887451,\n",
       "   0.6885755658149719,\n",
       "   0.7629178166389465,\n",
       "   0.7384986281394958,\n",
       "   1.1973007917404175,\n",
       "   0.7380744814872742,\n",
       "   0.5740038156509399,\n",
       "   0.7019008994102478,\n",
       "   0.5901484489440918,\n",
       "   0.7768926024436951,\n",
       "   0.715773344039917,\n",
       "   0.636970579624176,\n",
       "   0.6779065728187561,\n",
       "   0.8542079329490662,\n",
       "   0.7831234931945801,\n",
       "   0.706101655960083,\n",
       "   0.4305434226989746,\n",
       "   0.4719915986061096,\n",
       "   0.5475062727928162,\n",
       "   0.6585534811019897,\n",
       "   0.390272855758667,\n",
       "   0.5391809344291687,\n",
       "   0.6693770885467529,\n",
       "   1.1687147617340088,\n",
       "   0.4577687680721283,\n",
       "   0.6522970199584961,\n",
       "   0.4513804018497467,\n",
       "   0.7172877192497253,\n",
       "   0.7290600538253784,\n",
       "   0.6321137547492981,\n",
       "   0.6161838173866272,\n",
       "   1.0912139415740967,\n",
       "   0.5287492871284485,\n",
       "   0.4378422796726227,\n",
       "   0.47547590732574463,\n",
       "   0.4982599914073944,\n",
       "   0.6661875247955322,\n",
       "   0.5918413996696472,\n",
       "   0.5478827357292175,\n",
       "   0.6401815414428711,\n",
       "   0.6680614948272705,\n",
       "   0.7339208126068115,\n",
       "   0.47875380516052246,\n",
       "   0.5158359408378601,\n",
       "   0.863548994064331,\n",
       "   0.4527030885219574,\n",
       "   0.5448372960090637,\n",
       "   0.7842663526535034,\n",
       "   0.952172040939331,\n",
       "   0.7528366446495056,\n",
       "   0.5708276629447937,\n",
       "   0.4841406047344208,\n",
       "   0.454170823097229,\n",
       "   0.9032771587371826,\n",
       "   0.5732111930847168,\n",
       "   0.7309675216674805,\n",
       "   0.7474843263626099,\n",
       "   0.7399004101753235,\n",
       "   0.7153311967849731,\n",
       "   0.6195186972618103,\n",
       "   0.5044630765914917,\n",
       "   0.6822726726531982,\n",
       "   0.5800197124481201,\n",
       "   0.5439531207084656,\n",
       "   0.5100648999214172,\n",
       "   0.8433547019958496,\n",
       "   0.7279801368713379,\n",
       "   0.6859745979309082,\n",
       "   0.6008884310722351,\n",
       "   0.41553881764411926,\n",
       "   0.6349242329597473,\n",
       "   0.5888237953186035,\n",
       "   0.9281405806541443,\n",
       "   0.6114510893821716,\n",
       "   0.8594719767570496,\n",
       "   0.6091007590293884,\n",
       "   0.6781314015388489,\n",
       "   0.7669848203659058,\n",
       "   0.6086501479148865,\n",
       "   0.6571800708770752,\n",
       "   0.6224963665008545,\n",
       "   0.6534108519554138,\n",
       "   0.6267393231391907,\n",
       "   0.8132420778274536,\n",
       "   0.37472355365753174,\n",
       "   0.5440769195556641,\n",
       "   0.44978824257850647,\n",
       "   0.5635855793952942,\n",
       "   0.5444916486740112,\n",
       "   0.5356719493865967,\n",
       "   0.6665717363357544,\n",
       "   0.4960726499557495,\n",
       "   0.8888676166534424,\n",
       "   0.5103474855422974,\n",
       "   0.6040600538253784,\n",
       "   0.543822169303894,\n",
       "   0.4857340455055237,\n",
       "   0.5185972452163696,\n",
       "   0.5062335729598999,\n",
       "   0.5228747725486755,\n",
       "   0.6763761043548584,\n",
       "   0.6321964859962463,\n",
       "   0.6043456196784973,\n",
       "   0.43843457102775574,\n",
       "   0.7587895393371582,\n",
       "   0.6496341228485107,\n",
       "   0.7569003701210022,\n",
       "   0.6835764646530151,\n",
       "   0.6602222919464111,\n",
       "   0.42388930916786194,\n",
       "   0.5076625347137451,\n",
       "   0.47907790541648865,\n",
       "   0.5638474225997925,\n",
       "   0.5122871994972229,\n",
       "   0.5168207287788391,\n",
       "   0.48423489928245544,\n",
       "   0.5502832531929016,\n",
       "   0.6844545602798462,\n",
       "   0.5693299174308777,\n",
       "   0.6391867995262146,\n",
       "   0.6489625573158264,\n",
       "   0.5742594599723816,\n",
       "   0.37723666429519653,\n",
       "   0.5852177739143372,\n",
       "   0.6210023164749146,\n",
       "   0.5440136790275574,\n",
       "   0.5497487783432007,\n",
       "   0.4436091482639313,\n",
       "   0.7736925482749939,\n",
       "   0.6664314270019531,\n",
       "   0.5847173929214478,\n",
       "   0.8803797960281372,\n",
       "   0.5872788429260254,\n",
       "   0.8001510500907898,\n",
       "   0.7452690005302429,\n",
       "   0.42400622367858887,\n",
       "   0.5286225080490112,\n",
       "   0.7464520335197449,\n",
       "   0.8701189756393433,\n",
       "   0.5572852492332458,\n",
       "   0.6154943704605103,\n",
       "   0.5191419720649719,\n",
       "   0.6069042682647705,\n",
       "   0.504412829875946,\n",
       "   0.5091197490692139,\n",
       "   0.6628885865211487,\n",
       "   0.4577971398830414,\n",
       "   0.5880874991416931,\n",
       "   0.5881015062332153,\n",
       "   0.6284719705581665,\n",
       "   0.670795738697052,\n",
       "   0.6057278513908386,\n",
       "   0.6970345377922058,\n",
       "   0.45446035265922546,\n",
       "   0.6187432408332825,\n",
       "   0.6031437516212463,\n",
       "   0.5174619555473328,\n",
       "   0.966129720211029,\n",
       "   0.5030089020729065,\n",
       "   0.5246751308441162,\n",
       "   0.4989434480667114,\n",
       "   0.6157273054122925,\n",
       "   0.53823322057724,\n",
       "   0.7062418460845947,\n",
       "   0.5327696204185486,\n",
       "   0.3948822319507599,\n",
       "   1.0656160116195679,\n",
       "   0.6307406425476074,\n",
       "   0.5237175822257996,\n",
       "   0.6089282631874084,\n",
       "   0.4535304605960846,\n",
       "   0.46248859167099,\n",
       "   0.4910130798816681,\n",
       "   0.5964431762695312,\n",
       "   0.6030354499816895,\n",
       "   0.5702076554298401,\n",
       "   0.7536522746086121,\n",
       "   0.5546074509620667,\n",
       "   0.39874279499053955,\n",
       "   0.5868319272994995,\n",
       "   0.7947847247123718,\n",
       "   0.6269795298576355,\n",
       "   0.4161507487297058,\n",
       "   0.5158392190933228,\n",
       "   0.34873199462890625,\n",
       "   0.9290374517440796,\n",
       "   0.770490825176239,\n",
       "   0.7215510606765747,\n",
       "   0.4395028352737427,\n",
       "   0.5815858840942383,\n",
       "   0.3903426229953766,\n",
       "   0.6699795722961426,\n",
       "   0.3895970582962036,\n",
       "   0.3287416696548462,\n",
       "   0.5516451001167297,\n",
       "   0.7331077456474304,\n",
       "   0.6773183941841125,\n",
       "   0.7061900496482849,\n",
       "   0.6089250445365906,\n",
       "   0.4317002594470978,\n",
       "   0.5674262642860413,\n",
       "   0.6851263046264648,\n",
       "   0.36587536334991455,\n",
       "   0.6714337468147278,\n",
       "   0.47289252281188965,\n",
       "   0.3597681522369385,\n",
       "   0.4921051263809204,\n",
       "   0.4284893870353699,\n",
       "   0.6678891777992249,\n",
       "   0.741826057434082,\n",
       "   0.7956802248954773,\n",
       "   0.4900674521923065,\n",
       "   0.656603991985321,\n",
       "   0.3646617531776428,\n",
       "   0.42813101410865784,\n",
       "   0.6406707167625427,\n",
       "   0.3677530586719513,\n",
       "   0.5904606580734253,\n",
       "   0.774694561958313,\n",
       "   0.5635966658592224,\n",
       "   0.5819734334945679,\n",
       "   0.5936662554740906,\n",
       "   0.5163794755935669,\n",
       "   0.35787203907966614,\n",
       "   0.6417086720466614,\n",
       "   0.6116384267807007,\n",
       "   0.5125069618225098,\n",
       "   0.5135876536369324,\n",
       "   0.6027969717979431,\n",
       "   0.5904892683029175,\n",
       "   0.5190146565437317,\n",
       "   0.4296676516532898,\n",
       "   0.45384886860847473,\n",
       "   0.6875813603401184,\n",
       "   0.6447187662124634,\n",
       "   0.362697958946228,\n",
       "   0.40190351009368896,\n",
       "   0.5794072151184082,\n",
       "   0.4440626800060272,\n",
       "   0.42252346873283386,\n",
       "   0.5119126439094543,\n",
       "   0.7238743901252747,\n",
       "   0.6513763070106506,\n",
       "   0.48200222849845886,\n",
       "   0.8742304444313049,\n",
       "   0.4657314121723175,\n",
       "   0.44096702337265015,\n",
       "   0.4328443109989166,\n",
       "   0.6120078563690186,\n",
       "   0.5315658450126648,\n",
       "   0.6314302682876587,\n",
       "   0.4611510932445526,\n",
       "   0.698133647441864,\n",
       "   0.5908457040786743,\n",
       "   0.3963542878627777,\n",
       "   0.35858625173568726,\n",
       "   0.5635120272636414,\n",
       "   0.8119320869445801,\n",
       "   0.5039297342300415,\n",
       "   0.5322288274765015,\n",
       "   0.3476547300815582,\n",
       "   0.44452905654907227,\n",
       "   0.680181086063385,\n",
       "   0.5397188067436218,\n",
       "   0.4125971794128418,\n",
       "   1.0208805799484253,\n",
       "   0.7670556902885437,\n",
       "   0.3412966728210449,\n",
       "   0.5457533001899719,\n",
       "   0.6462741494178772,\n",
       "   0.7373965382575989,\n",
       "   0.6669449806213379,\n",
       "   0.6906368732452393,\n",
       "   0.5819250345230103,\n",
       "   0.5185977220535278,\n",
       "   0.4140720069408417,\n",
       "   0.5970402359962463,\n",
       "   0.4670301675796509,\n",
       "   0.49265533685684204,\n",
       "   0.42700573801994324,\n",
       "   0.48982173204421997,\n",
       "   0.4460418224334717,\n",
       "   0.8927298784255981,\n",
       "   1.1731268167495728,\n",
       "   0.5591881275177002,\n",
       "   0.646611213684082,\n",
       "   0.5901979207992554,\n",
       "   0.550786018371582,\n",
       "   0.680528998374939,\n",
       "   0.5483578443527222,\n",
       "   0.7334350347518921,\n",
       "   0.5684356093406677,\n",
       "   0.7450746893882751,\n",
       "   0.7108153700828552,\n",
       "   0.6884757280349731,\n",
       "   0.6671496629714966,\n",
       "   0.5308337211608887,\n",
       "   0.463169127702713,\n",
       "   0.5643749833106995,\n",
       "   0.48681870102882385,\n",
       "   0.948883056640625,\n",
       "   0.46311286091804504,\n",
       "   0.5564707517623901,\n",
       "   0.5397810339927673,\n",
       "   0.8785330057144165,\n",
       "   0.4742482602596283,\n",
       "   0.3580634593963623,\n",
       "   0.5296000242233276,\n",
       "   0.4759847819805145,\n",
       "   0.627415120601654,\n",
       "   0.44975703954696655,\n",
       "   0.6905079483985901,\n",
       "   0.44184353947639465,\n",
       "   0.5379250049591064,\n",
       "   0.6788017749786377,\n",
       "   0.5717983841896057,\n",
       "   0.528131902217865,\n",
       "   0.7721915245056152,\n",
       "   0.5366532206535339,\n",
       "   0.43805643916130066,\n",
       "   0.8132138848304749,\n",
       "   0.5456158518791199,\n",
       "   0.5297282934188843,\n",
       "   0.45315077900886536,\n",
       "   0.4606472849845886,\n",
       "   0.5533812642097473,\n",
       "   0.46794673800468445,\n",
       "   0.6498217582702637,\n",
       "   0.6798253655433655,\n",
       "   0.606096088886261,\n",
       "   0.46045783162117004,\n",
       "   0.4312532842159271,\n",
       "   0.5200421214103699,\n",
       "   0.5084241032600403,\n",
       "   0.4213496744632721,\n",
       "   0.5551450252532959,\n",
       "   0.41028252243995667,\n",
       "   0.5710642337799072,\n",
       "   0.7268951535224915],\n",
       "  'train_jsc_loss': [0.4955841898918152,\n",
       "   0.5223848819732666,\n",
       "   0.5049086213111877,\n",
       "   0.4902406930923462,\n",
       "   0.48609644174575806,\n",
       "   0.5378007888793945,\n",
       "   0.499767541885376,\n",
       "   0.5548886060714722,\n",
       "   0.5000183582305908,\n",
       "   0.5122959017753601,\n",
       "   0.49290385842323303,\n",
       "   0.5727696418762207,\n",
       "   0.4868100583553314,\n",
       "   0.4426310062408447,\n",
       "   0.46687066555023193,\n",
       "   0.5590617656707764,\n",
       "   0.43294891715049744,\n",
       "   0.4941491484642029,\n",
       "   0.5817866325378418,\n",
       "   0.4847589135169983,\n",
       "   0.6028764843940735,\n",
       "   0.5397117137908936,\n",
       "   0.5289232134819031,\n",
       "   0.48062387108802795,\n",
       "   0.5935959815979004,\n",
       "   0.5262661576271057,\n",
       "   0.49567487835884094,\n",
       "   0.4928283393383026,\n",
       "   0.48527416586875916,\n",
       "   0.46296632289886475,\n",
       "   0.4924015402793884,\n",
       "   0.5048298239707947,\n",
       "   0.5007047653198242,\n",
       "   0.4906645119190216,\n",
       "   0.4547394812107086,\n",
       "   0.44977718591690063,\n",
       "   0.463852196931839,\n",
       "   0.5324249267578125,\n",
       "   0.551164984703064,\n",
       "   0.5592778921127319,\n",
       "   0.5035728812217712,\n",
       "   0.4425753355026245,\n",
       "   0.5222245454788208,\n",
       "   0.44996944069862366,\n",
       "   0.4697960317134857,\n",
       "   0.4683563709259033,\n",
       "   0.5227580666542053,\n",
       "   0.5129145979881287,\n",
       "   0.5039835572242737,\n",
       "   0.49938860535621643,\n",
       "   0.5660682916641235,\n",
       "   0.505519449710846,\n",
       "   0.5161231160163879,\n",
       "   0.4873001277446747,\n",
       "   0.491476833820343,\n",
       "   0.47114744782447815,\n",
       "   0.680695652961731,\n",
       "   0.5211340188980103,\n",
       "   0.49808478355407715,\n",
       "   0.5072659254074097,\n",
       "   0.4841728210449219,\n",
       "   0.49345892667770386,\n",
       "   0.45986470580101013,\n",
       "   0.5995674729347229,\n",
       "   0.5289026498794556,\n",
       "   0.6043872237205505,\n",
       "   0.4681644141674042,\n",
       "   0.45454517006874084,\n",
       "   0.5380855202674866,\n",
       "   0.53304123878479,\n",
       "   0.45156022906303406,\n",
       "   0.45098334550857544,\n",
       "   0.6193300485610962,\n",
       "   0.4568043053150177,\n",
       "   0.4654434621334076,\n",
       "   0.4934238791465759,\n",
       "   0.4798586666584015,\n",
       "   0.5073009133338928,\n",
       "   0.4705754816532135,\n",
       "   0.5623183250427246,\n",
       "   0.47237613797187805,\n",
       "   0.4874463677406311,\n",
       "   0.5594066381454468,\n",
       "   0.4928908050060272,\n",
       "   0.5172111988067627,\n",
       "   0.4699386656284332,\n",
       "   0.44798001646995544,\n",
       "   0.4630323052406311,\n",
       "   0.5224321484565735,\n",
       "   0.4704810678958893,\n",
       "   0.48823583126068115,\n",
       "   0.4673757255077362,\n",
       "   0.48570287227630615,\n",
       "   0.4574795961380005,\n",
       "   0.46565181016921997,\n",
       "   0.48704609274864197,\n",
       "   0.47709643840789795,\n",
       "   0.5069963335990906,\n",
       "   0.5812229514122009,\n",
       "   0.5227217674255371,\n",
       "   0.47246959805488586,\n",
       "   0.5370985269546509,\n",
       "   0.5060713887214661,\n",
       "   0.4671493172645569,\n",
       "   0.49341845512390137,\n",
       "   0.4620681703090668,\n",
       "   0.5281995534896851,\n",
       "   0.5381833910942078,\n",
       "   0.5677062273025513,\n",
       "   0.46299415826797485,\n",
       "   0.5072144865989685,\n",
       "   0.5695933699607849,\n",
       "   0.4772472381591797,\n",
       "   0.47766804695129395,\n",
       "   0.5056736469268799,\n",
       "   0.5585814714431763,\n",
       "   0.5096004009246826,\n",
       "   0.4943895637989044,\n",
       "   0.556219220161438,\n",
       "   0.45808184146881104,\n",
       "   0.4918871521949768,\n",
       "   0.44100192189216614,\n",
       "   0.5101410150527954,\n",
       "   0.5031545162200928,\n",
       "   0.5308928489685059,\n",
       "   0.4354426860809326,\n",
       "   0.5408231616020203,\n",
       "   0.4959873557090759,\n",
       "   0.46804165840148926,\n",
       "   0.5241715312004089,\n",
       "   0.4751928150653839,\n",
       "   0.4884873628616333,\n",
       "   0.5456020832061768,\n",
       "   0.5448145866394043,\n",
       "   0.4664325714111328,\n",
       "   0.5532113909721375,\n",
       "   0.49609169363975525,\n",
       "   0.5248634815216064,\n",
       "   0.5674371719360352,\n",
       "   0.47780364751815796,\n",
       "   0.48588114976882935,\n",
       "   0.5006973147392273,\n",
       "   0.4679067134857178,\n",
       "   0.4873014986515045,\n",
       "   0.4781356751918793,\n",
       "   0.5522658824920654,\n",
       "   0.4424358010292053,\n",
       "   0.4504739046096802,\n",
       "   0.4512884318828583,\n",
       "   0.5638108253479004,\n",
       "   0.5242372155189514,\n",
       "   0.5062069296836853,\n",
       "   0.5852091908454895,\n",
       "   0.4554835259914398,\n",
       "   0.4791697561740875,\n",
       "   0.46517258882522583,\n",
       "   0.5595969557762146,\n",
       "   0.5320819616317749,\n",
       "   0.4986765682697296,\n",
       "   0.4733944833278656,\n",
       "   0.42503342032432556,\n",
       "   0.51802659034729,\n",
       "   0.5099748969078064,\n",
       "   0.4783742129802704,\n",
       "   0.441079318523407,\n",
       "   0.44729796051979065,\n",
       "   0.4795023500919342,\n",
       "   0.5737037062644958,\n",
       "   0.5652778148651123,\n",
       "   0.4761159420013428,\n",
       "   0.5675435662269592,\n",
       "   0.5128092765808105,\n",
       "   0.41502365469932556,\n",
       "   0.4936503469944,\n",
       "   0.45398128032684326,\n",
       "   0.5580207705497742,\n",
       "   0.5205872654914856,\n",
       "   0.45277491211891174,\n",
       "   0.44657760858535767,\n",
       "   0.5609468817710876,\n",
       "   0.48288577795028687,\n",
       "   0.5028476715087891,\n",
       "   0.4896203577518463,\n",
       "   0.5967421531677246,\n",
       "   0.49441081285476685,\n",
       "   0.4708877205848694,\n",
       "   0.46434080600738525,\n",
       "   0.4585750699043274,\n",
       "   0.5012437701225281,\n",
       "   0.5103451609611511,\n",
       "   0.5019302368164062,\n",
       "   0.49622416496276855,\n",
       "   0.42700910568237305,\n",
       "   0.5289461016654968,\n",
       "   0.4921266734600067,\n",
       "   0.4274080991744995,\n",
       "   0.5276334881782532,\n",
       "   0.5037493705749512,\n",
       "   0.4998624622821808,\n",
       "   0.45485928654670715,\n",
       "   0.5475884079933167,\n",
       "   0.6057308912277222,\n",
       "   0.47408828139305115,\n",
       "   0.526226818561554,\n",
       "   0.5394003987312317,\n",
       "   0.5370601415634155,\n",
       "   0.48887839913368225,\n",
       "   0.4825049936771393,\n",
       "   0.48891016840934753,\n",
       "   0.5022619366645813,\n",
       "   0.4906899333000183,\n",
       "   0.5761435031890869,\n",
       "   0.47568702697753906,\n",
       "   0.5606788992881775,\n",
       "   0.46524596214294434,\n",
       "   0.521765410900116,\n",
       "   0.46140196919441223,\n",
       "   0.4669618308544159,\n",
       "   0.49447518587112427,\n",
       "   0.5643200874328613,\n",
       "   0.44925904273986816,\n",
       "   0.5018525123596191,\n",
       "   0.5192018747329712,\n",
       "   0.5485740900039673,\n",
       "   0.4557923674583435,\n",
       "   0.464769572019577,\n",
       "   0.43867266178131104,\n",
       "   0.4823473393917084,\n",
       "   0.4406105875968933,\n",
       "   0.4645196199417114,\n",
       "   0.4452793598175049,\n",
       "   0.545979380607605,\n",
       "   0.44586560130119324,\n",
       "   0.5771223902702332,\n",
       "   0.4562087953090668,\n",
       "   0.4455583989620209,\n",
       "   0.45812907814979553,\n",
       "   0.5434690117835999,\n",
       "   0.4900704324245453,\n",
       "   0.4585457146167755,\n",
       "   0.5141671895980835,\n",
       "   0.4923568367958069,\n",
       "   0.4982740879058838,\n",
       "   0.49211427569389343,\n",
       "   0.499500572681427,\n",
       "   0.5399935245513916,\n",
       "   0.43372538685798645,\n",
       "   0.4705592691898346,\n",
       "   0.4663934111595154,\n",
       "   0.6245126128196716,\n",
       "   0.5089591145515442,\n",
       "   0.5589704513549805,\n",
       "   0.5087043046951294,\n",
       "   0.5536355376243591,\n",
       "   0.5199129581451416,\n",
       "   0.44692233204841614,\n",
       "   0.5678758025169373,\n",
       "   0.4243062734603882,\n",
       "   0.47341665625572205,\n",
       "   0.49253541231155396,\n",
       "   0.477253258228302,\n",
       "   0.49572765827178955,\n",
       "   0.574180006980896,\n",
       "   0.4934367835521698,\n",
       "   0.5836805701255798,\n",
       "   0.5085798501968384,\n",
       "   0.5273030996322632,\n",
       "   0.4510071873664856,\n",
       "   0.4636218547821045,\n",
       "   0.416940301656723,\n",
       "   0.4763203561306,\n",
       "   0.5027116537094116,\n",
       "   0.5106560587882996,\n",
       "   0.4369925260543823,\n",
       "   0.5495190024375916,\n",
       "   0.4616259038448334,\n",
       "   0.49532076716423035,\n",
       "   0.4444981813430786,\n",
       "   0.44939830899238586,\n",
       "   0.4687756597995758,\n",
       "   0.4789698123931885,\n",
       "   0.5079103708267212,\n",
       "   0.47065216302871704,\n",
       "   0.5507567524909973,\n",
       "   0.4835697114467621,\n",
       "   0.4550299346446991,\n",
       "   0.5605086088180542,\n",
       "   0.5128628611564636,\n",
       "   0.4803358316421509,\n",
       "   0.4748080372810364,\n",
       "   0.48457854986190796,\n",
       "   0.48031938076019287,\n",
       "   0.5464049577713013,\n",
       "   0.5829254984855652,\n",
       "   0.4642315208911896,\n",
       "   0.4649845361709595,\n",
       "   0.511711835861206,\n",
       "   0.47481152415275574,\n",
       "   0.4582008123397827,\n",
       "   0.4582374691963196,\n",
       "   0.49242910742759705,\n",
       "   0.4898532032966614,\n",
       "   0.4668503701686859,\n",
       "   0.5514186024665833,\n",
       "   0.4663858115673065,\n",
       "   0.5293071866035461,\n",
       "   0.49112626910209656,\n",
       "   0.5147207975387573,\n",
       "   0.46059873700141907,\n",
       "   0.5049978494644165,\n",
       "   0.46844443678855896,\n",
       "   0.5178781747817993,\n",
       "   0.48146653175354004,\n",
       "   0.5155575275421143,\n",
       "   0.632540225982666,\n",
       "   0.49312925338745117,\n",
       "   0.5370890498161316,\n",
       "   0.46629956364631653,\n",
       "   0.5074931383132935,\n",
       "   0.5279357433319092,\n",
       "   0.4927469789981842,\n",
       "   0.497583270072937,\n",
       "   0.47025808691978455,\n",
       "   0.45018815994262695,\n",
       "   0.567128598690033,\n",
       "   0.47337040305137634,\n",
       "   0.4134994149208069,\n",
       "   0.5157071948051453,\n",
       "   0.5386461019515991,\n",
       "   0.5462436079978943,\n",
       "   0.5889607071876526,\n",
       "   0.4948524534702301,\n",
       "   0.45764023065567017,\n",
       "   0.5053095817565918,\n",
       "   0.5384932160377502,\n",
       "   0.5251836180686951,\n",
       "   0.5036658644676208,\n",
       "   0.5841960906982422,\n",
       "   0.47656136751174927,\n",
       "   0.4588854908943176,\n",
       "   0.4493216574192047,\n",
       "   0.46096429228782654,\n",
       "   0.47688230872154236,\n",
       "   0.4279579222202301,\n",
       "   0.4996704161167145,\n",
       "   0.5076972246170044,\n",
       "   0.45919299125671387,\n",
       "   0.5120434761047363,\n",
       "   0.4556125998497009,\n",
       "   0.4619797170162201,\n",
       "   0.5695490837097168,\n",
       "   0.5147555470466614,\n",
       "   0.4281938970088959,\n",
       "   0.47722917795181274,\n",
       "   0.44769787788391113,\n",
       "   0.5539140105247498,\n",
       "   0.5324515700340271,\n",
       "   0.5353512763977051,\n",
       "   0.4754239618778229,\n",
       "   0.46384426951408386,\n",
       "   0.46739742159843445,\n",
       "   0.5024858713150024,\n",
       "   0.4560115933418274,\n",
       "   0.4309781789779663,\n",
       "   0.46127307415008545,\n",
       "   0.5709545612335205,\n",
       "   0.4350411891937256,\n",
       "   0.4931362271308899,\n",
       "   0.48664000630378723,\n",
       "   0.4677501618862152,\n",
       "   0.511666476726532,\n",
       "   0.4762763977050781,\n",
       "   0.43198010325431824,\n",
       "   0.5215945243835449,\n",
       "   0.44972747564315796,\n",
       "   0.48178181052207947,\n",
       "   0.4397018253803253,\n",
       "   0.47899627685546875,\n",
       "   0.4707839787006378,\n",
       "   0.5366024374961853,\n",
       "   0.4917336404323578,\n",
       "   0.4966476857662201,\n",
       "   0.4386464059352875,\n",
       "   0.539989709854126,\n",
       "   0.4510256350040436,\n",
       "   0.6042304635047913,\n",
       "   0.47985124588012695,\n",
       "   0.5843066573143005,\n",
       "   0.4532249867916107,\n",
       "   0.4690104126930237,\n",
       "   0.49890604615211487,\n",
       "   0.41485849022865295,\n",
       "   0.48585087060928345,\n",
       "   0.48364344239234924,\n",
       "   0.49177446961402893,\n",
       "   0.560131847858429,\n",
       "   0.505462110042572,\n",
       "   0.50007563829422,\n",
       "   0.5671961903572083,\n",
       "   0.4678424596786499,\n",
       "   0.5104085206985474,\n",
       "   0.5455257892608643,\n",
       "   0.4927215576171875,\n",
       "   0.6176584959030151,\n",
       "   0.5655955672264099,\n",
       "   0.4724140167236328,\n",
       "   0.5057100653648376,\n",
       "   0.5201639533042908,\n",
       "   0.4983426332473755,\n",
       "   0.6855157613754272,\n",
       "   0.4752187430858612,\n",
       "   0.49352535605430603,\n",
       "   0.4550110995769501,\n",
       "   0.4472000002861023,\n",
       "   0.49528035521507263,\n",
       "   0.43596151471138,\n",
       "   0.4611992835998535,\n",
       "   0.39258548617362976,\n",
       "   0.5316347479820251,\n",
       "   0.5272247791290283,\n",
       "   0.543753445148468,\n",
       "   0.5384796857833862,\n",
       "   0.5172960162162781,\n",
       "   0.47738027572631836,\n",
       "   0.4579005837440491,\n",
       "   0.6211820840835571,\n",
       "   0.49859824776649475,\n",
       "   0.45041123032569885,\n",
       "   0.5332687497138977,\n",
       "   0.4719313085079193,\n",
       "   0.5254337787628174,\n",
       "   0.5281435251235962,\n",
       "   0.5059993863105774,\n",
       "   0.5662359595298767,\n",
       "   0.46482205390930176,\n",
       "   0.5498594045639038,\n",
       "   0.5480313301086426,\n",
       "   0.5867282748222351,\n",
       "   0.5011176466941833,\n",
       "   0.5360139012336731,\n",
       "   0.46100860834121704,\n",
       "   0.45937982201576233,\n",
       "   0.5478575229644775,\n",
       "   0.5606074929237366,\n",
       "   0.4477100074291229,\n",
       "   0.46345990896224976,\n",
       "   0.48091915249824524,\n",
       "   0.4933019280433655,\n",
       "   0.46100854873657227,\n",
       "   0.5172640681266785,\n",
       "   0.47835007309913635,\n",
       "   0.46184489130973816,\n",
       "   0.4907919764518738,\n",
       "   0.45469212532043457,\n",
       "   0.513709545135498,\n",
       "   0.4912014901638031,\n",
       "   0.44917216897010803,\n",
       "   0.4544675648212433,\n",
       "   0.45253562927246094,\n",
       "   0.5318803787231445,\n",
       "   0.4950137734413147,\n",
       "   0.49055278301239014,\n",
       "   0.5535314679145813,\n",
       "   0.5518875122070312,\n",
       "   0.5503708124160767,\n",
       "   0.4541516602039337,\n",
       "   0.4778117835521698,\n",
       "   0.48290494084358215,\n",
       "   0.4866437017917633,\n",
       "   0.4940189719200134,\n",
       "   0.49039125442504883,\n",
       "   0.5585886240005493,\n",
       "   0.5239949822425842,\n",
       "   0.5264346599578857,\n",
       "   0.4753509759902954,\n",
       "   0.5136197805404663,\n",
       "   0.5251990556716919,\n",
       "   0.5732904672622681,\n",
       "   0.47582998871803284,\n",
       "   0.46317535638809204,\n",
       "   0.4710195064544678,\n",
       "   0.4703032076358795,\n",
       "   0.5255247354507446,\n",
       "   0.4461468756198883,\n",
       "   0.4899258613586426,\n",
       "   0.45537492632865906,\n",
       "   0.5422166585922241,\n",
       "   0.5115915536880493,\n",
       "   0.452343612909317,\n",
       "   0.41970211267471313,\n",
       "   0.47356489300727844,\n",
       "   0.4591508209705353,\n",
       "   0.4765119254589081,\n",
       "   0.5033679008483887,\n",
       "   0.46134239435195923,\n",
       "   0.5045189261436462,\n",
       "   0.44704127311706543,\n",
       "   0.5294763445854187,\n",
       "   0.49437010288238525,\n",
       "   0.535585880279541],\n",
       "  'train_ff_loss': [0.30639103055000305,\n",
       "   0.43236997723579407,\n",
       "   0.4633191227912903,\n",
       "   0.4195469319820404,\n",
       "   0.3331330418586731,\n",
       "   0.42960307002067566,\n",
       "   0.3771957755088806,\n",
       "   0.265288382768631,\n",
       "   0.28619980812072754,\n",
       "   0.42432901263237,\n",
       "   0.3369484841823578,\n",
       "   0.41151925921440125,\n",
       "   0.3877337574958801,\n",
       "   0.32128700613975525,\n",
       "   0.40118977427482605,\n",
       "   0.2669919431209564,\n",
       "   0.2738330066204071,\n",
       "   0.5196470618247986,\n",
       "   0.273660808801651,\n",
       "   0.4068552851676941,\n",
       "   0.36792755126953125,\n",
       "   0.3931398093700409,\n",
       "   0.4110986888408661,\n",
       "   0.3801545202732086,\n",
       "   0.42606788873672485,\n",
       "   0.3119299113750458,\n",
       "   0.4013230800628662,\n",
       "   0.3221602737903595,\n",
       "   0.3170193135738373,\n",
       "   0.3978925943374634,\n",
       "   0.3893599510192871,\n",
       "   0.4328761696815491,\n",
       "   0.31252431869506836,\n",
       "   0.36759069561958313,\n",
       "   0.35703858733177185,\n",
       "   0.37831270694732666,\n",
       "   0.2777058482170105,\n",
       "   0.34101828932762146,\n",
       "   0.2928027808666229,\n",
       "   0.4289102256298065,\n",
       "   0.38087618350982666,\n",
       "   0.4292581379413605,\n",
       "   0.37726202607154846,\n",
       "   0.22410809993743896,\n",
       "   0.30292460322380066,\n",
       "   0.3661603033542633,\n",
       "   0.33881789445877075,\n",
       "   0.2739172577857971,\n",
       "   0.3854129910469055,\n",
       "   0.3208436071872711,\n",
       "   0.44091328978538513,\n",
       "   0.3746829628944397,\n",
       "   0.3245755732059479,\n",
       "   0.40798017382621765,\n",
       "   0.3046717643737793,\n",
       "   0.3925490379333496,\n",
       "   0.4146207571029663,\n",
       "   0.4137873649597168,\n",
       "   0.43919724225997925,\n",
       "   0.4460964500904083,\n",
       "   0.29721859097480774,\n",
       "   0.40097737312316895,\n",
       "   0.366408109664917,\n",
       "   0.4979354739189148,\n",
       "   0.34323155879974365,\n",
       "   0.40796563029289246,\n",
       "   0.44159287214279175,\n",
       "   0.3287718594074249,\n",
       "   0.4109712839126587,\n",
       "   0.34129512310028076,\n",
       "   0.33629897236824036,\n",
       "   0.4178764522075653,\n",
       "   0.44938352704048157,\n",
       "   0.4366436004638672,\n",
       "   0.3593007028102875,\n",
       "   0.3011448383331299,\n",
       "   0.32481032609939575,\n",
       "   0.32217738032341003,\n",
       "   0.38444146513938904,\n",
       "   0.43236595392227173,\n",
       "   0.32069140672683716,\n",
       "   0.3174314796924591,\n",
       "   0.3822697103023529,\n",
       "   0.41783925890922546,\n",
       "   0.3738333582878113,\n",
       "   0.34750014543533325,\n",
       "   0.41376543045043945,\n",
       "   0.321531742811203,\n",
       "   0.3842809200286865,\n",
       "   0.35829877853393555,\n",
       "   0.3985786437988281,\n",
       "   0.38870641589164734,\n",
       "   0.3873763084411621,\n",
       "   0.3476923406124115,\n",
       "   0.34847840666770935,\n",
       "   0.2989669144153595,\n",
       "   0.3197118639945984,\n",
       "   0.5355594754219055,\n",
       "   0.32921087741851807,\n",
       "   0.2901279032230377,\n",
       "   0.3856932520866394,\n",
       "   0.34897691011428833,\n",
       "   0.3333897590637207,\n",
       "   0.3863881528377533,\n",
       "   0.42188119888305664,\n",
       "   0.3651416301727295,\n",
       "   0.41167256236076355,\n",
       "   0.3059646785259247,\n",
       "   0.35329827666282654,\n",
       "   0.32085877656936646,\n",
       "   0.33040758967399597,\n",
       "   0.4167932868003845,\n",
       "   0.32353898882865906,\n",
       "   0.3029593229293823,\n",
       "   0.3865157663822174,\n",
       "   0.3098626732826233,\n",
       "   0.5292406678199768,\n",
       "   0.3783133625984192,\n",
       "   0.39911603927612305,\n",
       "   0.4921615421772003,\n",
       "   0.2946728765964508,\n",
       "   0.4129542112350464,\n",
       "   0.3170877695083618,\n",
       "   0.3785494267940521,\n",
       "   0.44208529591560364,\n",
       "   0.39084741473197937,\n",
       "   0.43406781554222107,\n",
       "   0.38122397661209106,\n",
       "   0.29479578137397766,\n",
       "   0.3584575653076172,\n",
       "   0.3529147207736969,\n",
       "   0.37805038690567017,\n",
       "   0.312796950340271,\n",
       "   0.44975289702415466,\n",
       "   0.3516283631324768,\n",
       "   0.3596898019313812,\n",
       "   0.3409988582134247,\n",
       "   0.33369675278663635,\n",
       "   0.42471277713775635,\n",
       "   0.2691422402858734,\n",
       "   0.29753100872039795,\n",
       "   0.277168333530426,\n",
       "   0.3406529724597931,\n",
       "   0.3426961600780487,\n",
       "   0.3797041177749634,\n",
       "   0.3620387017726898,\n",
       "   0.384164035320282,\n",
       "   0.5251011848449707,\n",
       "   0.3371097147464752,\n",
       "   0.46252039074897766,\n",
       "   0.43811267614364624,\n",
       "   0.42681315541267395,\n",
       "   0.42813074588775635,\n",
       "   0.35919472575187683,\n",
       "   0.42561179399490356,\n",
       "   0.3452213406562805,\n",
       "   0.30214250087738037,\n",
       "   0.4185172915458679,\n",
       "   0.31595146656036377,\n",
       "   0.2598680555820465,\n",
       "   0.44390806555747986,\n",
       "   0.2576325535774231,\n",
       "   0.3124760091304779,\n",
       "   0.47974398732185364,\n",
       "   0.39458703994750977,\n",
       "   0.3987758755683899,\n",
       "   0.41131797432899475,\n",
       "   0.5025060772895813,\n",
       "   0.4232739508152008,\n",
       "   0.3060755729675293,\n",
       "   0.4202995300292969,\n",
       "   0.3998830020427704,\n",
       "   0.32782435417175293,\n",
       "   0.3749200999736786,\n",
       "   0.40453100204467773,\n",
       "   0.4733242094516754,\n",
       "   0.37755635380744934,\n",
       "   0.30863651633262634,\n",
       "   0.3443736135959625,\n",
       "   0.37318697571754456,\n",
       "   0.3846646249294281,\n",
       "   0.36527860164642334,\n",
       "   0.320005863904953,\n",
       "   0.3013410270214081,\n",
       "   0.3594391345977783,\n",
       "   0.4384528696537018,\n",
       "   0.35767900943756104,\n",
       "   0.25097090005874634,\n",
       "   0.35888057947158813,\n",
       "   0.4086955785751343,\n",
       "   0.3680872321128845,\n",
       "   0.4547533392906189,\n",
       "   0.3505232334136963,\n",
       "   0.4016313850879669,\n",
       "   0.34712424874305725,\n",
       "   0.32465726137161255,\n",
       "   0.27508723735809326,\n",
       "   0.37215015292167664,\n",
       "   0.3384823799133301,\n",
       "   0.32124850153923035,\n",
       "   0.32481229305267334,\n",
       "   0.3278419077396393,\n",
       "   0.34168943762779236,\n",
       "   0.3956667184829712,\n",
       "   0.30876195430755615,\n",
       "   0.20304973423480988,\n",
       "   0.34980541467666626,\n",
       "   0.3363828957080841,\n",
       "   0.38351428508758545,\n",
       "   0.35118141770362854,\n",
       "   0.3115752935409546,\n",
       "   0.27812623977661133,\n",
       "   0.2999401092529297,\n",
       "   0.4254665672779083,\n",
       "   0.2956450581550598,\n",
       "   0.5041537284851074,\n",
       "   0.2924250662326813,\n",
       "   0.2982403635978699,\n",
       "   0.3389440178871155,\n",
       "   0.28962019085884094,\n",
       "   0.3626759946346283,\n",
       "   0.3211058974266052,\n",
       "   0.2859143018722534,\n",
       "   0.323455274105072,\n",
       "   0.37056833505630493,\n",
       "   0.32058286666870117,\n",
       "   0.3170200288295746,\n",
       "   0.3994531035423279,\n",
       "   0.3147771656513214,\n",
       "   0.41278988122940063,\n",
       "   0.2671506404876709,\n",
       "   0.3132605254650116,\n",
       "   0.3335527777671814,\n",
       "   0.4337252080440521,\n",
       "   0.33094319701194763,\n",
       "   0.3662753105163574,\n",
       "   0.44190114736557007,\n",
       "   0.353320837020874,\n",
       "   0.41609078645706177,\n",
       "   0.3850647211074829,\n",
       "   0.37509965896606445,\n",
       "   0.30801644921302795,\n",
       "   0.2825225591659546,\n",
       "   0.32100120186805725,\n",
       "   0.3737565577030182,\n",
       "   0.32820168137550354,\n",
       "   0.23206064105033875,\n",
       "   0.5140820741653442,\n",
       "   0.41169995069503784,\n",
       "   0.30368608236312866,\n",
       "   0.45751625299453735,\n",
       "   0.32652339339256287,\n",
       "   0.33552271127700806,\n",
       "   0.29785770177841187,\n",
       "   0.3402738869190216,\n",
       "   0.36302411556243896,\n",
       "   0.42925703525543213,\n",
       "   0.3824276924133301,\n",
       "   0.3121336102485657,\n",
       "   0.33355557918548584,\n",
       "   0.4105260670185089,\n",
       "   0.36952149868011475,\n",
       "   0.38638725876808167,\n",
       "   0.3349330425262451,\n",
       "   0.28843793272972107,\n",
       "   0.3741222023963928,\n",
       "   0.3904002010822296,\n",
       "   0.36157289147377014,\n",
       "   0.30834388732910156,\n",
       "   0.3052079677581787,\n",
       "   0.4524688720703125,\n",
       "   0.3307173550128937,\n",
       "   0.37960219383239746,\n",
       "   0.42228007316589355,\n",
       "   0.45241212844848633,\n",
       "   0.36168190836906433,\n",
       "   0.4095362424850464,\n",
       "   0.2940211892127991,\n",
       "   0.3330966830253601,\n",
       "   0.3150216042995453,\n",
       "   0.2583010196685791,\n",
       "   0.38124018907546997,\n",
       "   0.40382856130599976,\n",
       "   0.3335651159286499,\n",
       "   0.354320764541626,\n",
       "   0.31508660316467285,\n",
       "   0.35658955574035645,\n",
       "   0.3535003364086151,\n",
       "   0.31318190693855286,\n",
       "   0.41193729639053345,\n",
       "   0.4059849977493286,\n",
       "   0.44248589873313904,\n",
       "   0.33883994817733765,\n",
       "   0.3426014184951782,\n",
       "   0.5036516189575195,\n",
       "   0.29209834337234497,\n",
       "   0.2630934417247772,\n",
       "   0.2932943105697632,\n",
       "   0.44366025924682617,\n",
       "   0.3711617588996887,\n",
       "   0.3088228404521942,\n",
       "   0.3819703161716461,\n",
       "   0.2929421365261078,\n",
       "   0.37675338983535767,\n",
       "   0.3414994776248932,\n",
       "   0.3224961757659912,\n",
       "   0.5164395570755005,\n",
       "   0.28061914443969727,\n",
       "   0.30341005325317383,\n",
       "   0.4093140959739685,\n",
       "   0.2862698435783386,\n",
       "   0.40700119733810425,\n",
       "   0.4415540099143982,\n",
       "   0.345181941986084,\n",
       "   0.39306581020355225,\n",
       "   0.454143226146698,\n",
       "   0.35473862290382385,\n",
       "   0.5214850902557373,\n",
       "   0.3765876591205597,\n",
       "   0.35766974091529846,\n",
       "   0.33716750144958496,\n",
       "   0.3210434913635254,\n",
       "   0.31726348400115967,\n",
       "   0.30341118574142456,\n",
       "   0.2964582145214081,\n",
       "   0.3615265190601349,\n",
       "   0.24527613818645477,\n",
       "   0.3733416199684143,\n",
       "   0.32947492599487305,\n",
       "   0.422782301902771,\n",
       "   0.32905885577201843,\n",
       "   0.3654867708683014,\n",
       "   0.3583530783653259,\n",
       "   0.43950334191322327,\n",
       "   0.32950571179389954,\n",
       "   0.40553298592567444,\n",
       "   0.3274851143360138,\n",
       "   0.3045823872089386,\n",
       "   0.29641589522361755,\n",
       "   0.3156414330005646,\n",
       "   0.3574490547180176,\n",
       "   0.2749400734901428,\n",
       "   0.3403913080692291,\n",
       "   0.3651795983314514,\n",
       "   0.48233258724212646,\n",
       "   0.32781755924224854,\n",
       "   0.43046873807907104,\n",
       "   0.3358585238456726,\n",
       "   0.3410099446773529,\n",
       "   0.4353466033935547,\n",
       "   0.3885558247566223,\n",
       "   0.3683567941188812,\n",
       "   0.3300045132637024,\n",
       "   0.4836511015892029,\n",
       "   0.5310142040252686,\n",
       "   0.3901999294757843,\n",
       "   0.24573472142219543,\n",
       "   0.40730971097946167,\n",
       "   0.2239132672548294,\n",
       "   0.2699413001537323,\n",
       "   0.44411247968673706,\n",
       "   0.3203762173652649,\n",
       "   0.3422766625881195,\n",
       "   0.36928316950798035,\n",
       "   0.5489611625671387,\n",
       "   0.2918457090854645,\n",
       "   0.32686612010002136,\n",
       "   0.35026752948760986,\n",
       "   0.4078007638454437,\n",
       "   0.3005848228931427,\n",
       "   0.3664662837982178,\n",
       "   0.3305678367614746,\n",
       "   0.28182074427604675,\n",
       "   0.3063458502292633,\n",
       "   0.28074851632118225,\n",
       "   0.32991984486579895,\n",
       "   0.49118348956108093,\n",
       "   0.4176871180534363,\n",
       "   0.34662777185440063,\n",
       "   0.4572369456291199,\n",
       "   0.2726455330848694,\n",
       "   0.26239901781082153,\n",
       "   0.34318071603775024,\n",
       "   0.31932124495506287,\n",
       "   0.28851017355918884,\n",
       "   0.4137610197067261,\n",
       "   0.439685195684433,\n",
       "   0.26850372552871704,\n",
       "   0.33064091205596924,\n",
       "   0.3281930088996887,\n",
       "   0.30009475350379944,\n",
       "   0.5218653678894043,\n",
       "   0.3784806430339813,\n",
       "   0.38888248801231384,\n",
       "   0.2816089391708374,\n",
       "   0.3595674932003021,\n",
       "   0.34282150864601135,\n",
       "   0.3198978900909424,\n",
       "   0.3654163181781769,\n",
       "   0.2871098518371582,\n",
       "   0.44058433175086975,\n",
       "   0.3090441823005676,\n",
       "   0.4069409668445587,\n",
       "   0.25240230560302734,\n",
       "   0.33660924434661865,\n",
       "   0.27838441729545593,\n",
       "   0.26716411113739014,\n",
       "   0.3691079020500183,\n",
       "   0.35302412509918213,\n",
       "   0.3279435336589813,\n",
       "   0.3836057484149933,\n",
       "   0.3367713987827301,\n",
       "   0.33981260657310486,\n",
       "   0.3551959693431854,\n",
       "   0.3284951448440552,\n",
       "   0.3533060848712921,\n",
       "   0.34277811646461487,\n",
       "   0.37061119079589844,\n",
       "   0.3811792731285095,\n",
       "   0.4187691807746887,\n",
       "   0.3977877199649811,\n",
       "   0.2954612672328949,\n",
       "   0.28651824593544006,\n",
       "   0.5798879265785217,\n",
       "   0.39464378356933594,\n",
       "   0.3723159730434418,\n",
       "   0.23799389600753784,\n",
       "   0.22366562485694885,\n",
       "   0.4594793915748596,\n",
       "   0.3326071500778198,\n",
       "   0.2693551778793335,\n",
       "   0.4091179072856903,\n",
       "   0.4515080451965332,\n",
       "   0.39416736364364624,\n",
       "   0.40642067790031433,\n",
       "   0.4063512980937958,\n",
       "   0.3139079809188843,\n",
       "   0.23094594478607178,\n",
       "   0.317475289106369,\n",
       "   0.3202219307422638,\n",
       "   0.38538259267807007,\n",
       "   0.33627358078956604,\n",
       "   0.3059421181678772,\n",
       "   0.4550134241580963,\n",
       "   0.3224451541900635,\n",
       "   0.3784394860267639,\n",
       "   0.3816107511520386,\n",
       "   0.31926560401916504,\n",
       "   0.2532181739807129,\n",
       "   0.41086363792419434,\n",
       "   0.3267849087715149,\n",
       "   0.2743438184261322,\n",
       "   0.3620425760746002,\n",
       "   0.2999122142791748,\n",
       "   0.2997981309890747,\n",
       "   0.4233969449996948,\n",
       "   0.3921487033367157,\n",
       "   0.37843769788742065,\n",
       "   0.3181154727935791,\n",
       "   0.39107853174209595,\n",
       "   0.39035889506340027,\n",
       "   0.3218974471092224,\n",
       "   0.4067618250846863,\n",
       "   0.34867075085639954,\n",
       "   0.3336225152015686,\n",
       "   0.3272028863430023,\n",
       "   0.3055347204208374,\n",
       "   0.3822973668575287,\n",
       "   0.3815903663635254,\n",
       "   0.3830597400665283,\n",
       "   0.31450220942497253,\n",
       "   0.25027889013290405,\n",
       "   0.48134881258010864,\n",
       "   0.3650745153427124,\n",
       "   0.3603326082229614,\n",
       "   0.3950154185295105,\n",
       "   0.4139285981655121,\n",
       "   0.3451887369155884,\n",
       "   0.2000681459903717,\n",
       "   0.30948859453201294,\n",
       "   0.31189608573913574,\n",
       "   0.3545697331428528,\n",
       "   0.2963378429412842,\n",
       "   0.33786624670028687,\n",
       "   0.2968813180923462,\n",
       "   0.40655794739723206,\n",
       "   0.4411163330078125,\n",
       "   0.2929624915122986,\n",
       "   0.2739798426628113,\n",
       "   0.3221830427646637,\n",
       "   0.29405584931373596,\n",
       "   0.37937596440315247,\n",
       "   0.3826355040073395,\n",
       "   0.38936880230903625,\n",
       "   0.3550090789794922,\n",
       "   0.31324562430381775,\n",
       "   0.3565140962600708,\n",
       "   0.37944260239601135,\n",
       "   0.3061840236186981,\n",
       "   0.4309616982936859]},\n",
       " 1: {'lr': 1e-06,\n",
       "  'best_loss_epoch': 22,\n",
       "  'best_acc_epoch': 75,\n",
       "  'best_r2_epoch': 231,\n",
       "  'pce_loss': [0.6564298868179321,\n",
       "   0.6657987236976624,\n",
       "   0.6584653258323669,\n",
       "   0.5966988801956177,\n",
       "   0.4965691864490509,\n",
       "   0.45469799637794495,\n",
       "   0.39500564336776733,\n",
       "   0.3787907660007477,\n",
       "   0.35525938868522644,\n",
       "   0.3430067002773285,\n",
       "   0.3217325210571289,\n",
       "   0.3156394362449646,\n",
       "   0.3054167926311493,\n",
       "   0.30670684576034546,\n",
       "   0.29967600107192993,\n",
       "   0.29979997873306274,\n",
       "   0.298591673374176,\n",
       "   0.31114059686660767,\n",
       "   0.30231526494026184,\n",
       "   0.31601211428642273,\n",
       "   0.2985740303993225,\n",
       "   0.2766808271408081,\n",
       "   0.2828048765659332,\n",
       "   0.2881260812282562,\n",
       "   0.2893405556678772,\n",
       "   0.298127681016922,\n",
       "   0.28421929478645325,\n",
       "   0.2727113366127014,\n",
       "   0.2682879567146301,\n",
       "   0.2658494710922241,\n",
       "   0.2674095034599304,\n",
       "   0.26027920842170715,\n",
       "   0.2603164315223694,\n",
       "   0.25644001364707947,\n",
       "   0.2528972029685974,\n",
       "   0.2494015395641327,\n",
       "   0.2528296411037445,\n",
       "   0.25239068269729614,\n",
       "   0.2591966986656189,\n",
       "   0.25458166003227234,\n",
       "   0.25913843512535095,\n",
       "   0.27418622374534607,\n",
       "   0.27314406633377075,\n",
       "   0.27466997504234314,\n",
       "   0.28126034140586853,\n",
       "   0.27903619408607483,\n",
       "   0.2830560505390167,\n",
       "   0.28180816769599915,\n",
       "   0.27479469776153564,\n",
       "   0.27212879061698914,\n",
       "   0.27286282181739807,\n",
       "   0.27011555433273315,\n",
       "   0.28652217984199524,\n",
       "   0.28424736857414246,\n",
       "   0.2823695242404938,\n",
       "   0.28380823135375977,\n",
       "   0.2886146903038025,\n",
       "   0.29243209958076477,\n",
       "   0.2890162169933319,\n",
       "   0.2752983272075653,\n",
       "   0.28385505080223083,\n",
       "   0.2842371165752411,\n",
       "   0.26719388365745544,\n",
       "   0.2816455364227295,\n",
       "   0.2773783802986145,\n",
       "   0.26879629492759705,\n",
       "   0.25552019476890564,\n",
       "   0.27031829953193665,\n",
       "   0.28705692291259766,\n",
       "   0.28678134083747864,\n",
       "   0.28376370668411255,\n",
       "   0.2735613286495209,\n",
       "   0.27316585183143616,\n",
       "   0.2584376931190491,\n",
       "   0.2573263645172119,\n",
       "   0.2585911452770233,\n",
       "   0.273732990026474,\n",
       "   0.26707229018211365,\n",
       "   0.287519246339798,\n",
       "   0.2968076169490814,\n",
       "   0.27969861030578613,\n",
       "   0.28637343645095825,\n",
       "   0.29487207531929016,\n",
       "   0.2984500825405121,\n",
       "   0.30379191040992737,\n",
       "   0.2868432104587555,\n",
       "   0.27410826086997986,\n",
       "   0.2693597376346588,\n",
       "   0.26487869024276733,\n",
       "   0.270103394985199,\n",
       "   0.2747235596179962,\n",
       "   0.2634265720844269,\n",
       "   0.2653070390224457,\n",
       "   0.26246047019958496,\n",
       "   0.2741706073284149,\n",
       "   0.2767781913280487,\n",
       "   0.2771579325199127,\n",
       "   0.2799961566925049,\n",
       "   0.2877524197101593,\n",
       "   0.2662767469882965,\n",
       "   0.25209155678749084,\n",
       "   0.2639766335487366,\n",
       "   0.2814713716506958,\n",
       "   0.276084303855896,\n",
       "   0.2728036642074585,\n",
       "   0.27538052201271057,\n",
       "   0.26642850041389465,\n",
       "   0.24539215862751007,\n",
       "   0.24406962096691132,\n",
       "   0.2370183765888214,\n",
       "   0.24340002238750458,\n",
       "   0.24129429459571838,\n",
       "   0.2389935404062271,\n",
       "   0.23783622682094574,\n",
       "   0.23774488270282745,\n",
       "   0.2487296313047409,\n",
       "   0.24647356569766998,\n",
       "   0.2624870240688324,\n",
       "   0.26126402616500854,\n",
       "   0.2860845625400543,\n",
       "   0.28769078850746155,\n",
       "   0.30163148045539856,\n",
       "   0.31010955572128296,\n",
       "   0.29524922370910645,\n",
       "   0.29505205154418945,\n",
       "   0.30243900418281555,\n",
       "   0.28637614846229553,\n",
       "   0.2977938950061798,\n",
       "   0.28042906522750854,\n",
       "   0.2832473814487457,\n",
       "   0.27349433302879333,\n",
       "   0.2844655215740204,\n",
       "   0.2887187600135803,\n",
       "   0.29093387722969055,\n",
       "   0.29302671551704407,\n",
       "   0.2942779064178467,\n",
       "   0.2980620563030243,\n",
       "   0.30127573013305664,\n",
       "   0.2975568473339081,\n",
       "   0.29296383261680603,\n",
       "   0.2920418381690979,\n",
       "   0.2761857211589813,\n",
       "   0.30647626519203186,\n",
       "   0.3017345070838928,\n",
       "   0.30689123272895813,\n",
       "   0.30454984307289124,\n",
       "   0.27423110604286194,\n",
       "   0.26808100938796997,\n",
       "   0.28008195757865906,\n",
       "   0.274929404258728,\n",
       "   0.2648940682411194,\n",
       "   0.27358439564704895,\n",
       "   0.26043179631233215,\n",
       "   0.23929734528064728,\n",
       "   0.2530962824821472,\n",
       "   0.25688251852989197,\n",
       "   0.26412665843963623,\n",
       "   0.25051072239875793,\n",
       "   0.26065531373023987,\n",
       "   0.2540697455406189,\n",
       "   0.2592962384223938,\n",
       "   0.25821706652641296,\n",
       "   0.2598411738872528,\n",
       "   0.25764572620391846,\n",
       "   0.25984880328178406,\n",
       "   0.27504032850265503,\n",
       "   0.2788970172405243,\n",
       "   0.27495077252388,\n",
       "   0.2769030034542084,\n",
       "   0.2943585515022278,\n",
       "   0.2750125527381897,\n",
       "   0.2665247619152069,\n",
       "   0.25789880752563477,\n",
       "   0.2579365074634552,\n",
       "   0.24897287786006927,\n",
       "   0.25351065397262573,\n",
       "   0.2547867000102997,\n",
       "   0.2530207335948944,\n",
       "   0.24437616765499115,\n",
       "   0.23861674964427948,\n",
       "   0.23139773309230804,\n",
       "   0.22292017936706543,\n",
       "   0.22704754769802094,\n",
       "   0.20706871151924133,\n",
       "   0.21821650862693787,\n",
       "   0.21918371319770813,\n",
       "   0.21932317316532135,\n",
       "   0.2145383656024933,\n",
       "   0.20558950304985046,\n",
       "   0.20638824999332428,\n",
       "   0.21264255046844482,\n",
       "   0.21222230792045593,\n",
       "   0.21259662508964539,\n",
       "   0.21818938851356506,\n",
       "   0.22243253886699677,\n",
       "   0.2251485139131546,\n",
       "   0.23484842479228973,\n",
       "   0.24503518640995026,\n",
       "   0.24761685729026794,\n",
       "   0.24527709186077118,\n",
       "   0.24081182479858398,\n",
       "   0.23653997480869293,\n",
       "   0.23861609399318695,\n",
       "   0.24198219180107117,\n",
       "   0.24250952899456024,\n",
       "   0.23552611470222473,\n",
       "   0.22764496505260468,\n",
       "   0.21778221428394318,\n",
       "   0.22457729279994965,\n",
       "   0.2293815165758133,\n",
       "   0.24689330160617828,\n",
       "   0.23890899121761322,\n",
       "   0.25339967012405396,\n",
       "   0.25991231203079224,\n",
       "   0.2555011510848999,\n",
       "   0.2512487471103668,\n",
       "   0.253436416387558,\n",
       "   0.24625326693058014,\n",
       "   0.26037248969078064,\n",
       "   0.262516587972641,\n",
       "   0.24697883427143097,\n",
       "   0.25883749127388,\n",
       "   0.24888192117214203,\n",
       "   0.23987773060798645,\n",
       "   0.2459271401166916,\n",
       "   0.25506648421287537,\n",
       "   0.24664698541164398,\n",
       "   0.25388458371162415,\n",
       "   0.24262014031410217,\n",
       "   0.23883159458637238,\n",
       "   0.2342156320810318,\n",
       "   0.2522205114364624,\n",
       "   0.25282084941864014,\n",
       "   0.24560515582561493,\n",
       "   0.23908738791942596,\n",
       "   0.2406989187002182,\n",
       "   0.241302028298378,\n",
       "   0.24057285487651825,\n",
       "   0.2533426582813263,\n",
       "   0.25461384654045105,\n",
       "   0.2520459294319153,\n",
       "   0.2472294569015503,\n",
       "   0.2466781586408615,\n",
       "   0.24683503806591034,\n",
       "   0.23898015916347504,\n",
       "   0.24057848751544952,\n",
       "   0.25459709763526917,\n",
       "   0.2480679452419281,\n",
       "   0.24645249545574188,\n",
       "   0.24910888075828552,\n",
       "   0.2474278211593628,\n",
       "   0.2553652226924896,\n",
       "   0.24463994801044464,\n",
       "   0.24673707783222198,\n",
       "   0.22593364119529724,\n",
       "   0.22706195712089539,\n",
       "   0.227281853556633,\n",
       "   0.23236680030822754,\n",
       "   0.23059208691120148,\n",
       "   0.23849473893642426,\n",
       "   0.24301639199256897,\n",
       "   0.24371810257434845,\n",
       "   0.2557657063007355,\n",
       "   0.2494133859872818,\n",
       "   0.251024454832077,\n",
       "   0.2461668998003006,\n",
       "   0.24502669274806976,\n",
       "   0.23482121527194977,\n",
       "   0.24394099414348602,\n",
       "   0.24335689842700958,\n",
       "   0.24051165580749512,\n",
       "   0.22615453600883484,\n",
       "   0.21982350945472717,\n",
       "   0.2178318351507187,\n",
       "   0.21973185241222382,\n",
       "   0.2234273999929428,\n",
       "   0.2300083339214325,\n",
       "   0.2344534546136856,\n",
       "   0.2314254194498062,\n",
       "   0.23532699048519135,\n",
       "   0.22496382892131805,\n",
       "   0.23359094560146332,\n",
       "   0.22562210261821747,\n",
       "   0.22292037308216095,\n",
       "   0.23383618891239166,\n",
       "   0.23159119486808777,\n",
       "   0.22108350694179535,\n",
       "   0.2149803787469864,\n",
       "   0.19710884988307953,\n",
       "   0.2222236841917038,\n",
       "   0.21467916667461395,\n",
       "   0.22274549305438995,\n",
       "   0.2378721535205841,\n",
       "   0.23513291776180267,\n",
       "   0.23215846717357635,\n",
       "   0.22228781878948212,\n",
       "   0.22813937067985535,\n",
       "   0.2191634178161621,\n",
       "   0.22639037668704987,\n",
       "   0.24487508833408356,\n",
       "   0.24989767372608185,\n",
       "   0.25930219888687134,\n",
       "   0.26666855812072754,\n",
       "   0.2647172510623932,\n",
       "   0.2588144838809967,\n",
       "   0.25348934531211853,\n",
       "   0.26451364159584045,\n",
       "   0.2610735595226288,\n",
       "   0.261405885219574,\n",
       "   0.2404312640428543,\n",
       "   0.25149592757225037,\n",
       "   0.2603495717048645,\n",
       "   0.2565760910511017,\n",
       "   0.281315416097641,\n",
       "   0.2666982412338257,\n",
       "   0.2684984505176544,\n",
       "   0.2852798104286194,\n",
       "   0.27713173627853394,\n",
       "   0.2659702003002167,\n",
       "   0.2424965649843216,\n",
       "   0.23756586015224457,\n",
       "   0.23164352774620056,\n",
       "   0.23536981642246246,\n",
       "   0.22715091705322266,\n",
       "   0.21771803498268127,\n",
       "   0.2122102677822113,\n",
       "   0.21757730841636658,\n",
       "   0.2370840460062027,\n",
       "   0.23755237460136414,\n",
       "   0.23893332481384277,\n",
       "   0.2349294275045395,\n",
       "   0.21520547568798065,\n",
       "   0.21638023853302002,\n",
       "   0.20868335664272308,\n",
       "   0.21343891322612762,\n",
       "   0.21670344471931458,\n",
       "   0.21646800637245178,\n",
       "   0.21916308999061584,\n",
       "   0.22794440388679504,\n",
       "   0.23081542551517487,\n",
       "   0.23429591953754425,\n",
       "   0.23379619419574738,\n",
       "   0.21906690299510956,\n",
       "   0.21713821589946747,\n",
       "   0.21063873171806335,\n",
       "   0.20610032975673676,\n",
       "   0.20676670968532562,\n",
       "   0.1968991607427597,\n",
       "   0.20538605749607086,\n",
       "   0.21282343566417694,\n",
       "   0.214683398604393,\n",
       "   0.22750987112522125,\n",
       "   0.22716443240642548,\n",
       "   0.2330334186553955,\n",
       "   0.24864326417446136,\n",
       "   0.2531145513057709,\n",
       "   0.2440422624349594,\n",
       "   0.2404627948999405,\n",
       "   0.25153690576553345,\n",
       "   0.2281210869550705,\n",
       "   0.24290786683559418,\n",
       "   0.2410437911748886,\n",
       "   0.24013231694698334,\n",
       "   0.2349560707807541,\n",
       "   0.2395155429840088,\n",
       "   0.23883391916751862,\n",
       "   0.23624078929424286,\n",
       "   0.2349274456501007,\n",
       "   0.22701573371887207,\n",
       "   0.23361720144748688,\n",
       "   0.23520994186401367,\n",
       "   0.2497587949037552,\n",
       "   0.25982365012168884,\n",
       "   0.25922858715057373,\n",
       "   0.2560300827026367,\n",
       "   0.25877276062965393,\n",
       "   0.2536483407020569,\n",
       "   0.2439737617969513,\n",
       "   0.2459193617105484,\n",
       "   0.24315212666988373,\n",
       "   0.23571300506591797,\n",
       "   0.233698770403862,\n",
       "   0.23875513672828674,\n",
       "   0.25031957030296326,\n",
       "   0.2509715259075165,\n",
       "   0.24934710562229156,\n",
       "   0.250277042388916,\n",
       "   0.2523937225341797,\n",
       "   0.24137680232524872,\n",
       "   0.2469196766614914,\n",
       "   0.24469222128391266,\n",
       "   0.23896953463554382,\n",
       "   0.2460080087184906,\n",
       "   0.24580290913581848,\n",
       "   0.2283385992050171,\n",
       "   0.229352205991745,\n",
       "   0.22950036823749542,\n",
       "   0.22133967280387878,\n",
       "   0.2225680649280548,\n",
       "   0.22719977796077728,\n",
       "   0.2220553159713745,\n",
       "   0.21995197236537933,\n",
       "   0.2299097627401352,\n",
       "   0.23791828751564026,\n",
       "   0.24299989640712738,\n",
       "   0.23403480648994446,\n",
       "   0.22403977811336517,\n",
       "   0.22844573855400085,\n",
       "   0.22844383120536804,\n",
       "   0.22223636507987976,\n",
       "   0.2190539687871933,\n",
       "   0.21097470819950104,\n",
       "   0.2283140867948532,\n",
       "   0.22547557950019836,\n",
       "   0.23344554007053375,\n",
       "   0.24003377556800842,\n",
       "   0.2418815642595291,\n",
       "   0.23718556761741638,\n",
       "   0.2426958531141281,\n",
       "   0.24337990581989288,\n",
       "   0.24891631305217743,\n",
       "   0.2672332227230072,\n",
       "   0.27014556527137756,\n",
       "   0.278124064207077,\n",
       "   0.2756820321083069,\n",
       "   0.2806638777256012,\n",
       "   0.2838997542858124,\n",
       "   0.3042728006839752,\n",
       "   0.2990846335887909,\n",
       "   0.3023945987224579,\n",
       "   0.2824435532093048,\n",
       "   0.2817026972770691,\n",
       "   0.2698097229003906,\n",
       "   0.2837488055229187,\n",
       "   0.2767854630947113,\n",
       "   0.28090453147888184,\n",
       "   0.263380229473114,\n",
       "   0.27416953444480896,\n",
       "   0.2901799976825714,\n",
       "   0.3010943531990051,\n",
       "   0.3143705427646637,\n",
       "   0.3139597773551941,\n",
       "   0.30231401324272156,\n",
       "   0.30028027296066284,\n",
       "   0.28386539220809937,\n",
       "   0.2779269218444824,\n",
       "   0.26152366399765015,\n",
       "   0.24979853630065918,\n",
       "   0.2521471679210663,\n",
       "   0.23540136218070984,\n",
       "   0.22819159924983978,\n",
       "   0.2345755398273468,\n",
       "   0.23653854429721832,\n",
       "   0.23929010331630707,\n",
       "   0.23189930617809296,\n",
       "   0.22255156934261322,\n",
       "   0.2274782508611679,\n",
       "   0.221320241689682,\n",
       "   0.22867491841316223,\n",
       "   0.22698405385017395,\n",
       "   0.22726456820964813,\n",
       "   0.22595299780368805,\n",
       "   0.2143680602312088,\n",
       "   0.2090880423784256,\n",
       "   0.21850919723510742,\n",
       "   0.2237100750207901,\n",
       "   0.21956562995910645,\n",
       "   0.19892854988574982,\n",
       "   0.21060451865196228,\n",
       "   0.20692013204097748,\n",
       "   0.21111534535884857,\n",
       "   0.21999764442443848,\n",
       "   0.23639194667339325,\n",
       "   0.22601531445980072,\n",
       "   0.23005564510822296,\n",
       "   0.22302725911140442,\n",
       "   0.21898381412029266,\n",
       "   0.2290782779455185,\n",
       "   0.2321247160434723,\n",
       "   0.24072086811065674,\n",
       "   0.24154898524284363,\n",
       "   0.2330225557088852,\n",
       "   0.21884119510650635,\n",
       "   0.21899795532226562,\n",
       "   0.21597562730312347,\n",
       "   0.217144176363945,\n",
       "   0.2274124175310135,\n",
       "   0.21785783767700195,\n",
       "   0.21392036974430084,\n",
       "   0.21729113161563873,\n",
       "   0.22714920341968536,\n",
       "   0.23316994309425354,\n",
       "   0.23765377700328827,\n",
       "   0.2425088733434677,\n",
       "   0.25632455945014954,\n",
       "   0.25222185254096985,\n",
       "   0.24437864124774933,\n",
       "   0.2478395402431488,\n",
       "   0.24685904383659363,\n",
       "   0.23842816054821014],\n",
       "  'voc_loss': [18.75828742980957,\n",
       "   11.162355422973633,\n",
       "   8.212972640991211,\n",
       "   5.606579780578613,\n",
       "   4.334367275238037,\n",
       "   3.4688150882720947,\n",
       "   2.663066864013672,\n",
       "   2.1357600688934326,\n",
       "   1.7540321350097656,\n",
       "   1.3872357606887817,\n",
       "   1.2034472227096558,\n",
       "   1.011354684829712,\n",
       "   0.8168448209762573,\n",
       "   0.6678794026374817,\n",
       "   0.5122882127761841,\n",
       "   0.43129464983940125,\n",
       "   0.38015180826187134,\n",
       "   0.328291118144989,\n",
       "   0.3020157516002655,\n",
       "   0.25475436449050903,\n",
       "   0.2171713262796402,\n",
       "   0.1858428716659546,\n",
       "   0.14365704357624054,\n",
       "   0.12028317153453827,\n",
       "   0.101430244743824,\n",
       "   0.07820726186037064,\n",
       "   0.07349389046430588,\n",
       "   0.06109744310379028,\n",
       "   0.05306237190961838,\n",
       "   0.055420804768800735,\n",
       "   0.04885641857981682,\n",
       "   0.05370554327964783,\n",
       "   0.051937080919742584,\n",
       "   0.044857777655124664,\n",
       "   0.04886099696159363,\n",
       "   0.034344688057899475,\n",
       "   0.027209797874093056,\n",
       "   0.022139456123113632,\n",
       "   0.020760009065270424,\n",
       "   0.01705778017640114,\n",
       "   0.018572811037302017,\n",
       "   0.017071546986699104,\n",
       "   0.019279610365629196,\n",
       "   0.015505755320191383,\n",
       "   0.017809558659791946,\n",
       "   0.016249049454927444,\n",
       "   0.02305445447564125,\n",
       "   0.019240889698266983,\n",
       "   0.020431125536561012,\n",
       "   0.017596280202269554,\n",
       "   0.015505758114159107,\n",
       "   0.016951216384768486,\n",
       "   0.014552501030266285,\n",
       "   0.02382074110209942,\n",
       "   0.025244100019335747,\n",
       "   0.024151895195245743,\n",
       "   0.025526318699121475,\n",
       "   0.02494892291724682,\n",
       "   0.022282734513282776,\n",
       "   0.021173903718590736,\n",
       "   0.01341363787651062,\n",
       "   0.006832674145698547,\n",
       "   0.007022505160421133,\n",
       "   0.00867520086467266,\n",
       "   0.005945699755102396,\n",
       "   0.004591910634189844,\n",
       "   0.004469589330255985,\n",
       "   0.004246007185429335,\n",
       "   0.0031295863445848227,\n",
       "   0.003645986784249544,\n",
       "   0.0038199068512767553,\n",
       "   0.00510991457849741,\n",
       "   0.006501428782939911,\n",
       "   0.006223827134817839,\n",
       "   0.005104136187583208,\n",
       "   0.0032427869737148285,\n",
       "   0.0034679730888456106,\n",
       "   0.004395213909447193,\n",
       "   0.0036196112632751465,\n",
       "   0.003735409816727042,\n",
       "   0.002599573927000165,\n",
       "   0.0025703399442136288,\n",
       "   0.0027692916337400675,\n",
       "   0.0016732992371544242,\n",
       "   0.001708149560727179,\n",
       "   0.0014573299558833241,\n",
       "   0.0016892937710508704,\n",
       "   0.0015866894973441958,\n",
       "   0.0016595375491306186,\n",
       "   0.003193620825186372,\n",
       "   0.0024611044209450483,\n",
       "   0.0017510857433080673,\n",
       "   0.0019234194187447429,\n",
       "   0.0023052431643009186,\n",
       "   0.004031392280012369,\n",
       "   0.0042462460696697235,\n",
       "   0.003969017416238785,\n",
       "   0.0059501719661056995,\n",
       "   0.003826605388894677,\n",
       "   0.0049732401967048645,\n",
       "   0.0042737992480397224,\n",
       "   0.004553685896098614,\n",
       "   0.0044079734943807125,\n",
       "   0.004322133492678404,\n",
       "   0.0046732304617762566,\n",
       "   0.005494728684425354,\n",
       "   0.005370136816054583,\n",
       "   0.005815496202558279,\n",
       "   0.005398531910032034,\n",
       "   0.00662333145737648,\n",
       "   0.010732186958193779,\n",
       "   0.009819301776587963,\n",
       "   0.01319965161383152,\n",
       "   0.012340960092842579,\n",
       "   0.01315920241177082,\n",
       "   0.01258816383779049,\n",
       "   0.011477787978947163,\n",
       "   0.010815358720719814,\n",
       "   0.010852242819964886,\n",
       "   0.01393827237188816,\n",
       "   0.013106021098792553,\n",
       "   0.014201716519892216,\n",
       "   0.01485779695212841,\n",
       "   0.014527476392686367,\n",
       "   0.014216474257409573,\n",
       "   0.01266727689653635,\n",
       "   0.015626776963472366,\n",
       "   0.015398566611111164,\n",
       "   0.01650957763195038,\n",
       "   0.01719212904572487,\n",
       "   0.01906898431479931,\n",
       "   0.023612501099705696,\n",
       "   0.02142307348549366,\n",
       "   0.02114715240895748,\n",
       "   0.019045550376176834,\n",
       "   0.019502654671669006,\n",
       "   0.018503278493881226,\n",
       "   0.017711633816361427,\n",
       "   0.017450062558054924,\n",
       "   0.015529071912169456,\n",
       "   0.015294677577912807,\n",
       "   0.0159353818744421,\n",
       "   0.014723754487931728,\n",
       "   0.017539991065859795,\n",
       "   0.018810240551829338,\n",
       "   0.018885700032114983,\n",
       "   0.016341155394911766,\n",
       "   0.017319850623607635,\n",
       "   0.020315071567893028,\n",
       "   0.020273922011256218,\n",
       "   0.02431582286953926,\n",
       "   0.01945970393717289,\n",
       "   0.019146140664815903,\n",
       "   0.02339721843600273,\n",
       "   0.02192908339202404,\n",
       "   0.02095797099173069,\n",
       "   0.022505030035972595,\n",
       "   0.02618648111820221,\n",
       "   0.025578584522008896,\n",
       "   0.02398204430937767,\n",
       "   0.02209744229912758,\n",
       "   0.02326706238090992,\n",
       "   0.022481145337224007,\n",
       "   0.021358830854296684,\n",
       "   0.019954608753323555,\n",
       "   0.01977398805320263,\n",
       "   0.01947123184800148,\n",
       "   0.022571584209799767,\n",
       "   0.024865740910172462,\n",
       "   0.02597120590507984,\n",
       "   0.0240639578551054,\n",
       "   0.025646980851888657,\n",
       "   0.029973123222589493,\n",
       "   0.028724515810608864,\n",
       "   0.026471873745322227,\n",
       "   0.027681956067681313,\n",
       "   0.02701619639992714,\n",
       "   0.030477354303002357,\n",
       "   0.035346828401088715,\n",
       "   0.03662529215216637,\n",
       "   0.037320829927921295,\n",
       "   0.04732422158122063,\n",
       "   0.052270326763391495,\n",
       "   0.06033540517091751,\n",
       "   0.06437089294195175,\n",
       "   0.07939008623361588,\n",
       "   0.07243473827838898,\n",
       "   0.0696437805891037,\n",
       "   0.0727517381310463,\n",
       "   0.07285331189632416,\n",
       "   0.07512164115905762,\n",
       "   0.07231331616640091,\n",
       "   0.07295851409435272,\n",
       "   0.06406963616609573,\n",
       "   0.06372017413377762,\n",
       "   0.05850539356470108,\n",
       "   0.05568543076515198,\n",
       "   0.06444095075130463,\n",
       "   0.06494145095348358,\n",
       "   0.06686536967754364,\n",
       "   0.07311732321977615,\n",
       "   0.06719408184289932,\n",
       "   0.06930733472108841,\n",
       "   0.06674428284168243,\n",
       "   0.0739704966545105,\n",
       "   0.07277590036392212,\n",
       "   0.06526952981948853,\n",
       "   0.07021339237689972,\n",
       "   0.06267048418521881,\n",
       "   0.054262127727270126,\n",
       "   0.05635597184300423,\n",
       "   0.054129958152770996,\n",
       "   0.0460180900990963,\n",
       "   0.05022289603948593,\n",
       "   0.0454615093767643,\n",
       "   0.04677819460630417,\n",
       "   0.05367603525519371,\n",
       "   0.058069150894880295,\n",
       "   0.07196909189224243,\n",
       "   0.07280422747135162,\n",
       "   0.07664889097213745,\n",
       "   0.06828407198190689,\n",
       "   0.05698370188474655,\n",
       "   0.05732668563723564,\n",
       "   0.07707453519105911,\n",
       "   0.06959302723407745,\n",
       "   0.07426531612873077,\n",
       "   0.07568083703517914,\n",
       "   0.05768558755517006,\n",
       "   0.056387901306152344,\n",
       "   0.06070259213447571,\n",
       "   0.06711507588624954,\n",
       "   0.06028652936220169,\n",
       "   0.0638897642493248,\n",
       "   0.062289584428071976,\n",
       "   0.06538219004869461,\n",
       "   0.07056912779808044,\n",
       "   0.06740331649780273,\n",
       "   0.05861015245318413,\n",
       "   0.05651523917913437,\n",
       "   0.0447038933634758,\n",
       "   0.040002238005399704,\n",
       "   0.04355775937438011,\n",
       "   0.04784337431192398,\n",
       "   0.04225257784128189,\n",
       "   0.040150728076696396,\n",
       "   0.05283317714929581,\n",
       "   0.04214772209525108,\n",
       "   0.042750388383865356,\n",
       "   0.050567056983709335,\n",
       "   0.05571293458342552,\n",
       "   0.05855727568268776,\n",
       "   0.0630153939127922,\n",
       "   0.05881486460566521,\n",
       "   0.06925363838672638,\n",
       "   0.0668334886431694,\n",
       "   0.0792495459318161,\n",
       "   0.0745692104101181,\n",
       "   0.07105692476034164,\n",
       "   0.05891473963856697,\n",
       "   0.058279335498809814,\n",
       "   0.05735413357615471,\n",
       "   0.05957603082060814,\n",
       "   0.061350904405117035,\n",
       "   0.06629101186990738,\n",
       "   0.07259518653154373,\n",
       "   0.08282981812953949,\n",
       "   0.08056914061307907,\n",
       "   0.08524376153945923,\n",
       "   0.08008108288049698,\n",
       "   0.07798850536346436,\n",
       "   0.07432357221841812,\n",
       "   0.06680984050035477,\n",
       "   0.07916788011789322,\n",
       "   0.06873414665460587,\n",
       "   0.07397054880857468,\n",
       "   0.0771331787109375,\n",
       "   0.07873409986495972,\n",
       "   0.08659076690673828,\n",
       "   0.08681338280439377,\n",
       "   0.0835413709282875,\n",
       "   0.08763215690851212,\n",
       "   0.08183375746011734,\n",
       "   0.0783248022198677,\n",
       "   0.07908706367015839,\n",
       "   0.07431383430957794,\n",
       "   0.08197499066591263,\n",
       "   0.07072386890649796,\n",
       "   0.07388728111982346,\n",
       "   0.07594840973615646,\n",
       "   0.07732518017292023,\n",
       "   0.07685991376638412,\n",
       "   0.08025317639112473,\n",
       "   0.07774996757507324,\n",
       "   0.08071954548358917,\n",
       "   0.07386370748281479,\n",
       "   0.07058484852313995,\n",
       "   0.07310253381729126,\n",
       "   0.07867903262376785,\n",
       "   0.07312583178281784,\n",
       "   0.07084929943084717,\n",
       "   0.07161108404397964,\n",
       "   0.07786693423986435,\n",
       "   0.07163850218057632,\n",
       "   0.06457649916410446,\n",
       "   0.07296483963727951,\n",
       "   0.06188073381781578,\n",
       "   0.05967698618769646,\n",
       "   0.05851108953356743,\n",
       "   0.05542523413896561,\n",
       "   0.04448157921433449,\n",
       "   0.040629543364048004,\n",
       "   0.047500234097242355,\n",
       "   0.04678109660744667,\n",
       "   0.04633554443717003,\n",
       "   0.04228449612855911,\n",
       "   0.03646758571267128,\n",
       "   0.04005234315991402,\n",
       "   0.045453425496816635,\n",
       "   0.04239863529801369,\n",
       "   0.047847066074609756,\n",
       "   0.050665877759456635,\n",
       "   0.0489569827914238,\n",
       "   0.049018632620573044,\n",
       "   0.05419503524899483,\n",
       "   0.05541759356856346,\n",
       "   0.05035797879099846,\n",
       "   0.04181452840566635,\n",
       "   0.041254740208387375,\n",
       "   0.03607085347175598,\n",
       "   0.04052906483411789,\n",
       "   0.04377369210124016,\n",
       "   0.047329485416412354,\n",
       "   0.04789171740412712,\n",
       "   0.04153593257069588,\n",
       "   0.03604677692055702,\n",
       "   0.038714226335287094,\n",
       "   0.03878629952669144,\n",
       "   0.05017365887761116,\n",
       "   0.04900972917675972,\n",
       "   0.04429187998175621,\n",
       "   0.04160735011100769,\n",
       "   0.04367804899811745,\n",
       "   0.04589158296585083,\n",
       "   0.0421433188021183,\n",
       "   0.04022153839468956,\n",
       "   0.03914565220475197,\n",
       "   0.04278009012341499,\n",
       "   0.04334539547562599,\n",
       "   0.04572637006640434,\n",
       "   0.05089304968714714,\n",
       "   0.047088246792554855,\n",
       "   0.042712513357400894,\n",
       "   0.03863383084535599,\n",
       "   0.037436939775943756,\n",
       "   0.03555179759860039,\n",
       "   0.04157239571213722,\n",
       "   0.03635300323367119,\n",
       "   0.05083983391523361,\n",
       "   0.05185549706220627,\n",
       "   0.04589070752263069,\n",
       "   0.044092535972595215,\n",
       "   0.04872379079461098,\n",
       "   0.0461614690721035,\n",
       "   0.0460929349064827,\n",
       "   0.04537857323884964,\n",
       "   0.04765641316771507,\n",
       "   0.05155744031071663,\n",
       "   0.05212683603167534,\n",
       "   0.049154870212078094,\n",
       "   0.05414748936891556,\n",
       "   0.052566371858119965,\n",
       "   0.04585925489664078,\n",
       "   0.04510226100683212,\n",
       "   0.0406222902238369,\n",
       "   0.0410282276570797,\n",
       "   0.04773357883095741,\n",
       "   0.055360618978738785,\n",
       "   0.04982007294893265,\n",
       "   0.05338854342699051,\n",
       "   0.05589408427476883,\n",
       "   0.058059800416231155,\n",
       "   0.05600855126976967,\n",
       "   0.046614862978458405,\n",
       "   0.045511309057474136,\n",
       "   0.0421062707901001,\n",
       "   0.04111742228269577,\n",
       "   0.04345184937119484,\n",
       "   0.03951168432831764,\n",
       "   0.04337889701128006,\n",
       "   0.041875068098306656,\n",
       "   0.038330163806676865,\n",
       "   0.03827172517776489,\n",
       "   0.03649839758872986,\n",
       "   0.03386880084872246,\n",
       "   0.033625680953264236,\n",
       "   0.03137553483247757,\n",
       "   0.024535106495022774,\n",
       "   0.027759814634919167,\n",
       "   0.02418399043381214,\n",
       "   0.02468658797442913,\n",
       "   0.025223776698112488,\n",
       "   0.02569565176963806,\n",
       "   0.02703927643597126,\n",
       "   0.02777842991054058,\n",
       "   0.023741155862808228,\n",
       "   0.025579731911420822,\n",
       "   0.025260159745812416,\n",
       "   0.020981639623641968,\n",
       "   0.01809282787144184,\n",
       "   0.01947605237364769,\n",
       "   0.020698321983218193,\n",
       "   0.024284830316901207,\n",
       "   0.031174365431070328,\n",
       "   0.03291649743914604,\n",
       "   0.02810332365334034,\n",
       "   0.027338743209838867,\n",
       "   0.02427619695663452,\n",
       "   0.025672104209661484,\n",
       "   0.028609607368707657,\n",
       "   0.0271633081138134,\n",
       "   0.021766817197203636,\n",
       "   0.024563563987612724,\n",
       "   0.03169666603207588,\n",
       "   0.02386343479156494,\n",
       "   0.021686723455786705,\n",
       "   0.021991267800331116,\n",
       "   0.02636677585542202,\n",
       "   0.023966683074831963,\n",
       "   0.02685547061264515,\n",
       "   0.029512405395507812,\n",
       "   0.029026761651039124,\n",
       "   0.03389137610793114,\n",
       "   0.035575781017541885,\n",
       "   0.039812956005334854,\n",
       "   0.04024573042988777,\n",
       "   0.0397552065551281,\n",
       "   0.040987759828567505,\n",
       "   0.042205262929201126,\n",
       "   0.03980955481529236,\n",
       "   0.03221165016293526,\n",
       "   0.03541308641433716,\n",
       "   0.034130971878767014,\n",
       "   0.029146725311875343,\n",
       "   0.03051009587943554,\n",
       "   0.03343958407640457,\n",
       "   0.036273520439863205,\n",
       "   0.032724831253290176,\n",
       "   0.026233693584799767,\n",
       "   0.02454940415918827,\n",
       "   0.026170959696173668,\n",
       "   0.022138075903058052,\n",
       "   0.022144746035337448,\n",
       "   0.02236759662628174,\n",
       "   0.022180765867233276,\n",
       "   0.025331782177090645,\n",
       "   0.02272595651447773,\n",
       "   0.021732956171035767,\n",
       "   0.022012880071997643,\n",
       "   0.022273004055023193,\n",
       "   0.02981153316795826,\n",
       "   0.026801656931638718,\n",
       "   0.02771514467895031,\n",
       "   0.029547588899731636,\n",
       "   0.025342481210827827,\n",
       "   0.025102365761995316,\n",
       "   0.027752937749028206,\n",
       "   0.03199049085378647,\n",
       "   0.02878291718661785,\n",
       "   0.029053611680865288,\n",
       "   0.025370175018906593,\n",
       "   0.020218679681420326,\n",
       "   0.019044453278183937,\n",
       "   0.023531386628746986,\n",
       "   0.024505246430635452,\n",
       "   0.02179718390107155,\n",
       "   0.014975524507462978,\n",
       "   0.016924666240811348,\n",
       "   0.018165981397032738,\n",
       "   0.014577400870621204,\n",
       "   0.012805976904928684,\n",
       "   0.012622790411114693,\n",
       "   0.012461910024285316,\n",
       "   0.014572163112461567,\n",
       "   0.01682146266102791,\n",
       "   0.0152138601988554,\n",
       "   0.017035653814673424,\n",
       "   0.018685951828956604,\n",
       "   0.019282322376966476,\n",
       "   0.017497999593615532,\n",
       "   0.024210503324866295,\n",
       "   0.02370520494878292,\n",
       "   0.02953154966235161,\n",
       "   0.02932770363986492,\n",
       "   0.02674773707985878,\n",
       "   0.024471526965498924,\n",
       "   0.01947227492928505,\n",
       "   0.019212782382965088,\n",
       "   0.017720472067594528,\n",
       "   0.017643196508288383],\n",
       "  'jsc_loss': [17.750810623168945,\n",
       "   7.4527482986450195,\n",
       "   3.850940704345703,\n",
       "   2.2034575939178467,\n",
       "   1.246465802192688,\n",
       "   0.6808146238327026,\n",
       "   0.3351062834262848,\n",
       "   0.15353891253471375,\n",
       "   0.05322171375155449,\n",
       "   0.015412603504955769,\n",
       "   0.0011750459671020508,\n",
       "   0.0147614860907197,\n",
       "   0.027532121166586876,\n",
       "   0.06406687945127487,\n",
       "   0.08747732639312744,\n",
       "   0.12366980314254761,\n",
       "   0.14561061561107635,\n",
       "   0.19037269055843353,\n",
       "   0.23781059682369232,\n",
       "   0.278096467256546,\n",
       "   0.30457422137260437,\n",
       "   0.31043940782546997,\n",
       "   0.32889270782470703,\n",
       "   0.36703208088874817,\n",
       "   0.4038143754005432,\n",
       "   0.42296504974365234,\n",
       "   0.4323388636112213,\n",
       "   0.4637300968170166,\n",
       "   0.45697611570358276,\n",
       "   0.4743385910987854,\n",
       "   0.49501651525497437,\n",
       "   0.5468273758888245,\n",
       "   0.5548034310340881,\n",
       "   0.5508025884628296,\n",
       "   0.5645176768302917,\n",
       "   0.5664708614349365,\n",
       "   0.5546961426734924,\n",
       "   0.574687659740448,\n",
       "   0.5726893544197083,\n",
       "   0.5835405588150024,\n",
       "   0.5859895944595337,\n",
       "   0.583551824092865,\n",
       "   0.5733805894851685,\n",
       "   0.5884009003639221,\n",
       "   0.5977419018745422,\n",
       "   0.6118707060813904,\n",
       "   0.6365473866462708,\n",
       "   0.6300696730613708,\n",
       "   0.6709795594215393,\n",
       "   0.6710847020149231,\n",
       "   0.667026937007904,\n",
       "   0.6862081289291382,\n",
       "   0.7108830809593201,\n",
       "   0.6985649466514587,\n",
       "   0.7032307982444763,\n",
       "   0.6928383708000183,\n",
       "   0.7052752375602722,\n",
       "   0.7075794339179993,\n",
       "   0.6830947995185852,\n",
       "   0.6797967553138733,\n",
       "   0.6794651746749878,\n",
       "   0.6820937395095825,\n",
       "   0.6632474064826965,\n",
       "   0.6562592387199402,\n",
       "   0.6530910730361938,\n",
       "   0.6506067514419556,\n",
       "   0.6473297476768494,\n",
       "   0.6607285737991333,\n",
       "   0.6630500555038452,\n",
       "   0.6429711580276489,\n",
       "   0.6416261792182922,\n",
       "   0.6447797417640686,\n",
       "   0.6399358510971069,\n",
       "   0.6269100308418274,\n",
       "   0.602046012878418,\n",
       "   0.5945287942886353,\n",
       "   0.6100589036941528,\n",
       "   0.6097450852394104,\n",
       "   0.6332622170448303,\n",
       "   0.64896160364151,\n",
       "   0.642154335975647,\n",
       "   0.649636447429657,\n",
       "   0.6245976090431213,\n",
       "   0.6500206589698792,\n",
       "   0.6649184226989746,\n",
       "   0.6640042662620544,\n",
       "   0.6762751340866089,\n",
       "   0.6467453837394714,\n",
       "   0.6224746704101562,\n",
       "   0.6170976758003235,\n",
       "   0.6097561120986938,\n",
       "   0.6213240027427673,\n",
       "   0.6260504126548767,\n",
       "   0.6344860792160034,\n",
       "   0.6471978425979614,\n",
       "   0.6686164140701294,\n",
       "   0.678489089012146,\n",
       "   0.6962178349494934,\n",
       "   0.7037988901138306,\n",
       "   0.6881829500198364,\n",
       "   0.6983672380447388,\n",
       "   0.7172269225120544,\n",
       "   0.7248634099960327,\n",
       "   0.7225407958030701,\n",
       "   0.7367342710494995,\n",
       "   0.7230062484741211,\n",
       "   0.7262111306190491,\n",
       "   0.7206569314002991,\n",
       "   0.6920326948165894,\n",
       "   0.6786836385726929,\n",
       "   0.6668696403503418,\n",
       "   0.6842842102050781,\n",
       "   0.6794914603233337,\n",
       "   0.6795275807380676,\n",
       "   0.6516607999801636,\n",
       "   0.6442794799804688,\n",
       "   0.6349332928657532,\n",
       "   0.6337413787841797,\n",
       "   0.6262311935424805,\n",
       "   0.6382641196250916,\n",
       "   0.6461753845214844,\n",
       "   0.6685864329338074,\n",
       "   0.6900973320007324,\n",
       "   0.6953915953636169,\n",
       "   0.6778231263160706,\n",
       "   0.6632621884346008,\n",
       "   0.6549790501594543,\n",
       "   0.6516519784927368,\n",
       "   0.6307759881019592,\n",
       "   0.6386705040931702,\n",
       "   0.6270068883895874,\n",
       "   0.6393347978591919,\n",
       "   0.6166203618049622,\n",
       "   0.6183554530143738,\n",
       "   0.6400206685066223,\n",
       "   0.6426992416381836,\n",
       "   0.6236535310745239,\n",
       "   0.621390700340271,\n",
       "   0.6132634878158569,\n",
       "   0.6059583425521851,\n",
       "   0.6158121824264526,\n",
       "   0.6151038408279419,\n",
       "   0.6456834077835083,\n",
       "   0.6419402360916138,\n",
       "   0.6287046074867249,\n",
       "   0.6471832990646362,\n",
       "   0.612956166267395,\n",
       "   0.6043404340744019,\n",
       "   0.6195330023765564,\n",
       "   0.6331307291984558,\n",
       "   0.620625913143158,\n",
       "   0.617245078086853,\n",
       "   0.6522286534309387,\n",
       "   0.6615235209465027,\n",
       "   0.6552901864051819,\n",
       "   0.6690837144851685,\n",
       "   0.6892999410629272,\n",
       "   0.6779770255088806,\n",
       "   0.6785814166069031,\n",
       "   0.6951047778129578,\n",
       "   0.7012572884559631,\n",
       "   0.6925825476646423,\n",
       "   0.6791247725486755,\n",
       "   0.7069514989852905,\n",
       "   0.6791129112243652,\n",
       "   0.6645375490188599,\n",
       "   0.687498927116394,\n",
       "   0.6716758012771606,\n",
       "   0.6763724684715271,\n",
       "   0.688136637210846,\n",
       "   0.7120099663734436,\n",
       "   0.7049503326416016,\n",
       "   0.7117792963981628,\n",
       "   0.7223014235496521,\n",
       "   0.6961716413497925,\n",
       "   0.6898541450500488,\n",
       "   0.676979660987854,\n",
       "   0.68185955286026,\n",
       "   0.7044548988342285,\n",
       "   0.6928386688232422,\n",
       "   0.6913976669311523,\n",
       "   0.6877780556678772,\n",
       "   0.6751035451889038,\n",
       "   0.6775820851325989,\n",
       "   0.6918319463729858,\n",
       "   0.6862539649009705,\n",
       "   0.6989521980285645,\n",
       "   0.6843293309211731,\n",
       "   0.7080492377281189,\n",
       "   0.7254262566566467,\n",
       "   0.7273290753364563,\n",
       "   0.7297409772872925,\n",
       "   0.7330735325813293,\n",
       "   0.758152961730957,\n",
       "   0.766103208065033,\n",
       "   0.7581386566162109,\n",
       "   0.7652212381362915,\n",
       "   0.7477355003356934,\n",
       "   0.7261313199996948,\n",
       "   0.7260474562644958,\n",
       "   0.7468544244766235,\n",
       "   0.7065480351448059,\n",
       "   0.6957124471664429,\n",
       "   0.6894908547401428,\n",
       "   0.6896035075187683,\n",
       "   0.6842368841171265,\n",
       "   0.6795620322227478,\n",
       "   0.678159773349762,\n",
       "   0.689419150352478,\n",
       "   0.7203130722045898,\n",
       "   0.7201728224754333,\n",
       "   0.724886417388916,\n",
       "   0.7022204995155334,\n",
       "   0.7072655558586121,\n",
       "   0.6883317232131958,\n",
       "   0.6547587513923645,\n",
       "   0.6667843461036682,\n",
       "   0.6733726859092712,\n",
       "   0.6603459715843201,\n",
       "   0.6428676843643188,\n",
       "   0.6317457556724548,\n",
       "   0.6587179899215698,\n",
       "   0.6546269655227661,\n",
       "   0.6564550995826721,\n",
       "   0.643559992313385,\n",
       "   0.6287697553634644,\n",
       "   0.6177034378051758,\n",
       "   0.6332892179489136,\n",
       "   0.6188198924064636,\n",
       "   0.6310874223709106,\n",
       "   0.6525009870529175,\n",
       "   0.6456709504127502,\n",
       "   0.6367300748825073,\n",
       "   0.6254709362983704,\n",
       "   0.6302957534790039,\n",
       "   0.6345008015632629,\n",
       "   0.6125169992446899,\n",
       "   0.6287516951560974,\n",
       "   0.6335959434509277,\n",
       "   0.6321752667427063,\n",
       "   0.6632293462753296,\n",
       "   0.6887407302856445,\n",
       "   0.7053585648536682,\n",
       "   0.7157118916511536,\n",
       "   0.7131503224372864,\n",
       "   0.7032333016395569,\n",
       "   0.6790353059768677,\n",
       "   0.6815140843391418,\n",
       "   0.6853479146957397,\n",
       "   0.6895301938056946,\n",
       "   0.6888367533683777,\n",
       "   0.6945690512657166,\n",
       "   0.7129591107368469,\n",
       "   0.7028099298477173,\n",
       "   0.7190064191818237,\n",
       "   0.7245370149612427,\n",
       "   0.7314732670783997,\n",
       "   0.699338436126709,\n",
       "   0.7219258546829224,\n",
       "   0.7370479702949524,\n",
       "   0.7195615172386169,\n",
       "   0.7187015414237976,\n",
       "   0.7289215326309204,\n",
       "   0.7342689633369446,\n",
       "   0.7085394859313965,\n",
       "   0.6994648575782776,\n",
       "   0.7163850665092468,\n",
       "   0.7208027839660645,\n",
       "   0.6941385269165039,\n",
       "   0.7077655792236328,\n",
       "   0.7126297950744629,\n",
       "   0.7238864898681641,\n",
       "   0.7341895699501038,\n",
       "   0.7058890461921692,\n",
       "   0.7198298573493958,\n",
       "   0.7083983421325684,\n",
       "   0.6900858879089355,\n",
       "   0.6955513954162598,\n",
       "   0.7015830874443054,\n",
       "   0.7036605477333069,\n",
       "   0.7024661302566528,\n",
       "   0.7095575928688049,\n",
       "   0.7479199171066284,\n",
       "   0.7149389386177063,\n",
       "   0.7085323333740234,\n",
       "   0.7093108296394348,\n",
       "   0.7117509245872498,\n",
       "   0.704356849193573,\n",
       "   0.6744948625564575,\n",
       "   0.6726244688034058,\n",
       "   0.6840264201164246,\n",
       "   0.6917924284934998,\n",
       "   0.7086992263793945,\n",
       "   0.6835557818412781,\n",
       "   0.693909764289856,\n",
       "   0.6705209612846375,\n",
       "   0.6380892992019653,\n",
       "   0.6430120468139648,\n",
       "   0.6416414976119995,\n",
       "   0.6263266801834106,\n",
       "   0.6334571838378906,\n",
       "   0.653346061706543,\n",
       "   0.6818196177482605,\n",
       "   0.7059345245361328,\n",
       "   0.688581645488739,\n",
       "   0.6707068681716919,\n",
       "   0.6522911787033081,\n",
       "   0.6607483625411987,\n",
       "   0.7109915018081665,\n",
       "   0.709895670413971,\n",
       "   0.7016380429267883,\n",
       "   0.697780191898346,\n",
       "   0.6594590544700623,\n",
       "   0.6536741256713867,\n",
       "   0.6829144358634949,\n",
       "   0.7147380113601685,\n",
       "   0.7387531995773315,\n",
       "   0.7470182776451111,\n",
       "   0.7369338274002075,\n",
       "   0.7051925659179688,\n",
       "   0.7068489193916321,\n",
       "   0.7146249413490295,\n",
       "   0.6942091584205627,\n",
       "   0.6889525651931763,\n",
       "   0.7051457166671753,\n",
       "   0.6999143362045288,\n",
       "   0.6936525106430054,\n",
       "   0.7052114605903625,\n",
       "   0.7064128518104553,\n",
       "   0.7336772680282593,\n",
       "   0.750649094581604,\n",
       "   0.712034285068512,\n",
       "   0.7001129984855652,\n",
       "   0.6975888609886169,\n",
       "   0.6985324621200562,\n",
       "   0.6772101521492004,\n",
       "   0.6733108162879944,\n",
       "   0.6668176651000977,\n",
       "   0.6413906812667847,\n",
       "   0.664239764213562,\n",
       "   0.6813737750053406,\n",
       "   0.6899606585502625,\n",
       "   0.7000845670700073,\n",
       "   0.6863301396369934,\n",
       "   0.6971758008003235,\n",
       "   0.737653911113739,\n",
       "   0.7209678888320923,\n",
       "   0.7350922226905823,\n",
       "   0.7273200750350952,\n",
       "   0.7001188397407532,\n",
       "   0.6918845772743225,\n",
       "   0.7074758410453796,\n",
       "   0.6985500454902649,\n",
       "   0.672106146812439,\n",
       "   0.6966762542724609,\n",
       "   0.6892805695533752,\n",
       "   0.6779183149337769,\n",
       "   0.6711639761924744,\n",
       "   0.6695712804794312,\n",
       "   0.6719138026237488,\n",
       "   0.6874409317970276,\n",
       "   0.6825264096260071,\n",
       "   0.6796931028366089,\n",
       "   0.6782776117324829,\n",
       "   0.6843878030776978,\n",
       "   0.6951745748519897,\n",
       "   0.7002511620521545,\n",
       "   0.6722133755683899,\n",
       "   0.6566208600997925,\n",
       "   0.67699134349823,\n",
       "   0.6875622868537903,\n",
       "   0.6933282613754272,\n",
       "   0.6760721206665039,\n",
       "   0.6735391616821289,\n",
       "   0.663946270942688,\n",
       "   0.674443244934082,\n",
       "   0.6936034560203552,\n",
       "   0.7303608059883118,\n",
       "   0.7443938851356506,\n",
       "   0.7618180513381958,\n",
       "   0.7572535276412964,\n",
       "   0.7464628219604492,\n",
       "   0.7458568811416626,\n",
       "   0.7839746475219727,\n",
       "   0.7800988554954529,\n",
       "   0.7720251679420471,\n",
       "   0.8049383163452148,\n",
       "   0.7891303896903992,\n",
       "   0.7612345814704895,\n",
       "   0.772546112537384,\n",
       "   0.7633137702941895,\n",
       "   0.7560872435569763,\n",
       "   0.7642977833747864,\n",
       "   0.7419244050979614,\n",
       "   0.7431479096412659,\n",
       "   0.7367939352989197,\n",
       "   0.7350296974182129,\n",
       "   0.7328637838363647,\n",
       "   0.7360066771507263,\n",
       "   0.7358990907669067,\n",
       "   0.736454963684082,\n",
       "   0.7363861203193665,\n",
       "   0.7191134095191956,\n",
       "   0.7175673246383667,\n",
       "   0.7259334921836853,\n",
       "   0.7438329458236694,\n",
       "   0.7462870478630066,\n",
       "   0.745631992816925,\n",
       "   0.7412957549095154,\n",
       "   0.732725203037262,\n",
       "   0.7139805555343628,\n",
       "   0.713283360004425,\n",
       "   0.7061437368392944,\n",
       "   0.7073953747749329,\n",
       "   0.723893940448761,\n",
       "   0.7089511752128601,\n",
       "   0.7413945198059082,\n",
       "   0.7186883091926575,\n",
       "   0.7335381507873535,\n",
       "   0.725462794303894,\n",
       "   0.7134736180305481,\n",
       "   0.7000397443771362,\n",
       "   0.711491048336029,\n",
       "   0.6955832242965698,\n",
       "   0.6905421018600464,\n",
       "   0.7027838826179504,\n",
       "   0.7208475470542908,\n",
       "   0.7167478799819946,\n",
       "   0.6876420378684998,\n",
       "   0.6678982973098755,\n",
       "   0.6652735471725464,\n",
       "   0.6767167448997498,\n",
       "   0.7010434865951538,\n",
       "   0.7340227961540222,\n",
       "   0.7217822074890137,\n",
       "   0.711837112903595,\n",
       "   0.7005805969238281,\n",
       "   0.6952269673347473,\n",
       "   0.7075857520103455,\n",
       "   0.6991693377494812,\n",
       "   0.7098755240440369,\n",
       "   0.6938254237174988,\n",
       "   0.69712895154953,\n",
       "   0.7044251561164856,\n",
       "   0.6936710476875305,\n",
       "   0.6902896165847778,\n",
       "   0.6615731120109558,\n",
       "   0.6592463254928589,\n",
       "   0.6445208191871643,\n",
       "   0.6247915625572205,\n",
       "   0.6201763153076172,\n",
       "   0.6229113936424255,\n",
       "   0.6429647207260132,\n",
       "   0.6504189372062683,\n",
       "   0.6614966988563538,\n",
       "   0.6881531476974487,\n",
       "   0.6959288716316223,\n",
       "   0.6816738843917847,\n",
       "   0.6879755258560181,\n",
       "   0.6868600845336914,\n",
       "   0.7031874060630798,\n",
       "   0.7194378972053528,\n",
       "   0.7270873188972473,\n",
       "   0.717675507068634,\n",
       "   0.7421861290931702,\n",
       "   0.7259859442710876,\n",
       "   0.7257727980613708,\n",
       "   0.7166728973388672,\n",
       "   0.7394156455993652,\n",
       "   0.7241262793540955,\n",
       "   0.7535436749458313,\n",
       "   0.7526664137840271,\n",
       "   0.7177550196647644,\n",
       "   0.7246559858322144,\n",
       "   0.7376141548156738,\n",
       "   0.7502345442771912,\n",
       "   0.7569228410720825,\n",
       "   0.7751766443252563,\n",
       "   0.7719277143478394,\n",
       "   0.7444929480552673,\n",
       "   0.7334961891174316,\n",
       "   0.7213002443313599,\n",
       "   0.7210574746131897,\n",
       "   0.7278861999511719,\n",
       "   0.7111073732376099,\n",
       "   0.7092012166976929,\n",
       "   0.7251185774803162,\n",
       "   0.7224326729774475,\n",
       "   0.7323698997497559,\n",
       "   0.7447642087936401,\n",
       "   0.7577160596847534,\n",
       "   0.7481859922409058,\n",
       "   0.7153674364089966,\n",
       "   0.722230851650238,\n",
       "   0.6975725889205933,\n",
       "   0.7077903747558594,\n",
       "   0.7092102766036987,\n",
       "   0.7056059241294861,\n",
       "   0.7131023406982422,\n",
       "   0.7027510404586792],\n",
       "  'ff_loss': [1.63880455493927,\n",
       "   0.6742035746574402,\n",
       "   0.3245871365070343,\n",
       "   0.18096771836280823,\n",
       "   0.06230532377958298,\n",
       "   0.02299243025481701,\n",
       "   0.012045465409755707,\n",
       "   0.00556687219068408,\n",
       "   0.005441787652671337,\n",
       "   0.005491469521075487,\n",
       "   0.006860163528472185,\n",
       "   0.009530263021588326,\n",
       "   0.015346414409577847,\n",
       "   0.018883077427744865,\n",
       "   0.02158883400261402,\n",
       "   0.023564262315630913,\n",
       "   0.02967476285994053,\n",
       "   0.031436894088983536,\n",
       "   0.03553956374526024,\n",
       "   0.04593092203140259,\n",
       "   0.04598158597946167,\n",
       "   0.05468546599149704,\n",
       "   0.06217210739850998,\n",
       "   0.06519101560115814,\n",
       "   0.0705542340874672,\n",
       "   0.0757676213979721,\n",
       "   0.08237264305353165,\n",
       "   0.09145039319992065,\n",
       "   0.08577760308980942,\n",
       "   0.08772385865449905,\n",
       "   0.08693262189626694,\n",
       "   0.09086515009403229,\n",
       "   0.08791700750589371,\n",
       "   0.08965332061052322,\n",
       "   0.08113694936037064,\n",
       "   0.07669230550527573,\n",
       "   0.08118412643671036,\n",
       "   0.08684821426868439,\n",
       "   0.09470747411251068,\n",
       "   0.1022527664899826,\n",
       "   0.10561390221118927,\n",
       "   0.10881693661212921,\n",
       "   0.10760398954153061,\n",
       "   0.11322088539600372,\n",
       "   0.11614281684160233,\n",
       "   0.11161548644304276,\n",
       "   0.10898588597774506,\n",
       "   0.11407630145549774,\n",
       "   0.10786460340023041,\n",
       "   0.11545216292142868,\n",
       "   0.11243271082639694,\n",
       "   0.11462145298719406,\n",
       "   0.12430029362440109,\n",
       "   0.12486666440963745,\n",
       "   0.11779004335403442,\n",
       "   0.1223832368850708,\n",
       "   0.1316913366317749,\n",
       "   0.11660929024219513,\n",
       "   0.11230655759572983,\n",
       "   0.11472063511610031,\n",
       "   0.11426805704832077,\n",
       "   0.11554902791976929,\n",
       "   0.1053660437464714,\n",
       "   0.10787685215473175,\n",
       "   0.10046442598104477,\n",
       "   0.10294032841920853,\n",
       "   0.10000409930944443,\n",
       "   0.09632474929094315,\n",
       "   0.0980210155248642,\n",
       "   0.09214049577713013,\n",
       "   0.09062366187572479,\n",
       "   0.0868421196937561,\n",
       "   0.09716086834669113,\n",
       "   0.10735372453927994,\n",
       "   0.10196361690759659,\n",
       "   0.09549535810947418,\n",
       "   0.08596312999725342,\n",
       "   0.0897257849574089,\n",
       "   0.08469897508621216,\n",
       "   0.09212026745080948,\n",
       "   0.09989647567272186,\n",
       "   0.09127762168645859,\n",
       "   0.09916217625141144,\n",
       "   0.09414196759462357,\n",
       "   0.09670151770114899,\n",
       "   0.09838105738162994,\n",
       "   0.1000867635011673,\n",
       "   0.09695912152528763,\n",
       "   0.09886407852172852,\n",
       "   0.10397278517484665,\n",
       "   0.09454559534788132,\n",
       "   0.091081403195858,\n",
       "   0.09810443967580795,\n",
       "   0.09246976673603058,\n",
       "   0.09845613688230515,\n",
       "   0.08853495866060257,\n",
       "   0.08695236593484879,\n",
       "   0.08902646601200104,\n",
       "   0.09187246114015579,\n",
       "   0.09567241370677948,\n",
       "   0.09064556658267975,\n",
       "   0.0894472673535347,\n",
       "   0.08614061027765274,\n",
       "   0.0863185003399849,\n",
       "   0.08816570043563843,\n",
       "   0.08796685189008713,\n",
       "   0.08691926300525665,\n",
       "   0.08500482141971588,\n",
       "   0.08939210325479507,\n",
       "   0.08702965825796127,\n",
       "   0.09427326172590256,\n",
       "   0.09862152487039566,\n",
       "   0.1012982502579689,\n",
       "   0.1010260358452797,\n",
       "   0.09600760042667389,\n",
       "   0.0912991389632225,\n",
       "   0.09311233460903168,\n",
       "   0.0904449075460434,\n",
       "   0.09252297878265381,\n",
       "   0.10093953460454941,\n",
       "   0.09092884510755539,\n",
       "   0.09428970515727997,\n",
       "   0.09210040420293808,\n",
       "   0.08254140615463257,\n",
       "   0.08054903894662857,\n",
       "   0.07931727916002274,\n",
       "   0.08126481622457504,\n",
       "   0.09082160890102386,\n",
       "   0.08352167904376984,\n",
       "   0.08380816131830215,\n",
       "   0.08522012084722519,\n",
       "   0.09552615135908127,\n",
       "   0.09831167012453079,\n",
       "   0.09651391953229904,\n",
       "   0.10530626028776169,\n",
       "   0.09545256197452545,\n",
       "   0.10790765285491943,\n",
       "   0.10898329317569733,\n",
       "   0.109413281083107,\n",
       "   0.10354667156934738,\n",
       "   0.11097653210163116,\n",
       "   0.1100182756781578,\n",
       "   0.11948943883180618,\n",
       "   0.12674084305763245,\n",
       "   0.1211918368935585,\n",
       "   0.12363703548908234,\n",
       "   0.12251666933298111,\n",
       "   0.12771427631378174,\n",
       "   0.12397551536560059,\n",
       "   0.12136360257863998,\n",
       "   0.1173841580748558,\n",
       "   0.1078466922044754,\n",
       "   0.10379990190267563,\n",
       "   0.10682376474142075,\n",
       "   0.10891643911600113,\n",
       "   0.11152594536542892,\n",
       "   0.10499078035354614,\n",
       "   0.11958170682191849,\n",
       "   0.12319272011518478,\n",
       "   0.11645296961069107,\n",
       "   0.11281885206699371,\n",
       "   0.10921529680490494,\n",
       "   0.10745713859796524,\n",
       "   0.10465135425329208,\n",
       "   0.11386597901582718,\n",
       "   0.10548065602779388,\n",
       "   0.09446419030427933,\n",
       "   0.09426696598529816,\n",
       "   0.09914761036634445,\n",
       "   0.09621249884366989,\n",
       "   0.0996660366654396,\n",
       "   0.0924338549375534,\n",
       "   0.0973387211561203,\n",
       "   0.09712895005941391,\n",
       "   0.09992805868387222,\n",
       "   0.09885071218013763,\n",
       "   0.09553386270999908,\n",
       "   0.09148187935352325,\n",
       "   0.08537130057811737,\n",
       "   0.08495824784040451,\n",
       "   0.08347847312688828,\n",
       "   0.0866679698228836,\n",
       "   0.0848141759634018,\n",
       "   0.08480750024318695,\n",
       "   0.08259475976228714,\n",
       "   0.07967383414506912,\n",
       "   0.0803794413805008,\n",
       "   0.07996999472379684,\n",
       "   0.0762685090303421,\n",
       "   0.08565294742584229,\n",
       "   0.09422983974218369,\n",
       "   0.09824216365814209,\n",
       "   0.1008557602763176,\n",
       "   0.10321737825870514,\n",
       "   0.10736168920993805,\n",
       "   0.120106041431427,\n",
       "   0.11515849828720093,\n",
       "   0.11995439231395721,\n",
       "   0.11446673423051834,\n",
       "   0.11434566229581833,\n",
       "   0.11355404555797577,\n",
       "   0.11974722146987915,\n",
       "   0.108860082924366,\n",
       "   0.12352097779512405,\n",
       "   0.12420344352722168,\n",
       "   0.1180567592382431,\n",
       "   0.1124701052904129,\n",
       "   0.11068881303071976,\n",
       "   0.10457732528448105,\n",
       "   0.1080661341547966,\n",
       "   0.11731430143117905,\n",
       "   0.11147322505712509,\n",
       "   0.10783662647008896,\n",
       "   0.12445318698883057,\n",
       "   0.11744169145822525,\n",
       "   0.11947304010391235,\n",
       "   0.11198721081018448,\n",
       "   0.10528744012117386,\n",
       "   0.09615687280893326,\n",
       "   0.10299909859895706,\n",
       "   0.10642792284488678,\n",
       "   0.1046919897198677,\n",
       "   0.10155551880598068,\n",
       "   0.10374075919389725,\n",
       "   0.10514684021472931,\n",
       "   0.1087271198630333,\n",
       "   0.11248799413442612,\n",
       "   0.11499358713626862,\n",
       "   0.11599280685186386,\n",
       "   0.12356831133365631,\n",
       "   0.10960221290588379,\n",
       "   0.09614914655685425,\n",
       "   0.09534119069576263,\n",
       "   0.09607351571321487,\n",
       "   0.1012837365269661,\n",
       "   0.09572016447782516,\n",
       "   0.09497334808111191,\n",
       "   0.089235819876194,\n",
       "   0.09635242074728012,\n",
       "   0.08958063274621964,\n",
       "   0.09009552001953125,\n",
       "   0.09573165327310562,\n",
       "   0.10045202821493149,\n",
       "   0.09502530843019485,\n",
       "   0.10015114396810532,\n",
       "   0.09840929508209229,\n",
       "   0.10674957185983658,\n",
       "   0.11190969496965408,\n",
       "   0.11055941134691238,\n",
       "   0.11788757145404816,\n",
       "   0.10742579400539398,\n",
       "   0.10318440198898315,\n",
       "   0.11010880768299103,\n",
       "   0.12250926345586777,\n",
       "   0.1237533912062645,\n",
       "   0.13282589614391327,\n",
       "   0.14862187206745148,\n",
       "   0.15008844435214996,\n",
       "   0.14805836975574493,\n",
       "   0.14133521914482117,\n",
       "   0.14006564021110535,\n",
       "   0.13614770770072937,\n",
       "   0.1266283392906189,\n",
       "   0.12928691506385803,\n",
       "   0.1349806934595108,\n",
       "   0.13509789109230042,\n",
       "   0.1405714452266693,\n",
       "   0.14249959588050842,\n",
       "   0.14187917113304138,\n",
       "   0.1361343413591385,\n",
       "   0.12908880412578583,\n",
       "   0.13315631449222565,\n",
       "   0.13709478080272675,\n",
       "   0.12593954801559448,\n",
       "   0.1222596988081932,\n",
       "   0.11272025108337402,\n",
       "   0.11974779516458511,\n",
       "   0.11871151626110077,\n",
       "   0.10984382778406143,\n",
       "   0.10830116271972656,\n",
       "   0.103959821164608,\n",
       "   0.10462156683206558,\n",
       "   0.11011815816164017,\n",
       "   0.10936196893453598,\n",
       "   0.11154109239578247,\n",
       "   0.11051671206951141,\n",
       "   0.10565827786922455,\n",
       "   0.11167649179697037,\n",
       "   0.10786759853363037,\n",
       "   0.10769752413034439,\n",
       "   0.09657267481088638,\n",
       "   0.09195273369550705,\n",
       "   0.10338016599416733,\n",
       "   0.09628226608037949,\n",
       "   0.10293936729431152,\n",
       "   0.1068924143910408,\n",
       "   0.10680241137742996,\n",
       "   0.1068197712302208,\n",
       "   0.10566829890012741,\n",
       "   0.11158745735883713,\n",
       "   0.11273717135190964,\n",
       "   0.11933881789445877,\n",
       "   0.1201225146651268,\n",
       "   0.11558227241039276,\n",
       "   0.11377251148223877,\n",
       "   0.10961256921291351,\n",
       "   0.10732978582382202,\n",
       "   0.10392892360687256,\n",
       "   0.10691934078931808,\n",
       "   0.09909423440694809,\n",
       "   0.09704302251338959,\n",
       "   0.09475637972354889,\n",
       "   0.0940239205956459,\n",
       "   0.10143890231847763,\n",
       "   0.10450609028339386,\n",
       "   0.10706434398889542,\n",
       "   0.10919258743524551,\n",
       "   0.10425575077533722,\n",
       "   0.09400800615549088,\n",
       "   0.0936291292309761,\n",
       "   0.08576565235853195,\n",
       "   0.08647734671831131,\n",
       "   0.08763439953327179,\n",
       "   0.08213194459676743,\n",
       "   0.09576953947544098,\n",
       "   0.09852007776498795,\n",
       "   0.09088098257780075,\n",
       "   0.08682768046855927,\n",
       "   0.09356306493282318,\n",
       "   0.08496476709842682,\n",
       "   0.08626595884561539,\n",
       "   0.0929836705327034,\n",
       "   0.0891149714589119,\n",
       "   0.08872110396623611,\n",
       "   0.09027326107025146,\n",
       "   0.09022336453199387,\n",
       "   0.08673214912414551,\n",
       "   0.10037768632173538,\n",
       "   0.10937456786632538,\n",
       "   0.10385002195835114,\n",
       "   0.10859169811010361,\n",
       "   0.11984004825353622,\n",
       "   0.11482298374176025,\n",
       "   0.10266395658254623,\n",
       "   0.10624778270721436,\n",
       "   0.10906446725130081,\n",
       "   0.10653264075517654,\n",
       "   0.10489476472139359,\n",
       "   0.09499248117208481,\n",
       "   0.09012231975793839,\n",
       "   0.08948303014039993,\n",
       "   0.08546387404203415,\n",
       "   0.0853341817855835,\n",
       "   0.08463772386312485,\n",
       "   0.0937366932630539,\n",
       "   0.09632065147161484,\n",
       "   0.09274368733167648,\n",
       "   0.09157717227935791,\n",
       "   0.09776950627565384,\n",
       "   0.10061115771532059,\n",
       "   0.09790675342082977,\n",
       "   0.09878731518983841,\n",
       "   0.1016516461968422,\n",
       "   0.0994868129491806,\n",
       "   0.10009486228227615,\n",
       "   0.10696975141763687,\n",
       "   0.09906784445047379,\n",
       "   0.09221986681222916,\n",
       "   0.08760173618793488,\n",
       "   0.09063217788934708,\n",
       "   0.08873089402914047,\n",
       "   0.08579184114933014,\n",
       "   0.08656490594148636,\n",
       "   0.08936449140310287,\n",
       "   0.09362854063510895,\n",
       "   0.10029256343841553,\n",
       "   0.11222423613071442,\n",
       "   0.11166683584451675,\n",
       "   0.11346595734357834,\n",
       "   0.10827024281024933,\n",
       "   0.11318633705377579,\n",
       "   0.10342443734407425,\n",
       "   0.090767040848732,\n",
       "   0.09565122425556183,\n",
       "   0.09789353609085083,\n",
       "   0.10261160880327225,\n",
       "   0.11524766683578491,\n",
       "   0.11363514512777328,\n",
       "   0.1060367301106453,\n",
       "   0.1099269911646843,\n",
       "   0.10485494881868362,\n",
       "   0.10787636786699295,\n",
       "   0.10849031060934067,\n",
       "   0.10715766996145248,\n",
       "   0.11727295815944672,\n",
       "   0.11884572356939316,\n",
       "   0.11732641607522964,\n",
       "   0.11212009191513062,\n",
       "   0.1216486245393753,\n",
       "   0.12471513450145721,\n",
       "   0.12442590296268463,\n",
       "   0.1257457435131073,\n",
       "   0.13326290249824524,\n",
       "   0.13060303032398224,\n",
       "   0.13501468300819397,\n",
       "   0.13271185755729675,\n",
       "   0.14081141352653503,\n",
       "   0.1339637041091919,\n",
       "   0.12711022794246674,\n",
       "   0.12755168974399567,\n",
       "   0.13514384627342224,\n",
       "   0.1418110430240631,\n",
       "   0.13715000450611115,\n",
       "   0.13376256823539734,\n",
       "   0.1312231421470642,\n",
       "   0.1228681206703186,\n",
       "   0.12074314057826996,\n",
       "   0.13234694302082062,\n",
       "   0.1319621503353119,\n",
       "   0.1332702934741974,\n",
       "   0.14091624319553375,\n",
       "   0.14342643320560455,\n",
       "   0.13944029808044434,\n",
       "   0.1462302953004837,\n",
       "   0.14938108623027802,\n",
       "   0.13682310283184052,\n",
       "   0.1331421434879303,\n",
       "   0.13265596330165863,\n",
       "   0.1293209046125412,\n",
       "   0.13086238503456116,\n",
       "   0.12139684706926346,\n",
       "   0.13104651868343353,\n",
       "   0.12377934157848358,\n",
       "   0.11603174358606339,\n",
       "   0.12226162850856781,\n",
       "   0.11723939329385757,\n",
       "   0.11713559925556183,\n",
       "   0.11419384181499481,\n",
       "   0.1087975725531578,\n",
       "   0.12027554959058762,\n",
       "   0.10927224904298782,\n",
       "   0.11084742099046707,\n",
       "   0.11170696467161179,\n",
       "   0.107895627617836,\n",
       "   0.10440228879451752,\n",
       "   0.10871682316064835,\n",
       "   0.1145978644490242,\n",
       "   0.10911555588245392,\n",
       "   0.11540254205465317,\n",
       "   0.11314665526151657,\n",
       "   0.11748207360506058,\n",
       "   0.11545455455780029,\n",
       "   0.10042636096477509,\n",
       "   0.09832926094532013,\n",
       "   0.10291271656751633,\n",
       "   0.10374383628368378,\n",
       "   0.10864804685115814,\n",
       "   0.11082172393798828,\n",
       "   0.11539693176746368,\n",
       "   0.11549382656812668,\n",
       "   0.1027636006474495,\n",
       "   0.1008915826678276,\n",
       "   0.09835336357355118,\n",
       "   0.09408257901668549,\n",
       "   0.1005728617310524,\n",
       "   0.09897105395793915,\n",
       "   0.10185419023036957,\n",
       "   0.11286676675081253,\n",
       "   0.12664540112018585,\n",
       "   0.12573924660682678,\n",
       "   0.12239379435777664,\n",
       "   0.11979705095291138,\n",
       "   0.12306470423936844,\n",
       "   0.11605715751647949,\n",
       "   0.11447932571172714,\n",
       "   0.11971832066774368,\n",
       "   0.11831475794315338,\n",
       "   0.11327201128005981,\n",
       "   0.12156181782484055,\n",
       "   0.12700428068637848,\n",
       "   0.12808535993099213,\n",
       "   0.12719176709651947,\n",
       "   0.11925631761550903,\n",
       "   0.11587036401033401,\n",
       "   0.12151838093996048,\n",
       "   0.1148008182644844,\n",
       "   0.11692208051681519,\n",
       "   0.11865811049938202,\n",
       "   0.11671509593725204,\n",
       "   0.11162906885147095,\n",
       "   0.10006870329380035,\n",
       "   0.0935683399438858,\n",
       "   0.1015254408121109,\n",
       "   0.10466789454221725,\n",
       "   0.10267067700624466,\n",
       "   0.1150503009557724,\n",
       "   0.1073802188038826,\n",
       "   0.11409018933773041,\n",
       "   0.11455642431974411,\n",
       "   0.11440348625183105],\n",
       "  'test_losses': [38.80433249473572,\n",
       "   19.955106019973755,\n",
       "   13.046965807676315,\n",
       "   8.587703973054886,\n",
       "   6.139707587659359,\n",
       "   4.627320138737559,\n",
       "   3.4052242562174797,\n",
       "   2.673656619619578,\n",
       "   2.167955025099218,\n",
       "   1.7511465339921415,\n",
       "   1.533214953262359,\n",
       "   1.3512858701869845,\n",
       "   1.1651401491835713,\n",
       "   1.0575362052768469,\n",
       "   0.9210303742438555,\n",
       "   0.8783286940306425,\n",
       "   0.8540288601070642,\n",
       "   0.8612412996590137,\n",
       "   0.8776811771094799,\n",
       "   0.8947938680648804,\n",
       "   0.8663011640310287,\n",
       "   0.8276485726237297,\n",
       "   0.8175267353653908,\n",
       "   0.8406323492527008,\n",
       "   0.8651394098997116,\n",
       "   0.8750676140189171,\n",
       "   0.8724246919155121,\n",
       "   0.888989269733429,\n",
       "   0.8641040474176407,\n",
       "   0.8833327256143093,\n",
       "   0.8982150591909885,\n",
       "   0.9516772776842117,\n",
       "   0.9549739509820938,\n",
       "   0.941753700375557,\n",
       "   0.9474128261208534,\n",
       "   0.9269093945622444,\n",
       "   0.9159197080880404,\n",
       "   0.9360660128295422,\n",
       "   0.9473535362631083,\n",
       "   0.9574327655136585,\n",
       "   0.9693147428333759,\n",
       "   0.9836265314370394,\n",
       "   0.973408255726099,\n",
       "   0.9917975161224604,\n",
       "   1.012954618781805,\n",
       "   1.0187714360654354,\n",
       "   1.0516437776386738,\n",
       "   1.0451950319111347,\n",
       "   1.0740699861198664,\n",
       "   1.0762619357556105,\n",
       "   1.0678282277658582,\n",
       "   1.0878963526338339,\n",
       "   1.1362580554559827,\n",
       "   1.131499720737338,\n",
       "   1.1286344658583403,\n",
       "   1.1231817342340946,\n",
       "   1.151107583194971,\n",
       "   1.141569746658206,\n",
       "   1.1067003086209297,\n",
       "   1.0909896213561296,\n",
       "   1.09100192040205,\n",
       "   1.0887125581502914,\n",
       "   1.0428298390470445,\n",
       "   1.054456828162074,\n",
       "   1.0368795790709555,\n",
       "   1.026935285422951,\n",
       "   1.0073236310854554,\n",
       "   1.0316176298074424,\n",
       "   1.051257580285892,\n",
       "   1.0255389814265072,\n",
       "   1.0198334546294063,\n",
       "   1.010293104685843,\n",
       "   1.0167640000581741,\n",
       "   0.9989252756349742,\n",
       "   0.9664401304908097,\n",
       "   0.9518580846488476,\n",
       "   0.9732229968067259,\n",
       "   0.9709383742883801,\n",
       "   1.0091000497341156,\n",
       "   1.041624897858128,\n",
       "   1.0243489958811551,\n",
       "   1.0298578455112875,\n",
       "   1.021401152247563,\n",
       "   1.0442860083421692,\n",
       "   1.0671200003707781,\n",
       "   1.0506858640583232,\n",
       "   1.052159452228807,\n",
       "   1.014650932396762,\n",
       "   0.9878769767237827,\n",
       "   0.9943674767855555,\n",
       "   0.9814863714855164,\n",
       "   0.9775830637663603,\n",
       "   0.9913853107718751,\n",
       "   0.9917215593159199,\n",
       "   1.0238559790886939,\n",
       "   1.0381758101284504,\n",
       "   1.0465684048831463,\n",
       "   1.071190629620105,\n",
       "   1.0872503763530403,\n",
       "   1.0551053509116173,\n",
       "   1.045378160662949,\n",
       "   1.0752045093104243,\n",
       "   1.096883365418762,\n",
       "   1.0892657334916294,\n",
       "   1.1023768661543727,\n",
       "   1.0918483510613441,\n",
       "   1.084929030854255,\n",
       "   1.0568694076500833,\n",
       "   1.0308929509483278,\n",
       "   1.009355004876852,\n",
       "   1.0152751114219427,\n",
       "   1.0340193314477801,\n",
       "   1.0329829026013613,\n",
       "   1.0307308034971356,\n",
       "   0.9985724855214357,\n",
       "   0.9968964140862226,\n",
       "   0.985996981151402,\n",
       "   0.9974886691197753,\n",
       "   0.9908704413101077,\n",
       "   1.0392264891415834,\n",
       "   1.0379010392352939,\n",
       "   1.0787093350663781,\n",
       "   1.1071650888770819,\n",
       "   1.0877097016200423,\n",
       "   1.0676406910642982,\n",
       "   1.0576857486739755,\n",
       "   1.0382467918097973,\n",
       "   1.0556660490110517,\n",
       "   1.011236310005188,\n",
       "   1.022918175905943,\n",
       "   1.0047903265804052,\n",
       "   1.0429389718919992,\n",
       "   1.025073865428567,\n",
       "   1.0269504021853209,\n",
       "   1.057399194687605,\n",
       "   1.0519323647022247,\n",
       "   1.0481265187263489,\n",
       "   1.0493613574653864,\n",
       "   1.037683678790927,\n",
       "   1.017997918650508,\n",
       "   1.0341252302750945,\n",
       "   1.0172432195395231,\n",
       "   1.086372866295278,\n",
       "   1.0879555772989988,\n",
       "   1.0755979176610708,\n",
       "   1.0942558776587248,\n",
       "   1.0260450970381498,\n",
       "   1.0174555703997612,\n",
       "   1.043905546888709,\n",
       "   1.04969765804708,\n",
       "   1.0272199623286724,\n",
       "   1.0181358698755503,\n",
       "   1.0356064923107624,\n",
       "   1.0310418494045734,\n",
       "   1.0392319913953543,\n",
       "   1.05845014937222,\n",
       "   1.0809224098920822,\n",
       "   1.0742559358477592,\n",
       "   1.0880080349743366,\n",
       "   1.0896095372736454,\n",
       "   1.0954698212444782,\n",
       "   1.0832819733768702,\n",
       "   1.0689042303711176,\n",
       "   1.0906074102967978,\n",
       "   1.0727823022753,\n",
       "   1.0648325216025114,\n",
       "   1.0803313665091991,\n",
       "   1.0634651239961386,\n",
       "   1.0772888232022524,\n",
       "   1.1046788934618235,\n",
       "   1.1107525136321783,\n",
       "   1.0895559303462505,\n",
       "   1.0969899483025074,\n",
       "   1.10609139688313,\n",
       "   1.0715444516390562,\n",
       "   1.0698974672704935,\n",
       "   1.05431642010808,\n",
       "   1.05683952011168,\n",
       "   1.0695491954684258,\n",
       "   1.0530389584600925,\n",
       "   1.04359470307827,\n",
       "   1.0446904264390469,\n",
       "   1.039235595613718,\n",
       "   1.0297937020659447,\n",
       "   1.0570141077041626,\n",
       "   1.0645015984773636,\n",
       "   1.0710895508527756,\n",
       "   1.048481471836567,\n",
       "   1.0626589879393578,\n",
       "   1.0903207659721375,\n",
       "   1.1093231067061424,\n",
       "   1.1125187650322914,\n",
       "   1.119484432041645,\n",
       "   1.143629364669323,\n",
       "   1.1596176102757454,\n",
       "   1.1618986055254936,\n",
       "   1.1709135919809341,\n",
       "   1.1771660298109055,\n",
       "   1.1531563624739647,\n",
       "   1.152535580098629,\n",
       "   1.1743376180529594,\n",
       "   1.1300293132662773,\n",
       "   1.1124959588050842,\n",
       "   1.1217383071780205,\n",
       "   1.1302869766950607,\n",
       "   1.1105956584215164,\n",
       "   1.084946632385254,\n",
       "   1.0768441930413246,\n",
       "   1.0812442526221275,\n",
       "   1.1120228506624699,\n",
       "   1.140736397355795,\n",
       "   1.1293985918164253,\n",
       "   1.1094748862087727,\n",
       "   1.1418539509177208,\n",
       "   1.1067360751330853,\n",
       "   1.0722587332129478,\n",
       "   1.0858840085566044,\n",
       "   1.0829825438559055,\n",
       "   1.0888444259762764,\n",
       "   1.0811875984072685,\n",
       "   1.06180140376091,\n",
       "   1.0905315428972244,\n",
       "   1.0620481073856354,\n",
       "   1.0574002750217915,\n",
       "   1.071708507835865,\n",
       "   1.0621563866734505,\n",
       "   1.0511037334799767,\n",
       "   1.0778482258319855,\n",
       "   1.0351184271275997,\n",
       "   1.0498752295970917,\n",
       "   1.0570214241743088,\n",
       "   1.0611556842923164,\n",
       "   1.0451786443591118,\n",
       "   1.031039372086525,\n",
       "   1.032956462353468,\n",
       "   1.036302074790001,\n",
       "   1.0193615034222603,\n",
       "   1.0259636864066124,\n",
       "   1.0419011749327183,\n",
       "   1.0328849852085114,\n",
       "   1.050074689090252,\n",
       "   1.0717040784657001,\n",
       "   1.0960465110838413,\n",
       "   1.1054156124591827,\n",
       "   1.0945342034101486,\n",
       "   1.082371812313795,\n",
       "   1.0932151526212692,\n",
       "   1.0836394466459751,\n",
       "   1.0851102098822594,\n",
       "   1.1070937030017376,\n",
       "   1.09940330311656,\n",
       "   1.111675951629877,\n",
       "   1.1307232603430748,\n",
       "   1.1308711357414722,\n",
       "   1.1379470899701118,\n",
       "   1.1512583568692207,\n",
       "   1.1866265386343002,\n",
       "   1.1563628911972046,\n",
       "   1.1716332361102104,\n",
       "   1.1757926680147648,\n",
       "   1.160922884941101,\n",
       "   1.1559214852750301,\n",
       "   1.170891609042883,\n",
       "   1.1743201687932014,\n",
       "   1.1608356460928917,\n",
       "   1.1533248350024223,\n",
       "   1.1848130226135254,\n",
       "   1.1786927357316017,\n",
       "   1.1652024537324905,\n",
       "   1.1673379018902779,\n",
       "   1.1602187603712082,\n",
       "   1.1575209125876427,\n",
       "   1.1579177007079124,\n",
       "   1.1288283094763756,\n",
       "   1.1305555552244186,\n",
       "   1.1185165420174599,\n",
       "   1.1169751957058907,\n",
       "   1.1274504661560059,\n",
       "   1.1294431015849113,\n",
       "   1.1341020837426186,\n",
       "   1.1149311512708664,\n",
       "   1.135402262210846,\n",
       "   1.1654939353466034,\n",
       "   1.125546082854271,\n",
       "   1.132996678352356,\n",
       "   1.125732570886612,\n",
       "   1.1204677000641823,\n",
       "   1.1017375886440277,\n",
       "   1.0533585920929909,\n",
       "   1.0784940868616104,\n",
       "   1.0726034417748451,\n",
       "   1.0833505690097809,\n",
       "   1.1302047222852707,\n",
       "   1.0927209332585335,\n",
       "   1.109727144241333,\n",
       "   1.0735649019479752,\n",
       "   1.0436159297823906,\n",
       "   1.042097769677639,\n",
       "   1.0523792058229446,\n",
       "   1.0559150576591492,\n",
       "   1.0669413283467293,\n",
       "   1.1035981625318527,\n",
       "   1.1464776247739792,\n",
       "   1.157872550189495,\n",
       "   1.125745140016079,\n",
       "   1.1067736223340034,\n",
       "   1.0860153399407864,\n",
       "   1.0854278318583965,\n",
       "   1.137827817350626,\n",
       "   1.104846403002739,\n",
       "   1.0946585722267628,\n",
       "   1.0935156866908073,\n",
       "   1.0575593002140522,\n",
       "   1.083209540694952,\n",
       "   1.1004543118178844,\n",
       "   1.1325853019952774,\n",
       "   1.1696931831538677,\n",
       "   1.1684581078588963,\n",
       "   1.1423654593527317,\n",
       "   1.0837168954312801,\n",
       "   1.0780274979770184,\n",
       "   1.083411693572998,\n",
       "   1.0661703571677208,\n",
       "   1.0472540594637394,\n",
       "   1.0728283263742924,\n",
       "   1.0660622753202915,\n",
       "   1.0524687804281712,\n",
       "   1.0709377154707909,\n",
       "   1.07878303155303,\n",
       "   1.0936462134122849,\n",
       "   1.1123735457658768,\n",
       "   1.0639971233904362,\n",
       "   1.0529376938939095,\n",
       "   1.0428850390017033,\n",
       "   1.0437805689871311,\n",
       "   1.020183738321066,\n",
       "   1.0152251981198788,\n",
       "   1.0251447409391403,\n",
       "   1.0288833118975163,\n",
       "   1.0479149408638477,\n",
       "   1.0685532726347446,\n",
       "   1.0852042511105537,\n",
       "   1.0776525028049946,\n",
       "   1.052023895084858,\n",
       "   1.0562056340277195,\n",
       "   1.0930402465164661,\n",
       "   1.0734128914773464,\n",
       "   1.0796662382781506,\n",
       "   1.0710440091788769,\n",
       "   1.0487909652292728,\n",
       "   1.0469440557062626,\n",
       "   1.06753783300519,\n",
       "   1.0537611730396748,\n",
       "   1.0284111201763153,\n",
       "   1.07649315148592,\n",
       "   1.0742675699293613,\n",
       "   1.05627666041255,\n",
       "   1.039556946605444,\n",
       "   1.069717526435852,\n",
       "   1.0525015443563461,\n",
       "   1.0741462595760822,\n",
       "   1.0664500519633293,\n",
       "   1.0702008567750454,\n",
       "   1.058881964534521,\n",
       "   1.0700911432504654,\n",
       "   1.0863568186759949,\n",
       "   1.0832162089645863,\n",
       "   1.0509181283414364,\n",
       "   1.0233651660382748,\n",
       "   1.050395593047142,\n",
       "   1.06565061211586,\n",
       "   1.0814452692866325,\n",
       "   1.0683199316263199,\n",
       "   1.0672345012426376,\n",
       "   1.0542271845042706,\n",
       "   1.0745367966592312,\n",
       "   1.107209611684084,\n",
       "   1.1413620226085186,\n",
       "   1.15359927713871,\n",
       "   1.1666289642453194,\n",
       "   1.162046954035759,\n",
       "   1.1416458301246166,\n",
       "   1.131387609988451,\n",
       "   1.1765603050589561,\n",
       "   1.1744752265512943,\n",
       "   1.166090153157711,\n",
       "   1.2115804478526115,\n",
       "   1.198611106723547,\n",
       "   1.1481597982347012,\n",
       "   1.1727716773748398,\n",
       "   1.1547360084950924,\n",
       "   1.14126330986619,\n",
       "   1.1570678278803825,\n",
       "   1.1313833817839622,\n",
       "   1.1226282678544521,\n",
       "   1.118617545813322,\n",
       "   1.1132320165634155,\n",
       "   1.090858655050397,\n",
       "   1.1079831812530756,\n",
       "   1.1119979936629534,\n",
       "   1.1076227705925703,\n",
       "   1.1073076128959656,\n",
       "   1.107981726527214,\n",
       "   1.1131279189139605,\n",
       "   1.1317265015095472,\n",
       "   1.1343207657337189,\n",
       "   1.1367179714143276,\n",
       "   1.1333015952259302,\n",
       "   1.1178314536809921,\n",
       "   1.1006060857325792,\n",
       "   1.087654422968626,\n",
       "   1.0867674332112074,\n",
       "   1.09589265845716,\n",
       "   1.097807887941599,\n",
       "   1.121479120105505,\n",
       "   1.0999563951045275,\n",
       "   1.1313579678535461,\n",
       "   1.112497016787529,\n",
       "   1.133868258446455,\n",
       "   1.130722600966692,\n",
       "   1.1304694823920727,\n",
       "   1.1324662175029516,\n",
       "   1.1456404756754637,\n",
       "   1.1516342498362064,\n",
       "   1.1394686549901962,\n",
       "   1.1419575866311789,\n",
       "   1.1598807126283646,\n",
       "   1.1800434198230505,\n",
       "   1.1400142591446638,\n",
       "   1.1280107516795397,\n",
       "   1.0986263528466225,\n",
       "   1.1184927225112915,\n",
       "   1.1285239271819592,\n",
       "   1.1693791262805462,\n",
       "   1.1606422550976276,\n",
       "   1.1502267681062222,\n",
       "   1.120851632207632,\n",
       "   1.1245781034231186,\n",
       "   1.1487685851752758,\n",
       "   1.1603487953543663,\n",
       "   1.1657299660146236,\n",
       "   1.154045708477497,\n",
       "   1.1452809013426304,\n",
       "   1.1417477820068598,\n",
       "   1.112448824569583,\n",
       "   1.1103729456663132,\n",
       "   1.0739681608974934,\n",
       "   1.0508852489292622,\n",
       "   1.0383042227476835,\n",
       "   0.9978889841586351,\n",
       "   0.9920209478586912,\n",
       "   0.9950795639306307,\n",
       "   1.002074372023344,\n",
       "   1.0104058980941772,\n",
       "   1.0184894874691963,\n",
       "   1.0397803355008364,\n",
       "   1.054781125858426,\n",
       "   1.0355488061904907,\n",
       "   1.0540602561086416,\n",
       "   1.0516109690070152,\n",
       "   1.0630271080881357,\n",
       "   1.0730841346085072,\n",
       "   1.0675238873809576,\n",
       "   1.0503937173634768,\n",
       "   1.0866106692701578,\n",
       "   1.0737694390118122,\n",
       "   1.074945555999875,\n",
       "   1.060458704829216,\n",
       "   1.1054484825581312,\n",
       "   1.085839269682765,\n",
       "   1.112422989681363,\n",
       "   1.1126797888427973,\n",
       "   1.09625612385571,\n",
       "   1.0902598444372416,\n",
       "   1.1066543720662594,\n",
       "   1.1147773079574108,\n",
       "   1.1091969376429915,\n",
       "   1.134451599791646,\n",
       "   1.143780229613185,\n",
       "   1.1267954977229238,\n",
       "   1.115936511196196,\n",
       "   1.0941373575478792,\n",
       "   1.0716168973594904,\n",
       "   1.077326682396233,\n",
       "   1.0654228441417217,\n",
       "   1.0563600715249777,\n",
       "   1.0864887293428183,\n",
       "   1.077634572982788,\n",
       "   1.0822876878082752,\n",
       "   1.0911824088543653,\n",
       "   1.1091444697231054,\n",
       "   1.098629480227828,\n",
       "   1.0840782038867474,\n",
       "   1.098735323175788,\n",
       "   1.0833155624568462,\n",
       "   1.0995340552181005,\n",
       "   1.0804414115846157,\n",
       "   1.0867484360933304,\n",
       "   1.0922382809221745,\n",
       "   1.0732258837670088],\n",
       "  'pce_acc': [82.81202697753906,\n",
       "   83.47443389892578,\n",
       "   83.10357666015625,\n",
       "   79.14817810058594,\n",
       "   72.2005386352539,\n",
       "   69.09680938720703,\n",
       "   64.41555786132812,\n",
       "   63.07448196411133,\n",
       "   61.089473724365234,\n",
       "   60.034549713134766,\n",
       "   58.151859283447266,\n",
       "   57.59648513793945,\n",
       "   56.65333938598633,\n",
       "   56.7711181640625,\n",
       "   56.112152099609375,\n",
       "   56.135589599609375,\n",
       "   56.019718170166016,\n",
       "   57.1881217956543,\n",
       "   56.3603401184082,\n",
       "   57.60264587402344,\n",
       "   55.912078857421875,\n",
       "   53.77595901489258,\n",
       "   54.32987976074219,\n",
       "   54.801265716552734,\n",
       "   54.90630340576172,\n",
       "   55.704254150390625,\n",
       "   54.38162612915039,\n",
       "   53.19123840332031,\n",
       "   52.77025604248047,\n",
       "   52.597415924072266,\n",
       "   52.776092529296875,\n",
       "   52.052528381347656,\n",
       "   51.9958610534668,\n",
       "   51.50929641723633,\n",
       "   51.12223815917969,\n",
       "   50.7962760925293,\n",
       "   51.15271759033203,\n",
       "   51.0289306640625,\n",
       "   51.76390838623047,\n",
       "   51.30015182495117,\n",
       "   51.807891845703125,\n",
       "   53.257896423339844,\n",
       "   53.135841369628906,\n",
       "   53.20991134643555,\n",
       "   53.887115478515625,\n",
       "   53.693634033203125,\n",
       "   54.074256896972656,\n",
       "   53.939788818359375,\n",
       "   53.26668167114258,\n",
       "   53.17049026489258,\n",
       "   53.16469192504883,\n",
       "   52.803688049316406,\n",
       "   54.364784240722656,\n",
       "   54.1917610168457,\n",
       "   53.985923767089844,\n",
       "   54.129493713378906,\n",
       "   54.6976432800293,\n",
       "   55.0269775390625,\n",
       "   54.73686981201172,\n",
       "   53.3358154296875,\n",
       "   54.14316177368164,\n",
       "   54.00214767456055,\n",
       "   52.26141357421875,\n",
       "   53.79964065551758,\n",
       "   53.39408493041992,\n",
       "   52.46635055541992,\n",
       "   51.24264907836914,\n",
       "   52.82923126220703,\n",
       "   54.498779296875,\n",
       "   54.44425964355469,\n",
       "   54.060752868652344,\n",
       "   53.10748291015625,\n",
       "   53.08688735961914,\n",
       "   51.63008499145508,\n",
       "   51.46563720703125,\n",
       "   51.582008361816406,\n",
       "   53.120338439941406,\n",
       "   52.400733947753906,\n",
       "   54.57590866088867,\n",
       "   55.458003997802734,\n",
       "   53.81382751464844,\n",
       "   54.42308044433594,\n",
       "   55.173744201660156,\n",
       "   55.558692932128906,\n",
       "   56.057228088378906,\n",
       "   54.378273010253906,\n",
       "   53.046852111816406,\n",
       "   52.64665603637695,\n",
       "   52.27159881591797,\n",
       "   52.809791564941406,\n",
       "   53.42707824707031,\n",
       "   52.18147659301758,\n",
       "   52.358863830566406,\n",
       "   52.10203552246094,\n",
       "   53.36159896850586,\n",
       "   53.60421371459961,\n",
       "   53.62479019165039,\n",
       "   53.84653091430664,\n",
       "   54.59747314453125,\n",
       "   52.388179779052734,\n",
       "   50.91285705566406,\n",
       "   52.02629089355469,\n",
       "   53.733402252197266,\n",
       "   53.223018646240234,\n",
       "   52.93416213989258,\n",
       "   53.29327392578125,\n",
       "   52.41828155517578,\n",
       "   50.2326545715332,\n",
       "   50.20829391479492,\n",
       "   49.4836540222168,\n",
       "   50.24129867553711,\n",
       "   49.92921447753906,\n",
       "   49.67353439331055,\n",
       "   49.55168151855469,\n",
       "   49.5322380065918,\n",
       "   50.61734390258789,\n",
       "   50.38138198852539,\n",
       "   52.047061920166016,\n",
       "   52.00649642944336,\n",
       "   54.503787994384766,\n",
       "   54.697566986083984,\n",
       "   55.965274810791016,\n",
       "   56.798397064208984,\n",
       "   55.449241638183594,\n",
       "   55.448089599609375,\n",
       "   56.06626892089844,\n",
       "   54.52448272705078,\n",
       "   55.61024475097656,\n",
       "   53.84484100341797,\n",
       "   54.1330451965332,\n",
       "   53.216583251953125,\n",
       "   54.25291061401367,\n",
       "   54.697669982910156,\n",
       "   54.832489013671875,\n",
       "   54.97609329223633,\n",
       "   55.18193817138672,\n",
       "   55.49908447265625,\n",
       "   55.81184005737305,\n",
       "   55.45486068725586,\n",
       "   55.03068542480469,\n",
       "   54.86697006225586,\n",
       "   53.2674446105957,\n",
       "   56.313262939453125,\n",
       "   55.74724197387695,\n",
       "   56.13884353637695,\n",
       "   55.9581298828125,\n",
       "   52.92936706542969,\n",
       "   52.411277770996094,\n",
       "   53.63760757446289,\n",
       "   53.05939865112305,\n",
       "   52.02273178100586,\n",
       "   52.91555404663086,\n",
       "   51.64224624633789,\n",
       "   49.413055419921875,\n",
       "   50.872676849365234,\n",
       "   51.17279815673828,\n",
       "   51.91006088256836,\n",
       "   50.47639083862305,\n",
       "   51.51704406738281,\n",
       "   50.914363861083984,\n",
       "   51.42768096923828,\n",
       "   51.31298828125,\n",
       "   51.4671745300293,\n",
       "   51.320701599121094,\n",
       "   51.50602340698242,\n",
       "   53.104801177978516,\n",
       "   53.46339797973633,\n",
       "   53.01675033569336,\n",
       "   53.3184928894043,\n",
       "   55.06325912475586,\n",
       "   53.221435546875,\n",
       "   52.25571823120117,\n",
       "   51.3213996887207,\n",
       "   51.448673248291016,\n",
       "   50.53947448730469,\n",
       "   51.09062957763672,\n",
       "   51.2765998840332,\n",
       "   51.1374397277832,\n",
       "   50.236480712890625,\n",
       "   49.527740478515625,\n",
       "   48.79154586791992,\n",
       "   47.91440200805664,\n",
       "   48.36064147949219,\n",
       "   46.15122604370117,\n",
       "   47.40098571777344,\n",
       "   47.502811431884766,\n",
       "   47.56432342529297,\n",
       "   47.0577507019043,\n",
       "   46.027801513671875,\n",
       "   46.16053009033203,\n",
       "   46.73270797729492,\n",
       "   46.62506103515625,\n",
       "   46.61594772338867,\n",
       "   47.25657272338867,\n",
       "   47.67494201660156,\n",
       "   48.06809616088867,\n",
       "   49.12153625488281,\n",
       "   50.20240020751953,\n",
       "   50.51854705810547,\n",
       "   50.28853988647461,\n",
       "   49.73814010620117,\n",
       "   49.27043151855469,\n",
       "   49.51898956298828,\n",
       "   49.88317108154297,\n",
       "   49.960941314697266,\n",
       "   49.25770568847656,\n",
       "   48.45420455932617,\n",
       "   47.385475158691406,\n",
       "   48.17096710205078,\n",
       "   48.68218231201172,\n",
       "   50.53474426269531,\n",
       "   49.64338684082031,\n",
       "   51.21356201171875,\n",
       "   51.858192443847656,\n",
       "   51.38722610473633,\n",
       "   51.00885772705078,\n",
       "   51.274532318115234,\n",
       "   50.48178482055664,\n",
       "   51.921661376953125,\n",
       "   52.12641525268555,\n",
       "   50.507198333740234,\n",
       "   51.726985931396484,\n",
       "   50.628108978271484,\n",
       "   49.70026779174805,\n",
       "   50.39313888549805,\n",
       "   51.36561584472656,\n",
       "   50.463199615478516,\n",
       "   51.1552619934082,\n",
       "   49.99102020263672,\n",
       "   49.547096252441406,\n",
       "   49.13270950317383,\n",
       "   51.02669143676758,\n",
       "   51.10313415527344,\n",
       "   50.46611404418945,\n",
       "   49.80134963989258,\n",
       "   49.98020935058594,\n",
       "   50.11268997192383,\n",
       "   50.01881790161133,\n",
       "   51.2880973815918,\n",
       "   51.51044845581055,\n",
       "   51.250694274902344,\n",
       "   50.747352600097656,\n",
       "   50.71038055419922,\n",
       "   50.727783203125,\n",
       "   49.902374267578125,\n",
       "   49.948760986328125,\n",
       "   51.38874053955078,\n",
       "   50.675601959228516,\n",
       "   50.439090728759766,\n",
       "   50.71881866455078,\n",
       "   50.558929443359375,\n",
       "   51.42093276977539,\n",
       "   50.25221252441406,\n",
       "   50.48843765258789,\n",
       "   48.26908874511719,\n",
       "   48.41048812866211,\n",
       "   48.37982940673828,\n",
       "   48.901710510253906,\n",
       "   48.750213623046875,\n",
       "   49.64488983154297,\n",
       "   50.079078674316406,\n",
       "   50.108367919921875,\n",
       "   51.4348258972168,\n",
       "   50.806610107421875,\n",
       "   50.93934631347656,\n",
       "   50.42469024658203,\n",
       "   50.31654357910156,\n",
       "   49.144134521484375,\n",
       "   50.196285247802734,\n",
       "   50.0256462097168,\n",
       "   49.76766586303711,\n",
       "   48.32209014892578,\n",
       "   47.497955322265625,\n",
       "   47.31815719604492,\n",
       "   47.6301383972168,\n",
       "   47.944583892822266,\n",
       "   48.74310302734375,\n",
       "   49.29836654663086,\n",
       "   48.93653869628906,\n",
       "   49.2923698425293,\n",
       "   48.18265151977539,\n",
       "   49.057769775390625,\n",
       "   48.19594192504883,\n",
       "   47.96236038208008,\n",
       "   49.11217498779297,\n",
       "   48.82747268676758,\n",
       "   47.773189544677734,\n",
       "   47.140384674072266,\n",
       "   45.180442810058594,\n",
       "   48.03512954711914,\n",
       "   47.19828796386719,\n",
       "   48.0893669128418,\n",
       "   49.74321746826172,\n",
       "   49.42106628417969,\n",
       "   49.07509231567383,\n",
       "   47.99119567871094,\n",
       "   48.551937103271484,\n",
       "   47.55635070800781,\n",
       "   48.404273986816406,\n",
       "   50.36752700805664,\n",
       "   50.89218521118164,\n",
       "   51.95711135864258,\n",
       "   52.71774673461914,\n",
       "   52.48192596435547,\n",
       "   51.84715270996094,\n",
       "   51.25088119506836,\n",
       "   52.3454475402832,\n",
       "   51.94536590576172,\n",
       "   52.05098342895508,\n",
       "   49.88125991821289,\n",
       "   51.04068374633789,\n",
       "   51.95577621459961,\n",
       "   51.55162811279297,\n",
       "   54.0731086730957,\n",
       "   52.513145446777344,\n",
       "   52.72758483886719,\n",
       "   54.35305404663086,\n",
       "   53.49723815917969,\n",
       "   52.40336608886719,\n",
       "   50.013362884521484,\n",
       "   49.5010986328125,\n",
       "   48.85637283325195,\n",
       "   49.295650482177734,\n",
       "   48.407657623291016,\n",
       "   47.3166389465332,\n",
       "   46.73954391479492,\n",
       "   47.30233383178711,\n",
       "   49.38090515136719,\n",
       "   49.51280212402344,\n",
       "   49.62333297729492,\n",
       "   49.1463508605957,\n",
       "   46.9705924987793,\n",
       "   47.046512603759766,\n",
       "   46.3209342956543,\n",
       "   46.84254837036133,\n",
       "   47.26546096801758,\n",
       "   47.2215461730957,\n",
       "   47.461795806884766,\n",
       "   48.31489944458008,\n",
       "   48.65616226196289,\n",
       "   49.11435317993164,\n",
       "   49.11201858520508,\n",
       "   47.53696823120117,\n",
       "   47.2684440612793,\n",
       "   46.49542999267578,\n",
       "   45.98774719238281,\n",
       "   46.0325813293457,\n",
       "   44.87633514404297,\n",
       "   45.865234375,\n",
       "   46.71935272216797,\n",
       "   46.89141845703125,\n",
       "   48.30687713623047,\n",
       "   48.28876495361328,\n",
       "   48.87936019897461,\n",
       "   50.526607513427734,\n",
       "   50.97345733642578,\n",
       "   50.08040237426758,\n",
       "   49.77958679199219,\n",
       "   50.94521713256836,\n",
       "   48.48128890991211,\n",
       "   50.008792877197266,\n",
       "   49.69101333618164,\n",
       "   49.605377197265625,\n",
       "   49.111907958984375,\n",
       "   49.58528137207031,\n",
       "   49.568965911865234,\n",
       "   49.34054946899414,\n",
       "   49.210609436035156,\n",
       "   48.4234733581543,\n",
       "   49.07661437988281,\n",
       "   49.26477813720703,\n",
       "   50.75644302368164,\n",
       "   51.906883239746094,\n",
       "   51.81495666503906,\n",
       "   51.508480072021484,\n",
       "   51.791236877441406,\n",
       "   51.260929107666016,\n",
       "   50.195068359375,\n",
       "   50.48793411254883,\n",
       "   50.15934753417969,\n",
       "   49.35362243652344,\n",
       "   49.095314025878906,\n",
       "   49.71759796142578,\n",
       "   50.89188766479492,\n",
       "   51.017032623291016,\n",
       "   50.86210250854492,\n",
       "   50.920631408691406,\n",
       "   51.141578674316406,\n",
       "   49.92081832885742,\n",
       "   50.4346923828125,\n",
       "   50.232547760009766,\n",
       "   49.71372985839844,\n",
       "   50.46776580810547,\n",
       "   50.448787689208984,\n",
       "   48.56858444213867,\n",
       "   48.69367218017578,\n",
       "   48.611698150634766,\n",
       "   47.73910903930664,\n",
       "   47.80815124511719,\n",
       "   48.33767318725586,\n",
       "   47.81410598754883,\n",
       "   47.57110595703125,\n",
       "   48.56563949584961,\n",
       "   49.4830436706543,\n",
       "   50.02046203613281,\n",
       "   49.03426742553711,\n",
       "   47.96855163574219,\n",
       "   48.468997955322266,\n",
       "   48.5604362487793,\n",
       "   47.8197021484375,\n",
       "   47.5487174987793,\n",
       "   46.66126251220703,\n",
       "   48.56825256347656,\n",
       "   48.18604278564453,\n",
       "   49.07768630981445,\n",
       "   49.676292419433594,\n",
       "   49.87290573120117,\n",
       "   49.39544677734375,\n",
       "   49.99015808105469,\n",
       "   50.141937255859375,\n",
       "   50.70411682128906,\n",
       "   52.626426696777344,\n",
       "   52.85495376586914,\n",
       "   53.688716888427734,\n",
       "   53.478973388671875,\n",
       "   54.01720428466797,\n",
       "   54.27592468261719,\n",
       "   56.2748908996582,\n",
       "   55.699951171875,\n",
       "   55.99342727661133,\n",
       "   54.14311218261719,\n",
       "   54.109012603759766,\n",
       "   52.91358184814453,\n",
       "   54.35659408569336,\n",
       "   53.708351135253906,\n",
       "   54.08381271362305,\n",
       "   52.31776809692383,\n",
       "   53.39954376220703,\n",
       "   54.96445846557617,\n",
       "   56.0107421875,\n",
       "   57.245689392089844,\n",
       "   57.2108268737793,\n",
       "   56.11587142944336,\n",
       "   55.93015670776367,\n",
       "   54.37126922607422,\n",
       "   53.76469421386719,\n",
       "   52.173500061035156,\n",
       "   50.83672332763672,\n",
       "   51.1525993347168,\n",
       "   49.35309982299805,\n",
       "   48.560302734375,\n",
       "   49.18232727050781,\n",
       "   49.32886505126953,\n",
       "   49.68048095703125,\n",
       "   48.82844924926758,\n",
       "   47.80574035644531,\n",
       "   48.33868408203125,\n",
       "   47.615779876708984,\n",
       "   48.410728454589844,\n",
       "   48.246131896972656,\n",
       "   48.38972473144531,\n",
       "   48.329105377197266,\n",
       "   47.09934997558594,\n",
       "   46.513206481933594,\n",
       "   47.43387985229492,\n",
       "   48.05105209350586,\n",
       "   47.53728103637695,\n",
       "   45.24835968017578,\n",
       "   46.590389251708984,\n",
       "   46.18855285644531,\n",
       "   46.68912887573242,\n",
       "   47.73148727416992,\n",
       "   49.50592041015625,\n",
       "   48.433345794677734,\n",
       "   48.737205505371094,\n",
       "   47.976829528808594,\n",
       "   47.638126373291016,\n",
       "   48.75749206542969,\n",
       "   49.00133514404297,\n",
       "   49.89518737792969,\n",
       "   49.986900329589844,\n",
       "   49.057437896728516,\n",
       "   47.4983024597168,\n",
       "   47.418418884277344,\n",
       "   47.17616271972656,\n",
       "   47.41323471069336,\n",
       "   48.565956115722656,\n",
       "   47.529541015625,\n",
       "   47.079410552978516,\n",
       "   47.39588165283203,\n",
       "   48.40310287475586,\n",
       "   49.093990325927734,\n",
       "   49.6058235168457,\n",
       "   50.13579559326172,\n",
       "   51.6013069152832,\n",
       "   51.15431594848633,\n",
       "   50.41364288330078,\n",
       "   50.6232795715332,\n",
       "   50.55788803100586,\n",
       "   49.661888122558594],\n",
       "  'voc_acc': [533.2200317382812,\n",
       "   411.23358154296875,\n",
       "   352.7123718261719,\n",
       "   291.3120422363281,\n",
       "   256.07720947265625,\n",
       "   229.0374755859375,\n",
       "   200.66253662109375,\n",
       "   179.6377410888672,\n",
       "   162.79820251464844,\n",
       "   144.70843505859375,\n",
       "   134.72010803222656,\n",
       "   123.47325134277344,\n",
       "   110.93126678466797,\n",
       "   100.30099487304688,\n",
       "   87.73846435546875,\n",
       "   80.43636322021484,\n",
       "   75.46858215332031,\n",
       "   70.09815216064453,\n",
       "   67.20884704589844,\n",
       "   61.605655670166016,\n",
       "   56.810707092285156,\n",
       "   52.4585075378418,\n",
       "   45.94997787475586,\n",
       "   41.8535270690918,\n",
       "   38.29674530029297,\n",
       "   33.41009521484375,\n",
       "   32.347251892089844,\n",
       "   29.2850399017334,\n",
       "   27.16144371032715,\n",
       "   27.832374572753906,\n",
       "   26.1037654876709,\n",
       "   27.490785598754883,\n",
       "   26.95305633544922,\n",
       "   24.801597595214844,\n",
       "   26.10592269897461,\n",
       "   21.52466583251953,\n",
       "   18.825061798095703,\n",
       "   16.581825256347656,\n",
       "   15.919987678527832,\n",
       "   14.36180305480957,\n",
       "   15.212156295776367,\n",
       "   14.30677604675293,\n",
       "   15.439457893371582,\n",
       "   13.390229225158691,\n",
       "   14.681048393249512,\n",
       "   14.000454902648926,\n",
       "   17.387332916259766,\n",
       "   15.791397094726562,\n",
       "   16.302291870117188,\n",
       "   14.90678596496582,\n",
       "   13.857471466064453,\n",
       "   14.526542663574219,\n",
       "   13.484029769897461,\n",
       "   17.9066104888916,\n",
       "   18.53955078125,\n",
       "   17.950664520263672,\n",
       "   18.3835506439209,\n",
       "   18.08714485168457,\n",
       "   16.892765045166016,\n",
       "   16.632970809936523,\n",
       "   12.92258071899414,\n",
       "   7.858486652374268,\n",
       "   7.87756872177124,\n",
       "   9.131537437438965,\n",
       "   6.948991775512695,\n",
       "   5.930331707000732,\n",
       "   5.7735276222229,\n",
       "   5.949789047241211,\n",
       "   5.137564659118652,\n",
       "   5.203614711761475,\n",
       "   5.397180557250977,\n",
       "   6.881961345672607,\n",
       "   8.2643461227417,\n",
       "   7.768853664398193,\n",
       "   6.936326026916504,\n",
       "   5.034544944763184,\n",
       "   5.0725812911987305,\n",
       "   5.752131938934326,\n",
       "   5.052394866943359,\n",
       "   5.318227291107178,\n",
       "   4.817967891693115,\n",
       "   4.854164123535156,\n",
       "   4.900629043579102,\n",
       "   4.515669345855713,\n",
       "   4.84339714050293,\n",
       "   4.349493026733398,\n",
       "   4.573940277099609,\n",
       "   4.510181427001953,\n",
       "   4.631970405578613,\n",
       "   5.4727067947387695,\n",
       "   5.01763916015625,\n",
       "   4.774364948272705,\n",
       "   5.030820369720459,\n",
       "   5.527146816253662,\n",
       "   6.012996673583984,\n",
       "   6.144290447235107,\n",
       "   6.697650909423828,\n",
       "   7.038741588592529,\n",
       "   7.116458415985107,\n",
       "   7.4735798835754395,\n",
       "   7.72879695892334,\n",
       "   7.538819313049316,\n",
       "   7.886473178863525,\n",
       "   7.818889617919922,\n",
       "   8.03640365600586,\n",
       "   8.265511512756348,\n",
       "   8.692961692810059,\n",
       "   9.092912673950195,\n",
       "   8.822417259216309,\n",
       "   8.883684158325195,\n",
       "   9.578332901000977,\n",
       "   9.188681602478027,\n",
       "   10.89367389678955,\n",
       "   10.412115097045898,\n",
       "   10.703875541687012,\n",
       "   10.593594551086426,\n",
       "   10.20291519165039,\n",
       "   10.414173126220703,\n",
       "   10.643852233886719,\n",
       "   10.537619590759277,\n",
       "   10.400575637817383,\n",
       "   10.925664901733398,\n",
       "   11.321002006530762,\n",
       "   11.230097770690918,\n",
       "   11.095409393310547,\n",
       "   10.813793182373047,\n",
       "   11.361580848693848,\n",
       "   11.233431816101074,\n",
       "   11.023908615112305,\n",
       "   11.359169006347656,\n",
       "   12.466337203979492,\n",
       "   14.954984664916992,\n",
       "   13.695372581481934,\n",
       "   13.830465316772461,\n",
       "   12.812082290649414,\n",
       "   13.47637939453125,\n",
       "   12.566208839416504,\n",
       "   12.106602668762207,\n",
       "   12.039990425109863,\n",
       "   11.756608963012695,\n",
       "   11.712543487548828,\n",
       "   11.57129955291748,\n",
       "   11.303526878356934,\n",
       "   12.161965370178223,\n",
       "   13.03837776184082,\n",
       "   12.964816093444824,\n",
       "   12.302416801452637,\n",
       "   12.207724571228027,\n",
       "   13.604799270629883,\n",
       "   13.431803703308105,\n",
       "   15.816216468811035,\n",
       "   13.269613265991211,\n",
       "   12.906877517700195,\n",
       "   14.902637481689453,\n",
       "   13.949470520019531,\n",
       "   13.664499282836914,\n",
       "   14.113612174987793,\n",
       "   16.03739356994629,\n",
       "   15.613601684570312,\n",
       "   14.90058422088623,\n",
       "   13.861759185791016,\n",
       "   14.369832992553711,\n",
       "   13.968810081481934,\n",
       "   13.628670692443848,\n",
       "   13.621100425720215,\n",
       "   14.037592887878418,\n",
       "   14.3353271484375,\n",
       "   14.66983413696289,\n",
       "   15.073473930358887,\n",
       "   15.2318696975708,\n",
       "   14.983227729797363,\n",
       "   15.531129837036133,\n",
       "   16.574249267578125,\n",
       "   16.45517349243164,\n",
       "   16.622323989868164,\n",
       "   16.947011947631836,\n",
       "   15.93015193939209,\n",
       "   16.8867130279541,\n",
       "   18.0408935546875,\n",
       "   18.74687385559082,\n",
       "   18.981822967529297,\n",
       "   22.686059951782227,\n",
       "   23.985567092895508,\n",
       "   26.34630584716797,\n",
       "   27.442724227905273,\n",
       "   31.68445587158203,\n",
       "   29.844934463500977,\n",
       "   28.926414489746094,\n",
       "   29.869409561157227,\n",
       "   30.1329345703125,\n",
       "   30.737442016601562,\n",
       "   30.053810119628906,\n",
       "   30.289844512939453,\n",
       "   28.08795928955078,\n",
       "   28.20375633239746,\n",
       "   26.70513916015625,\n",
       "   26.20372200012207,\n",
       "   28.734830856323242,\n",
       "   28.95996856689453,\n",
       "   29.539798736572266,\n",
       "   31.120681762695312,\n",
       "   29.641454696655273,\n",
       "   30.11249542236328,\n",
       "   29.4742488861084,\n",
       "   31.382274627685547,\n",
       "   31.3048152923584,\n",
       "   29.47830581665039,\n",
       "   30.795455932617188,\n",
       "   29.104814529418945,\n",
       "   27.078834533691406,\n",
       "   27.710407257080078,\n",
       "   27.048168182373047,\n",
       "   24.734596252441406,\n",
       "   26.075172424316406,\n",
       "   24.642786026000977,\n",
       "   25.06049346923828,\n",
       "   27.075904846191406,\n",
       "   28.19906997680664,\n",
       "   31.81658172607422,\n",
       "   32.21424102783203,\n",
       "   33.1655158996582,\n",
       "   31.216304779052734,\n",
       "   28.450315475463867,\n",
       "   28.60481071472168,\n",
       "   33.518367767333984,\n",
       "   31.746789932250977,\n",
       "   32.767295837402344,\n",
       "   33.12022018432617,\n",
       "   28.64906120300293,\n",
       "   28.43400001525879,\n",
       "   29.547521591186523,\n",
       "   31.135377883911133,\n",
       "   29.42318344116211,\n",
       "   30.27028465270996,\n",
       "   29.914113998413086,\n",
       "   30.670347213745117,\n",
       "   31.977771759033203,\n",
       "   31.253314971923828,\n",
       "   29.151517868041992,\n",
       "   28.676387786865234,\n",
       "   25.406068801879883,\n",
       "   24.041168212890625,\n",
       "   25.202713012695312,\n",
       "   26.47995948791504,\n",
       "   24.79159927368164,\n",
       "   24.11001205444336,\n",
       "   27.860942840576172,\n",
       "   24.806039810180664,\n",
       "   25.097259521484375,\n",
       "   27.405733108520508,\n",
       "   28.8248291015625,\n",
       "   29.559589385986328,\n",
       "   30.677932739257812,\n",
       "   29.634052276611328,\n",
       "   32.21236038208008,\n",
       "   31.652381896972656,\n",
       "   34.52211380004883,\n",
       "   33.506710052490234,\n",
       "   32.71494674682617,\n",
       "   29.79002571105957,\n",
       "   29.638391494750977,\n",
       "   29.408899307250977,\n",
       "   29.99466896057129,\n",
       "   30.445465087890625,\n",
       "   31.655078887939453,\n",
       "   33.124046325683594,\n",
       "   35.377960205078125,\n",
       "   34.87441635131836,\n",
       "   35.865692138671875,\n",
       "   34.750343322753906,\n",
       "   34.26490020751953,\n",
       "   33.41733932495117,\n",
       "   31.62250518798828,\n",
       "   34.44206237792969,\n",
       "   32.0323600769043,\n",
       "   33.21660614013672,\n",
       "   33.93848419189453,\n",
       "   34.291988372802734,\n",
       "   35.987274169921875,\n",
       "   36.0389518737793,\n",
       "   35.34496307373047,\n",
       "   36.22168731689453,\n",
       "   34.9956169128418,\n",
       "   34.234066009521484,\n",
       "   34.395713806152344,\n",
       "   33.319950103759766,\n",
       "   35.0193977355957,\n",
       "   32.48115921020508,\n",
       "   33.21097946166992,\n",
       "   33.67850112915039,\n",
       "   33.98131561279297,\n",
       "   33.85382080078125,\n",
       "   34.59601593017578,\n",
       "   34.040252685546875,\n",
       "   34.695762634277344,\n",
       "   33.164756774902344,\n",
       "   32.410400390625,\n",
       "   32.99427032470703,\n",
       "   34.26339340209961,\n",
       "   32.99738311767578,\n",
       "   32.472957611083984,\n",
       "   32.631710052490234,\n",
       "   34.05439376831055,\n",
       "   32.648277282714844,\n",
       "   30.956735610961914,\n",
       "   32.93811798095703,\n",
       "   30.268733978271484,\n",
       "   29.6899356842041,\n",
       "   29.383281707763672,\n",
       "   28.568622589111328,\n",
       "   25.494226455688477,\n",
       "   24.31596565246582,\n",
       "   26.38578987121582,\n",
       "   26.18499755859375,\n",
       "   26.056522369384766,\n",
       "   24.833999633789062,\n",
       "   22.999814987182617,\n",
       "   24.143699645996094,\n",
       "   25.80180549621582,\n",
       "   24.884929656982422,\n",
       "   26.494583129882812,\n",
       "   27.275657653808594,\n",
       "   26.793045043945312,\n",
       "   26.811126708984375,\n",
       "   28.242006301879883,\n",
       "   28.556520462036133,\n",
       "   27.16650390625,\n",
       "   24.664573669433594,\n",
       "   24.507274627685547,\n",
       "   22.82622528076172,\n",
       "   24.261192321777344,\n",
       "   25.27293586730957,\n",
       "   26.298606872558594,\n",
       "   26.47454071044922,\n",
       "   24.567874908447266,\n",
       "   22.818275451660156,\n",
       "   23.68376922607422,\n",
       "   23.69900131225586,\n",
       "   27.074331283569336,\n",
       "   26.750940322875977,\n",
       "   25.39060401916504,\n",
       "   24.5841007232666,\n",
       "   25.231138229370117,\n",
       "   25.880983352661133,\n",
       "   24.75729751586914,\n",
       "   24.160606384277344,\n",
       "   23.802852630615234,\n",
       "   24.937702178955078,\n",
       "   25.094688415527344,\n",
       "   25.811115264892578,\n",
       "   27.280197143554688,\n",
       "   26.184621810913086,\n",
       "   24.884138107299805,\n",
       "   23.63883399963379,\n",
       "   23.23716163635254,\n",
       "   22.621583938598633,\n",
       "   24.564756393432617,\n",
       "   22.85576057434082,\n",
       "   27.258256912231445,\n",
       "   27.551687240600586,\n",
       "   25.852123260498047,\n",
       "   25.324954986572266,\n",
       "   26.67981719970703,\n",
       "   25.918134689331055,\n",
       "   25.895339965820312,\n",
       "   25.68214225769043,\n",
       "   26.332416534423828,\n",
       "   27.443307876586914,\n",
       "   27.607282638549805,\n",
       "   26.776334762573242,\n",
       "   28.141822814941406,\n",
       "   27.712940216064453,\n",
       "   25.83236312866211,\n",
       "   25.617345809936523,\n",
       "   24.253551483154297,\n",
       "   24.357070922851562,\n",
       "   26.365657806396484,\n",
       "   28.472238540649414,\n",
       "   26.963632583618164,\n",
       "   27.944522857666016,\n",
       "   28.623109817504883,\n",
       "   29.199203491210938,\n",
       "   28.66461944580078,\n",
       "   26.069265365600586,\n",
       "   25.75424575805664,\n",
       "   24.73814582824707,\n",
       "   24.42039680480957,\n",
       "   25.137096405029297,\n",
       "   23.90817642211914,\n",
       "   25.109222412109375,\n",
       "   24.650325775146484,\n",
       "   23.53641700744629,\n",
       "   23.517656326293945,\n",
       "   22.92424774169922,\n",
       "   22.038719177246094,\n",
       "   21.949918746948242,\n",
       "   21.161161422729492,\n",
       "   18.53523063659668,\n",
       "   19.80878257751465,\n",
       "   18.41329002380371,\n",
       "   18.631574630737305,\n",
       "   18.815345764160156,\n",
       "   18.97970962524414,\n",
       "   19.519039154052734,\n",
       "   19.80193328857422,\n",
       "   18.20313835144043,\n",
       "   18.962507247924805,\n",
       "   18.825037002563477,\n",
       "   17.01363754272461,\n",
       "   15.659393310546875,\n",
       "   16.274341583251953,\n",
       "   16.851139068603516,\n",
       "   18.367197036743164,\n",
       "   21.023204803466797,\n",
       "   21.642927169799805,\n",
       "   19.887481689453125,\n",
       "   19.581321716308594,\n",
       "   18.351863861083984,\n",
       "   18.911130905151367,\n",
       "   20.049150466918945,\n",
       "   19.491580963134766,\n",
       "   17.290916442871094,\n",
       "   18.4989070892334,\n",
       "   21.22591209411621,\n",
       "   18.21074867248535,\n",
       "   17.26910400390625,\n",
       "   17.40926742553711,\n",
       "   19.241241455078125,\n",
       "   18.264209747314453,\n",
       "   19.457347869873047,\n",
       "   20.450332641601562,\n",
       "   20.2860050201416,\n",
       "   22.050823211669922,\n",
       "   22.621522903442383,\n",
       "   24.002492904663086,\n",
       "   24.128700256347656,\n",
       "   23.976369857788086,\n",
       "   24.35211753845215,\n",
       "   24.722667694091797,\n",
       "   23.944374084472656,\n",
       "   21.41257667541504,\n",
       "   22.51288414001465,\n",
       "   22.095745086669922,\n",
       "   20.317398071289062,\n",
       "   20.81314468383789,\n",
       "   21.85719871520996,\n",
       "   22.807971954345703,\n",
       "   21.585960388183594,\n",
       "   19.159324645996094,\n",
       "   18.472639083862305,\n",
       "   19.12020492553711,\n",
       "   17.469663619995117,\n",
       "   17.458843231201172,\n",
       "   17.541669845581055,\n",
       "   17.455127716064453,\n",
       "   18.782642364501953,\n",
       "   17.69153594970703,\n",
       "   17.272613525390625,\n",
       "   17.371082305908203,\n",
       "   17.43972396850586,\n",
       "   20.477407455444336,\n",
       "   19.331321716308594,\n",
       "   19.673152923583984,\n",
       "   20.403453826904297,\n",
       "   18.770347595214844,\n",
       "   18.624881744384766,\n",
       "   19.67844009399414,\n",
       "   21.27277183532715,\n",
       "   20.09905433654785,\n",
       "   20.2115535736084,\n",
       "   18.755428314208984,\n",
       "   16.52484893798828,\n",
       "   15.972905158996582,\n",
       "   17.977067947387695,\n",
       "   18.373943328857422,\n",
       "   17.224084854125977,\n",
       "   13.913043022155762,\n",
       "   14.93222427368164,\n",
       "   15.546183586120605,\n",
       "   13.684887886047363,\n",
       "   12.608613014221191,\n",
       "   12.450995445251465,\n",
       "   12.364632606506348,\n",
       "   13.65544319152832,\n",
       "   14.819613456726074,\n",
       "   13.995461463928223,\n",
       "   14.929636001586914,\n",
       "   15.802023887634277,\n",
       "   16.128759384155273,\n",
       "   15.235856056213379,\n",
       "   18.271268844604492,\n",
       "   18.063570022583008,\n",
       "   20.3726863861084,\n",
       "   20.30866241455078,\n",
       "   19.3160400390625,\n",
       "   18.3969669342041,\n",
       "   16.18876838684082,\n",
       "   16.079620361328125,\n",
       "   15.347509384155273,\n",
       "   15.294466972351074],\n",
       "  'jsc_acc': [450.9501037597656,\n",
       "   292.2119140625,\n",
       "   210.03244018554688,\n",
       "   158.8765869140625,\n",
       "   119.48453521728516,\n",
       "   88.2861557006836,\n",
       "   61.91825866699219,\n",
       "   41.875362396240234,\n",
       "   24.577857971191406,\n",
       "   13.069989204406738,\n",
       "   3.3592371940612793,\n",
       "   12.765937805175781,\n",
       "   17.578752517700195,\n",
       "   26.98027229309082,\n",
       "   31.566497802734375,\n",
       "   37.57992172241211,\n",
       "   40.788944244384766,\n",
       "   46.65526580810547,\n",
       "   52.15300369262695,\n",
       "   56.38618469238281,\n",
       "   59.01724624633789,\n",
       "   59.597599029541016,\n",
       "   61.35031509399414,\n",
       "   64.81410217285156,\n",
       "   67.99417877197266,\n",
       "   69.59339904785156,\n",
       "   70.36018371582031,\n",
       "   72.8757095336914,\n",
       "   72.34896087646484,\n",
       "   73.71678924560547,\n",
       "   75.3066635131836,\n",
       "   79.14835357666016,\n",
       "   79.72425842285156,\n",
       "   79.4370346069336,\n",
       "   80.41473388671875,\n",
       "   80.55367279052734,\n",
       "   79.71276092529297,\n",
       "   81.13702392578125,\n",
       "   80.98975372314453,\n",
       "   81.75318908691406,\n",
       "   81.92755889892578,\n",
       "   81.75483703613281,\n",
       "   81.04222869873047,\n",
       "   82.08688354492188,\n",
       "   82.74131774902344,\n",
       "   83.70826721191406,\n",
       "   85.38501739501953,\n",
       "   84.95392608642578,\n",
       "   87.66753387451172,\n",
       "   87.6762924194336,\n",
       "   87.41048431396484,\n",
       "   88.6545181274414,\n",
       "   90.23468780517578,\n",
       "   89.45686340332031,\n",
       "   89.74757385253906,\n",
       "   89.08440399169922,\n",
       "   89.86888885498047,\n",
       "   90.0100326538086,\n",
       "   88.41510772705078,\n",
       "   88.2086410522461,\n",
       "   88.19442749023438,\n",
       "   88.35943603515625,\n",
       "   87.14105224609375,\n",
       "   86.68318939208984,\n",
       "   86.47611999511719,\n",
       "   86.30180358886719,\n",
       "   86.09064483642578,\n",
       "   86.98993682861328,\n",
       "   87.13668060302734,\n",
       "   85.80712127685547,\n",
       "   85.67449188232422,\n",
       "   85.89285278320312,\n",
       "   85.56729888916016,\n",
       "   84.6968994140625,\n",
       "   83.01896667480469,\n",
       "   82.48881530761719,\n",
       "   83.57862091064453,\n",
       "   83.546630859375,\n",
       "   85.15323638916016,\n",
       "   86.20291137695312,\n",
       "   85.75135803222656,\n",
       "   86.25045776367188,\n",
       "   84.57142639160156,\n",
       "   86.27216339111328,\n",
       "   87.24942016601562,\n",
       "   87.18034362792969,\n",
       "   87.9879379272461,\n",
       "   86.04568481445312,\n",
       "   84.41963958740234,\n",
       "   84.04790496826172,\n",
       "   83.54166412353516,\n",
       "   84.32308959960938,\n",
       "   84.62590026855469,\n",
       "   85.21126556396484,\n",
       "   86.07494354248047,\n",
       "   87.50120544433594,\n",
       "   88.14855194091797,\n",
       "   89.29041290283203,\n",
       "   89.76719665527344,\n",
       "   88.7620849609375,\n",
       "   89.41133880615234,\n",
       "   90.6135025024414,\n",
       "   91.09632110595703,\n",
       "   90.958984375,\n",
       "   91.85700988769531,\n",
       "   91.00180053710938,\n",
       "   91.20320892333984,\n",
       "   90.85494232177734,\n",
       "   89.02555847167969,\n",
       "   88.16217041015625,\n",
       "   87.3968505859375,\n",
       "   88.52982330322266,\n",
       "   88.21306610107422,\n",
       "   88.2053451538086,\n",
       "   86.39039611816406,\n",
       "   85.90198516845703,\n",
       "   85.28314208984375,\n",
       "   85.20065307617188,\n",
       "   84.68836212158203,\n",
       "   85.49809265136719,\n",
       "   86.02880096435547,\n",
       "   87.49726867675781,\n",
       "   88.892822265625,\n",
       "   89.23477172851562,\n",
       "   88.10169219970703,\n",
       "   87.1577377319336,\n",
       "   86.61235046386719,\n",
       "   86.38489532470703,\n",
       "   84.99861145019531,\n",
       "   85.52928924560547,\n",
       "   84.74242401123047,\n",
       "   85.5728988647461,\n",
       "   84.04125213623047,\n",
       "   84.15523529052734,\n",
       "   85.61730194091797,\n",
       "   85.79277038574219,\n",
       "   84.5103759765625,\n",
       "   84.34782409667969,\n",
       "   83.77330017089844,\n",
       "   83.27527618408203,\n",
       "   83.95564270019531,\n",
       "   83.91090393066406,\n",
       "   85.97197723388672,\n",
       "   85.71502685546875,\n",
       "   84.82591247558594,\n",
       "   86.06607055664062,\n",
       "   83.74949645996094,\n",
       "   83.16515350341797,\n",
       "   84.21209716796875,\n",
       "   85.14295959472656,\n",
       "   84.27925109863281,\n",
       "   84.0669174194336,\n",
       "   86.41775512695312,\n",
       "   87.0324935913086,\n",
       "   86.61625671386719,\n",
       "   87.54066467285156,\n",
       "   88.84905242919922,\n",
       "   88.12824249267578,\n",
       "   88.1707534790039,\n",
       "   89.24010467529297,\n",
       "   89.63204956054688,\n",
       "   89.07611083984375,\n",
       "   88.2098159790039,\n",
       "   89.99301147460938,\n",
       "   88.2060546875,\n",
       "   87.25860595703125,\n",
       "   88.74964141845703,\n",
       "   87.72674560546875,\n",
       "   88.03062438964844,\n",
       "   88.78892517089844,\n",
       "   90.3100814819336,\n",
       "   89.85741424560547,\n",
       "   90.28913116455078,\n",
       "   90.94768524169922,\n",
       "   89.2897720336914,\n",
       "   88.87637329101562,\n",
       "   88.02862548828125,\n",
       "   88.33793640136719,\n",
       "   89.78866577148438,\n",
       "   89.04039001464844,\n",
       "   88.96186828613281,\n",
       "   88.7234878540039,\n",
       "   87.90591430664062,\n",
       "   88.0576400756836,\n",
       "   88.97796630859375,\n",
       "   88.5951156616211,\n",
       "   89.4358901977539,\n",
       "   88.4858627319336,\n",
       "   90.00582885742188,\n",
       "   91.1231460571289,\n",
       "   91.25391387939453,\n",
       "   91.39253234863281,\n",
       "   91.61843872070312,\n",
       "   93.16056060791016,\n",
       "   93.65805053710938,\n",
       "   93.1752700805664,\n",
       "   93.61802673339844,\n",
       "   92.53077697753906,\n",
       "   91.19155883789062,\n",
       "   91.17473602294922,\n",
       "   92.48591613769531,\n",
       "   89.94695281982422,\n",
       "   89.24018859863281,\n",
       "   88.84725952148438,\n",
       "   88.84928894042969,\n",
       "   88.49405670166016,\n",
       "   88.16400909423828,\n",
       "   88.05602264404297,\n",
       "   88.7970962524414,\n",
       "   90.77605438232422,\n",
       "   90.77555084228516,\n",
       "   91.05615997314453,\n",
       "   89.60566711425781,\n",
       "   89.9386978149414,\n",
       "   88.76839447021484,\n",
       "   86.56172943115234,\n",
       "   87.35441589355469,\n",
       "   87.79190826416016,\n",
       "   86.94393157958984,\n",
       "   85.78251647949219,\n",
       "   85.01576232910156,\n",
       "   86.8231430053711,\n",
       "   86.54943084716797,\n",
       "   86.67237091064453,\n",
       "   85.79299926757812,\n",
       "   84.77606964111328,\n",
       "   83.99629211425781,\n",
       "   85.0649185180664,\n",
       "   84.0867691040039,\n",
       "   84.92146301269531,\n",
       "   86.32402038574219,\n",
       "   85.83673095703125,\n",
       "   85.24853515625,\n",
       "   84.49411010742188,\n",
       "   84.82176208496094,\n",
       "   85.11343383789062,\n",
       "   83.62332916259766,\n",
       "   84.74750518798828,\n",
       "   85.08273315429688,\n",
       "   84.99625396728516,\n",
       "   87.09878540039062,\n",
       "   88.76870727539062,\n",
       "   89.83537292480469,\n",
       "   90.50621795654297,\n",
       "   90.35263061523438,\n",
       "   89.72161102294922,\n",
       "   88.1576156616211,\n",
       "   88.31607055664062,\n",
       "   88.54569244384766,\n",
       "   88.81695556640625,\n",
       "   88.78591918945312,\n",
       "   89.16592407226562,\n",
       "   90.32672882080078,\n",
       "   89.69462585449219,\n",
       "   90.73544311523438,\n",
       "   91.07929992675781,\n",
       "   91.51851654052734,\n",
       "   89.4873046875,\n",
       "   90.92987060546875,\n",
       "   91.86901092529297,\n",
       "   90.76363372802734,\n",
       "   90.71517944335938,\n",
       "   91.3628158569336,\n",
       "   91.6829605102539,\n",
       "   90.06434631347656,\n",
       "   89.47998046875,\n",
       "   90.55479431152344,\n",
       "   90.83109283447266,\n",
       "   89.12922668457031,\n",
       "   90.00247955322266,\n",
       "   90.31283569335938,\n",
       "   91.00452423095703,\n",
       "   91.650146484375,\n",
       "   89.84928131103516,\n",
       "   90.73129272460938,\n",
       "   90.01253509521484,\n",
       "   88.85079956054688,\n",
       "   89.22315979003906,\n",
       "   89.62391662597656,\n",
       "   89.76278686523438,\n",
       "   89.67684173583984,\n",
       "   90.1222915649414,\n",
       "   92.53695678710938,\n",
       "   90.47364807128906,\n",
       "   90.07038116455078,\n",
       "   90.11416625976562,\n",
       "   90.27064514160156,\n",
       "   89.78775787353516,\n",
       "   87.86433410644531,\n",
       "   87.74019622802734,\n",
       "   88.45731353759766,\n",
       "   88.98138427734375,\n",
       "   90.03776550292969,\n",
       "   88.41763305664062,\n",
       "   89.07798767089844,\n",
       "   87.56893157958984,\n",
       "   85.39253234863281,\n",
       "   85.72480010986328,\n",
       "   85.66429138183594,\n",
       "   84.60709381103516,\n",
       "   85.08321380615234,\n",
       "   86.41459655761719,\n",
       "   88.2773208618164,\n",
       "   89.83654022216797,\n",
       "   88.73296356201172,\n",
       "   87.59622192382812,\n",
       "   86.3974609375,\n",
       "   86.91951751708984,\n",
       "   90.1693344116211,\n",
       "   90.09752655029297,\n",
       "   89.55521392822266,\n",
       "   89.31065368652344,\n",
       "   86.81311798095703,\n",
       "   86.44967651367188,\n",
       "   88.32654571533203,\n",
       "   90.39445495605469,\n",
       "   91.85472869873047,\n",
       "   92.36674499511719,\n",
       "   91.7718505859375,\n",
       "   89.78485107421875,\n",
       "   89.91073608398438,\n",
       "   90.43509674072266,\n",
       "   89.12757873535156,\n",
       "   88.7855453491211,\n",
       "   89.81741333007812,\n",
       "   89.51799011230469,\n",
       "   89.13926696777344,\n",
       "   89.86360168457031,\n",
       "   89.92633819580078,\n",
       "   91.65397644042969,\n",
       "   92.70753479003906,\n",
       "   90.29591369628906,\n",
       "   89.5206298828125,\n",
       "   89.33332824707031,\n",
       "   89.41744232177734,\n",
       "   88.04227447509766,\n",
       "   87.79965209960938,\n",
       "   87.37550354003906,\n",
       "   85.71153259277344,\n",
       "   87.2340087890625,\n",
       "   88.35404205322266,\n",
       "   88.90913391113281,\n",
       "   89.5480728149414,\n",
       "   88.66327667236328,\n",
       "   89.36566925048828,\n",
       "   91.91487121582031,\n",
       "   90.88249206542969,\n",
       "   91.768798828125,\n",
       "   91.28314971923828,\n",
       "   89.56417083740234,\n",
       "   89.02589416503906,\n",
       "   90.02117919921875,\n",
       "   89.43231964111328,\n",
       "   87.72889709472656,\n",
       "   89.3204116821289,\n",
       "   88.82603454589844,\n",
       "   88.0853500366211,\n",
       "   87.63194274902344,\n",
       "   87.54032897949219,\n",
       "   87.70579528808594,\n",
       "   88.72119903564453,\n",
       "   88.39603424072266,\n",
       "   88.2197494506836,\n",
       "   88.1150894165039,\n",
       "   88.51371002197266,\n",
       "   89.21599578857422,\n",
       "   89.53121948242188,\n",
       "   87.7215347290039,\n",
       "   86.6873550415039,\n",
       "   88.00513458251953,\n",
       "   88.69712829589844,\n",
       "   89.06649017333984,\n",
       "   87.92601013183594,\n",
       "   87.78239440917969,\n",
       "   87.1394271850586,\n",
       "   87.83394622802734,\n",
       "   89.06671142578125,\n",
       "   91.40978240966797,\n",
       "   92.28640747070312,\n",
       "   93.35562896728516,\n",
       "   93.0949478149414,\n",
       "   92.42537689208984,\n",
       "   92.39369201660156,\n",
       "   94.74899291992188,\n",
       "   94.51838684082031,\n",
       "   94.0184097290039,\n",
       "   96.00321197509766,\n",
       "   95.0516128540039,\n",
       "   93.35623931884766,\n",
       "   94.04944610595703,\n",
       "   93.48179626464844,\n",
       "   93.04080963134766,\n",
       "   93.54175567626953,\n",
       "   92.15937805175781,\n",
       "   92.21366119384766,\n",
       "   91.80769348144531,\n",
       "   91.70808410644531,\n",
       "   91.55571746826172,\n",
       "   91.76692962646484,\n",
       "   91.77140045166016,\n",
       "   91.80003356933594,\n",
       "   91.78861999511719,\n",
       "   90.69268035888672,\n",
       "   90.60171508789062,\n",
       "   91.12767791748047,\n",
       "   92.2618408203125,\n",
       "   92.40412902832031,\n",
       "   92.34126281738281,\n",
       "   92.07524871826172,\n",
       "   91.52584838867188,\n",
       "   90.36476135253906,\n",
       "   90.32815551757812,\n",
       "   89.89096069335938,\n",
       "   89.97441101074219,\n",
       "   91.02977752685547,\n",
       "   90.09281158447266,\n",
       "   92.12974548339844,\n",
       "   90.69982147216797,\n",
       "   91.63450622558594,\n",
       "   91.11643981933594,\n",
       "   90.37403106689453,\n",
       "   89.50940704345703,\n",
       "   90.2530517578125,\n",
       "   89.2315902709961,\n",
       "   88.90241241455078,\n",
       "   89.69872283935547,\n",
       "   90.83341979980469,\n",
       "   90.5810546875,\n",
       "   88.71672821044922,\n",
       "   87.43108367919922,\n",
       "   87.26165771484375,\n",
       "   88.0033950805664,\n",
       "   89.57167053222656,\n",
       "   91.65560150146484,\n",
       "   90.88939666748047,\n",
       "   90.2574691772461,\n",
       "   89.52374267578125,\n",
       "   89.19567108154297,\n",
       "   89.98815155029297,\n",
       "   89.45105743408203,\n",
       "   90.12942504882812,\n",
       "   89.10202026367188,\n",
       "   89.32011413574219,\n",
       "   89.78873443603516,\n",
       "   89.10115814208984,\n",
       "   88.88003540039062,\n",
       "   87.01066589355469,\n",
       "   86.86737060546875,\n",
       "   85.89991760253906,\n",
       "   84.57131958007812,\n",
       "   84.2586898803711,\n",
       "   84.44032287597656,\n",
       "   85.78394317626953,\n",
       "   86.27458953857422,\n",
       "   87.00846099853516,\n",
       "   88.75592041015625,\n",
       "   89.2157974243164,\n",
       "   88.2612533569336,\n",
       "   88.690673828125,\n",
       "   88.60993957519531,\n",
       "   89.66900634765625,\n",
       "   90.70138549804688,\n",
       "   91.18928527832031,\n",
       "   90.5908203125,\n",
       "   92.14698028564453,\n",
       "   91.13969421386719,\n",
       "   91.11334228515625,\n",
       "   90.5455093383789,\n",
       "   91.98939514160156,\n",
       "   91.0353012084961,\n",
       "   92.872314453125,\n",
       "   92.80460357666016,\n",
       "   90.6289291381836,\n",
       "   91.06121063232422,\n",
       "   91.87359619140625,\n",
       "   92.66148376464844,\n",
       "   93.0792236328125,\n",
       "   94.19012451171875,\n",
       "   93.98179626464844,\n",
       "   92.29882049560547,\n",
       "   91.61595153808594,\n",
       "   90.8530044555664,\n",
       "   90.84772491455078,\n",
       "   91.2737808227539,\n",
       "   90.21268463134766,\n",
       "   90.08303833007812,\n",
       "   91.05902862548828,\n",
       "   90.88813018798828,\n",
       "   91.49935913085938,\n",
       "   92.26341247558594,\n",
       "   93.07637786865234,\n",
       "   92.47029113769531,\n",
       "   90.39381408691406,\n",
       "   90.84539794921875,\n",
       "   89.28502655029297,\n",
       "   89.97746276855469,\n",
       "   90.06640625,\n",
       "   89.84183502197266,\n",
       "   90.31525421142578,\n",
       "   89.67776489257812],\n",
       "  'ff_acc': [129.8175506591797,\n",
       "   83.08020782470703,\n",
       "   57.464141845703125,\n",
       "   42.6324348449707,\n",
       "   24.507526397705078,\n",
       "   14.199773788452148,\n",
       "   10.68366813659668,\n",
       "   6.084762096405029,\n",
       "   5.316501617431641,\n",
       "   5.261505126953125,\n",
       "   5.683196544647217,\n",
       "   6.260250568389893,\n",
       "   9.187037467956543,\n",
       "   10.228567123413086,\n",
       "   11.288384437561035,\n",
       "   11.79419994354248,\n",
       "   13.768423080444336,\n",
       "   14.459864616394043,\n",
       "   15.713669776916504,\n",
       "   18.68205451965332,\n",
       "   18.806610107421875,\n",
       "   21.066205978393555,\n",
       "   22.913558959960938,\n",
       "   23.526222229003906,\n",
       "   24.525142669677734,\n",
       "   25.365665435791016,\n",
       "   26.564863204956055,\n",
       "   28.283388137817383,\n",
       "   27.199295043945312,\n",
       "   27.40350341796875,\n",
       "   27.331117630004883,\n",
       "   28.12783432006836,\n",
       "   27.60686492919922,\n",
       "   28.0206298828125,\n",
       "   26.253992080688477,\n",
       "   25.664966583251953,\n",
       "   26.663150787353516,\n",
       "   27.792068481445312,\n",
       "   29.002660751342773,\n",
       "   30.390256881713867,\n",
       "   30.847190856933594,\n",
       "   31.394094467163086,\n",
       "   31.25477409362793,\n",
       "   32.10103988647461,\n",
       "   32.29054641723633,\n",
       "   31.799036026000977,\n",
       "   31.523242950439453,\n",
       "   32.18060302734375,\n",
       "   31.024070739746094,\n",
       "   32.35455322265625,\n",
       "   31.848711013793945,\n",
       "   32.31913757324219,\n",
       "   33.956764221191406,\n",
       "   34.216468811035156,\n",
       "   33.076927185058594,\n",
       "   33.86785125732422,\n",
       "   35.26243209838867,\n",
       "   32.74778747558594,\n",
       "   32.07627868652344,\n",
       "   32.33954620361328,\n",
       "   32.177982330322266,\n",
       "   32.39891052246094,\n",
       "   30.653085708618164,\n",
       "   31.13465690612793,\n",
       "   29.724905014038086,\n",
       "   29.96585464477539,\n",
       "   29.498586654663086,\n",
       "   28.609460830688477,\n",
       "   28.95858383178711,\n",
       "   27.82047462463379,\n",
       "   27.755577087402344,\n",
       "   27.05046272277832,\n",
       "   28.97437286376953,\n",
       "   30.941513061523438,\n",
       "   30.325820922851562,\n",
       "   29.2276611328125,\n",
       "   27.233198165893555,\n",
       "   27.740808486938477,\n",
       "   26.317087173461914,\n",
       "   28.019908905029297,\n",
       "   29.252498626708984,\n",
       "   27.696290969848633,\n",
       "   29.05797576904297,\n",
       "   28.013107299804688,\n",
       "   28.76918601989746,\n",
       "   29.077390670776367,\n",
       "   29.331928253173828,\n",
       "   28.691011428833008,\n",
       "   29.11209487915039,\n",
       "   30.218273162841797,\n",
       "   28.43964385986328,\n",
       "   27.774890899658203,\n",
       "   29.044559478759766,\n",
       "   28.102705001831055,\n",
       "   28.801456451416016,\n",
       "   26.818567276000977,\n",
       "   26.53249740600586,\n",
       "   27.006301879882812,\n",
       "   27.840417861938477,\n",
       "   28.424665451049805,\n",
       "   27.574310302734375,\n",
       "   27.359130859375,\n",
       "   26.819950103759766,\n",
       "   26.653289794921875,\n",
       "   26.69283676147461,\n",
       "   26.94602394104004,\n",
       "   26.532384872436523,\n",
       "   26.364269256591797,\n",
       "   26.981712341308594,\n",
       "   26.61125373840332,\n",
       "   27.965904235839844,\n",
       "   29.03614616394043,\n",
       "   29.464309692382812,\n",
       "   29.517824172973633,\n",
       "   28.642229080200195,\n",
       "   27.8323974609375,\n",
       "   28.25937843322754,\n",
       "   28.063636779785156,\n",
       "   28.29637908935547,\n",
       "   29.802248001098633,\n",
       "   28.230449676513672,\n",
       "   28.697919845581055,\n",
       "   28.500417709350586,\n",
       "   26.640533447265625,\n",
       "   26.065093994140625,\n",
       "   25.929349899291992,\n",
       "   26.356992721557617,\n",
       "   28.32196044921875,\n",
       "   27.031787872314453,\n",
       "   27.091907501220703,\n",
       "   27.26513671875,\n",
       "   29.045639038085938,\n",
       "   29.458967208862305,\n",
       "   29.352781295776367,\n",
       "   30.97001838684082,\n",
       "   29.2908935546875,\n",
       "   31.301651000976562,\n",
       "   31.531482696533203,\n",
       "   31.711292266845703,\n",
       "   30.66161346435547,\n",
       "   31.924278259277344,\n",
       "   31.58515167236328,\n",
       "   33.106300354003906,\n",
       "   34.34925842285156,\n",
       "   33.04473876953125,\n",
       "   33.46046829223633,\n",
       "   33.43417739868164,\n",
       "   34.3770637512207,\n",
       "   33.85773468017578,\n",
       "   33.304569244384766,\n",
       "   32.669471740722656,\n",
       "   31.213268280029297,\n",
       "   30.29073715209961,\n",
       "   30.901094436645508,\n",
       "   31.348094940185547,\n",
       "   31.765058517456055,\n",
       "   30.594310760498047,\n",
       "   33.011714935302734,\n",
       "   33.588504791259766,\n",
       "   32.49320983886719,\n",
       "   31.741539001464844,\n",
       "   31.269498825073242,\n",
       "   30.825645446777344,\n",
       "   30.182775497436523,\n",
       "   31.7504940032959,\n",
       "   30.381492614746094,\n",
       "   28.520124435424805,\n",
       "   28.240446090698242,\n",
       "   29.084991455078125,\n",
       "   28.503387451171875,\n",
       "   29.135005950927734,\n",
       "   28.14096450805664,\n",
       "   29.259910583496094,\n",
       "   29.06402587890625,\n",
       "   29.855844497680664,\n",
       "   29.46892738342285,\n",
       "   29.04021453857422,\n",
       "   27.962282180786133,\n",
       "   26.66376495361328,\n",
       "   26.762399673461914,\n",
       "   26.36035919189453,\n",
       "   27.057249069213867,\n",
       "   26.70697784423828,\n",
       "   26.856781005859375,\n",
       "   26.37680435180664,\n",
       "   25.905210494995117,\n",
       "   26.062110900878906,\n",
       "   25.82709503173828,\n",
       "   25.099149703979492,\n",
       "   26.683706283569336,\n",
       "   28.481821060180664,\n",
       "   29.162019729614258,\n",
       "   29.586347579956055,\n",
       "   30.024534225463867,\n",
       "   30.867332458496094,\n",
       "   33.0762939453125,\n",
       "   32.563297271728516,\n",
       "   33.28852844238281,\n",
       "   32.421234130859375,\n",
       "   32.719749450683594,\n",
       "   32.505680084228516,\n",
       "   33.43757629394531,\n",
       "   31.4328670501709,\n",
       "   33.8010368347168,\n",
       "   33.81485366821289,\n",
       "   32.861663818359375,\n",
       "   31.980579376220703,\n",
       "   31.68523406982422,\n",
       "   30.714508056640625,\n",
       "   30.905431747436523,\n",
       "   32.574073791503906,\n",
       "   31.767133712768555,\n",
       "   31.09026336669922,\n",
       "   33.7513542175293,\n",
       "   32.78963851928711,\n",
       "   33.36821365356445,\n",
       "   32.017398834228516,\n",
       "   30.666845321655273,\n",
       "   28.961753845214844,\n",
       "   30.425983428955078,\n",
       "   30.935304641723633,\n",
       "   30.58230209350586,\n",
       "   30.125125885009766,\n",
       "   30.34101104736328,\n",
       "   30.512672424316406,\n",
       "   31.038591384887695,\n",
       "   31.85248374938965,\n",
       "   32.196983337402344,\n",
       "   32.353519439697266,\n",
       "   33.34663772583008,\n",
       "   31.037822723388672,\n",
       "   28.709020614624023,\n",
       "   28.204423904418945,\n",
       "   28.31833267211914,\n",
       "   29.58799934387207,\n",
       "   28.617366790771484,\n",
       "   28.701290130615234,\n",
       "   27.670156478881836,\n",
       "   28.737178802490234,\n",
       "   27.632469177246094,\n",
       "   27.992626190185547,\n",
       "   28.705841064453125,\n",
       "   29.091934204101562,\n",
       "   27.929264068603516,\n",
       "   28.86691665649414,\n",
       "   29.028419494628906,\n",
       "   30.5242919921875,\n",
       "   31.43242835998535,\n",
       "   31.08454132080078,\n",
       "   32.233272552490234,\n",
       "   30.488828659057617,\n",
       "   29.627138137817383,\n",
       "   30.999040603637695,\n",
       "   33.403785705566406,\n",
       "   33.51786422729492,\n",
       "   35.07987976074219,\n",
       "   37.4445686340332,\n",
       "   37.80815124511719,\n",
       "   37.526466369628906,\n",
       "   36.4361686706543,\n",
       "   36.24102783203125,\n",
       "   35.91694641113281,\n",
       "   34.4292106628418,\n",
       "   34.77045440673828,\n",
       "   35.484745025634766,\n",
       "   35.55241012573242,\n",
       "   36.249061584472656,\n",
       "   36.478538513183594,\n",
       "   36.376914978027344,\n",
       "   35.69802474975586,\n",
       "   34.3975830078125,\n",
       "   35.12717819213867,\n",
       "   35.83359909057617,\n",
       "   34.31623077392578,\n",
       "   33.76696014404297,\n",
       "   32.253597259521484,\n",
       "   33.17687225341797,\n",
       "   33.14486312866211,\n",
       "   31.650060653686523,\n",
       "   31.370445251464844,\n",
       "   30.74118423461914,\n",
       "   30.771955490112305,\n",
       "   31.611173629760742,\n",
       "   31.42426872253418,\n",
       "   31.73923110961914,\n",
       "   31.734949111938477,\n",
       "   30.783018112182617,\n",
       "   32.065589904785156,\n",
       "   31.381839752197266,\n",
       "   31.281524658203125,\n",
       "   29.48923683166504,\n",
       "   28.60504150390625,\n",
       "   30.792665481567383,\n",
       "   29.342750549316406,\n",
       "   30.4691162109375,\n",
       "   31.09796142578125,\n",
       "   31.236549377441406,\n",
       "   31.09380340576172,\n",
       "   30.938989639282227,\n",
       "   32.00961685180664,\n",
       "   32.072410583496094,\n",
       "   33.08270263671875,\n",
       "   33.35761260986328,\n",
       "   32.6396598815918,\n",
       "   32.01840591430664,\n",
       "   31.55350112915039,\n",
       "   31.24353790283203,\n",
       "   30.75527572631836,\n",
       "   31.279420852661133,\n",
       "   30.16887664794922,\n",
       "   29.912263870239258,\n",
       "   29.526033401489258,\n",
       "   29.215272903442383,\n",
       "   30.675220489501953,\n",
       "   31.277633666992188,\n",
       "   31.720224380493164,\n",
       "   31.996627807617188,\n",
       "   31.093603134155273,\n",
       "   29.22739601135254,\n",
       "   29.121524810791016,\n",
       "   27.835237503051758,\n",
       "   28.11184310913086,\n",
       "   28.131484985351562,\n",
       "   26.89272117614746,\n",
       "   29.430622100830078,\n",
       "   30.164554595947266,\n",
       "   28.602249145507812,\n",
       "   27.724313735961914,\n",
       "   28.85199546813965,\n",
       "   27.228897094726562,\n",
       "   27.644121170043945,\n",
       "   28.871503829956055,\n",
       "   28.174686431884766,\n",
       "   28.19268798828125,\n",
       "   28.652402877807617,\n",
       "   28.525394439697266,\n",
       "   27.568265914916992,\n",
       "   30.114099502563477,\n",
       "   31.670974731445312,\n",
       "   30.7442569732666,\n",
       "   31.49455451965332,\n",
       "   33.25007629394531,\n",
       "   32.40201950073242,\n",
       "   30.118045806884766,\n",
       "   30.970829010009766,\n",
       "   31.427431106567383,\n",
       "   30.831876754760742,\n",
       "   30.831417083740234,\n",
       "   28.95382308959961,\n",
       "   28.176774978637695,\n",
       "   27.87623405456543,\n",
       "   27.231178283691406,\n",
       "   27.420696258544922,\n",
       "   27.192312240600586,\n",
       "   29.02027130126953,\n",
       "   29.527191162109375,\n",
       "   28.670242309570312,\n",
       "   28.437450408935547,\n",
       "   29.439958572387695,\n",
       "   30.032106399536133,\n",
       "   29.59548568725586,\n",
       "   29.494125366210938,\n",
       "   30.184545516967773,\n",
       "   29.68269920349121,\n",
       "   29.785018920898438,\n",
       "   31.067058563232422,\n",
       "   29.68156623840332,\n",
       "   28.369945526123047,\n",
       "   27.622095108032227,\n",
       "   28.270050048828125,\n",
       "   28.086814880371094,\n",
       "   27.448997497558594,\n",
       "   27.495315551757812,\n",
       "   28.083620071411133,\n",
       "   28.663331985473633,\n",
       "   29.977638244628906,\n",
       "   32.178260803222656,\n",
       "   32.055606842041016,\n",
       "   32.189205169677734,\n",
       "   31.174072265625,\n",
       "   32.00621795654297,\n",
       "   30.259183883666992,\n",
       "   28.189117431640625,\n",
       "   29.10913848876953,\n",
       "   29.64213752746582,\n",
       "   30.602582931518555,\n",
       "   32.76865005493164,\n",
       "   32.41203308105469,\n",
       "   31.125608444213867,\n",
       "   31.842439651489258,\n",
       "   31.04359245300293,\n",
       "   31.512832641601562,\n",
       "   31.790191650390625,\n",
       "   31.615978240966797,\n",
       "   33.26165771484375,\n",
       "   33.44664001464844,\n",
       "   33.19975662231445,\n",
       "   32.10363006591797,\n",
       "   33.637603759765625,\n",
       "   33.826717376708984,\n",
       "   33.715267181396484,\n",
       "   34.12682342529297,\n",
       "   35.12117385864258,\n",
       "   34.79471206665039,\n",
       "   35.462276458740234,\n",
       "   35.24057388305664,\n",
       "   36.5609245300293,\n",
       "   35.55778503417969,\n",
       "   34.46818161010742,\n",
       "   34.466487884521484,\n",
       "   35.60728073120117,\n",
       "   36.32197570800781,\n",
       "   35.659950256347656,\n",
       "   35.28947067260742,\n",
       "   35.10099411010742,\n",
       "   33.9544563293457,\n",
       "   33.774776458740234,\n",
       "   35.4573860168457,\n",
       "   35.430850982666016,\n",
       "   35.66205978393555,\n",
       "   36.59367370605469,\n",
       "   37.072486877441406,\n",
       "   36.63066482543945,\n",
       "   37.479454040527344,\n",
       "   37.93235778808594,\n",
       "   36.17271423339844,\n",
       "   35.673614501953125,\n",
       "   35.62643051147461,\n",
       "   35.188602447509766,\n",
       "   35.19889450073242,\n",
       "   33.68772888183594,\n",
       "   35.070098876953125,\n",
       "   33.65593719482422,\n",
       "   32.61441421508789,\n",
       "   33.26547622680664,\n",
       "   32.46540832519531,\n",
       "   32.50246047973633,\n",
       "   32.03887939453125,\n",
       "   31.092796325683594,\n",
       "   33.15078353881836,\n",
       "   31.40740966796875,\n",
       "   31.87592315673828,\n",
       "   31.969186782836914,\n",
       "   31.138334274291992,\n",
       "   30.539941787719727,\n",
       "   31.40576171875,\n",
       "   32.419830322265625,\n",
       "   31.42154312133789,\n",
       "   32.514747619628906,\n",
       "   32.03512191772461,\n",
       "   32.66291809082031,\n",
       "   32.33474349975586,\n",
       "   29.91632843017578,\n",
       "   29.7489013671875,\n",
       "   30.14786148071289,\n",
       "   30.57231330871582,\n",
       "   31.316862106323242,\n",
       "   31.743091583251953,\n",
       "   32.828609466552734,\n",
       "   33.048404693603516,\n",
       "   30.688732147216797,\n",
       "   30.40049934387207,\n",
       "   29.89969825744629,\n",
       "   29.320106506347656,\n",
       "   30.317861557006836,\n",
       "   30.049875259399414,\n",
       "   30.479446411132812,\n",
       "   32.395751953125,\n",
       "   34.59497833251953,\n",
       "   34.37955856323242,\n",
       "   33.97720718383789,\n",
       "   33.66691970825195,\n",
       "   34.24824905395508,\n",
       "   32.983558654785156,\n",
       "   32.595375061035156,\n",
       "   33.260189056396484,\n",
       "   33.02645492553711,\n",
       "   31.98694610595703,\n",
       "   33.156673431396484,\n",
       "   33.97278594970703,\n",
       "   34.28733825683594,\n",
       "   34.25899887084961,\n",
       "   33.12839889526367,\n",
       "   32.70233154296875,\n",
       "   33.59977722167969,\n",
       "   32.659366607666016,\n",
       "   33.337833404541016,\n",
       "   33.66606903076172,\n",
       "   33.40260314941406,\n",
       "   32.55853271484375,\n",
       "   30.373844146728516,\n",
       "   29.09093475341797,\n",
       "   30.437332153320312,\n",
       "   30.957826614379883,\n",
       "   30.46803092956543,\n",
       "   32.39396286010742,\n",
       "   30.985137939453125,\n",
       "   32.03087615966797,\n",
       "   32.15681457519531,\n",
       "   32.17655563354492],\n",
       "  'test_accs': [1196.7997131347656,\n",
       "   870.0001373291016,\n",
       "   703.3125305175781,\n",
       "   571.9692420959473,\n",
       "   472.2698097229004,\n",
       "   400.6202144622803,\n",
       "   337.68002128601074,\n",
       "   290.6723475456238,\n",
       "   253.78203582763672,\n",
       "   223.07447910308838,\n",
       "   201.91440105438232,\n",
       "   200.09592485427856,\n",
       "   194.35039615631104,\n",
       "   194.28095245361328,\n",
       "   186.70549869537354,\n",
       "   185.9460744857788,\n",
       "   186.04566764831543,\n",
       "   188.40140438079834,\n",
       "   191.4358606338501,\n",
       "   194.2765407562256,\n",
       "   190.5466423034668,\n",
       "   186.89827156066895,\n",
       "   184.54373168945312,\n",
       "   184.9951171875,\n",
       "   185.72237014770508,\n",
       "   184.07341384887695,\n",
       "   183.6539249420166,\n",
       "   183.6353759765625,\n",
       "   179.47995567321777,\n",
       "   181.5500831604004,\n",
       "   181.51763916015625,\n",
       "   186.81950187683105,\n",
       "   186.2800407409668,\n",
       "   183.76855850219727,\n",
       "   183.89688682556152,\n",
       "   178.53958129882812,\n",
       "   176.35369110107422,\n",
       "   176.53984832763672,\n",
       "   177.6763105392456,\n",
       "   177.80540084838867,\n",
       "   179.79479789733887,\n",
       "   180.71360397338867,\n",
       "   180.8723020553589,\n",
       "   180.78806400299072,\n",
       "   183.6000280380249,\n",
       "   183.2013921737671,\n",
       "   188.3698501586914,\n",
       "   186.86571502685547,\n",
       "   188.26057815551758,\n",
       "   188.10812187194824,\n",
       "   186.28135871887207,\n",
       "   188.30388641357422,\n",
       "   192.0402660369873,\n",
       "   195.77170372009277,\n",
       "   195.3499755859375,\n",
       "   195.03241348266602,\n",
       "   198.21251487731934,\n",
       "   195.8719425201416,\n",
       "   192.12102127075195,\n",
       "   190.5169734954834,\n",
       "   187.43815231323242,\n",
       "   182.618980884552,\n",
       "   177.9331202507019,\n",
       "   180.74902439117432,\n",
       "   176.5441017150879,\n",
       "   174.66434049606323,\n",
       "   172.6054081916809,\n",
       "   174.37841796875,\n",
       "   175.7316083908081,\n",
       "   173.27547025680542,\n",
       "   172.88800239562988,\n",
       "   172.9327597618103,\n",
       "   175.89290523529053,\n",
       "   175.0373511314392,\n",
       "   171.746750831604,\n",
       "   168.33302974700928,\n",
       "   169.00473880767822,\n",
       "   169.4403052330017,\n",
       "   171.0986270904541,\n",
       "   174.99905157089233,\n",
       "   173.6356520652771,\n",
       "   173.2239933013916,\n",
       "   173.7037754058838,\n",
       "   174.3596329689026,\n",
       "   176.91923141479492,\n",
       "   174.98550033569336,\n",
       "   174.94065856933594,\n",
       "   171.89353370666504,\n",
       "   170.43530368804932,\n",
       "   172.5486764907837,\n",
       "   170.426025390625,\n",
       "   169.05382204055786,\n",
       "   171.06014394760132,\n",
       "   170.9431529045105,\n",
       "   174.25099563598633,\n",
       "   174.06827688217163,\n",
       "   175.00349044799805,\n",
       "   177.181987285614,\n",
       "   179.32154607772827,\n",
       "   177.04851007461548,\n",
       "   175.62730312347412,\n",
       "   177.5377435684204,\n",
       "   179.5361466407776,\n",
       "   178.65418243408203,\n",
       "   179.52041244506836,\n",
       "   179.506609916687,\n",
       "   178.8468370437622,\n",
       "   176.54477882385254,\n",
       "   175.0379819869995,\n",
       "   173.14076232910156,\n",
       "   175.18238639831543,\n",
       "   176.68386554718018,\n",
       "   178.24458408355713,\n",
       "   177.6869659423828,\n",
       "   175.26873874664307,\n",
       "   174.94532108306885,\n",
       "   174.12681770324707,\n",
       "   175.72552490234375,\n",
       "   175.63508987426758,\n",
       "   180.34174823760986,\n",
       "   179.3573932647705,\n",
       "   183.08612823486328,\n",
       "   185.51263904571533,\n",
       "   182.55464458465576,\n",
       "   180.71028518676758,\n",
       "   179.96714973449707,\n",
       "   178.85540676116943,\n",
       "   181.55053234100342,\n",
       "   176.89914894104004,\n",
       "   178.11341094970703,\n",
       "   177.6904811859131,\n",
       "   183.8264331817627,\n",
       "   181.89326190948486,\n",
       "   182.17097091674805,\n",
       "   184.37549591064453,\n",
       "   183.74198150634766,\n",
       "   183.87732028961182,\n",
       "   183.79774951934814,\n",
       "   182.97944355010986,\n",
       "   180.72418403625488,\n",
       "   182.45943450927734,\n",
       "   180.33479976654053,\n",
       "   186.69506740570068,\n",
       "   187.9734926223755,\n",
       "   187.04787254333496,\n",
       "   188.44948482513428,\n",
       "   182.4154577255249,\n",
       "   182.1612195968628,\n",
       "   185.3122386932373,\n",
       "   184.93873119354248,\n",
       "   184.78767108917236,\n",
       "   181.46535301208496,\n",
       "   181.25761604309082,\n",
       "   182.24928092956543,\n",
       "   182.7864990234375,\n",
       "   184.1430206298828,\n",
       "   185.46703624725342,\n",
       "   187.65374183654785,\n",
       "   188.8899040222168,\n",
       "   187.54826259613037,\n",
       "   186.66302871704102,\n",
       "   186.0284309387207,\n",
       "   184.47144603729248,\n",
       "   185.12515926361084,\n",
       "   185.08367252349854,\n",
       "   184.78249263763428,\n",
       "   185.06849098205566,\n",
       "   183.65377616882324,\n",
       "   185.50758266448975,\n",
       "   187.58744144439697,\n",
       "   187.6497507095337,\n",
       "   185.7852268218994,\n",
       "   187.4446907043457,\n",
       "   187.91555786132812,\n",
       "   186.30741500854492,\n",
       "   186.38294219970703,\n",
       "   184.27559185028076,\n",
       "   184.32437133789062,\n",
       "   184.72980499267578,\n",
       "   184.0774040222168,\n",
       "   183.09559631347656,\n",
       "   186.38119888305664,\n",
       "   186.9591007232666,\n",
       "   187.4119529724121,\n",
       "   190.1984806060791,\n",
       "   193.687593460083,\n",
       "   192.90725898742676,\n",
       "   190.29712295532227,\n",
       "   191.00218963623047,\n",
       "   194.10031700134277,\n",
       "   197.20588493347168,\n",
       "   197.23342323303223,\n",
       "   198.1105785369873,\n",
       "   198.52962684631348,\n",
       "   200.4040813446045,\n",
       "   201.02479934692383,\n",
       "   201.50658226013184,\n",
       "   204.75653648376465,\n",
       "   203.09130859375,\n",
       "   203.7228240966797,\n",
       "   205.8504180908203,\n",
       "   202.2964153289795,\n",
       "   200.30454063415527,\n",
       "   202.00571632385254,\n",
       "   204.0073585510254,\n",
       "   201.9182415008545,\n",
       "   198.07709884643555,\n",
       "   197.92218780517578,\n",
       "   196.78738594055176,\n",
       "   197.44250297546387,\n",
       "   201.59477615356445,\n",
       "   199.51484870910645,\n",
       "   196.6440887451172,\n",
       "   201.62341690063477,\n",
       "   197.58804512023926,\n",
       "   195.99929428100586,\n",
       "   197.72225189208984,\n",
       "   197.1396083831787,\n",
       "   199.64392852783203,\n",
       "   200.54915618896484,\n",
       "   199.62378120422363,\n",
       "   200.34873580932617,\n",
       "   195.7529811859131,\n",
       "   195.31846046447754,\n",
       "   200.21717834472656,\n",
       "   198.92706680297852,\n",
       "   199.07927131652832,\n",
       "   201.53738403320312,\n",
       "   195.08036994934082,\n",
       "   196.2491970062256,\n",
       "   196.0420742034912,\n",
       "   196.70782089233398,\n",
       "   193.9792766571045,\n",
       "   193.54884147644043,\n",
       "   194.12522506713867,\n",
       "   194.38135719299316,\n",
       "   194.41508102416992,\n",
       "   193.68979454040527,\n",
       "   194.2595272064209,\n",
       "   192.81555938720703,\n",
       "   191.7481746673584,\n",
       "   192.26306915283203,\n",
       "   194.84040069580078,\n",
       "   195.64322471618652,\n",
       "   193.91352081298828,\n",
       "   192.8088035583496,\n",
       "   197.93159103393555,\n",
       "   195.23014068603516,\n",
       "   195.16658401489258,\n",
       "   199.17477989196777,\n",
       "   198.65850639343262,\n",
       "   199.77358436584473,\n",
       "   202.25591468811035,\n",
       "   203.2209014892578,\n",
       "   204.73475646972656,\n",
       "   206.22204971313477,\n",
       "   211.86502838134766,\n",
       "   209.70387649536133,\n",
       "   209.9214973449707,\n",
       "   207.7400951385498,\n",
       "   206.72213172912598,\n",
       "   206.14939308166504,\n",
       "   207.22152137756348,\n",
       "   207.7054901123047,\n",
       "   208.14351654052734,\n",
       "   208.58112716674805,\n",
       "   212.49835968017578,\n",
       "   211.32818222045898,\n",
       "   211.56811904907227,\n",
       "   210.47649383544922,\n",
       "   208.74298477172852,\n",
       "   207.87113189697266,\n",
       "   206.60420608520508,\n",
       "   205.92573165893555,\n",
       "   204.16075134277344,\n",
       "   203.4273223876953,\n",
       "   204.70925903320312,\n",
       "   205.95837783813477,\n",
       "   206.19779014587402,\n",
       "   206.4645538330078,\n",
       "   203.94564056396484,\n",
       "   206.17370414733887,\n",
       "   207.33968925476074,\n",
       "   204.0943431854248,\n",
       "   205.31750106811523,\n",
       "   203.99653816223145,\n",
       "   203.84625053405762,\n",
       "   201.47489166259766,\n",
       "   197.6375961303711,\n",
       "   200.7353515625,\n",
       "   199.12615394592285,\n",
       "   199.52961349487305,\n",
       "   205.16966438293457,\n",
       "   201.2217025756836,\n",
       "   203.3179588317871,\n",
       "   199.82284545898438,\n",
       "   197.5914192199707,\n",
       "   197.36922454833984,\n",
       "   199.27094841003418,\n",
       "   199.98162078857422,\n",
       "   200.52076721191406,\n",
       "   204.08612060546875,\n",
       "   208.40707397460938,\n",
       "   207.60640335083008,\n",
       "   203.5552577972412,\n",
       "   203.3387222290039,\n",
       "   200.25518035888672,\n",
       "   199.31009483337402,\n",
       "   202.88302040100098,\n",
       "   198.7162857055664,\n",
       "   196.00238800048828,\n",
       "   195.10842895507812,\n",
       "   193.9658088684082,\n",
       "   197.38300323486328,\n",
       "   198.17384719848633,\n",
       "   199.6762638092041,\n",
       "   201.20422554016113,\n",
       "   201.10128593444824,\n",
       "   199.20441818237305,\n",
       "   193.80466842651367,\n",
       "   193.74165534973145,\n",
       "   194.67897033691406,\n",
       "   193.34775924682617,\n",
       "   190.89705085754395,\n",
       "   194.8066806793213,\n",
       "   194.978609085083,\n",
       "   192.21035385131836,\n",
       "   191.633394241333,\n",
       "   192.7984104156494,\n",
       "   191.3324317932129,\n",
       "   193.75919914245605,\n",
       "   191.41094589233398,\n",
       "   191.04043579101562,\n",
       "   190.32149124145508,\n",
       "   189.48026847839355,\n",
       "   186.65140533447266,\n",
       "   186.2732334136963,\n",
       "   188.65040016174316,\n",
       "   192.77173805236816,\n",
       "   193.38536834716797,\n",
       "   194.35355377197266,\n",
       "   195.8553295135498,\n",
       "   194.71819877624512,\n",
       "   191.93074989318848,\n",
       "   191.58922576904297,\n",
       "   193.49065589904785,\n",
       "   191.54980278015137,\n",
       "   192.41425323486328,\n",
       "   191.19689559936523,\n",
       "   190.2714138031006,\n",
       "   191.07374382019043,\n",
       "   191.7438564300537,\n",
       "   190.0259189605713,\n",
       "   187.43940353393555,\n",
       "   192.1044521331787,\n",
       "   191.94826698303223,\n",
       "   191.4007511138916,\n",
       "   188.704740524292,\n",
       "   195.1837615966797,\n",
       "   193.77087783813477,\n",
       "   194.1776008605957,\n",
       "   192.9061279296875,\n",
       "   194.68948936462402,\n",
       "   192.82783126831055,\n",
       "   193.77935028076172,\n",
       "   195.5341625213623,\n",
       "   194.88575172424316,\n",
       "   192.74539756774902,\n",
       "   190.34020614624023,\n",
       "   192.1281337738037,\n",
       "   194.19054412841797,\n",
       "   194.98487091064453,\n",
       "   193.16057205200195,\n",
       "   193.2983169555664,\n",
       "   191.564790725708,\n",
       "   193.95989227294922,\n",
       "   198.8715591430664,\n",
       "   202.1326961517334,\n",
       "   201.92717933654785,\n",
       "   202.63357162475586,\n",
       "   203.0778980255127,\n",
       "   200.97907829284668,\n",
       "   198.96502685546875,\n",
       "   200.8192844390869,\n",
       "   200.9318027496338,\n",
       "   200.22124099731445,\n",
       "   204.11289024353027,\n",
       "   203.7423210144043,\n",
       "   198.3108425140381,\n",
       "   201.43580055236816,\n",
       "   199.40826225280762,\n",
       "   197.80378913879395,\n",
       "   199.31736946105957,\n",
       "   197.1483917236328,\n",
       "   196.08262252807617,\n",
       "   195.89792442321777,\n",
       "   194.68070030212402,\n",
       "   189.933687210083,\n",
       "   193.0214672088623,\n",
       "   192.3490810394287,\n",
       "   191.96098136901855,\n",
       "   192.30189514160156,\n",
       "   193.35920333862305,\n",
       "   194.39850997924805,\n",
       "   196.41234970092773,\n",
       "   194.73982048034668,\n",
       "   195.8961124420166,\n",
       "   195.19308280944824,\n",
       "   192.11750411987305,\n",
       "   189.47143173217773,\n",
       "   189.79510116577148,\n",
       "   190.16253280639648,\n",
       "   192.48636054992676,\n",
       "   194.47312927246094,\n",
       "   196.85138511657715,\n",
       "   193.61104202270508,\n",
       "   195.35874938964844,\n",
       "   193.9045181274414,\n",
       "   195.966646194458,\n",
       "   196.9695873260498,\n",
       "   197.16340255737305,\n",
       "   196.49923706054688,\n",
       "   198.2375774383545,\n",
       "   201.62567329406738,\n",
       "   198.52449226379395,\n",
       "   197.15774536132812,\n",
       "   198.1922264099121,\n",
       "   201.72361755371094,\n",
       "   197.86949157714844,\n",
       "   198.08075332641602,\n",
       "   195.54283142089844,\n",
       "   197.4685115814209,\n",
       "   198.19201278686523,\n",
       "   201.24813270568848,\n",
       "   201.8657169342041,\n",
       "   200.9353904724121,\n",
       "   198.3203411102295,\n",
       "   198.9862117767334,\n",
       "   200.76807403564453,\n",
       "   202.55695724487305,\n",
       "   200.19510078430176,\n",
       "   200.7016544342041,\n",
       "   199.50091743469238,\n",
       "   197.17462348937988,\n",
       "   194.82551383972168,\n",
       "   195.90769004821777,\n",
       "   194.41196823120117,\n",
       "   190.71159744262695,\n",
       "   188.72658920288086,\n",
       "   184.4321804046631,\n",
       "   184.60211563110352,\n",
       "   183.42705726623535,\n",
       "   182.48797988891602,\n",
       "   183.24564170837402,\n",
       "   183.43989944458008,\n",
       "   185.91661643981934,\n",
       "   186.56287956237793,\n",
       "   184.89273834228516,\n",
       "   187.30109405517578,\n",
       "   187.34420013427734,\n",
       "   189.2248706817627,\n",
       "   188.7623119354248,\n",
       "   187.86148643493652,\n",
       "   186.82758712768555,\n",
       "   188.66906929016113,\n",
       "   187.86550331115723,\n",
       "   188.80850982666016,\n",
       "   189.46239280700684,\n",
       "   193.27381706237793,\n",
       "   191.81496620178223,\n",
       "   192.2940788269043,\n",
       "   190.7278594970703,\n",
       "   190.3560037612915,\n",
       "   190.4551830291748,\n",
       "   191.58012008666992,\n",
       "   191.1225872039795,\n",
       "   187.6568479537964,\n",
       "   189.8667869567871,\n",
       "   191.6859884262085,\n",
       "   189.85168170928955,\n",
       "   188.4988031387329,\n",
       "   186.620436668396,\n",
       "   183.8390588760376,\n",
       "   185.04997444152832,\n",
       "   185.80823802947998,\n",
       "   184.15110111236572,\n",
       "   187.89245414733887,\n",
       "   187.88576412200928,\n",
       "   188.11013221740723,\n",
       "   187.4536828994751,\n",
       "   190.1245937347412,\n",
       "   188.71878623962402,\n",
       "   190.80965614318848,\n",
       "   192.24768257141113,\n",
       "   190.6704044342041,\n",
       "   191.92270851135254,\n",
       "   187.65395545959473,\n",
       "   188.57561111450195,\n",
       "   188.37746620178223,\n",
       "   186.81067562103271],\n",
       "  'pce_r2': [-80.02239643662813,\n",
       "   -94.16594731911313,\n",
       "   -119.04673567144089,\n",
       "   -130.97115253321167,\n",
       "   -126.39376956725782,\n",
       "   -128.28042361762166,\n",
       "   -129.92256788770155,\n",
       "   -125.42413180916216,\n",
       "   -126.85642529478626,\n",
       "   -129.68380954894025,\n",
       "   -131.10372407777734,\n",
       "   -127.71174328675866,\n",
       "   -125.88337907469952,\n",
       "   -124.62909808251959,\n",
       "   -122.86217976143956,\n",
       "   -133.23152504708023,\n",
       "   -128.4116250065228,\n",
       "   -131.441509272999,\n",
       "   -121.3619598400244,\n",
       "   -110.09077938119377,\n",
       "   -80.29896624840205,\n",
       "   -70.23198627100307,\n",
       "   -66.21990394848535,\n",
       "   -60.59095235036282,\n",
       "   -59.79448901735942,\n",
       "   -55.28014009442372,\n",
       "   -54.42448773927928,\n",
       "   -45.37817564391146,\n",
       "   -46.45525053931408,\n",
       "   -52.85560075620874,\n",
       "   -56.79377044302494,\n",
       "   -55.448503300884674,\n",
       "   -48.296470356982695,\n",
       "   -41.27991697335945,\n",
       "   -40.31762552395001,\n",
       "   -41.86695752219459,\n",
       "   -42.113592684558384,\n",
       "   -36.22470083213765,\n",
       "   -38.769945518332015,\n",
       "   -38.23886504260608,\n",
       "   -41.423379517167774,\n",
       "   -38.762379538571366,\n",
       "   -37.52952582052835,\n",
       "   -33.51472960912458,\n",
       "   -35.72387050232567,\n",
       "   -36.55740089226678,\n",
       "   -36.67988750174661,\n",
       "   -36.15271857911901,\n",
       "   -36.54531867922038,\n",
       "   -48.49301459182697,\n",
       "   -42.17107019824604,\n",
       "   -36.071093478368695,\n",
       "   -35.79555957016391,\n",
       "   -37.21683771142305,\n",
       "   -35.577185140231336,\n",
       "   -35.5841782416973,\n",
       "   -42.63216016354828,\n",
       "   -40.270931014034815,\n",
       "   -42.831036529411875,\n",
       "   -36.63343986689144,\n",
       "   -36.37988651307306,\n",
       "   -29.287818673289504,\n",
       "   -26.267348488789192,\n",
       "   -31.26908494290567,\n",
       "   -30.682936020992646,\n",
       "   -27.728925592239875,\n",
       "   -30.920237239096046,\n",
       "   -36.376772945051506,\n",
       "   -40.34596067580592,\n",
       "   -38.60709721767506,\n",
       "   -33.51149402121112,\n",
       "   -35.40354801998358,\n",
       "   -35.49589768776072,\n",
       "   -35.75738186241892,\n",
       "   -32.81016935755061,\n",
       "   -32.406694378286105,\n",
       "   -34.96447188304671,\n",
       "   -31.068407048154782,\n",
       "   -41.72006394782007,\n",
       "   -43.060034768871475,\n",
       "   -41.061319597298166,\n",
       "   -38.566952415579,\n",
       "   -35.96906295013665,\n",
       "   -38.41148233110454,\n",
       "   -38.879503025000254,\n",
       "   -33.58977716062901,\n",
       "   -29.085036644012614,\n",
       "   -31.60177243559604,\n",
       "   -34.66541186624043,\n",
       "   -35.725711237881654,\n",
       "   -47.3674795951554,\n",
       "   -37.28566104912545,\n",
       "   -36.630096059335614,\n",
       "   -38.02791181268748,\n",
       "   -45.74808206261957,\n",
       "   -45.18867624708072,\n",
       "   -45.47299124058039,\n",
       "   -42.03549083439792,\n",
       "   -43.07227525887201,\n",
       "   -33.430437324436845,\n",
       "   -30.70200712352913,\n",
       "   -28.335456006107847,\n",
       "   -29.686382083987574,\n",
       "   -29.390180738845814,\n",
       "   -31.614605074158092,\n",
       "   -36.486976282155574,\n",
       "   -35.71256575406906,\n",
       "   -31.904360094983204,\n",
       "   -37.4409291285524,\n",
       "   -38.06150020550443,\n",
       "   -44.98898772602926,\n",
       "   -39.568935928425766,\n",
       "   -37.82783756651959,\n",
       "   -37.65715239176805,\n",
       "   -36.27262504039183,\n",
       "   -34.232058937823865,\n",
       "   -33.863613314340256,\n",
       "   -37.16535566063007,\n",
       "   -41.49999449589663,\n",
       "   -46.848401559799846,\n",
       "   -50.74518279132447,\n",
       "   -47.71885733506924,\n",
       "   -53.01425560492474,\n",
       "   -55.96001072002718,\n",
       "   -60.942309390138554,\n",
       "   -52.41332507489615,\n",
       "   -47.498281454806005,\n",
       "   -48.44789094392296,\n",
       "   -38.91329900305469,\n",
       "   -39.25369221674312,\n",
       "   -40.76478560204353,\n",
       "   -39.46548287964407,\n",
       "   -43.56821429695758,\n",
       "   -38.32044332865925,\n",
       "   -35.254406832434256,\n",
       "   -40.7248614362636,\n",
       "   -38.183587370065574,\n",
       "   -39.1271980049632,\n",
       "   -38.258017076156015,\n",
       "   -37.92663234075293,\n",
       "   -35.89585292364531,\n",
       "   -31.566548180668498,\n",
       "   -42.55662973545591,\n",
       "   -35.589625162518125,\n",
       "   -32.196377154237965,\n",
       "   -32.667733914891144,\n",
       "   -26.038404072793288,\n",
       "   -28.506998201950136,\n",
       "   -31.134392016945824,\n",
       "   -28.337981847166546,\n",
       "   -25.64091889197592,\n",
       "   -27.577798181187585,\n",
       "   -27.68114666705629,\n",
       "   -24.87079764234407,\n",
       "   -26.421785188146124,\n",
       "   -25.310788795476036,\n",
       "   -25.878556532318065,\n",
       "   -23.34929538835873,\n",
       "   -23.552216896660074,\n",
       "   -24.455704459683567,\n",
       "   -24.762755817087054,\n",
       "   -24.82705578999739,\n",
       "   -24.8007134735643,\n",
       "   -26.55721306358845,\n",
       "   -25.872693637051206,\n",
       "   -29.863269568719183,\n",
       "   -29.386345851399323,\n",
       "   -27.590919961921458,\n",
       "   -31.388577759880775,\n",
       "   -36.28194577986732,\n",
       "   -35.02278025220552,\n",
       "   -29.1658828323429,\n",
       "   -26.380067001621438,\n",
       "   -29.98223659449929,\n",
       "   -30.3258092228638,\n",
       "   -33.21976556412049,\n",
       "   -36.77091632523807,\n",
       "   -38.52673114353572,\n",
       "   -36.92959285989654,\n",
       "   -31.464566276915853,\n",
       "   -31.636801619896268,\n",
       "   -32.89693414428856,\n",
       "   -32.843721157678296,\n",
       "   -30.92713602024527,\n",
       "   -33.44426347838841,\n",
       "   -34.19322551955004,\n",
       "   -37.535827673164384,\n",
       "   -36.639257263165376,\n",
       "   -35.059968779500515,\n",
       "   -38.24242069122728,\n",
       "   -31.47748388227091,\n",
       "   -29.46975313750771,\n",
       "   -27.273859808076015,\n",
       "   -28.272232902468566,\n",
       "   -27.145145618553766,\n",
       "   -30.73148075935143,\n",
       "   -32.012092443114,\n",
       "   -33.605599195732154,\n",
       "   -34.94224606438325,\n",
       "   -35.402455683888284,\n",
       "   -31.620496433288373,\n",
       "   -31.1077813855929,\n",
       "   -31.86603974617298,\n",
       "   -32.56359889815269,\n",
       "   -34.02775478354982,\n",
       "   -35.58669054909316,\n",
       "   -36.368020746749025,\n",
       "   -36.45510139407747,\n",
       "   -37.66552469197832,\n",
       "   -38.40210530723256,\n",
       "   -40.4950766948392,\n",
       "   -35.84474529249654,\n",
       "   -40.91491608969442,\n",
       "   -41.01758400424391,\n",
       "   -39.170419200493725,\n",
       "   -41.903456463766446,\n",
       "   -43.657579487209716,\n",
       "   -38.5276459908245,\n",
       "   -39.16857421650039,\n",
       "   -38.60709136672036,\n",
       "   -36.21480025572426,\n",
       "   -38.65278934016105,\n",
       "   -32.75110934716256,\n",
       "   -33.030006458576835,\n",
       "   -36.277596932515316,\n",
       "   -38.27455259376396,\n",
       "   -35.828119357069355,\n",
       "   -33.338135407675445,\n",
       "   -32.05069351172459,\n",
       "   -30.02299513549326,\n",
       "   -33.07092836201568,\n",
       "   -35.51245994034155,\n",
       "   -37.28981720823249,\n",
       "   -43.30966876653423,\n",
       "   -45.55614244335865,\n",
       "   -48.13933203558674,\n",
       "   -56.46739467099474,\n",
       "   -56.8123616203971,\n",
       "   -52.99950778029164,\n",
       "   -66.69933355092566,\n",
       "   -64.33134645204774,\n",
       "   -61.12512737434789,\n",
       "   -63.05450381272725,\n",
       "   -60.56770252440545,\n",
       "   -59.005872271476356,\n",
       "   -46.802280177635836,\n",
       "   -47.80371378940553,\n",
       "   -42.25756698665579,\n",
       "   -36.684946260656595,\n",
       "   -37.65463749351971,\n",
       "   -38.00723386536516,\n",
       "   -41.09121633381257,\n",
       "   -36.19455417614563,\n",
       "   -37.24534029919027,\n",
       "   -35.204453890210345,\n",
       "   -36.44879295048117,\n",
       "   -33.63929963041298,\n",
       "   -32.0437664239033,\n",
       "   -33.05230609848644,\n",
       "   -36.11863681274454,\n",
       "   -34.00824606256783,\n",
       "   -31.940126251886703,\n",
       "   -37.608477516985296,\n",
       "   -39.216129782867974,\n",
       "   -36.79554443389634,\n",
       "   -36.41555789264043,\n",
       "   -36.62965960350816,\n",
       "   -31.6762578655822,\n",
       "   -38.338238322877665,\n",
       "   -33.15397753756575,\n",
       "   -34.66355199647345,\n",
       "   -37.40035834950442,\n",
       "   -31.876610620097303,\n",
       "   -32.4030495580762,\n",
       "   -37.416108958879384,\n",
       "   -32.89235826432542,\n",
       "   -38.56362086609666,\n",
       "   -43.607040876653976,\n",
       "   -39.49781870635396,\n",
       "   -36.07717734365676,\n",
       "   -35.38294674561659,\n",
       "   -33.777064356379356,\n",
       "   -33.40410166959789,\n",
       "   -36.03226954015632,\n",
       "   -34.46194564914987,\n",
       "   -32.15596674790594,\n",
       "   -34.87605882019683,\n",
       "   -36.76328132330424,\n",
       "   -38.75085220163898,\n",
       "   -44.306322786851545,\n",
       "   -42.329228217398814,\n",
       "   -43.66705174603353,\n",
       "   -49.05589798931035,\n",
       "   -44.969780956377605,\n",
       "   -42.50283903482138,\n",
       "   -39.81458656034859,\n",
       "   -35.401296247412034,\n",
       "   -33.6820169022525,\n",
       "   -38.16994183871813,\n",
       "   -40.8481694366585,\n",
       "   -41.32925843717376,\n",
       "   -53.59431071461803,\n",
       "   -55.92094078466939,\n",
       "   -50.376757553753684,\n",
       "   -46.33886991083612,\n",
       "   -40.7154648126926,\n",
       "   -40.9406862803883,\n",
       "   -36.53095043239482,\n",
       "   -40.73133600901346,\n",
       "   -38.817727600151635,\n",
       "   -41.37949793925633,\n",
       "   -44.20660494818205,\n",
       "   -41.588269154369314,\n",
       "   -49.979566192079396,\n",
       "   -37.86408219640242,\n",
       "   -41.38164928526966,\n",
       "   -42.636460305923485,\n",
       "   -37.94612483808902,\n",
       "   -36.19346099230912,\n",
       "   -33.72257794318047,\n",
       "   -33.29769920397121,\n",
       "   -32.45626978203956,\n",
       "   -34.83191864768398,\n",
       "   -34.4865388440167,\n",
       "   -31.928153138616807,\n",
       "   -32.82735418998792,\n",
       "   -31.386849776111355,\n",
       "   -31.89417634490394,\n",
       "   -35.39729887899938,\n",
       "   -32.76320270096256,\n",
       "   -30.13813057047754,\n",
       "   -28.01086515818538,\n",
       "   -26.864442404230875,\n",
       "   -30.247776616025597,\n",
       "   -30.297973921723155,\n",
       "   -33.89147287318369,\n",
       "   -32.87121418024645,\n",
       "   -31.224294149045413,\n",
       "   -28.14296240817881,\n",
       "   -29.582078547306644,\n",
       "   -34.63491367344156,\n",
       "   -36.46582472144698,\n",
       "   -35.45377618264573,\n",
       "   -33.78727257145645,\n",
       "   -29.788507556795118,\n",
       "   -28.384996129625808,\n",
       "   -27.5122704022212,\n",
       "   -26.34408305557685,\n",
       "   -27.271492472801057,\n",
       "   -28.366834525878822,\n",
       "   -27.069330670252512,\n",
       "   -28.73230989978424,\n",
       "   -28.746022341494786,\n",
       "   -28.411323521007276,\n",
       "   -29.871824395347325,\n",
       "   -29.980604358393148,\n",
       "   -30.789966565874536,\n",
       "   -33.793078940545875,\n",
       "   -35.71594232110748,\n",
       "   -32.98746022720863,\n",
       "   -32.45920860341949,\n",
       "   -28.300950718875608,\n",
       "   -28.669006346328132,\n",
       "   -30.539831974752705,\n",
       "   -29.940813565383255,\n",
       "   -32.31863707355585,\n",
       "   -33.92519350884769,\n",
       "   -33.57161638487697,\n",
       "   -36.83880016718811,\n",
       "   -34.225994194099165,\n",
       "   -35.085695336529334,\n",
       "   -33.71777125844603,\n",
       "   -42.489369326519025,\n",
       "   -40.21358985018503,\n",
       "   -41.15254573018732,\n",
       "   -41.69035227845676,\n",
       "   -40.88491274078812,\n",
       "   -35.80240374212725,\n",
       "   -41.41817997849045,\n",
       "   -38.03059777967732,\n",
       "   -36.23375445341157,\n",
       "   -33.63727421108822,\n",
       "   -39.43980948167831,\n",
       "   -38.55735909523777,\n",
       "   -42.55276795713615,\n",
       "   -43.94392026516288,\n",
       "   -42.48626185762438,\n",
       "   -43.60367878767474,\n",
       "   -38.010969191710736,\n",
       "   -34.17532621359158,\n",
       "   -35.42185994169545,\n",
       "   -41.5638025934197,\n",
       "   -42.590035404701,\n",
       "   -43.01311011586993,\n",
       "   -39.911638504510286,\n",
       "   -41.19358323526426,\n",
       "   -35.18004080789749,\n",
       "   -36.03279474490211,\n",
       "   -31.39706146200166,\n",
       "   -32.799774380123246,\n",
       "   -32.82682220313977,\n",
       "   -33.16047403518542,\n",
       "   -30.327833876094573,\n",
       "   -34.46105377798245,\n",
       "   -35.920290472740426,\n",
       "   -33.25169663252748,\n",
       "   -32.243389181286204,\n",
       "   -33.36417411385047,\n",
       "   -38.4624206557802,\n",
       "   -33.33035160236131,\n",
       "   -37.226148500110476,\n",
       "   -36.05389823695259,\n",
       "   -36.814757702544625,\n",
       "   -32.33712156401854,\n",
       "   -35.14035907818521,\n",
       "   -31.72274543778189,\n",
       "   -31.451766345184865,\n",
       "   -31.895767896730334,\n",
       "   -33.03198444808792,\n",
       "   -36.73479903498375,\n",
       "   -36.6525099741808,\n",
       "   -42.97719545275401,\n",
       "   -39.9067065572065,\n",
       "   -44.9082806234003,\n",
       "   -47.48486470896953,\n",
       "   -51.512365793131295,\n",
       "   -46.77850938101625,\n",
       "   -57.0511532560718,\n",
       "   -47.36040710653475,\n",
       "   -45.9090121945499,\n",
       "   -47.053683866081066,\n",
       "   -50.112978805973036,\n",
       "   -46.03460860106692,\n",
       "   -56.8306044405092,\n",
       "   -58.16836498384755,\n",
       "   -54.750055951895725,\n",
       "   -47.96983430930406,\n",
       "   -51.37508306825192,\n",
       "   -59.08188637304019,\n",
       "   -62.11501409734918,\n",
       "   -64.39447954641417,\n",
       "   -62.78310320282933,\n",
       "   -58.08932887983472,\n",
       "   -56.521226348474144,\n",
       "   -53.108137889434175,\n",
       "   -49.909115356172585,\n",
       "   -50.66148461124221,\n",
       "   -38.157114816877424,\n",
       "   -44.05076484639369,\n",
       "   -39.43118378115469,\n",
       "   -37.967261434840815,\n",
       "   -35.38158776961072,\n",
       "   -32.28682935208594,\n",
       "   -34.51155743269725,\n",
       "   -31.139561759030386,\n",
       "   -29.450172412755443,\n",
       "   -29.372437726951233,\n",
       "   -26.958111052351846,\n",
       "   -27.29598003661256,\n",
       "   -28.357050364772796,\n",
       "   -33.00483089476161,\n",
       "   -37.88774832127843,\n",
       "   -40.21375657128793,\n",
       "   -39.49293702902903,\n",
       "   -32.930663612604356,\n",
       "   -35.83837142254762,\n",
       "   -33.694272149986,\n",
       "   -34.170195629220636,\n",
       "   -36.05490844852957,\n",
       "   -35.64786181446821,\n",
       "   -37.68655237206093,\n",
       "   -40.6727690382446,\n",
       "   -42.00250668589354,\n",
       "   -44.5620675760366,\n",
       "   -36.03378638584236,\n",
       "   -35.93486952616844,\n",
       "   -43.53860497521296,\n",
       "   -45.239623575376925,\n",
       "   -38.744675671890484,\n",
       "   -37.945125200868446,\n",
       "   -37.67931194349346,\n",
       "   -35.05270447774362,\n",
       "   -33.01956763527839,\n",
       "   -28.862998801924384,\n",
       "   -32.035938403431736,\n",
       "   -38.11663327860787,\n",
       "   -41.72850400436696,\n",
       "   -40.63971888987098,\n",
       "   -40.07159416715039,\n",
       "   -36.09724738291596,\n",
       "   -33.018159235677736,\n",
       "   -36.121069440783174,\n",
       "   -38.941856816509876,\n",
       "   -40.94230367270981,\n",
       "   -45.937181748083404,\n",
       "   -42.941210034375324,\n",
       "   -48.07312977192924,\n",
       "   -36.68516063550761,\n",
       "   -38.850966395381406,\n",
       "   -37.19800407669892],\n",
       "  'voc_r2': [-2205.314748915457,\n",
       "   -1069.3659586772112,\n",
       "   -895.5785338888624,\n",
       "   -532.0648955635888,\n",
       "   -425.6151361971641,\n",
       "   -359.8135975114956,\n",
       "   -337.80097869481193,\n",
       "   -271.34883804269947,\n",
       "   -274.8486107430964,\n",
       "   -215.82060248826187,\n",
       "   -179.50656795647603,\n",
       "   -165.73170610803132,\n",
       "   -149.4810759630654,\n",
       "   -146.02799548188338,\n",
       "   -107.21903056757291,\n",
       "   -90.2812396961872,\n",
       "   -80.57743707455474,\n",
       "   -74.33111759540678,\n",
       "   -70.11014376616114,\n",
       "   -54.57819306311056,\n",
       "   -47.80304881370409,\n",
       "   -40.404421992314454,\n",
       "   -30.615606801957036,\n",
       "   -23.6115037936457,\n",
       "   -19.973678237704195,\n",
       "   -15.53196783794365,\n",
       "   -14.86804087463358,\n",
       "   -12.064620442972545,\n",
       "   -10.648184987532696,\n",
       "   -11.350693569648937,\n",
       "   -10.962461335714515,\n",
       "   -12.319655050644487,\n",
       "   -11.389470375165443,\n",
       "   -9.08380588161542,\n",
       "   -10.958286820971429,\n",
       "   -7.6507959394411795,\n",
       "   -5.781825658872718,\n",
       "   -4.309077567283715,\n",
       "   -3.931606472036954,\n",
       "   -3.682382203129527,\n",
       "   -4.300194241237912,\n",
       "   -3.542739279834559,\n",
       "   -4.1273451942023955,\n",
       "   -3.0016713789989122,\n",
       "   -3.68544962537909,\n",
       "   -3.5988600978284406,\n",
       "   -5.907729267434315,\n",
       "   -5.353357533526483,\n",
       "   -5.490181651724638,\n",
       "   -4.509751378933264,\n",
       "   -3.9863835777497236,\n",
       "   -4.134245607788978,\n",
       "   -4.110577583101099,\n",
       "   -7.049274928428693,\n",
       "   -7.788394361471422,\n",
       "   -6.600524606141487,\n",
       "   -6.2730451135598875,\n",
       "   -5.849351799190016,\n",
       "   -4.970615812890654,\n",
       "   -5.569821181642132,\n",
       "   -3.9159291854302287,\n",
       "   -1.2244705191565908,\n",
       "   -1.172347647253749,\n",
       "   -1.499590123679131,\n",
       "   -0.9241990005480658,\n",
       "   -0.4716543365377095,\n",
       "   -0.5030511532409101,\n",
       "   -0.7761833111151688,\n",
       "   -0.19118441096472338,\n",
       "   -0.4472468119216222,\n",
       "   -0.5281220688296207,\n",
       "   -1.1386208066304726,\n",
       "   -1.7117900271421678,\n",
       "   -1.3574636029779308,\n",
       "   -1.1686525194573636,\n",
       "   -0.24964309277113195,\n",
       "   -0.30367842927708133,\n",
       "   -0.5943427867453752,\n",
       "   -0.3756947408990121,\n",
       "   -0.4864772178440131,\n",
       "   -0.11071702727324495,\n",
       "   -0.09797129909897917,\n",
       "   -0.19678832625342935,\n",
       "   0.21026730711977648,\n",
       "   0.21908587255553735,\n",
       "   0.2794719649007722,\n",
       "   0.17054806222094787,\n",
       "   0.26462409720678737,\n",
       "   0.23583868563924892,\n",
       "   -0.517059207983128,\n",
       "   -0.12015134738742472,\n",
       "   0.2494778244478313,\n",
       "   0.23756981135653354,\n",
       "   0.1920428541585376,\n",
       "   -0.16232107683870178,\n",
       "   -0.221251742710453,\n",
       "   -0.000485721760012936,\n",
       "   -0.27326763426974976,\n",
       "   0.10455009093499812,\n",
       "   -0.018588879106477885,\n",
       "   0.11164975225464358,\n",
       "   0.06181315901132023,\n",
       "   0.12556509147952044,\n",
       "   0.13805279231152323,\n",
       "   0.042417058726175405,\n",
       "   -0.11475592421271497,\n",
       "   -0.003637886466707485,\n",
       "   0.017675310399473565,\n",
       "   0.10839200701316065,\n",
       "   -0.14779641355530182,\n",
       "   -0.8789209793706851,\n",
       "   -0.6598437195913358,\n",
       "   -1.0490888479098759,\n",
       "   -0.7326919005851227,\n",
       "   -0.7425937521179007,\n",
       "   -0.5767639146182089,\n",
       "   -0.42305070443355786,\n",
       "   -0.23302523900966743,\n",
       "   -0.23088836039733596,\n",
       "   -0.6118236462126421,\n",
       "   -0.4708287385978067,\n",
       "   -0.49810818818163516,\n",
       "   -0.48101252104264747,\n",
       "   -0.42809687180657385,\n",
       "   -0.4175644860365624,\n",
       "   -0.2297222808399424,\n",
       "   -0.4585124079603684,\n",
       "   -0.43200141997631514,\n",
       "   -0.5859559216003549,\n",
       "   -0.6188656412201134,\n",
       "   -0.8115865293184836,\n",
       "   -1.2420846149860392,\n",
       "   -1.004281611276141,\n",
       "   -1.095428571651981,\n",
       "   -0.9510961992697817,\n",
       "   -1.1606002077038844,\n",
       "   -0.9286046749888559,\n",
       "   -0.8515200493584376,\n",
       "   -0.8547467307127967,\n",
       "   -0.5796214471434891,\n",
       "   -0.5505559654396526,\n",
       "   -0.6780770240133149,\n",
       "   -0.5887037649112425,\n",
       "   -0.8999522319795421,\n",
       "   -1.1014685393825445,\n",
       "   -1.0736601114890965,\n",
       "   -0.6763353378171335,\n",
       "   -0.8985003973220504,\n",
       "   -1.1619919771178848,\n",
       "   -1.1016389683631385,\n",
       "   -1.6665594215423396,\n",
       "   -1.1287279955858582,\n",
       "   -0.9918886431718241,\n",
       "   -1.3159715994313763,\n",
       "   -1.0945822095954094,\n",
       "   -1.0897158365208264,\n",
       "   -1.0847973462445282,\n",
       "   -1.4612550192479143,\n",
       "   -1.3336781164544163,\n",
       "   -1.218367688147393,\n",
       "   -0.9366873067162966,\n",
       "   -1.0931174579468128,\n",
       "   -0.9954467106708655,\n",
       "   -0.9035923209451402,\n",
       "   -0.6818137366363286,\n",
       "   -0.5547972557151246,\n",
       "   -0.4731183910060106,\n",
       "   -0.7068660145166927,\n",
       "   -0.9066905658337439,\n",
       "   -1.0852166794446365,\n",
       "   -0.7865892796384897,\n",
       "   -0.8122643560704113,\n",
       "   -1.07789125766976,\n",
       "   -0.8852603178297473,\n",
       "   -0.6286679380667417,\n",
       "   -0.6524343806788231,\n",
       "   -0.9139700649371494,\n",
       "   -1.0101565883353136,\n",
       "   -1.3065855416798633,\n",
       "   -1.4941236804567435,\n",
       "   -1.527838901675422,\n",
       "   -2.243136494372137,\n",
       "   -2.365238835043084,\n",
       "   -2.8268339891130907,\n",
       "   -3.04279706883362,\n",
       "   -4.526715024777641,\n",
       "   -3.8801916560465823,\n",
       "   -3.460049405561632,\n",
       "   -3.838588479063195,\n",
       "   -4.181212385394798,\n",
       "   -4.403989343830562,\n",
       "   -4.227529593616562,\n",
       "   -4.389740983436764,\n",
       "   -3.8911416216635013,\n",
       "   -4.180375626317384,\n",
       "   -3.6726641853521427,\n",
       "   -3.8822057947478514,\n",
       "   -4.861997722604362,\n",
       "   -5.111372165315425,\n",
       "   -5.492318535215912,\n",
       "   -6.147384195662317,\n",
       "   -5.572885506319009,\n",
       "   -5.600953440003921,\n",
       "   -5.393261943847727,\n",
       "   -6.396132121719871,\n",
       "   -7.01348697503666,\n",
       "   -6.357421858347564,\n",
       "   -7.189250488043077,\n",
       "   -7.188769396423066,\n",
       "   -7.1058118321903265,\n",
       "   -7.672700165824967,\n",
       "   -7.120038675296501,\n",
       "   -6.15202449775344,\n",
       "   -7.168847647802876,\n",
       "   -6.35927337740053,\n",
       "   -6.6227435813922755,\n",
       "   -7.783059055822715,\n",
       "   -8.02879872452129,\n",
       "   -10.966981720104474,\n",
       "   -13.232539965871535,\n",
       "   -14.794398641068335,\n",
       "   -13.410469946298592,\n",
       "   -12.29284571356625,\n",
       "   -13.212347259503511,\n",
       "   -20.069889107311734,\n",
       "   -17.242070610381756,\n",
       "   -16.83903178797071,\n",
       "   -17.763080278391577,\n",
       "   -12.56534386064629,\n",
       "   -14.146965353242594,\n",
       "   -15.013058146417364,\n",
       "   -16.367232372761876,\n",
       "   -14.612803298628599,\n",
       "   -14.393099536174525,\n",
       "   -14.77161426761194,\n",
       "   -15.213881979010193,\n",
       "   -17.49285043940663,\n",
       "   -17.410471996465017,\n",
       "   -17.231052579920767,\n",
       "   -18.41028600352677,\n",
       "   -15.331466108362736,\n",
       "   -15.28536466703142,\n",
       "   -18.556972027386117,\n",
       "   -21.251430756046457,\n",
       "   -17.605190480268192,\n",
       "   -15.927836957461167,\n",
       "   -23.04874894894334,\n",
       "   -19.060086212115507,\n",
       "   -23.863664185415605,\n",
       "   -31.935278703725956,\n",
       "   -38.407774745482946,\n",
       "   -39.89840740201599,\n",
       "   -42.574600909937395,\n",
       "   -41.39919114960412,\n",
       "   -51.88464325828366,\n",
       "   -53.22035771022058,\n",
       "   -69.94549108467747,\n",
       "   -76.55205022085265,\n",
       "   -78.13193027555457,\n",
       "   -73.36351392364058,\n",
       "   -77.73053748075066,\n",
       "   -81.85315553737293,\n",
       "   -99.24332615342615,\n",
       "   -109.29979901689852,\n",
       "   -126.50537133599406,\n",
       "   -134.84750701851837,\n",
       "   -146.7993572080137,\n",
       "   -132.88203779715516,\n",
       "   -138.18033445991344,\n",
       "   -129.39703310356856,\n",
       "   -111.24863017250183,\n",
       "   -93.89590825446956,\n",
       "   -70.34905074991873,\n",
       "   -79.44010062814155,\n",
       "   -61.4976413207571,\n",
       "   -59.12892185987648,\n",
       "   -63.56340913856236,\n",
       "   -64.31779968986008,\n",
       "   -70.80394616066509,\n",
       "   -72.56951251791045,\n",
       "   -70.11979628423705,\n",
       "   -76.88402265566228,\n",
       "   -74.48524195194707,\n",
       "   -73.50021623445708,\n",
       "   -71.95600745010739,\n",
       "   -65.56135512451637,\n",
       "   -72.32449535320828,\n",
       "   -59.483051272983836,\n",
       "   -61.9946696945117,\n",
       "   -63.80631905066657,\n",
       "   -63.635796682891495,\n",
       "   -57.86812952519224,\n",
       "   -58.54481199847457,\n",
       "   -56.179815246171444,\n",
       "   -58.532303141520174,\n",
       "   -53.64651318549181,\n",
       "   -51.72890610293863,\n",
       "   -53.70508995361241,\n",
       "   -60.466569228203745,\n",
       "   -53.22879091518104,\n",
       "   -51.956868015754196,\n",
       "   -49.413123065046754,\n",
       "   -53.68766316946694,\n",
       "   -50.860697820121295,\n",
       "   -44.49397713622864,\n",
       "   -48.95489101658521,\n",
       "   -39.99602006813318,\n",
       "   -36.48544490512112,\n",
       "   -35.02267299742173,\n",
       "   -32.51671756359704,\n",
       "   -25.565695333544905,\n",
       "   -23.043375523726787,\n",
       "   -28.028647180013134,\n",
       "   -27.950333795207907,\n",
       "   -27.705218996522255,\n",
       "   -24.318378710350334,\n",
       "   -21.24647729875456,\n",
       "   -22.97195221366237,\n",
       "   -27.18330245189738,\n",
       "   -25.06172998801227,\n",
       "   -28.513268755773122,\n",
       "   -29.336637722260264,\n",
       "   -28.114600418420896,\n",
       "   -28.178003565364374,\n",
       "   -31.676264445022966,\n",
       "   -31.52426934980464,\n",
       "   -27.719043236216514,\n",
       "   -22.69471749673681,\n",
       "   -23.06322487152464,\n",
       "   -19.25446009417758,\n",
       "   -21.722918725742005,\n",
       "   -24.392489707839868,\n",
       "   -25.42375404618374,\n",
       "   -26.5822197053893,\n",
       "   -22.078149614533746,\n",
       "   -19.26113666141389,\n",
       "   -20.62999087414968,\n",
       "   -20.359720946699678,\n",
       "   -25.354366592894454,\n",
       "   -25.00224007231704,\n",
       "   -23.03072080432669,\n",
       "   -21.949382496048706,\n",
       "   -23.86348730700172,\n",
       "   -24.74958485060832,\n",
       "   -22.639117633632768,\n",
       "   -21.50408084778105,\n",
       "   -20.26363900430914,\n",
       "   -22.417600690030454,\n",
       "   -22.098215962278317,\n",
       "   -23.679311337489526,\n",
       "   -26.177460951296716,\n",
       "   -23.33111070796459,\n",
       "   -20.992682279681787,\n",
       "   -19.954260949736845,\n",
       "   -18.805634061164227,\n",
       "   -18.079844097544882,\n",
       "   -21.6149416987115,\n",
       "   -17.485439701971874,\n",
       "   -25.707490910431694,\n",
       "   -26.886142904028805,\n",
       "   -23.39626657587913,\n",
       "   -22.69953067624198,\n",
       "   -25.384960113607985,\n",
       "   -22.99016246978357,\n",
       "   -22.86073978729828,\n",
       "   -22.34895557098667,\n",
       "   -22.920346660831548,\n",
       "   -25.381324383630655,\n",
       "   -26.09202595792351,\n",
       "   -24.423525737811357,\n",
       "   -26.317495568869884,\n",
       "   -25.53140297230029,\n",
       "   -22.95835641262913,\n",
       "   -22.899473265199184,\n",
       "   -20.45389718079078,\n",
       "   -19.862913533128435,\n",
       "   -23.369704352125027,\n",
       "   -27.128034335609104,\n",
       "   -24.678751607432655,\n",
       "   -26.243039069690216,\n",
       "   -27.932963488020288,\n",
       "   -29.58725533781614,\n",
       "   -28.649752620698525,\n",
       "   -23.962194924169594,\n",
       "   -23.717820192145474,\n",
       "   -22.171282086444457,\n",
       "   -21.09215210556756,\n",
       "   -22.474135539364124,\n",
       "   -19.92895445259863,\n",
       "   -22.19751842395151,\n",
       "   -21.44612707355172,\n",
       "   -19.568787441915727,\n",
       "   -19.52423403470616,\n",
       "   -18.137709573442468,\n",
       "   -16.79231219965009,\n",
       "   -16.537017203692418,\n",
       "   -15.428854850737498,\n",
       "   -11.612133558415609,\n",
       "   -13.21340389045358,\n",
       "   -11.776079263441453,\n",
       "   -12.234206597715906,\n",
       "   -11.920732263797664,\n",
       "   -11.733601628823365,\n",
       "   -12.573885273116845,\n",
       "   -12.915733009133055,\n",
       "   -11.106906729009228,\n",
       "   -12.169921881669573,\n",
       "   -11.879445017106322,\n",
       "   -9.67831827350107,\n",
       "   -8.055783980192803,\n",
       "   -8.336997705839266,\n",
       "   -9.118452249196185,\n",
       "   -10.504868166430144,\n",
       "   -13.849006040536754,\n",
       "   -14.69728348867302,\n",
       "   -12.421081655797341,\n",
       "   -11.847730352707067,\n",
       "   -10.336789600341486,\n",
       "   -10.871199397759327,\n",
       "   -12.144491644638224,\n",
       "   -11.438321198500695,\n",
       "   -9.237393302966563,\n",
       "   -10.882470013162703,\n",
       "   -14.420554615550612,\n",
       "   -10.567982177503556,\n",
       "   -9.372994937179774,\n",
       "   -9.596058113138964,\n",
       "   -12.05534612066974,\n",
       "   -10.748790940275526,\n",
       "   -12.716214093549125,\n",
       "   -13.744503266301649,\n",
       "   -13.849820759652815,\n",
       "   -16.924447762611724,\n",
       "   -17.77345051046912,\n",
       "   -20.111190235398983,\n",
       "   -19.944412202173854,\n",
       "   -19.80481367712372,\n",
       "   -20.071651731065753,\n",
       "   -20.50650296232837,\n",
       "   -18.25630678703581,\n",
       "   -14.778524165087639,\n",
       "   -16.243276441156684,\n",
       "   -16.07303008360464,\n",
       "   -13.630585238778359,\n",
       "   -14.16620831925641,\n",
       "   -15.700084932175493,\n",
       "   -16.852182907435196,\n",
       "   -14.886730741487758,\n",
       "   -11.545349031804585,\n",
       "   -10.628709950685282,\n",
       "   -11.3001339529882,\n",
       "   -9.661347717302826,\n",
       "   -9.491044387389126,\n",
       "   -9.426923010780675,\n",
       "   -9.286599648019328,\n",
       "   -10.860290461673957,\n",
       "   -9.547117168443533,\n",
       "   -9.21727729227069,\n",
       "   -9.076120888795133,\n",
       "   -8.72289588829938,\n",
       "   -12.350969881065923,\n",
       "   -11.012843166108997,\n",
       "   -11.216371102416693,\n",
       "   -12.633240058019977,\n",
       "   -10.607578090168216,\n",
       "   -9.883890708385263,\n",
       "   -11.095773107769435,\n",
       "   -13.406642223935924,\n",
       "   -12.000659594549651,\n",
       "   -12.306445081288963,\n",
       "   -10.293422505862338,\n",
       "   -7.845581235842653,\n",
       "   -7.294544408200899,\n",
       "   -9.239066714518732,\n",
       "   -9.560842400731634,\n",
       "   -8.435730754423641,\n",
       "   -5.546069893795787,\n",
       "   -6.365359644096865,\n",
       "   -6.877058154375787,\n",
       "   -5.325942860490209,\n",
       "   -4.328149395455257,\n",
       "   -4.081824552734965,\n",
       "   -4.059668937548256,\n",
       "   -5.195494578376759,\n",
       "   -5.963320788805337,\n",
       "   -5.416872015380114,\n",
       "   -6.06049120380511,\n",
       "   -7.160818524421295,\n",
       "   -7.7920968669365,\n",
       "   -6.7470022609634706,\n",
       "   -9.70762219553663,\n",
       "   -9.509306268639872,\n",
       "   -12.259529436609567,\n",
       "   -12.354581326690282,\n",
       "   -11.088904263676604,\n",
       "   -10.01780884659148,\n",
       "   -7.610550200965362,\n",
       "   -7.606934525501842,\n",
       "   -6.855551144755427,\n",
       "   -6.715219075290348],\n",
       "  'jsc_r2': [-2252.69234953985,\n",
       "   -2559.6737506272093,\n",
       "   -1718.9929654826176,\n",
       "   -1711.1224137804838,\n",
       "   -1313.3939952126052,\n",
       "   -852.0451919470703,\n",
       "   -568.112273932589,\n",
       "   -320.93064326748197,\n",
       "   -130.93910904900991,\n",
       "   -41.956884231719656,\n",
       "   -3.4525994563485627,\n",
       "   -45.60941342971812,\n",
       "   -82.74605237018709,\n",
       "   -230.80996859093298,\n",
       "   -390.89270207056944,\n",
       "   -778.8841245357268,\n",
       "   -893.6140846510513,\n",
       "   -1565.8419622421259,\n",
       "   -1699.1079398662537,\n",
       "   -1647.9293197435832,\n",
       "   -1998.2711577191908,\n",
       "   -2548.057101837893,\n",
       "   -1981.2933346233767,\n",
       "   -1518.4680923550118,\n",
       "   -1992.972018409357,\n",
       "   -1305.8548480074123,\n",
       "   -850.197842380897,\n",
       "   -1335.8940310705589,\n",
       "   -1484.1156436870913,\n",
       "   -2227.053128395322,\n",
       "   -1952.0449481098435,\n",
       "   -3676.1886810927945,\n",
       "   -3578.115135570704,\n",
       "   -4875.6130214643035,\n",
       "   -7305.554899387248,\n",
       "   -6855.028701838715,\n",
       "   -2706.474769133977,\n",
       "   -3328.94283887057,\n",
       "   -2684.8293238932665,\n",
       "   -9645.973063241689,\n",
       "   -3560.5044084313545,\n",
       "   -3934.2555060506884,\n",
       "   -3452.709135745204,\n",
       "   -2291.8008597119347,\n",
       "   -1728.6484381981063,\n",
       "   -2239.4368985679744,\n",
       "   -3845.3832532488063,\n",
       "   -3084.383821714585,\n",
       "   -5783.692850730683,\n",
       "   -6252.772033183802,\n",
       "   -8536.570425111355,\n",
       "   -3976.621685921144,\n",
       "   -3446.6538645896535,\n",
       "   -5595.023722483391,\n",
       "   -4267.615607033532,\n",
       "   -3069.445098547558,\n",
       "   -1965.2631670835833,\n",
       "   -1963.8373995009595,\n",
       "   -961.8112621805448,\n",
       "   -1367.0090376819442,\n",
       "   -1364.8504359222447,\n",
       "   -1537.9677219227203,\n",
       "   -2057.3014011974874,\n",
       "   -1923.543376914645,\n",
       "   -2137.629888496202,\n",
       "   -1398.3048758714626,\n",
       "   -1460.9295279261157,\n",
       "   -2314.1541743357834,\n",
       "   -1434.0265825603178,\n",
       "   -1327.6814443499236,\n",
       "   -598.9235162196951,\n",
       "   -676.2150488576655,\n",
       "   -576.9998559001515,\n",
       "   -659.2117042502789,\n",
       "   -1241.210611188643,\n",
       "   -935.8352992115399,\n",
       "   -1392.7612489209675,\n",
       "   -1136.8971626055359,\n",
       "   -1561.7819882882968,\n",
       "   -1428.6924183070537,\n",
       "   -1430.1690282261047,\n",
       "   -1371.8218055232012,\n",
       "   -1331.2768321192343,\n",
       "   -1480.9851730499058,\n",
       "   -1308.141939711137,\n",
       "   -958.4739118878251,\n",
       "   -1281.2553040399484,\n",
       "   -1115.7838284523157,\n",
       "   -1038.5167497554735,\n",
       "   -1051.8981652274892,\n",
       "   -914.860233327329,\n",
       "   -824.2003482924041,\n",
       "   -588.8330050436285,\n",
       "   -620.67681519173,\n",
       "   -663.6257771258156,\n",
       "   -859.8882536266469,\n",
       "   -1061.226317537009,\n",
       "   -1037.2971014021873,\n",
       "   -750.6562649700704,\n",
       "   -771.9279076773537,\n",
       "   -650.9197751554216,\n",
       "   -849.5503572752561,\n",
       "   -1025.889112977161,\n",
       "   -1688.6481978303484,\n",
       "   -2565.2313200845238,\n",
       "   -3506.3689325683567,\n",
       "   -6203.748976191399,\n",
       "   -7036.474609926892,\n",
       "   -20874.005535854856,\n",
       "   -10708.4966799749,\n",
       "   -11536.001113842067,\n",
       "   -18334.417114472657,\n",
       "   -18059.330198293974,\n",
       "   -9317.513834915635,\n",
       "   -16239.501584742293,\n",
       "   -17172.846846398454,\n",
       "   -8641.427716541904,\n",
       "   -17136.95952636454,\n",
       "   -21585.457849445233,\n",
       "   -17135.327535601926,\n",
       "   -26215.25707425473,\n",
       "   -15352.409412487405,\n",
       "   -13154.769802479817,\n",
       "   -14467.740615801578,\n",
       "   -5248.5212323702235,\n",
       "   -6109.38173225895,\n",
       "   -3345.093220616954,\n",
       "   -3262.3373381464326,\n",
       "   -3146.431578477183,\n",
       "   -2901.5438795495666,\n",
       "   -2803.784509373476,\n",
       "   -2519.437638409515,\n",
       "   -1916.2055652335503,\n",
       "   -1924.4897761582006,\n",
       "   -1530.3502295046715,\n",
       "   -2575.464420137643,\n",
       "   -2860.840991112671,\n",
       "   -2523.051272528403,\n",
       "   -1555.2393159417322,\n",
       "   -2138.8210828220144,\n",
       "   -1518.957779827756,\n",
       "   -1250.0849906834037,\n",
       "   -1745.6411312541272,\n",
       "   -1656.132609342849,\n",
       "   -1538.1839515320019,\n",
       "   -1514.5587583782226,\n",
       "   -933.0328187722106,\n",
       "   -942.3122645559413,\n",
       "   -1008.3868178261534,\n",
       "   -1735.5509686766045,\n",
       "   -1029.69555497647,\n",
       "   -1555.357949067154,\n",
       "   -1431.244740971954,\n",
       "   -1351.0673866034738,\n",
       "   -1582.6965333202945,\n",
       "   -1644.5560831782755,\n",
       "   -1392.559067934709,\n",
       "   -2689.0017237243446,\n",
       "   -4073.6396531684986,\n",
       "   -8196.388453355046,\n",
       "   -6342.772982566034,\n",
       "   -4247.631534179612,\n",
       "   -6105.508518616442,\n",
       "   -6417.695610055453,\n",
       "   -4228.980088315892,\n",
       "   -3851.347810953217,\n",
       "   -3943.937805220262,\n",
       "   -3887.5216866849423,\n",
       "   -4634.204192059773,\n",
       "   -8510.992019885362,\n",
       "   -9005.858590132428,\n",
       "   -2344.464180747641,\n",
       "   -3972.4375996589874,\n",
       "   -4493.431035043151,\n",
       "   -3381.9045621480923,\n",
       "   -1961.3568806563562,\n",
       "   -952.7851257214924,\n",
       "   -914.5360126495657,\n",
       "   -819.9913100737074,\n",
       "   -701.5114021788205,\n",
       "   -966.7925721227497,\n",
       "   -797.5884634379311,\n",
       "   -817.6802759785867,\n",
       "   -596.3192006681629,\n",
       "   -556.8649699854994,\n",
       "   -440.4876587846358,\n",
       "   -580.1757315112469,\n",
       "   -523.0891128496559,\n",
       "   -604.6026432577303,\n",
       "   -797.158680783068,\n",
       "   -1096.5868294407753,\n",
       "   -1058.6686674555244,\n",
       "   -1228.3978326420797,\n",
       "   -1318.2605381584826,\n",
       "   -1864.3627831908912,\n",
       "   -2928.038007750346,\n",
       "   -4640.303583352798,\n",
       "   -1783.9593583467604,\n",
       "   -2994.550689716174,\n",
       "   -1736.5061057480266,\n",
       "   -5276.261734629993,\n",
       "   -1752.6721672687859,\n",
       "   -1055.2076233283765,\n",
       "   -1528.7251165212479,\n",
       "   -1466.9992816428396,\n",
       "   -957.8215592808644,\n",
       "   -574.5633307799613,\n",
       "   -555.3963331489545,\n",
       "   -567.5543497727033,\n",
       "   -663.2372056197707,\n",
       "   -723.7656732435366,\n",
       "   -612.6181329151086,\n",
       "   -427.93468220804453,\n",
       "   -498.8820805029692,\n",
       "   -970.0351111678389,\n",
       "   -693.4782302478857,\n",
       "   -719.1605419979663,\n",
       "   -789.7554647981909,\n",
       "   -899.6881481612817,\n",
       "   -853.9192281252374,\n",
       "   -636.1140735746109,\n",
       "   -822.9084497929463,\n",
       "   -646.5790855653694,\n",
       "   -669.231558115241,\n",
       "   -468.00225125525986,\n",
       "   -371.55011013042565,\n",
       "   -300.8305544922797,\n",
       "   -337.89030568127447,\n",
       "   -334.3096111834997,\n",
       "   -374.3724519191388,\n",
       "   -308.3523583260483,\n",
       "   -254.1627020088033,\n",
       "   -265.6046229552417,\n",
       "   -254.02329051483284,\n",
       "   -248.95813051991064,\n",
       "   -268.5916441913675,\n",
       "   -249.43548276265764,\n",
       "   -280.6947074860784,\n",
       "   -306.29378139057593,\n",
       "   -334.5087408707874,\n",
       "   -461.36137602746106,\n",
       "   -497.4789789660659,\n",
       "   -532.8628465525861,\n",
       "   -752.3876404519277,\n",
       "   -1045.435522754947,\n",
       "   -907.8317711563353,\n",
       "   -811.0014629247815,\n",
       "   -893.9330563770061,\n",
       "   -684.261326729958,\n",
       "   -668.5920566933999,\n",
       "   -732.7906847102547,\n",
       "   -777.5215731733592,\n",
       "   -629.5490367444996,\n",
       "   -881.158418486474,\n",
       "   -1152.0876976809013,\n",
       "   -1079.638500150509,\n",
       "   -1193.7299774217681,\n",
       "   -1119.7734033066265,\n",
       "   -2534.230500436392,\n",
       "   -1532.707111522022,\n",
       "   -1212.465541813425,\n",
       "   -1575.3631414547967,\n",
       "   -2435.1235610832327,\n",
       "   -1643.9899647141578,\n",
       "   -1094.7532307009617,\n",
       "   -892.8568497866361,\n",
       "   -824.2439603349311,\n",
       "   -686.5096778279703,\n",
       "   -634.6094944543754,\n",
       "   -673.5239720713553,\n",
       "   -886.9768345206415,\n",
       "   -533.4964578817589,\n",
       "   -559.7191614360848,\n",
       "   -486.2602986307187,\n",
       "   -463.39635475500273,\n",
       "   -530.3925926360828,\n",
       "   -616.7315489971321,\n",
       "   -634.1481480211721,\n",
       "   -869.4266718703423,\n",
       "   -1118.5776441505802,\n",
       "   -1023.3217271403021,\n",
       "   -929.7053705339703,\n",
       "   -1311.1675004742856,\n",
       "   -1300.2939722816757,\n",
       "   -988.9813970041064,\n",
       "   -887.6072561204548,\n",
       "   -1133.049793073378,\n",
       "   -843.0715396884892,\n",
       "   -681.7965806860498,\n",
       "   -706.5966253709377,\n",
       "   -476.5281734079985,\n",
       "   -616.5697884854167,\n",
       "   -437.57448360373957,\n",
       "   -402.90657890780847,\n",
       "   -375.49691252994614,\n",
       "   -382.5698176597274,\n",
       "   -306.45310984672733,\n",
       "   -277.69694105573114,\n",
       "   -380.9201514708534,\n",
       "   -304.6375768162394,\n",
       "   -291.35916793901976,\n",
       "   -324.4872853712599,\n",
       "   -346.4918797316615,\n",
       "   -402.5344776701564,\n",
       "   -406.69762146054643,\n",
       "   -497.7780006806837,\n",
       "   -614.9336878804378,\n",
       "   -400.69515818683345,\n",
       "   -427.4998171357465,\n",
       "   -413.70554177757543,\n",
       "   -322.19252441893644,\n",
       "   -323.5531389616444,\n",
       "   -301.71853929256423,\n",
       "   -354.9778911318337,\n",
       "   -293.7715210034106,\n",
       "   -354.83537352343325,\n",
       "   -264.17601602521785,\n",
       "   -245.50805752672977,\n",
       "   -310.7369556274866,\n",
       "   -354.07408372695033,\n",
       "   -395.49620715262415,\n",
       "   -584.245094467482,\n",
       "   -558.8324545973637,\n",
       "   -614.5916326053213,\n",
       "   -542.2199620243409,\n",
       "   -1017.9286124505404,\n",
       "   -2618.5907893432836,\n",
       "   -1750.2590380678885,\n",
       "   -1600.919665070327,\n",
       "   -1457.7257084080936,\n",
       "   -1560.314955968852,\n",
       "   -2009.8623007597387,\n",
       "   -1064.168586133425,\n",
       "   -663.5630203061731,\n",
       "   -1203.7274822604754,\n",
       "   -1069.4292118009982,\n",
       "   -1407.8635740371944,\n",
       "   -1413.046195746994,\n",
       "   -2473.2866065635294,\n",
       "   -3297.5256325511828,\n",
       "   -4975.449912971393,\n",
       "   -5171.6752594032,\n",
       "   -1806.765032195853,\n",
       "   -2104.490980321848,\n",
       "   -2926.579256339136,\n",
       "   -1612.5082844529627,\n",
       "   -4196.962445882965,\n",
       "   -8074.1852920029005,\n",
       "   -7231.61772543845,\n",
       "   -2378.999423654174,\n",
       "   -1223.8209427648121,\n",
       "   -1099.508691403332,\n",
       "   -714.2134263889964,\n",
       "   -833.9351446973474,\n",
       "   -1078.2575347571574,\n",
       "   -593.4540956837634,\n",
       "   -584.2685158866583,\n",
       "   -510.7031515995738,\n",
       "   -627.3357069174415,\n",
       "   -842.5433784002445,\n",
       "   -1118.9047814140922,\n",
       "   -803.959001923509,\n",
       "   -873.6734894804245,\n",
       "   -658.5128935179825,\n",
       "   -666.3969341297811,\n",
       "   -805.4013474216774,\n",
       "   -631.2261258918402,\n",
       "   -701.6142125599316,\n",
       "   -554.8182749002559,\n",
       "   -406.36863236188265,\n",
       "   -469.4232625397918,\n",
       "   -495.93458769143984,\n",
       "   -366.1436472478494,\n",
       "   -447.17257234242874,\n",
       "   -391.46362298486036,\n",
       "   -442.23886578001276,\n",
       "   -434.0994294726794,\n",
       "   -510.891927703028,\n",
       "   -484.76579422828456,\n",
       "   -482.04121141451435,\n",
       "   -610.2860716051375,\n",
       "   -534.0013965179746,\n",
       "   -643.0616952067712,\n",
       "   -1435.1838963174303,\n",
       "   -1547.702646240501,\n",
       "   -1304.5791373728025,\n",
       "   -2121.4284622869677,\n",
       "   -2205.808059927564,\n",
       "   -1692.594949547845,\n",
       "   -1523.2383785091665,\n",
       "   -1251.6694984474675,\n",
       "   -1590.8652884676214,\n",
       "   -1415.793036340581,\n",
       "   -1133.3311283181392,\n",
       "   -658.6246311073384,\n",
       "   -509.48215959651606,\n",
       "   -566.6169029346917,\n",
       "   -453.40816127870414,\n",
       "   -562.0080648809993,\n",
       "   -745.6291867175221,\n",
       "   -665.908453603217,\n",
       "   -544.9202952937788,\n",
       "   -503.55760303370187,\n",
       "   -542.154957909384,\n",
       "   -549.7297342963349,\n",
       "   -754.9749853104267,\n",
       "   -626.4294604540631,\n",
       "   -427.7796319651422,\n",
       "   -460.8986061014302,\n",
       "   -334.9785266532369,\n",
       "   -396.1981506596523,\n",
       "   -471.0897040205516,\n",
       "   -657.9099615715294,\n",
       "   -712.4250512660882,\n",
       "   -1023.9978662520491,\n",
       "   -1433.998835732492,\n",
       "   -1985.2139299654714,\n",
       "   -1033.4736595071515,\n",
       "   -916.575910897053,\n",
       "   -647.1117364104007,\n",
       "   -864.4978949113525,\n",
       "   -662.8842677126859,\n",
       "   -1124.7370790076054,\n",
       "   -951.0520110580769,\n",
       "   -776.6095118633662,\n",
       "   -887.3477680791275,\n",
       "   -700.5192171233925,\n",
       "   -850.2725575999889,\n",
       "   -1032.8842825846716,\n",
       "   -1002.0938964375077,\n",
       "   -1222.4544370707156,\n",
       "   -1152.235825380557,\n",
       "   -1364.5625224070036,\n",
       "   -1752.904610075756,\n",
       "   -1364.1950520000025,\n",
       "   -930.8274394767415,\n",
       "   -632.5891649241004,\n",
       "   -981.9866134258913,\n",
       "   -949.0559527838736,\n",
       "   -1241.4531223036097,\n",
       "   -1168.4344478418675,\n",
       "   -978.6725259944028,\n",
       "   -1348.7268943619074,\n",
       "   -2164.8408598208443,\n",
       "   -2058.8120663476416,\n",
       "   -2194.0677871286425,\n",
       "   -1767.7416420990896,\n",
       "   -1448.3253753439983,\n",
       "   -1678.7885442583377,\n",
       "   -1375.9443753041937,\n",
       "   -1088.5582309943982,\n",
       "   -958.4900627546848,\n",
       "   -755.6293010823346,\n",
       "   -714.1758717108903,\n",
       "   -772.2060251627531,\n",
       "   -820.7417861253301,\n",
       "   -512.1333764208007,\n",
       "   -371.8905366767152,\n",
       "   -449.0311755700173,\n",
       "   -412.80850443934565,\n",
       "   -488.0663238917829,\n",
       "   -501.97585879821526,\n",
       "   -560.7460003442259,\n",
       "   -499.0505415000745,\n",
       "   -624.8820815369146,\n",
       "   -680.3176335429877,\n",
       "   -631.0893608628605,\n",
       "   -762.0133077957845,\n",
       "   -1323.9161179222392,\n",
       "   -1208.3113842951227,\n",
       "   -1249.4474908909265,\n",
       "   -973.2647644947319,\n",
       "   -747.4034429280781,\n",
       "   -671.2701442445837,\n",
       "   -759.5495648027838,\n",
       "   -908.4668345113427,\n",
       "   -1092.9628889499406,\n",
       "   -894.1200402784804,\n",
       "   -756.4973344272964,\n",
       "   -829.7203445828814,\n",
       "   -789.7591593533135,\n",
       "   -755.0880071153376,\n",
       "   -1025.3745277328524,\n",
       "   -1078.6102381922512,\n",
       "   -879.4758458679606,\n",
       "   -724.200432954232,\n",
       "   -487.951084958352,\n",
       "   -490.4802026853823,\n",
       "   -408.9047415012414,\n",
       "   -390.41814120673064,\n",
       "   -421.87199198223584,\n",
       "   -341.38542718054543,\n",
       "   -273.4799714081425,\n",
       "   -328.13814001675036,\n",
       "   -343.99173676752855,\n",
       "   -540.6591424351377,\n",
       "   -499.4621145490354,\n",
       "   -549.9533329371754,\n",
       "   -587.7334516820874,\n",
       "   -881.5034990956977],\n",
       "  'ff_r2': [-255.45088593989146,\n",
       "   -114.50877283459775,\n",
       "   -63.34794885278839,\n",
       "   -33.29577725866754,\n",
       "   -12.771469091100347,\n",
       "   -3.6843258421629814,\n",
       "   -1.1143953313002957,\n",
       "   0.047475070375238015,\n",
       "   0.043543713685343044,\n",
       "   0.0033843174931460496,\n",
       "   -0.06297115663783526,\n",
       "   -0.33249571384820675,\n",
       "   -0.9575193774356727,\n",
       "   -0.9805562951605535,\n",
       "   -1.1516012408207548,\n",
       "   -1.1582744385193005,\n",
       "   -1.4259872309474169,\n",
       "   -1.5807846721096328,\n",
       "   -1.8069435341828632,\n",
       "   -2.4549368597433663,\n",
       "   -2.55148241926832,\n",
       "   -3.231324296955642,\n",
       "   -3.9083449255818428,\n",
       "   -3.9603205286152035,\n",
       "   -4.02364261487805,\n",
       "   -3.937803091431274,\n",
       "   -4.172027697568819,\n",
       "   -4.751991753623989,\n",
       "   -4.287060640787175,\n",
       "   -4.116236685091432,\n",
       "   -4.165714707012484,\n",
       "   -4.54101619871785,\n",
       "   -4.387543511088117,\n",
       "   -4.6948837222947155,\n",
       "   -3.9289685391802704,\n",
       "   -4.137236142541025,\n",
       "   -4.666457506946861,\n",
       "   -5.197068986120185,\n",
       "   -5.205737966210606,\n",
       "   -5.847740042820789,\n",
       "   -5.769824914458941,\n",
       "   -6.038340255693956,\n",
       "   -6.154879612283571,\n",
       "   -6.313276431474103,\n",
       "   -5.772968571642407,\n",
       "   -6.073567876396496,\n",
       "   -6.35217261389633,\n",
       "   -6.192194233722097,\n",
       "   -5.442098999569353,\n",
       "   -6.169453147736964,\n",
       "   -5.911956098943286,\n",
       "   -6.361316970856139,\n",
       "   -7.437836518954642,\n",
       "   -8.250999574225546,\n",
       "   -7.484658499068317,\n",
       "   -8.152547702971168,\n",
       "   -8.920698969461014,\n",
       "   -6.857788697067061,\n",
       "   -6.641170296693074,\n",
       "   -6.414417959190549,\n",
       "   -6.124613837533244,\n",
       "   -6.203220232053734,\n",
       "   -5.335828367234975,\n",
       "   -5.659561014401561,\n",
       "   -4.860876790754116,\n",
       "   -4.630763589642753,\n",
       "   -4.531356653388728,\n",
       "   -3.9332815047764624,\n",
       "   -4.111912994368755,\n",
       "   -3.7109579717310206,\n",
       "   -3.9378650040273957,\n",
       "   -3.738804964385623,\n",
       "   -4.348152032671068,\n",
       "   -5.38651011956636,\n",
       "   -5.772856260224614,\n",
       "   -5.369617746351092,\n",
       "   -4.236644554741558,\n",
       "   -4.1571664424496415,\n",
       "   -3.244267236837744,\n",
       "   -4.0215721291197,\n",
       "   -4.21293184223817,\n",
       "   -3.767320195796386,\n",
       "   -4.062337886957109,\n",
       "   -3.611962955686262,\n",
       "   -4.1599816081921785,\n",
       "   -4.257827341711653,\n",
       "   -4.268306149137122,\n",
       "   -3.9758838584570144,\n",
       "   -4.192531367034466,\n",
       "   -4.878750394059172,\n",
       "   -4.119345449193944,\n",
       "   -3.8698292864139407,\n",
       "   -4.292979527998647,\n",
       "   -4.133826874802735,\n",
       "   -3.8930340349817962,\n",
       "   -3.227867741185686,\n",
       "   -3.170070369545029,\n",
       "   -3.3354620294439234,\n",
       "   -3.86695641080761,\n",
       "   -3.9008620883062637,\n",
       "   -3.7663588964830135,\n",
       "   -3.7110139717784607,\n",
       "   -3.672017720214394,\n",
       "   -3.4175243599711234,\n",
       "   -3.152466911304285,\n",
       "   -3.446345590465169,\n",
       "   -3.164888052592712,\n",
       "   -3.27149533846925,\n",
       "   -3.233010607582548,\n",
       "   -3.2133223253781633,\n",
       "   -3.543081168738518,\n",
       "   -4.1325697610430785,\n",
       "   -4.224001238922568,\n",
       "   -4.3624770932262855,\n",
       "   -4.12202499553467,\n",
       "   -3.9252512224058442,\n",
       "   -4.152763001941169,\n",
       "   -4.506464297609981,\n",
       "   -4.357950106977879,\n",
       "   -4.8747752285555865,\n",
       "   -4.722564838243243,\n",
       "   -4.672256986721558,\n",
       "   -4.913117018803103,\n",
       "   -4.139272890007571,\n",
       "   -3.723099955227603,\n",
       "   -3.8065870381678106,\n",
       "   -4.005351592456781,\n",
       "   -4.871528935203838,\n",
       "   -4.516876791747292,\n",
       "   -4.528461592962627,\n",
       "   -4.449491330275519,\n",
       "   -4.885848421201512,\n",
       "   -4.921812060229029,\n",
       "   -5.293363890167884,\n",
       "   -6.230387875614401,\n",
       "   -5.566990140317133,\n",
       "   -6.1147930546347045,\n",
       "   -6.324447982352337,\n",
       "   -6.698967589266267,\n",
       "   -6.035214207145075,\n",
       "   -6.664760094873714,\n",
       "   -6.050942747795655,\n",
       "   -6.723400269710055,\n",
       "   -7.731307847140034,\n",
       "   -5.885431777967082,\n",
       "   -6.090951315813018,\n",
       "   -6.402313642260598,\n",
       "   -7.268889342652043,\n",
       "   -7.20019239102076,\n",
       "   -6.502801761919963,\n",
       "   -6.232308995475991,\n",
       "   -5.833447187533504,\n",
       "   -5.016190466158233,\n",
       "   -5.430993548298251,\n",
       "   -5.838081637914967,\n",
       "   -5.963601860198777,\n",
       "   -5.3331167637794525,\n",
       "   -6.382289401488467,\n",
       "   -6.678927358179309,\n",
       "   -6.118558191902595,\n",
       "   -5.496145931777764,\n",
       "   -5.515415482316544,\n",
       "   -5.072596499964753,\n",
       "   -4.581724651782966,\n",
       "   -5.170424643161043,\n",
       "   -4.764154183231463,\n",
       "   -4.249115306360072,\n",
       "   -3.8704533192666677,\n",
       "   -4.0722318820659575,\n",
       "   -3.842305054649117,\n",
       "   -4.038623917013411,\n",
       "   -4.123159122154077,\n",
       "   -4.83366617094748,\n",
       "   -4.515508799067731,\n",
       "   -5.304151441044301,\n",
       "   -4.832553838840443,\n",
       "   -4.981127964396244,\n",
       "   -4.102201294211596,\n",
       "   -3.562224973127388,\n",
       "   -3.77138884250078,\n",
       "   -3.523437517778725,\n",
       "   -3.8304509181608886,\n",
       "   -3.7404074031339505,\n",
       "   -3.954965006715417,\n",
       "   -3.74503951211108,\n",
       "   -3.7532994681453085,\n",
       "   -3.8422832690091555,\n",
       "   -3.576855287650142,\n",
       "   -3.3986919703945118,\n",
       "   -3.5695953584709583,\n",
       "   -4.309471254227833,\n",
       "   -4.467597516067501,\n",
       "   -4.558022190231828,\n",
       "   -4.751195936133918,\n",
       "   -5.26140054162593,\n",
       "   -6.488078261839073,\n",
       "   -7.061868852475831,\n",
       "   -7.304304341764544,\n",
       "   -6.922434253181272,\n",
       "   -8.187675833976966,\n",
       "   -7.768286987571178,\n",
       "   -8.08565596130976,\n",
       "   -6.24395614809517,\n",
       "   -7.448687305893614,\n",
       "   -7.0979558305767245,\n",
       "   -6.644410888160907,\n",
       "   -6.291354874675423,\n",
       "   -6.156177162580739,\n",
       "   -5.88439926831283,\n",
       "   -5.1576239593167195,\n",
       "   -6.133276662120991,\n",
       "   -6.089298356798265,\n",
       "   -5.626977610267706,\n",
       "   -6.783721232574529,\n",
       "   -6.7278657021944825,\n",
       "   -7.891024137337935,\n",
       "   -6.677856984661178,\n",
       "   -5.501598510855118,\n",
       "   -4.637781593398128,\n",
       "   -5.698891124096144,\n",
       "   -5.749542427731557,\n",
       "   -5.488234969197784,\n",
       "   -5.451360751156482,\n",
       "   -5.244958266780677,\n",
       "   -5.217503187268297,\n",
       "   -5.24571196521389,\n",
       "   -5.970371499391587,\n",
       "   -5.975945311929003,\n",
       "   -6.05299871170517,\n",
       "   -5.97553189503383,\n",
       "   -4.971686357293205,\n",
       "   -4.197271396498606,\n",
       "   -3.6708845837799373,\n",
       "   -3.688897299640863,\n",
       "   -4.462445570300354,\n",
       "   -4.174920901503353,\n",
       "   -4.514810807046158,\n",
       "   -4.216270772529137,\n",
       "   -4.233641441084301,\n",
       "   -4.048582906693906,\n",
       "   -4.547929217963108,\n",
       "   -4.349518026594428,\n",
       "   -3.9040460994888706,\n",
       "   -3.431368285310863,\n",
       "   -3.66441832700633,\n",
       "   -4.213468298895224,\n",
       "   -4.7695337961403625,\n",
       "   -5.154189369514557,\n",
       "   -4.804430957866629,\n",
       "   -5.093484389025545,\n",
       "   -4.516295130211235,\n",
       "   -4.0873231109300106,\n",
       "   -4.738243527876214,\n",
       "   -6.4439220780738395,\n",
       "   -6.2872480180448065,\n",
       "   -7.463523068160246,\n",
       "   -9.087799759117773,\n",
       "   -10.141001143167609,\n",
       "   -9.906149628528198,\n",
       "   -8.618766078408168,\n",
       "   -8.499663233371917,\n",
       "   -9.405822750536885,\n",
       "   -8.211157143987437,\n",
       "   -8.18091355886473,\n",
       "   -8.046292655651191,\n",
       "   -8.302145145746445,\n",
       "   -8.281091236023626,\n",
       "   -8.223365674569514,\n",
       "   -8.064407746847316,\n",
       "   -8.284000530108209,\n",
       "   -6.779159120025978,\n",
       "   -7.491473303864526,\n",
       "   -8.329796423227311,\n",
       "   -8.075089397164906,\n",
       "   -7.8369053924716034,\n",
       "   -7.026872488009461,\n",
       "   -6.879373839088533,\n",
       "   -7.26526208481175,\n",
       "   -6.404239270377442,\n",
       "   -6.179453815129548,\n",
       "   -6.130106060813209,\n",
       "   -5.987596848106339,\n",
       "   -6.176614666760558,\n",
       "   -5.915949493181102,\n",
       "   -5.8873756516721345,\n",
       "   -6.2479463259810135,\n",
       "   -5.522538538728544,\n",
       "   -6.815759788965828,\n",
       "   -6.339355437163804,\n",
       "   -6.072035006914883,\n",
       "   -5.585794108633382,\n",
       "   -5.100427695513413,\n",
       "   -6.476189386137122,\n",
       "   -5.3353278342225146,\n",
       "   -5.740079371872889,\n",
       "   -5.902103537268255,\n",
       "   -6.330896701404315,\n",
       "   -5.92841039092608,\n",
       "   -5.920116315160587,\n",
       "   -6.65192768211583,\n",
       "   -6.356107748394443,\n",
       "   -6.636835038280077,\n",
       "   -7.272453703239085,\n",
       "   -6.913186983815109,\n",
       "   -5.808883292674382,\n",
       "   -6.13673441730777,\n",
       "   -6.206991099044891,\n",
       "   -6.210718676854338,\n",
       "   -6.5081351352571915,\n",
       "   -6.605754928671605,\n",
       "   -6.792875317526715,\n",
       "   -6.6363033827597455,\n",
       "   -6.018570274573061,\n",
       "   -7.2359750146185675,\n",
       "   -7.919532140647036,\n",
       "   -8.27951469550705,\n",
       "   -8.104152872518748,\n",
       "   -7.313433936631517,\n",
       "   -6.085383175603531,\n",
       "   -5.9355291145729145,\n",
       "   -5.735888196120108,\n",
       "   -6.23318766337692,\n",
       "   -5.739323093923852,\n",
       "   -4.810954591602735,\n",
       "   -5.855889575676439,\n",
       "   -6.887059611065068,\n",
       "   -5.5332086130933185,\n",
       "   -4.926578880098149,\n",
       "   -5.149698167574608,\n",
       "   -4.504981949951884,\n",
       "   -4.950757741479605,\n",
       "   -5.44902693932445,\n",
       "   -5.211677721407333,\n",
       "   -5.394256407279216,\n",
       "   -6.068672160786117,\n",
       "   -5.713055624567716,\n",
       "   -4.699285938887667,\n",
       "   -5.899960193611037,\n",
       "   -6.707062272257928,\n",
       "   -6.2598629257719,\n",
       "   -6.4843843521135485,\n",
       "   -7.161660721398944,\n",
       "   -6.634168386033979,\n",
       "   -5.101846067370036,\n",
       "   -5.947527776702167,\n",
       "   -6.085590952835799,\n",
       "   -5.446660827277806,\n",
       "   -6.058348341656363,\n",
       "   -4.9933906737701115,\n",
       "   -4.875623978495042,\n",
       "   -4.46600639643323,\n",
       "   -4.423866449662763,\n",
       "   -4.828450919604769,\n",
       "   -4.6291942547864595,\n",
       "   -5.631931609363689,\n",
       "   -5.997969164615543,\n",
       "   -5.108529973502008,\n",
       "   -4.995201440309782,\n",
       "   -5.178374017373972,\n",
       "   -5.6053969358881925,\n",
       "   -5.4599394793051435,\n",
       "   -4.931308282573531,\n",
       "   -5.56243111481237,\n",
       "   -5.0918779972143335,\n",
       "   -5.12695991608223,\n",
       "   -5.796622951483738,\n",
       "   -5.213911464030015,\n",
       "   -4.5926152154401345,\n",
       "   -4.502619860935988,\n",
       "   -4.868211697522428,\n",
       "   -5.116324446324948,\n",
       "   -4.7003866231468505,\n",
       "   -4.5454668573252786,\n",
       "   -4.841229655301207,\n",
       "   -4.744301663088547,\n",
       "   -5.506017013379508,\n",
       "   -7.006140738320315,\n",
       "   -6.819847324292772,\n",
       "   -6.425858560799335,\n",
       "   -5.626392963030785,\n",
       "   -5.997229827086436,\n",
       "   -5.0799176782889734,\n",
       "   -4.634053241819755,\n",
       "   -5.078515186659347,\n",
       "   -5.569670049836659,\n",
       "   -6.366140018803254,\n",
       "   -7.700192661903678,\n",
       "   -7.1349616362207655,\n",
       "   -6.368748681968452,\n",
       "   -6.921877517986397,\n",
       "   -6.63094670829607,\n",
       "   -6.725888281901262,\n",
       "   -7.45973887764082,\n",
       "   -7.58277699905363,\n",
       "   -8.621437554673914,\n",
       "   -8.451162118409115,\n",
       "   -8.238905283745657,\n",
       "   -6.69993263265839,\n",
       "   -7.570452382963495,\n",
       "   -6.742464934871137,\n",
       "   -6.535942332851015,\n",
       "   -7.3150362398918585,\n",
       "   -7.3222074428374135,\n",
       "   -7.391722024065103,\n",
       "   -7.826819582353343,\n",
       "   -8.162617712346481,\n",
       "   -9.494677662559928,\n",
       "   -8.81405470522597,\n",
       "   -7.933546973757544,\n",
       "   -7.628117957327337,\n",
       "   -8.301652351385727,\n",
       "   -7.740161843054752,\n",
       "   -7.497467177361655,\n",
       "   -7.758897671522734,\n",
       "   -8.369359895633375,\n",
       "   -8.151846533377709,\n",
       "   -8.693233035525642,\n",
       "   -9.42097844792978,\n",
       "   -9.494979901044049,\n",
       "   -9.898310981968246,\n",
       "   -9.540172426201607,\n",
       "   -10.576194287729052,\n",
       "   -11.069397090497644,\n",
       "   -10.916872905166027,\n",
       "   -11.386209072542908,\n",
       "   -10.216861774715825,\n",
       "   -10.183413409475161,\n",
       "   -10.282479142335067,\n",
       "   -10.335736053330507,\n",
       "   -9.153096994618421,\n",
       "   -7.895221845082293,\n",
       "   -8.267292075725749,\n",
       "   -6.578962076783423,\n",
       "   -6.5771893375096075,\n",
       "   -6.027214942480689,\n",
       "   -5.682615325498873,\n",
       "   -5.8351604524226595,\n",
       "   -5.701154650628883,\n",
       "   -5.2593713150671855,\n",
       "   -6.485938325619101,\n",
       "   -5.844721283085351,\n",
       "   -6.564077425727988,\n",
       "   -6.4667113968316725,\n",
       "   -5.612513594149072,\n",
       "   -5.357342793985461,\n",
       "   -6.038765632877236,\n",
       "   -6.574645497011579,\n",
       "   -5.903905937667792,\n",
       "   -6.576376451713643,\n",
       "   -6.102418558793622,\n",
       "   -6.203676126458971,\n",
       "   -6.052982887023363,\n",
       "   -5.286622235967307,\n",
       "   -5.614191757043462,\n",
       "   -5.0105578346864394,\n",
       "   -5.712106675612485,\n",
       "   -5.803993565708952,\n",
       "   -6.149203339073953,\n",
       "   -7.783452975383957,\n",
       "   -8.758624924028087,\n",
       "   -6.4342045766031974,\n",
       "   -6.368028508015917,\n",
       "   -6.008631809715327,\n",
       "   -6.177789358175766,\n",
       "   -6.237182728056298,\n",
       "   -6.168766694787068,\n",
       "   -6.223791750378702,\n",
       "   -7.429827028332523,\n",
       "   -8.88262307766896,\n",
       "   -8.410207492028265,\n",
       "   -8.74399148603687,\n",
       "   -8.995973304053868,\n",
       "   -9.842310756312328,\n",
       "   -8.141809990740237,\n",
       "   -7.4098669388179825,\n",
       "   -7.217811535042511,\n",
       "   -7.014040957111639,\n",
       "   -5.95783355617728,\n",
       "   -6.0591914740664246,\n",
       "   -6.335579737145213,\n",
       "   -6.864155087601978,\n",
       "   -7.204953928911783,\n",
       "   -6.918571150913892,\n",
       "   -7.063348301677815,\n",
       "   -7.5061320272582375,\n",
       "   -7.447581797877264,\n",
       "   -9.345435712219441,\n",
       "   -9.915232279296022,\n",
       "   -10.026272843812194,\n",
       "   -9.180062830728213,\n",
       "   -6.849747613713171,\n",
       "   -5.794579549926919,\n",
       "   -6.278953982097171,\n",
       "   -6.502466807739706,\n",
       "   -5.8564968255661185,\n",
       "   -6.365712058606997,\n",
       "   -5.478146508354098,\n",
       "   -5.741685334781116,\n",
       "   -5.85242710354362,\n",
       "   -5.945681428997153],\n",
       "  'test_r2s': [-4793.480380831827,\n",
       "   -3837.7144294581317,\n",
       "   -2796.966183895709,\n",
       "   -2407.454239135952,\n",
       "   -1878.1743700681272,\n",
       "   -1343.8235389183503,\n",
       "   -1036.950215846403,\n",
       "   -717.6561380489684,\n",
       "   -532.6006013732073,\n",
       "   -387.45791195142857,\n",
       "   -314.1258626472397,\n",
       "   -339.38535853835634,\n",
       "   -359.0680267853877,\n",
       "   -502.4476184504965,\n",
       "   -622.1255136404027,\n",
       "   -1003.5551637175136,\n",
       "   -1104.0291339630762,\n",
       "   -1773.1953737826411,\n",
       "   -1892.3869870066221,\n",
       "   -1815.0532290476308,\n",
       "   -2128.924655200565,\n",
       "   -2661.9248343981662,\n",
       "   -2082.0371902994007,\n",
       "   -1606.6308690276355,\n",
       "   -2076.7638282792987,\n",
       "   -1380.604759031211,\n",
       "   -923.6623986923787,\n",
       "   -1398.088818911067,\n",
       "   -1545.5061398547252,\n",
       "   -2295.3756594062706,\n",
       "   -2023.9668945955955,\n",
       "   -3748.4978556430415,\n",
       "   -3642.18861981394,\n",
       "   -4930.671628041573,\n",
       "   -7360.75978027135,\n",
       "   -6908.683691442892,\n",
       "   -2759.036644984355,\n",
       "   -3374.6736862561115,\n",
       "   -2732.736613849846,\n",
       "   -9693.742050530243,\n",
       "   -3611.997807104219,\n",
       "   -3982.5989651247883,\n",
       "   -3500.5208863722182,\n",
       "   -2334.6305371315325,\n",
       "   -1773.8307268974536,\n",
       "   -2285.666727434466,\n",
       "   -3894.3230426318837,\n",
       "   -3132.0820920609526,\n",
       "   -5831.170450061198,\n",
       "   -6311.944252302299,\n",
       "   -8588.639834986294,\n",
       "   -4023.188341978158,\n",
       "   -3493.997838261873,\n",
       "   -5647.540834697468,\n",
       "   -4318.465845034303,\n",
       "   -3119.782349098368,\n",
       "   -2023.0890713301526,\n",
       "   -2016.8154710112512,\n",
       "   -1016.2540848195404,\n",
       "   -1415.6267166896682,\n",
       "   -1411.2708654582812,\n",
       "   -1574.6832313472203,\n",
       "   -2090.0769257007655,\n",
       "   -1961.9716129956314,\n",
       "   -2174.0979003084967,\n",
       "   -1431.136219389883,\n",
       "   -1496.8841729718413,\n",
       "   -2355.2404120967267,\n",
       "   -1478.6756406414572,\n",
       "   -1370.4467463512512,\n",
       "   -636.9009973137631,\n",
       "   -716.4960226486652,\n",
       "   -618.5556956477255,\n",
       "   -701.713059835242,\n",
       "   -1280.9622893258756,\n",
       "   -973.8612544289482,\n",
       "   -1432.266043788033,\n",
       "   -1172.7170788828855,\n",
       "   -1607.1220142138536,\n",
       "   -1476.260502422889,\n",
       "   -1475.5539966929143,\n",
       "   -1414.2540494336756,\n",
       "   -1371.5050212825815,\n",
       "   -1522.798351029577,\n",
       "   -1350.9623384717738,\n",
       "   -996.042044425265,\n",
       "   -1314.438098770877,\n",
       "   -1151.096860649162,\n",
       "   -1077.1388543031092,\n",
       "   -1093.0196860674132,\n",
       "   -966.4672097190657,\n",
       "   -865.1063608034957,\n",
       "   -629.5185108196063,\n",
       "   -662.6465110250617,\n",
       "   -713.4292143002557,\n",
       "   -908.5260493576238,\n",
       "   -1109.8698648688944,\n",
       "   -1082.941321900299,\n",
       "   -797.490946548815,\n",
       "   -809.2777959692032,\n",
       "   -685.2764914231791,\n",
       "   -881.5350140941312,\n",
       "   -1059.1219476898834,\n",
       "   -1721.3178501368538,\n",
       "   -2599.95597501126,\n",
       "   -3546.41701036519,\n",
       "   -6242.630067884528,\n",
       "   -7071.6327900499455,\n",
       "   -20914.571083583978,\n",
       "   -10749.919298919338,\n",
       "   -11585.412103716206,\n",
       "   -18378.778463881717,\n",
       "   -18102.431125947325,\n",
       "   -9360.266156301213,\n",
       "   -16280.638828530338,\n",
       "   -17211.5809204733,\n",
       "   -8679.867143562618,\n",
       "   -17178.864371561787,\n",
       "   -21631.5466824085,\n",
       "   -17187.662536036496,\n",
       "   -26271.195650622896,\n",
       "   -15405.298634997376,\n",
       "   -13213.178187624588,\n",
       "   -14528.26799628342,\n",
       "   -5313.604206201626,\n",
       "   -6165.831366652854,\n",
       "   -3397.0553660721775,\n",
       "   -3316.0887594455357,\n",
       "   -3190.447710193585,\n",
       "   -2945.9448990004926,\n",
       "   -2849.8103728351134,\n",
       "   -2565.0310543253463,\n",
       "   -1965.6998732020131,\n",
       "   -1969.1990119486798,\n",
       "   -1572.78612041199,\n",
       "   -2622.916871921928,\n",
       "   -2906.0679762123605,\n",
       "   -2569.354438565077,\n",
       "   -1601.0510473378672,\n",
       "   -2183.362550817056,\n",
       "   -1562.0689488117146,\n",
       "   -1288.3805586358812,\n",
       "   -1795.5098650242044,\n",
       "   -1700.3534945844867,\n",
       "   -1577.3672290035895,\n",
       "   -1554.391103720416,\n",
       "   -966.1498718250816,\n",
       "   -978.9866524978655,\n",
       "   -1047.8833942112378,\n",
       "   -1771.4933912540541,\n",
       "   -1063.2353422854642,\n",
       "   -1589.897922431461,\n",
       "   -1464.9339667483403,\n",
       "   -1382.6851493935474,\n",
       "   -1616.050982355951,\n",
       "   -1676.9201896704712,\n",
       "   -1424.855538577051,\n",
       "   -2720.1945635334396,\n",
       "   -4105.204475539792,\n",
       "   -8228.18108369478,\n",
       "   -6373.968571621615,\n",
       "   -4279.0671229098725,\n",
       "   -6136.377275300642,\n",
       "   -6449.738140091768,\n",
       "   -4260.705020332741,\n",
       "   -3886.5300319608828,\n",
       "   -3978.0463847690276,\n",
       "   -3919.6899259806473,\n",
       "   -4670.571692267554,\n",
       "   -8552.201487399323,\n",
       "   -9045.706583581285,\n",
       "   -2378.5654870582084,\n",
       "   -4004.7292240892257,\n",
       "   -4528.814040754548,\n",
       "   -3418.1631907500673,\n",
       "   -2000.061634439996,\n",
       "   -995.4511400760638,\n",
       "   -958.1751016756483,\n",
       "   -861.7897134484111,\n",
       "   -738.2414809786939,\n",
       "   -1003.4806501621,\n",
       "   -836.5589849947527,\n",
       "   -856.629643374442,\n",
       "   -634.0281356842366,\n",
       "   -597.0970700448327,\n",
       "   -482.9608987971088,\n",
       "   -625.434034109467,\n",
       "   -566.765274806033,\n",
       "   -646.8998924866886,\n",
       "   -843.151909218161,\n",
       "   -1136.7777739211047,\n",
       "   -1096.8335477027163,\n",
       "   -1264.6194556238245,\n",
       "   -1355.1751086187485,\n",
       "   -1900.9497049773884,\n",
       "   -2968.930230956889,\n",
       "   -4683.259750443136,\n",
       "   -1829.7312596068614,\n",
       "   -3041.526742199054,\n",
       "   -1785.5885558011078,\n",
       "   -5321.797902246515,\n",
       "   -1797.4384901220076,\n",
       "   -1098.9185726626486,\n",
       "   -1574.1306646691419,\n",
       "   -1514.521124378686,\n",
       "   -1007.0661476931551,\n",
       "   -623.5801282597334,\n",
       "   -605.1968621936558,\n",
       "   -618.2930431294175,\n",
       "   -713.9027467185105,\n",
       "   -778.0667267663218,\n",
       "   -661.6722152396999,\n",
       "   -480.6286004057601,\n",
       "   -553.8522333875904,\n",
       "   -1022.2926694479276,\n",
       "   -749.8954544303823,\n",
       "   -777.2790375256599,\n",
       "   -841.8135080243919,\n",
       "   -954.4614856912847,\n",
       "   -911.4577505819254,\n",
       "   -692.8728148991349,\n",
       "   -880.4599440486037,\n",
       "   -697.0744013772546,\n",
       "   -720.7188701001021,\n",
       "   -529.5672404823553,\n",
       "   -432.3124452997853,\n",
       "   -359.4680771367114,\n",
       "   -394.9674666792705,\n",
       "   -384.9786472675758,\n",
       "   -424.5179443029085,\n",
       "   -361.40803119177457,\n",
       "   -310.2396657184053,\n",
       "   -321.1781280458827,\n",
       "   -315.41495611718244,\n",
       "   -313.7483328011816,\n",
       "   -336.11977910746776,\n",
       "   -327.91053868010516,\n",
       "   -359.13381187546963,\n",
       "   -380.7579831918726,\n",
       "   -423.66694333193374,\n",
       "   -545.5721178058346,\n",
       "   -578.2389890340396,\n",
       "   -618.3783684921883,\n",
       "   -837.6381420176905,\n",
       "   -1125.711003833698,\n",
       "   -974.7753565903275,\n",
       "   -886.6234594592707,\n",
       "   -960.404898945292,\n",
       "   -749.6143681338968,\n",
       "   -743.2754572796711,\n",
       "   -813.721988451314,\n",
       "   -862.5985200201178,\n",
       "   -713.0564353584589,\n",
       "   -966.2468720133422,\n",
       "   -1245.46404284744,\n",
       "   -1176.7711738793707,\n",
       "   -1306.4025678959765,\n",
       "   -1238.5102210945502,\n",
       "   -2655.320886438961,\n",
       "   -1650.8080283368154,\n",
       "   -1332.7039885901154,\n",
       "   -1698.5622459945932,\n",
       "   -2580.1865218976313,\n",
       "   -1800.686807072789,\n",
       "   -1266.1004391265033,\n",
       "   -1072.4220598435413,\n",
       "   -1015.9540683824765,\n",
       "   -859.2913391652772,\n",
       "   -819.1924749840139,\n",
       "   -844.3589832425978,\n",
       "   -1039.6681758096427,\n",
       "   -672.2841977895974,\n",
       "   -670.2746192293282,\n",
       "   -606.1785382141013,\n",
       "   -570.1470104271109,\n",
       "   -629.4407452482941,\n",
       "   -725.7379528408796,\n",
       "   -749.3382506724979,\n",
       "   -986.1326760077387,\n",
       "   -1233.403787827277,\n",
       "   -1134.954576230969,\n",
       "   -1046.354054394118,\n",
       "   -1425.233458762591,\n",
       "   -1415.7424075494703,\n",
       "   -1101.2867257550358,\n",
       "   -991.5725243188582,\n",
       "   -1245.7728857855116,\n",
       "   -946.1336320737431,\n",
       "   -788.8814580193642,\n",
       "   -820.7813022153707,\n",
       "   -588.0789924169222,\n",
       "   -723.2053974521558,\n",
       "   -551.6513829776616,\n",
       "   -509.39150294458005,\n",
       "   -482.27213407816055,\n",
       "   -481.93302094283604,\n",
       "   -399.9142088984823,\n",
       "   -371.01245830252213,\n",
       "   -485.4767788529359,\n",
       "   -405.3664648501948,\n",
       "   -391.0014021403421,\n",
       "   -434.13155418920473,\n",
       "   -463.3729373890369,\n",
       "   -510.68512002784644,\n",
       "   -503.3393518002856,\n",
       "   -593.5850909272693,\n",
       "   -702.0773853280042,\n",
       "   -479.9222722012037,\n",
       "   -509.7619612774389,\n",
       "   -491.64574186999573,\n",
       "   -395.9305930092644,\n",
       "   -397.43942281631297,\n",
       "   -377.3540259015197,\n",
       "   -440.1437661337396,\n",
       "   -367.26035433698235,\n",
       "   -428.8149162145603,\n",
       "   -336.1631065024146,\n",
       "   -313.73956851511264,\n",
       "   -380.19910224729665,\n",
       "   -418.793920772716,\n",
       "   -463.0430633084886,\n",
       "   -652.2711896351587,\n",
       "   -627.5182967573925,\n",
       "   -682.067129606305,\n",
       "   -611.6802691836572,\n",
       "   -1089.167295601398,\n",
       "   -2683.2298909687047,\n",
       "   -1809.7745107896274,\n",
       "   -1664.5298869884257,\n",
       "   -1514.2483531531855,\n",
       "   -1617.126763006551,\n",
       "   -2067.7146825650884,\n",
       "   -1121.668460305247,\n",
       "   -725.7872730348673,\n",
       "   -1262.1722779575184,\n",
       "   -1128.2948769601635,\n",
       "   -1466.0640650304783,\n",
       "   -1470.53017103635,\n",
       "   -2533.490997836861,\n",
       "   -3358.3698140965785,\n",
       "   -5039.5999318012755,\n",
       "   -5237.252127342094,\n",
       "   -1872.7164640715346,\n",
       "   -2168.1296838112826,\n",
       "   -2984.9544093062664,\n",
       "   -1668.4829523832054,\n",
       "   -4250.185016116773,\n",
       "   -8129.005324090164,\n",
       "   -7285.9808245473,\n",
       "   -2435.9211934960376,\n",
       "   -1281.5337407827944,\n",
       "   -1155.9959784607436,\n",
       "   -768.7805819297777,\n",
       "   -886.929923422878,\n",
       "   -1132.5669248230327,\n",
       "   -647.512513304317,\n",
       "   -641.7819541247463,\n",
       "   -566.9768716824013,\n",
       "   -693.9375141663546,\n",
       "   -908.0223784673701,\n",
       "   -1180.220196072696,\n",
       "   -859.8907916012002,\n",
       "   -933.2898870551729,\n",
       "   -717.134765959733,\n",
       "   -724.325447398545,\n",
       "   -865.8655630177037,\n",
       "   -693.2855775255495,\n",
       "   -765.1597685438793,\n",
       "   -622.2517208863035,\n",
       "   -469.8863639913156,\n",
       "   -535.9427778915159,\n",
       "   -559.884148545333,\n",
       "   -436.1368398443228,\n",
       "   -515.1268651131141,\n",
       "   -457.814367558927,\n",
       "   -509.2981486049774,\n",
       "   -505.3601873039128,\n",
       "   -580.6422131050572,\n",
       "   -557.2885843750071,\n",
       "   -551.9412412269127,\n",
       "   -680.4500193736559,\n",
       "   -602.305843745168,\n",
       "   -715.7853105509678,\n",
       "   -1502.781965523497,\n",
       "   -1619.5429044396194,\n",
       "   -1377.0604797432131,\n",
       "   -2192.7070689120633,\n",
       "   -2279.020835890824,\n",
       "   -1756.903621874123,\n",
       "   -1586.533100664696,\n",
       "   -1315.1684321710106,\n",
       "   -1658.723766784858,\n",
       "   -1485.3670446576289,\n",
       "   -1202.0647250065051,\n",
       "   -723.9500193661726,\n",
       "   -575.6639221538818,\n",
       "   -625.4647038770723,\n",
       "   -507.75302221468024,\n",
       "   -614.188982616418,\n",
       "   -796.9475052959579,\n",
       "   -717.5054247369237,\n",
       "   -597.3165378326538,\n",
       "   -552.9412459814573,\n",
       "   -596.5816189845484,\n",
       "   -606.3925773605616,\n",
       "   -807.4962063843099,\n",
       "   -680.3374491795789,\n",
       "   -481.837305801325,\n",
       "   -516.972892004469,\n",
       "   -383.99278019311834,\n",
       "   -450.06294921698776,\n",
       "   -524.0022163497551,\n",
       "   -712.7270546178657,\n",
       "   -766.3700765421662,\n",
       "   -1082.2048687145407,\n",
       "   -1486.2945093594487,\n",
       "   -2037.206659698889,\n",
       "   -1085.1271954521533,\n",
       "   -969.9740746439443,\n",
       "   -705.8893380719909,\n",
       "   -922.1288985102357,\n",
       "   -725.6750507561355,\n",
       "   -1186.5956526684722,\n",
       "   -1021.2977192021938,\n",
       "   -846.0485678223821,\n",
       "   -958.4499905841544,\n",
       "   -767.077198027023,\n",
       "   -929.6615361190655,\n",
       "   -1101.3292166848123,\n",
       "   -1069.872219720225,\n",
       "   -1291.1478460481806,\n",
       "   -1224.4659170219084,\n",
       "   -1434.1005408474657,\n",
       "   -1834.085854364244,\n",
       "   -1448.5018221617297,\n",
       "   -1011.2045229563099,\n",
       "   -706.1989733629508,\n",
       "   -1059.1345028758378,\n",
       "   -1033.9037134343093,\n",
       "   -1328.310381513614,\n",
       "   -1253.4521728364548,\n",
       "   -1064.262983064117,\n",
       "   -1429.3559647221784,\n",
       "   -2240.605185002246,\n",
       "   -2131.4437553503176,\n",
       "   -2265.7157530498675,\n",
       "   -1841.8299551147786,\n",
       "   -1507.2731268400312,\n",
       "   -1740.9610345882497,\n",
       "   -1432.1066875948275,\n",
       "   -1144.0293025086862,\n",
       "   -1009.5859811286217,\n",
       "   -802.6937970577769,\n",
       "   -763.7285439114117,\n",
       "   -817.6427444044892,\n",
       "   -866.764355675372,\n",
       "   -556.8569248819044,\n",
       "   -414.2151283604117,\n",
       "   -493.1867294708089,\n",
       "   -458.64707561644593,\n",
       "   -539.8563292442136,\n",
       "   -557.2444787936186,\n",
       "   -618.1847598276458,\n",
       "   -557.3545079452992,\n",
       "   -674.6575059677434,\n",
       "   -732.2086623687077,\n",
       "   -682.1031978709947,\n",
       "   -817.0199726772736,\n",
       "   -1380.8543090429873,\n",
       "   -1264.6758986829082,\n",
       "   -1306.1714572548867,\n",
       "   -1030.779088072873,\n",
       "   -806.5428047784849,\n",
       "   -733.2130885258792,\n",
       "   -812.5540605281759,\n",
       "   -960.0552463269772,\n",
       "   -1149.061604776061,\n",
       "   -951.6828570541314,\n",
       "   -808.1782597276291,\n",
       "   -879.3269923813851,\n",
       "   -838.6307757798642,\n",
       "   -801.427490074728,\n",
       "   -1069.372335456593,\n",
       "   -1119.73207987423,\n",
       "   -924.981237087456,\n",
       "   -775.1815200460974,\n",
       "   -545.0855158787435,\n",
       "   -548.1959723789706,\n",
       "   -466.7947053791405,\n",
       "   -442.4424536813383,\n",
       "   -471.4475210271634,\n",
       "   -392.8103824398954,\n",
       "   -330.9603116433591,\n",
       "   -387.9374918238902,\n",
       "   -406.8743196048547,\n",
       "   -599.9838733747115,\n",
       "   -560.6239410302842,\n",
       "   -599.987113432966,\n",
       "   -639.2923963257679,\n",
       "   -931.3624036766843],\n",
       "  'train_pce_loss': [0.3560909330844879,\n",
       "   0.3494891822338104,\n",
       "   0.34216660261154175,\n",
       "   0.3371444046497345,\n",
       "   0.329547256231308,\n",
       "   0.333605021238327,\n",
       "   0.34348705410957336,\n",
       "   0.3295060098171234,\n",
       "   0.31984806060791016,\n",
       "   0.3454091548919678,\n",
       "   0.364702969789505,\n",
       "   0.34014835953712463,\n",
       "   0.346525102853775,\n",
       "   0.3386831283569336,\n",
       "   0.3177514970302582,\n",
       "   0.36513248085975647,\n",
       "   0.31945493817329407,\n",
       "   0.346218079328537,\n",
       "   0.3351219892501831,\n",
       "   0.3606218695640564,\n",
       "   0.41443371772766113,\n",
       "   0.3524497151374817,\n",
       "   0.3503921329975128,\n",
       "   0.3491990268230438,\n",
       "   0.35423552989959717,\n",
       "   0.3672579228878021,\n",
       "   0.3822862207889557,\n",
       "   0.3539183437824249,\n",
       "   0.3780546486377716,\n",
       "   0.3856537640094757,\n",
       "   0.3378809988498688,\n",
       "   0.3796939253807068,\n",
       "   0.3767334818840027,\n",
       "   0.32610899209976196,\n",
       "   0.2947878837585449,\n",
       "   0.36642253398895264,\n",
       "   0.3851194977760315,\n",
       "   0.34879207611083984,\n",
       "   0.34516578912734985,\n",
       "   0.30171433091163635,\n",
       "   0.36099642515182495,\n",
       "   0.37661466002464294,\n",
       "   0.39785271883010864,\n",
       "   0.34587153792381287,\n",
       "   0.32079195976257324,\n",
       "   0.3371083438396454,\n",
       "   0.35499870777130127,\n",
       "   0.34492748975753784,\n",
       "   0.3545595407485962,\n",
       "   0.35482272505760193,\n",
       "   0.3981163501739502,\n",
       "   0.32597842812538147,\n",
       "   0.3624764680862427,\n",
       "   0.33201590180397034,\n",
       "   0.323047399520874,\n",
       "   0.3709492087364197,\n",
       "   0.35842064023017883,\n",
       "   0.36150744557380676,\n",
       "   0.327287882566452,\n",
       "   0.33635497093200684,\n",
       "   0.34900251030921936,\n",
       "   0.34183573722839355,\n",
       "   0.32852649688720703,\n",
       "   0.34626176953315735,\n",
       "   0.3410753011703491,\n",
       "   0.3603355884552002,\n",
       "   0.35489869117736816,\n",
       "   0.3943737745285034,\n",
       "   0.34890323877334595,\n",
       "   0.32538241147994995,\n",
       "   0.3480782210826874,\n",
       "   0.38678646087646484,\n",
       "   0.3546634018421173,\n",
       "   0.3944889008998871,\n",
       "   0.3365485370159149,\n",
       "   0.33900830149650574,\n",
       "   0.3466317057609558,\n",
       "   0.35199376940727234,\n",
       "   0.3454461097717285,\n",
       "   0.3237967789173126,\n",
       "   0.37030163407325745,\n",
       "   0.3640187382698059,\n",
       "   0.3095875084400177,\n",
       "   0.36996057629585266,\n",
       "   0.2986901104450226,\n",
       "   0.36463722586631775,\n",
       "   0.3258007764816284,\n",
       "   0.33804774284362793,\n",
       "   0.3308619558811188,\n",
       "   0.30865785479545593,\n",
       "   0.3335409164428711,\n",
       "   0.3117455542087555,\n",
       "   0.3092399835586548,\n",
       "   0.36933061480522156,\n",
       "   0.37073591351509094,\n",
       "   0.34253236651420593,\n",
       "   0.2998090088367462,\n",
       "   0.35350897908210754,\n",
       "   0.32875558733940125,\n",
       "   0.3541416525840759,\n",
       "   0.34786084294319153,\n",
       "   0.3770812451839447,\n",
       "   0.38233068585395813,\n",
       "   0.36562249064445496,\n",
       "   0.33500999212265015,\n",
       "   0.41473472118377686,\n",
       "   0.37307921051979065,\n",
       "   0.35334891080856323,\n",
       "   0.3495223820209503,\n",
       "   0.35390299558639526,\n",
       "   0.3476320505142212,\n",
       "   0.3431628346443176,\n",
       "   0.3234243094921112,\n",
       "   0.38501155376434326,\n",
       "   0.3269602060317993,\n",
       "   0.32486778497695923,\n",
       "   0.3499622642993927,\n",
       "   0.3148995339870453,\n",
       "   0.3381350338459015,\n",
       "   0.3688899278640747,\n",
       "   0.3464936316013336,\n",
       "   0.3432866334915161,\n",
       "   0.3462132215499878,\n",
       "   0.33907178044319153,\n",
       "   0.3114386200904846,\n",
       "   0.32149365544319153,\n",
       "   0.3460371196269989,\n",
       "   0.3211691975593567,\n",
       "   0.34955766797065735,\n",
       "   0.32198530435562134,\n",
       "   0.3593178391456604,\n",
       "   0.3536376953125,\n",
       "   0.33182528614997864,\n",
       "   0.33883345127105713,\n",
       "   0.33081868290901184,\n",
       "   0.3115754723548889,\n",
       "   0.3498023450374603,\n",
       "   0.3062439262866974,\n",
       "   0.3635255992412567,\n",
       "   0.32677045464515686,\n",
       "   0.36283019185066223,\n",
       "   0.3291151821613312,\n",
       "   0.3510103225708008,\n",
       "   0.340581476688385,\n",
       "   0.3655064105987549,\n",
       "   0.3319302499294281,\n",
       "   0.3484765291213989,\n",
       "   0.31345686316490173,\n",
       "   0.3458581566810608,\n",
       "   0.3468152582645416,\n",
       "   0.4041576683521271,\n",
       "   0.3397187292575836,\n",
       "   0.33876028656959534,\n",
       "   0.3480164110660553,\n",
       "   0.32740673422813416,\n",
       "   0.3317624628543854,\n",
       "   0.34035637974739075,\n",
       "   0.3238522410392761,\n",
       "   0.3230869174003601,\n",
       "   0.35902079939842224,\n",
       "   0.33216196298599243,\n",
       "   0.3101308345794678,\n",
       "   0.3378037214279175,\n",
       "   0.3862576186656952,\n",
       "   0.32023364305496216,\n",
       "   0.33753660321235657,\n",
       "   0.34987881779670715,\n",
       "   0.31787121295928955,\n",
       "   0.3312775194644928,\n",
       "   0.364233136177063,\n",
       "   0.3775312602519989,\n",
       "   0.37312471866607666,\n",
       "   0.33371955156326294,\n",
       "   0.3172817528247833,\n",
       "   0.32073771953582764,\n",
       "   0.3408350944519043,\n",
       "   0.34518060088157654,\n",
       "   0.3472311198711395,\n",
       "   0.35726067423820496,\n",
       "   0.37800389528274536,\n",
       "   0.3217252492904663,\n",
       "   0.337513267993927,\n",
       "   0.34904909133911133,\n",
       "   0.312392920255661,\n",
       "   0.40686729550361633,\n",
       "   0.3370681703090668,\n",
       "   0.32082486152648926,\n",
       "   0.3229770362377167,\n",
       "   0.32130709290504456,\n",
       "   0.2963414788246155,\n",
       "   0.35738450288772583,\n",
       "   0.305801659822464,\n",
       "   0.32427141070365906,\n",
       "   0.35966554284095764,\n",
       "   0.3933843672275543,\n",
       "   0.3430333137512207,\n",
       "   0.3395989239215851,\n",
       "   0.34395134449005127,\n",
       "   0.35058683156967163,\n",
       "   0.3517509698867798,\n",
       "   0.3428841233253479,\n",
       "   0.35113662481307983,\n",
       "   0.3317137360572815,\n",
       "   0.29721954464912415,\n",
       "   0.30857959389686584,\n",
       "   0.34374186396598816,\n",
       "   0.34547945857048035,\n",
       "   0.28602316975593567,\n",
       "   0.34749501943588257,\n",
       "   0.3500848412513733,\n",
       "   0.3446350395679474,\n",
       "   0.4145738482475281,\n",
       "   0.3474017381668091,\n",
       "   0.3253300189971924,\n",
       "   0.3538847863674164,\n",
       "   0.3067440688610077,\n",
       "   0.3356018364429474,\n",
       "   0.33645185828208923,\n",
       "   0.3301438093185425,\n",
       "   0.35907265543937683,\n",
       "   0.3533749282360077,\n",
       "   0.32885992527008057,\n",
       "   0.3605830669403076,\n",
       "   0.3379250764846802,\n",
       "   0.3892221748828888,\n",
       "   0.33860889077186584,\n",
       "   0.31204694509506226,\n",
       "   0.31976595520973206,\n",
       "   0.2969036400318146,\n",
       "   0.3466908037662506,\n",
       "   0.36393389105796814,\n",
       "   0.3564792275428772,\n",
       "   0.3659154772758484,\n",
       "   0.33678165078163147,\n",
       "   0.3350194990634918,\n",
       "   0.33692121505737305,\n",
       "   0.337091863155365,\n",
       "   0.32945841550827026,\n",
       "   0.3381674587726593,\n",
       "   0.32684653997421265,\n",
       "   0.3608413636684418,\n",
       "   0.3059927821159363,\n",
       "   0.332396924495697,\n",
       "   0.3042207658290863,\n",
       "   0.3222352862358093,\n",
       "   0.31404975056648254,\n",
       "   0.35393521189689636,\n",
       "   0.33969956636428833,\n",
       "   0.3267142176628113,\n",
       "   0.3329964876174927,\n",
       "   0.30982324481010437,\n",
       "   0.32436248660087585,\n",
       "   0.34743732213974,\n",
       "   0.35051971673965454,\n",
       "   0.324712872505188,\n",
       "   0.3664490580558777,\n",
       "   0.3300727605819702,\n",
       "   0.33361750841140747,\n",
       "   0.33663204312324524,\n",
       "   0.3546730875968933,\n",
       "   0.3159603178501129,\n",
       "   0.32768145203590393,\n",
       "   0.3436705768108368,\n",
       "   0.33608877658843994,\n",
       "   0.3378009498119354,\n",
       "   0.3463444411754608,\n",
       "   0.3545382022857666,\n",
       "   0.3511556088924408,\n",
       "   0.3093995153903961,\n",
       "   0.32494449615478516,\n",
       "   0.36431923508644104,\n",
       "   0.3247337341308594,\n",
       "   0.3773488998413086,\n",
       "   0.32131001353263855,\n",
       "   0.35005515813827515,\n",
       "   0.3625982403755188,\n",
       "   0.32696911692619324,\n",
       "   0.34389886260032654,\n",
       "   0.36572912335395813,\n",
       "   0.301526814699173,\n",
       "   0.3603399097919464,\n",
       "   0.3583453595638275,\n",
       "   0.31046736240386963,\n",
       "   0.32805749773979187,\n",
       "   0.31955933570861816,\n",
       "   0.3408268690109253,\n",
       "   0.3392687141895294,\n",
       "   0.35584643483161926,\n",
       "   0.34791725873947144,\n",
       "   0.35047850012779236,\n",
       "   0.3390219211578369,\n",
       "   0.3354155123233795,\n",
       "   0.2844713032245636,\n",
       "   0.3668193221092224,\n",
       "   0.3482842445373535,\n",
       "   0.35189002752304077,\n",
       "   0.32673409581184387,\n",
       "   0.33755195140838623,\n",
       "   0.3470088243484497,\n",
       "   0.3184482455253601,\n",
       "   0.3238276541233063,\n",
       "   0.3037443459033966,\n",
       "   0.3592691123485565,\n",
       "   0.34894102811813354,\n",
       "   0.3366217017173767,\n",
       "   0.33937564492225647,\n",
       "   0.33288660645484924,\n",
       "   0.3170730173587799,\n",
       "   0.2991868257522583,\n",
       "   0.3076171278953552,\n",
       "   0.3449469804763794,\n",
       "   0.3401302695274353,\n",
       "   0.363754004240036,\n",
       "   0.32980644702911377,\n",
       "   0.30845215916633606,\n",
       "   0.3562658131122589,\n",
       "   0.33545002341270447,\n",
       "   0.3253694772720337,\n",
       "   0.3149605393409729,\n",
       "   0.33200570940971375,\n",
       "   0.31413745880126953,\n",
       "   0.34570518136024475,\n",
       "   0.3128768801689148,\n",
       "   0.3540850579738617,\n",
       "   0.3398323655128479,\n",
       "   0.3251219093799591,\n",
       "   0.3131856918334961,\n",
       "   0.38167381286621094,\n",
       "   0.33953768014907837,\n",
       "   0.34439465403556824,\n",
       "   0.32124003767967224,\n",
       "   0.3407357633113861,\n",
       "   0.3595057427883148,\n",
       "   0.314647376537323,\n",
       "   0.3526051640510559,\n",
       "   0.3198000490665436,\n",
       "   0.37159106135368347,\n",
       "   0.35996460914611816,\n",
       "   0.34794995188713074,\n",
       "   0.3417995274066925,\n",
       "   0.345081090927124,\n",
       "   0.347668319940567,\n",
       "   0.35529497265815735,\n",
       "   0.31129464507102966,\n",
       "   0.34385251998901367,\n",
       "   0.35629141330718994,\n",
       "   0.29294976592063904,\n",
       "   0.29788076877593994,\n",
       "   0.3560677766799927,\n",
       "   0.3400116562843323,\n",
       "   0.3349257707595825,\n",
       "   0.34915250539779663,\n",
       "   0.3195390999317169,\n",
       "   0.36438605189323425,\n",
       "   0.32007458806037903,\n",
       "   0.3206709325313568,\n",
       "   0.3630138039588928,\n",
       "   0.34221351146698,\n",
       "   0.34797683358192444,\n",
       "   0.34571048617362976,\n",
       "   0.37208738923072815,\n",
       "   0.3266499936580658,\n",
       "   0.3379121422767639,\n",
       "   0.3419402837753296,\n",
       "   0.3304492235183716,\n",
       "   0.35040560364723206,\n",
       "   0.32557767629623413,\n",
       "   0.344407320022583,\n",
       "   0.3724805414676666,\n",
       "   0.33667030930519104,\n",
       "   0.32073426246643066,\n",
       "   0.34710073471069336,\n",
       "   0.31827670335769653,\n",
       "   0.37342020869255066,\n",
       "   0.3330168128013611,\n",
       "   0.34063905477523804,\n",
       "   0.3342895209789276,\n",
       "   0.38379591703414917,\n",
       "   0.3006153404712677,\n",
       "   0.32772597670555115,\n",
       "   0.295308381319046,\n",
       "   0.3534492552280426,\n",
       "   0.3632165193557739,\n",
       "   0.3744768500328064,\n",
       "   0.3411000072956085,\n",
       "   0.3286241888999939,\n",
       "   0.342632532119751,\n",
       "   0.335498183965683,\n",
       "   0.33741602301597595,\n",
       "   0.35052627325057983,\n",
       "   0.3124125003814697,\n",
       "   0.35020485520362854,\n",
       "   0.33709338307380676,\n",
       "   0.32649606466293335,\n",
       "   0.33692821860313416,\n",
       "   0.35008570551872253,\n",
       "   0.317893922328949,\n",
       "   0.37014341354370117,\n",
       "   0.32548266649246216,\n",
       "   0.3311537206172943,\n",
       "   0.3153569996356964,\n",
       "   0.3297215402126312,\n",
       "   0.3739670217037201,\n",
       "   0.306688517332077,\n",
       "   0.33143362402915955,\n",
       "   0.3225119113922119,\n",
       "   0.35283562541007996,\n",
       "   0.35289672017097473,\n",
       "   0.33774900436401367,\n",
       "   0.31852611899375916,\n",
       "   0.3374435305595398,\n",
       "   0.3322840631008148,\n",
       "   0.3467305302619934,\n",
       "   0.2911566197872162,\n",
       "   0.3523029386997223,\n",
       "   0.3531992733478546,\n",
       "   0.36153021454811096,\n",
       "   0.336061954498291,\n",
       "   0.3426656723022461,\n",
       "   0.3255481421947479,\n",
       "   0.34024161100387573,\n",
       "   0.3764384984970093,\n",
       "   0.33626505732536316,\n",
       "   0.33998268842697144,\n",
       "   0.3389510214328766,\n",
       "   0.3246634006500244,\n",
       "   0.3576156198978424,\n",
       "   0.37012773752212524,\n",
       "   0.326210081577301,\n",
       "   0.3475443720817566,\n",
       "   0.32546207308769226,\n",
       "   0.3241446614265442,\n",
       "   0.3265882730484009,\n",
       "   0.32174476981163025,\n",
       "   0.3178451955318451,\n",
       "   0.3379943072795868,\n",
       "   0.3075372874736786,\n",
       "   0.3549140989780426,\n",
       "   0.3711434602737427,\n",
       "   0.33701324462890625,\n",
       "   0.3644225299358368,\n",
       "   0.3393135964870453,\n",
       "   0.3258388042449951,\n",
       "   0.3137885332107544,\n",
       "   0.3495301604270935,\n",
       "   0.3164459764957428,\n",
       "   0.330068439245224,\n",
       "   0.33935219049453735,\n",
       "   0.3434136211872101,\n",
       "   0.3352179527282715,\n",
       "   0.3344646394252777,\n",
       "   0.3256217837333679,\n",
       "   0.3644483685493469,\n",
       "   0.3763582706451416,\n",
       "   0.3041146397590637,\n",
       "   0.37665659189224243,\n",
       "   0.31691598892211914,\n",
       "   0.30324655771255493,\n",
       "   0.3388267159461975,\n",
       "   0.34127479791641235,\n",
       "   0.36015796661376953,\n",
       "   0.3878736197948456,\n",
       "   0.3559662103652954,\n",
       "   0.3040646016597748,\n",
       "   0.3299502730369568,\n",
       "   0.31173276901245117,\n",
       "   0.3528021574020386,\n",
       "   0.32241541147232056,\n",
       "   0.324178546667099,\n",
       "   0.32843711972236633,\n",
       "   0.38498738408088684,\n",
       "   0.3161552846431732,\n",
       "   0.33408427238464355,\n",
       "   0.3382425606250763,\n",
       "   0.36900627613067627,\n",
       "   0.3403193950653076,\n",
       "   0.3741343021392822,\n",
       "   0.3124294579029083,\n",
       "   0.3176281154155731,\n",
       "   0.3474601209163666,\n",
       "   0.3201010525226593,\n",
       "   0.33572790026664734,\n",
       "   0.3393092751502991,\n",
       "   0.3636217415332794,\n",
       "   0.3109966516494751,\n",
       "   0.38431498408317566,\n",
       "   0.3351241946220398,\n",
       "   0.29378873109817505,\n",
       "   0.31816011667251587,\n",
       "   0.37768664956092834,\n",
       "   0.36166828870773315,\n",
       "   0.3312487006187439,\n",
       "   0.3518025875091553,\n",
       "   0.39186885952949524,\n",
       "   0.3336053192615509,\n",
       "   0.3733041286468506,\n",
       "   0.3653094470500946,\n",
       "   0.3248234689235687,\n",
       "   0.3155938386917114,\n",
       "   0.2837894856929779],\n",
       "  'train_voc_loss': [1.296923279762268,\n",
       "   1.50185227394104,\n",
       "   1.2518726587295532,\n",
       "   0.8549108505249023,\n",
       "   1.0831596851348877,\n",
       "   1.0845234394073486,\n",
       "   1.1895372867584229,\n",
       "   1.4480822086334229,\n",
       "   1.203007698059082,\n",
       "   1.0543590784072876,\n",
       "   0.8443240523338318,\n",
       "   1.2398779392242432,\n",
       "   1.2020175457000732,\n",
       "   1.2467035055160522,\n",
       "   1.0966458320617676,\n",
       "   1.2670904397964478,\n",
       "   1.3794516324996948,\n",
       "   1.006775140762329,\n",
       "   1.3065266609191895,\n",
       "   1.0674247741699219,\n",
       "   1.5802764892578125,\n",
       "   1.0598945617675781,\n",
       "   1.3608887195587158,\n",
       "   1.2669696807861328,\n",
       "   1.0602211952209473,\n",
       "   1.300301432609558,\n",
       "   1.187786340713501,\n",
       "   1.1216626167297363,\n",
       "   1.1389753818511963,\n",
       "   0.9125732183456421,\n",
       "   1.1817131042480469,\n",
       "   1.097856879234314,\n",
       "   1.047350525856018,\n",
       "   0.9660247564315796,\n",
       "   1.335180401802063,\n",
       "   1.1682180166244507,\n",
       "   1.1600196361541748,\n",
       "   1.3534668684005737,\n",
       "   1.3919192552566528,\n",
       "   1.1843465566635132,\n",
       "   1.223470687866211,\n",
       "   1.2277867794036865,\n",
       "   1.158084511756897,\n",
       "   1.1592687368392944,\n",
       "   0.934638261795044,\n",
       "   0.989461362361908,\n",
       "   0.901297926902771,\n",
       "   1.074190378189087,\n",
       "   1.5325192213058472,\n",
       "   1.2948763370513916,\n",
       "   1.4959841966629028,\n",
       "   1.2859227657318115,\n",
       "   1.20743727684021,\n",
       "   0.9868724942207336,\n",
       "   1.1563777923583984,\n",
       "   1.144752025604248,\n",
       "   1.3439549207687378,\n",
       "   1.0640124082565308,\n",
       "   1.5197967290878296,\n",
       "   1.0657644271850586,\n",
       "   1.1653730869293213,\n",
       "   1.4655073881149292,\n",
       "   1.267796516418457,\n",
       "   0.9438232183456421,\n",
       "   1.1866693496704102,\n",
       "   0.8695092797279358,\n",
       "   1.1562243700027466,\n",
       "   1.0185381174087524,\n",
       "   1.1041011810302734,\n",
       "   1.119240641593933,\n",
       "   1.1315622329711914,\n",
       "   1.054207682609558,\n",
       "   0.8822481632232666,\n",
       "   1.0771371126174927,\n",
       "   0.9672341346740723,\n",
       "   1.0837618112564087,\n",
       "   1.185062050819397,\n",
       "   1.0281556844711304,\n",
       "   0.8597242832183838,\n",
       "   1.1630359888076782,\n",
       "   0.9958698749542236,\n",
       "   0.8659164905548096,\n",
       "   1.2710363864898682,\n",
       "   1.3029519319534302,\n",
       "   1.251198172569275,\n",
       "   1.0935229063034058,\n",
       "   1.1985372304916382,\n",
       "   0.971413254737854,\n",
       "   0.7990534901618958,\n",
       "   1.2475777864456177,\n",
       "   0.8845838308334351,\n",
       "   1.1600594520568848,\n",
       "   1.2245597839355469,\n",
       "   1.0976756811141968,\n",
       "   0.9843159914016724,\n",
       "   0.9548577070236206,\n",
       "   1.213005542755127,\n",
       "   0.8989111185073853,\n",
       "   0.8990778923034668,\n",
       "   0.9031339287757874,\n",
       "   0.9768481254577637,\n",
       "   0.8211243152618408,\n",
       "   0.9485681653022766,\n",
       "   1.1224929094314575,\n",
       "   1.2461899518966675,\n",
       "   1.098488450050354,\n",
       "   1.1023272275924683,\n",
       "   1.1226788759231567,\n",
       "   1.0949641466140747,\n",
       "   1.0943567752838135,\n",
       "   1.3439098596572876,\n",
       "   1.005826711654663,\n",
       "   1.300508975982666,\n",
       "   1.3173576593399048,\n",
       "   0.9743696451187134,\n",
       "   1.0043567419052124,\n",
       "   0.9647920727729797,\n",
       "   0.7984915971755981,\n",
       "   0.8020705580711365,\n",
       "   0.9312507510185242,\n",
       "   0.8156182169914246,\n",
       "   1.164181113243103,\n",
       "   0.855101466178894,\n",
       "   1.1404834985733032,\n",
       "   1.2887847423553467,\n",
       "   1.4247682094573975,\n",
       "   0.7921804189682007,\n",
       "   0.8422229886054993,\n",
       "   0.8537040948867798,\n",
       "   0.9697853326797485,\n",
       "   0.9801616072654724,\n",
       "   0.9563065767288208,\n",
       "   1.1415047645568848,\n",
       "   0.8668957948684692,\n",
       "   0.9803754687309265,\n",
       "   1.0946786403656006,\n",
       "   0.8948953747749329,\n",
       "   0.8226079344749451,\n",
       "   0.8867133259773254,\n",
       "   0.8173357844352722,\n",
       "   0.8405722379684448,\n",
       "   0.8127287030220032,\n",
       "   1.2337219715118408,\n",
       "   1.164282202720642,\n",
       "   1.043864130973816,\n",
       "   1.2828216552734375,\n",
       "   0.9766008257865906,\n",
       "   1.330176830291748,\n",
       "   0.7627552151679993,\n",
       "   0.8263260126113892,\n",
       "   1.2697274684906006,\n",
       "   0.9309669733047485,\n",
       "   0.7420425415039062,\n",
       "   1.1211352348327637,\n",
       "   0.7731133103370667,\n",
       "   0.7721906900405884,\n",
       "   0.9310619235038757,\n",
       "   0.9070160984992981,\n",
       "   0.939400851726532,\n",
       "   1.0237435102462769,\n",
       "   1.0203920602798462,\n",
       "   0.9094523787498474,\n",
       "   0.9530622959136963,\n",
       "   1.0346899032592773,\n",
       "   0.9382310509681702,\n",
       "   0.9154829382896423,\n",
       "   0.801552951335907,\n",
       "   1.28883695602417,\n",
       "   1.1315242052078247,\n",
       "   1.1935033798217773,\n",
       "   0.8337281346321106,\n",
       "   0.7212699055671692,\n",
       "   0.9777956604957581,\n",
       "   1.007126808166504,\n",
       "   1.0920192003250122,\n",
       "   0.8982486128807068,\n",
       "   1.0200430154800415,\n",
       "   0.6300322413444519,\n",
       "   0.6044424176216125,\n",
       "   0.9116080403327942,\n",
       "   1.0253300666809082,\n",
       "   0.8462585210800171,\n",
       "   1.2912023067474365,\n",
       "   0.9797652363777161,\n",
       "   0.7923699617385864,\n",
       "   1.002472996711731,\n",
       "   0.9169079065322876,\n",
       "   0.8773452639579773,\n",
       "   0.835253894329071,\n",
       "   0.9138657450675964,\n",
       "   0.7005791068077087,\n",
       "   0.751498818397522,\n",
       "   0.9270185232162476,\n",
       "   0.7758327722549438,\n",
       "   0.9040916562080383,\n",
       "   0.9587357640266418,\n",
       "   0.9181172251701355,\n",
       "   0.7327560782432556,\n",
       "   0.9119199514389038,\n",
       "   0.8982753157615662,\n",
       "   1.096183180809021,\n",
       "   0.7857432961463928,\n",
       "   0.9375740885734558,\n",
       "   0.9941112399101257,\n",
       "   1.115816593170166,\n",
       "   0.6949436664581299,\n",
       "   1.100766658782959,\n",
       "   0.6200911402702332,\n",
       "   0.913535475730896,\n",
       "   0.8943491578102112,\n",
       "   0.7706300020217896,\n",
       "   0.7675064206123352,\n",
       "   1.0975561141967773,\n",
       "   0.8751596808433533,\n",
       "   0.7418028116226196,\n",
       "   0.6400213241577148,\n",
       "   0.7170192003250122,\n",
       "   0.7074732184410095,\n",
       "   0.7387242317199707,\n",
       "   0.7508054375648499,\n",
       "   0.6795795559883118,\n",
       "   0.6715942025184631,\n",
       "   1.18365478515625,\n",
       "   0.9297992587089539,\n",
       "   0.9393342733383179,\n",
       "   0.939136266708374,\n",
       "   0.795600950717926,\n",
       "   0.9709892272949219,\n",
       "   0.7052808403968811,\n",
       "   0.782084047794342,\n",
       "   1.1894986629486084,\n",
       "   0.8783013224601746,\n",
       "   0.8189013600349426,\n",
       "   0.5394384860992432,\n",
       "   0.611060619354248,\n",
       "   0.7667999863624573,\n",
       "   1.0643374919891357,\n",
       "   0.8726640343666077,\n",
       "   0.825981855392456,\n",
       "   0.9071497917175293,\n",
       "   0.6570514440536499,\n",
       "   1.1083595752716064,\n",
       "   0.8772619366645813,\n",
       "   0.5709611773490906,\n",
       "   0.7444781064987183,\n",
       "   0.4968161880970001,\n",
       "   0.7016903758049011,\n",
       "   0.9344792366027832,\n",
       "   0.8202405571937561,\n",
       "   0.7694056034088135,\n",
       "   0.6780843138694763,\n",
       "   0.5353725552558899,\n",
       "   0.8722820281982422,\n",
       "   0.9001389145851135,\n",
       "   0.9455745220184326,\n",
       "   0.6961417198181152,\n",
       "   0.9180511832237244,\n",
       "   0.8116165995597839,\n",
       "   0.7891826629638672,\n",
       "   0.8357996940612793,\n",
       "   0.5808097720146179,\n",
       "   0.6057007908821106,\n",
       "   0.8498828411102295,\n",
       "   0.6146289706230164,\n",
       "   1.0314990282058716,\n",
       "   0.5738726854324341,\n",
       "   0.634119987487793,\n",
       "   0.8496155142784119,\n",
       "   0.6178510189056396,\n",
       "   0.7172974944114685,\n",
       "   1.1318488121032715,\n",
       "   0.7284716367721558,\n",
       "   0.5505810379981995,\n",
       "   0.9063451290130615,\n",
       "   0.8117623329162598,\n",
       "   0.7757211923599243,\n",
       "   0.7976535558700562,\n",
       "   0.6000939607620239,\n",
       "   0.6445323824882507,\n",
       "   0.633638858795166,\n",
       "   0.8753758072853088,\n",
       "   0.7065083980560303,\n",
       "   0.7721648812294006,\n",
       "   0.5850839614868164,\n",
       "   1.0057178735733032,\n",
       "   0.781621515750885,\n",
       "   0.7499313950538635,\n",
       "   0.7374250292778015,\n",
       "   0.69693922996521,\n",
       "   0.6994811296463013,\n",
       "   0.7527392506599426,\n",
       "   0.627025842666626,\n",
       "   0.6941218972206116,\n",
       "   0.6309248208999634,\n",
       "   0.7440098524093628,\n",
       "   0.6353355050086975,\n",
       "   0.9010184407234192,\n",
       "   0.6644561886787415,\n",
       "   0.853774905204773,\n",
       "   0.7604672312736511,\n",
       "   0.669174313545227,\n",
       "   1.2737454175949097,\n",
       "   0.6213799118995667,\n",
       "   0.6163834929466248,\n",
       "   0.7996432185173035,\n",
       "   0.7826911211013794,\n",
       "   0.8455586433410645,\n",
       "   0.8264997005462646,\n",
       "   0.7997912764549255,\n",
       "   0.6295257210731506,\n",
       "   0.6373060345649719,\n",
       "   0.9354050159454346,\n",
       "   0.6221777200698853,\n",
       "   0.46482786536216736,\n",
       "   0.5348199009895325,\n",
       "   0.6856658458709717,\n",
       "   0.6736794710159302,\n",
       "   0.7668866515159607,\n",
       "   0.9187468886375427,\n",
       "   0.7272148132324219,\n",
       "   0.7638978958129883,\n",
       "   0.49455130100250244,\n",
       "   0.5481177568435669,\n",
       "   0.6473473310470581,\n",
       "   0.8428345918655396,\n",
       "   0.5971577167510986,\n",
       "   0.5978676080703735,\n",
       "   0.6163989305496216,\n",
       "   0.7083750367164612,\n",
       "   0.6041122674942017,\n",
       "   0.8301237225532532,\n",
       "   0.5420856475830078,\n",
       "   0.7509546279907227,\n",
       "   0.6235235333442688,\n",
       "   0.5407119989395142,\n",
       "   0.5282061696052551,\n",
       "   0.3616602122783661,\n",
       "   0.42836377024650574,\n",
       "   0.7597166895866394,\n",
       "   0.48247984051704407,\n",
       "   0.6951791644096375,\n",
       "   0.5501307845115662,\n",
       "   0.6234211325645447,\n",
       "   0.5439674854278564,\n",
       "   0.39047038555145264,\n",
       "   0.41159820556640625,\n",
       "   0.8584432601928711,\n",
       "   1.023537039756775,\n",
       "   0.6317178010940552,\n",
       "   0.531067967414856,\n",
       "   0.5034462809562683,\n",
       "   0.7166509032249451,\n",
       "   0.682368814945221,\n",
       "   0.6774253249168396,\n",
       "   0.35173025727272034,\n",
       "   0.6087678670883179,\n",
       "   0.9768930673599243,\n",
       "   0.5385009050369263,\n",
       "   0.8751058578491211,\n",
       "   0.5528968572616577,\n",
       "   0.6200270652770996,\n",
       "   0.5880804657936096,\n",
       "   0.5761052370071411,\n",
       "   0.8070583343505859,\n",
       "   0.7405813336372375,\n",
       "   0.6091065406799316,\n",
       "   0.5036734938621521,\n",
       "   0.517004132270813,\n",
       "   0.4967762529850006,\n",
       "   0.8692724704742432,\n",
       "   0.795442521572113,\n",
       "   0.5062701106071472,\n",
       "   0.5209826827049255,\n",
       "   0.5464327931404114,\n",
       "   0.4589473009109497,\n",
       "   0.9360579252243042,\n",
       "   0.5212090015411377,\n",
       "   0.5063039064407349,\n",
       "   0.5478056073188782,\n",
       "   0.8019049763679504,\n",
       "   0.5847084522247314,\n",
       "   0.7388032674789429,\n",
       "   0.5959066152572632,\n",
       "   0.5723266005516052,\n",
       "   0.5323544144630432,\n",
       "   0.47385790944099426,\n",
       "   0.5918169617652893,\n",
       "   0.5277068614959717,\n",
       "   0.5542958974838257,\n",
       "   0.73965984582901,\n",
       "   0.5696581602096558,\n",
       "   0.5128458142280579,\n",
       "   0.40494218468666077,\n",
       "   0.5158290863037109,\n",
       "   0.49593955278396606,\n",
       "   0.7570406794548035,\n",
       "   0.4954604208469391,\n",
       "   0.41754981875419617,\n",
       "   0.4971895217895508,\n",
       "   0.6433959603309631,\n",
       "   0.7155877351760864,\n",
       "   0.5330213904380798,\n",
       "   0.4455134868621826,\n",
       "   0.6843568086624146,\n",
       "   0.43821921944618225,\n",
       "   0.9100263118743896,\n",
       "   0.5034738183021545,\n",
       "   0.5311872363090515,\n",
       "   0.5901378989219666,\n",
       "   0.5398790240287781,\n",
       "   0.5944963097572327,\n",
       "   0.5813260078430176,\n",
       "   0.5340998768806458,\n",
       "   0.6689890623092651,\n",
       "   0.48496147990226746,\n",
       "   0.6136597394943237,\n",
       "   0.622606635093689,\n",
       "   0.6088734865188599,\n",
       "   0.43973812460899353,\n",
       "   0.5017756223678589,\n",
       "   0.5970622897148132,\n",
       "   0.8196037411689758,\n",
       "   0.5698283314704895,\n",
       "   0.5916041731834412,\n",
       "   0.6190468668937683,\n",
       "   0.5003920197486877,\n",
       "   0.40558114647865295,\n",
       "   0.6517537832260132,\n",
       "   0.46108871698379517,\n",
       "   0.4932229220867157,\n",
       "   0.6612305641174316,\n",
       "   0.8708871006965637,\n",
       "   0.45148321986198425,\n",
       "   0.5955250859260559,\n",
       "   0.4989125430583954,\n",
       "   0.5351560711860657,\n",
       "   0.4341721832752228,\n",
       "   0.5209593772888184,\n",
       "   0.5349698662757874,\n",
       "   0.9180043339729309,\n",
       "   0.5634024739265442,\n",
       "   0.5996708273887634,\n",
       "   0.49216049909591675,\n",
       "   0.596946656703949,\n",
       "   0.49685025215148926,\n",
       "   0.6785258054733276,\n",
       "   0.9767226576805115,\n",
       "   0.568132758140564,\n",
       "   0.7748323678970337,\n",
       "   0.5186456441879272,\n",
       "   0.49721479415893555,\n",
       "   0.5132527351379395,\n",
       "   0.43644607067108154,\n",
       "   0.5555925965309143,\n",
       "   0.4672127366065979,\n",
       "   0.44805920124053955,\n",
       "   0.49348923563957214,\n",
       "   0.49001753330230713,\n",
       "   0.44585371017456055,\n",
       "   0.4985654652118683,\n",
       "   0.421948105096817,\n",
       "   0.5906256437301636,\n",
       "   0.5941476821899414,\n",
       "   0.48834672570228577,\n",
       "   0.41462045907974243,\n",
       "   0.3429998457431793,\n",
       "   0.5952632427215576,\n",
       "   0.621904194355011,\n",
       "   0.49446961283683777,\n",
       "   0.55701744556427,\n",
       "   0.4154759645462036,\n",
       "   0.5319445133209229,\n",
       "   0.4784868359565735,\n",
       "   0.716509997844696,\n",
       "   0.4928880035877228,\n",
       "   0.6679766774177551,\n",
       "   0.551491916179657,\n",
       "   0.4025033712387085,\n",
       "   0.5277700424194336,\n",
       "   0.5873048901557922,\n",
       "   0.7708795666694641,\n",
       "   0.6487701535224915,\n",
       "   0.47073712944984436,\n",
       "   0.6775376796722412,\n",
       "   0.6130499839782715,\n",
       "   0.5884542465209961,\n",
       "   0.5835707187652588,\n",
       "   0.5745087265968323,\n",
       "   0.5870887637138367,\n",
       "   0.5627140998840332,\n",
       "   0.5357021689414978,\n",
       "   0.6325693726539612,\n",
       "   0.8437829613685608,\n",
       "   0.7322143316268921,\n",
       "   0.664634108543396,\n",
       "   0.5344217419624329,\n",
       "   0.5162661671638489,\n",
       "   0.5281295776367188,\n",
       "   0.4059124290943146,\n",
       "   0.49589911103248596],\n",
       "  'train_jsc_loss': [0.5024717450141907,\n",
       "   0.4704260230064392,\n",
       "   0.5368364453315735,\n",
       "   0.45601412653923035,\n",
       "   0.4931771159172058,\n",
       "   0.4288713037967682,\n",
       "   0.6425598859786987,\n",
       "   0.46904999017715454,\n",
       "   0.4945922791957855,\n",
       "   0.4507659077644348,\n",
       "   0.452170729637146,\n",
       "   0.4367902874946594,\n",
       "   0.4717077314853668,\n",
       "   0.4475383460521698,\n",
       "   0.45750707387924194,\n",
       "   0.5215411186218262,\n",
       "   0.5071468949317932,\n",
       "   0.5260686874389648,\n",
       "   0.4383220076560974,\n",
       "   0.5064287781715393,\n",
       "   0.4999353289604187,\n",
       "   0.4225952625274658,\n",
       "   0.446616530418396,\n",
       "   0.46701285243034363,\n",
       "   0.5843899846076965,\n",
       "   0.5045803785324097,\n",
       "   0.5301111936569214,\n",
       "   0.4387401044368744,\n",
       "   0.5190267562866211,\n",
       "   0.555953323841095,\n",
       "   0.46209514141082764,\n",
       "   0.6247935891151428,\n",
       "   0.5307009220123291,\n",
       "   0.5230445265769958,\n",
       "   0.5100198984146118,\n",
       "   0.4413193464279175,\n",
       "   0.5321363210678101,\n",
       "   0.4840753376483917,\n",
       "   0.41263023018836975,\n",
       "   0.40722519159317017,\n",
       "   0.4437471032142639,\n",
       "   0.4076026678085327,\n",
       "   0.5017387866973877,\n",
       "   0.4698464870452881,\n",
       "   0.4103655517101288,\n",
       "   0.4480561316013336,\n",
       "   0.46727803349494934,\n",
       "   0.4407857060432434,\n",
       "   0.477183073759079,\n",
       "   0.497123122215271,\n",
       "   0.5126879215240479,\n",
       "   0.4770418405532837,\n",
       "   0.546531081199646,\n",
       "   0.45769140124320984,\n",
       "   0.44186800718307495,\n",
       "   0.47655072808265686,\n",
       "   0.5003026127815247,\n",
       "   0.49581122398376465,\n",
       "   0.46153900027275085,\n",
       "   0.4545802175998688,\n",
       "   0.473759263753891,\n",
       "   0.4930753707885742,\n",
       "   0.4599762260913849,\n",
       "   0.5342627763748169,\n",
       "   0.585822582244873,\n",
       "   0.44795095920562744,\n",
       "   0.4642954170703888,\n",
       "   0.5513179302215576,\n",
       "   0.48556771874427795,\n",
       "   0.49425598978996277,\n",
       "   0.5114468932151794,\n",
       "   0.4600079655647278,\n",
       "   0.45218315720558167,\n",
       "   0.49464645981788635,\n",
       "   0.4911448657512665,\n",
       "   0.47102469205856323,\n",
       "   0.5159637928009033,\n",
       "   0.39673668146133423,\n",
       "   0.45771294832229614,\n",
       "   0.4541708827018738,\n",
       "   0.44368991255760193,\n",
       "   0.4589267075061798,\n",
       "   0.43003514409065247,\n",
       "   0.5372317433357239,\n",
       "   0.4477062523365021,\n",
       "   0.4393005073070526,\n",
       "   0.44155189394950867,\n",
       "   0.558681070804596,\n",
       "   0.4667627513408661,\n",
       "   0.44316163659095764,\n",
       "   0.4113342761993408,\n",
       "   0.45545604825019836,\n",
       "   0.4778287708759308,\n",
       "   0.5069524049758911,\n",
       "   0.46328288316726685,\n",
       "   0.5382065176963806,\n",
       "   0.53669673204422,\n",
       "   0.5367549061775208,\n",
       "   0.44289007782936096,\n",
       "   0.38802722096443176,\n",
       "   0.49628153443336487,\n",
       "   0.5629752278327942,\n",
       "   0.4167761206626892,\n",
       "   0.47458750009536743,\n",
       "   0.5689218044281006,\n",
       "   0.4906216561794281,\n",
       "   0.43378567695617676,\n",
       "   0.45575669407844543,\n",
       "   0.4509148597717285,\n",
       "   0.4614444077014923,\n",
       "   0.44750019907951355,\n",
       "   0.4838373363018036,\n",
       "   0.41993775963783264,\n",
       "   0.43750303983688354,\n",
       "   0.4980228543281555,\n",
       "   0.4091164171695709,\n",
       "   0.4543057084083557,\n",
       "   0.5074611902236938,\n",
       "   0.585673987865448,\n",
       "   0.48436230421066284,\n",
       "   0.4553230404853821,\n",
       "   0.42629748582839966,\n",
       "   0.42400458455085754,\n",
       "   0.4465671181678772,\n",
       "   0.48001763224601746,\n",
       "   0.36625829339027405,\n",
       "   0.39125868678092957,\n",
       "   0.4433990716934204,\n",
       "   0.5740881562232971,\n",
       "   0.46922340989112854,\n",
       "   0.4586888253688812,\n",
       "   0.46730414032936096,\n",
       "   0.464428573846817,\n",
       "   0.41526517271995544,\n",
       "   0.45144322514533997,\n",
       "   0.5143464803695679,\n",
       "   0.43153804540634155,\n",
       "   0.43075674772262573,\n",
       "   0.43322181701660156,\n",
       "   0.4203356206417084,\n",
       "   0.4647056758403778,\n",
       "   0.5316336154937744,\n",
       "   0.5126485228538513,\n",
       "   0.4098775386810303,\n",
       "   0.4279065728187561,\n",
       "   0.47387585043907166,\n",
       "   0.5630502700805664,\n",
       "   0.4778083264827728,\n",
       "   0.4632670283317566,\n",
       "   0.49704089760780334,\n",
       "   0.4149334728717804,\n",
       "   0.478076308965683,\n",
       "   0.47227850556373596,\n",
       "   0.4757232964038849,\n",
       "   0.44581010937690735,\n",
       "   0.4541673958301544,\n",
       "   0.48142075538635254,\n",
       "   0.39828380942344666,\n",
       "   0.582608163356781,\n",
       "   0.5383822321891785,\n",
       "   0.42338791489601135,\n",
       "   0.41736409068107605,\n",
       "   0.44112929701805115,\n",
       "   0.5156930088996887,\n",
       "   0.4705718755722046,\n",
       "   0.4765174686908722,\n",
       "   0.5030533075332642,\n",
       "   0.516089677810669,\n",
       "   0.4713366627693176,\n",
       "   0.5115182399749756,\n",
       "   0.5019985437393188,\n",
       "   0.5030316114425659,\n",
       "   0.4817339777946472,\n",
       "   0.41147589683532715,\n",
       "   0.5400096774101257,\n",
       "   0.5148816704750061,\n",
       "   0.46739715337753296,\n",
       "   0.4420028626918793,\n",
       "   0.5141329765319824,\n",
       "   0.4801243841648102,\n",
       "   0.46885550022125244,\n",
       "   0.40233057737350464,\n",
       "   0.38397470116615295,\n",
       "   0.46938955783843994,\n",
       "   0.4175780415534973,\n",
       "   0.4163716733455658,\n",
       "   0.43311914801597595,\n",
       "   0.45092055201530457,\n",
       "   0.4863716661930084,\n",
       "   0.5303295850753784,\n",
       "   0.5181127786636353,\n",
       "   0.5985797047615051,\n",
       "   0.44701245427131653,\n",
       "   0.5003414750099182,\n",
       "   0.4045242965221405,\n",
       "   0.4142306447029114,\n",
       "   0.5026703476905823,\n",
       "   0.41421180963516235,\n",
       "   0.5001723766326904,\n",
       "   0.4106568396091461,\n",
       "   0.4258003830909729,\n",
       "   0.35602477192878723,\n",
       "   0.43859925866127014,\n",
       "   0.5380758047103882,\n",
       "   0.4467082619667053,\n",
       "   0.5564954876899719,\n",
       "   0.411245733499527,\n",
       "   0.4126942753791809,\n",
       "   0.45239388942718506,\n",
       "   0.4695780873298645,\n",
       "   0.5226089358329773,\n",
       "   0.44393572211265564,\n",
       "   0.40178146958351135,\n",
       "   0.4439229965209961,\n",
       "   0.4895382821559906,\n",
       "   0.46609067916870117,\n",
       "   0.3754938840866089,\n",
       "   0.5726337432861328,\n",
       "   0.4945805072784424,\n",
       "   0.5254704356193542,\n",
       "   0.4647439122200012,\n",
       "   0.43683061003685,\n",
       "   0.5000056028366089,\n",
       "   0.47285163402557373,\n",
       "   0.460342675447464,\n",
       "   0.4181476831436157,\n",
       "   0.4293094873428345,\n",
       "   0.43045374751091003,\n",
       "   0.5048008561134338,\n",
       "   0.5618619322776794,\n",
       "   0.44014835357666016,\n",
       "   0.40026482939720154,\n",
       "   0.507654070854187,\n",
       "   0.49452558159828186,\n",
       "   0.43921616673469543,\n",
       "   0.4608030319213867,\n",
       "   0.4261293113231659,\n",
       "   0.5493602156639099,\n",
       "   0.5644981861114502,\n",
       "   0.5283423066139221,\n",
       "   0.4644683003425598,\n",
       "   0.4072413742542267,\n",
       "   0.49935492873191833,\n",
       "   0.5381987690925598,\n",
       "   0.4536150097846985,\n",
       "   0.4132642149925232,\n",
       "   0.4662250578403473,\n",
       "   0.44877803325653076,\n",
       "   0.42073068022727966,\n",
       "   0.44329866766929626,\n",
       "   0.48413750529289246,\n",
       "   0.5705661773681641,\n",
       "   0.4149133861064911,\n",
       "   0.46718883514404297,\n",
       "   0.42406755685806274,\n",
       "   0.43887126445770264,\n",
       "   0.5266804695129395,\n",
       "   0.4430381655693054,\n",
       "   0.42783358693122864,\n",
       "   0.5423545241355896,\n",
       "   0.45213958621025085,\n",
       "   0.4706885516643524,\n",
       "   0.46399959921836853,\n",
       "   0.49634310603141785,\n",
       "   0.48101845383644104,\n",
       "   0.45511770248413086,\n",
       "   0.4285271465778351,\n",
       "   0.49890318512916565,\n",
       "   0.40526971220970154,\n",
       "   0.42725425958633423,\n",
       "   0.5056594610214233,\n",
       "   0.4667476415634155,\n",
       "   0.4796775281429291,\n",
       "   0.47725024819374084,\n",
       "   0.4371181130409241,\n",
       "   0.3790381848812103,\n",
       "   0.5518373250961304,\n",
       "   0.4303244352340698,\n",
       "   0.4481888711452484,\n",
       "   0.4938909411430359,\n",
       "   0.3616825044155121,\n",
       "   0.4269528090953827,\n",
       "   0.43026670813560486,\n",
       "   0.4218097925186157,\n",
       "   0.5542663335800171,\n",
       "   0.4097371995449066,\n",
       "   0.5074535012245178,\n",
       "   0.44537416100502014,\n",
       "   0.4468422532081604,\n",
       "   0.5168869495391846,\n",
       "   0.4269096851348877,\n",
       "   0.4431940019130707,\n",
       "   0.39991921186447144,\n",
       "   0.4489068388938904,\n",
       "   0.4155322313308716,\n",
       "   0.4559508264064789,\n",
       "   0.3727344870567322,\n",
       "   0.4288689196109772,\n",
       "   0.4302571713924408,\n",
       "   0.4231429398059845,\n",
       "   0.5158912539482117,\n",
       "   0.4667700529098511,\n",
       "   0.3820282220840454,\n",
       "   0.5474520921707153,\n",
       "   0.37724074721336365,\n",
       "   0.47771310806274414,\n",
       "   0.4049758017063141,\n",
       "   0.46826568245887756,\n",
       "   0.47120508551597595,\n",
       "   0.47126126289367676,\n",
       "   0.4332709014415741,\n",
       "   0.46069952845573425,\n",
       "   0.4734465181827545,\n",
       "   0.44578611850738525,\n",
       "   0.4189339578151703,\n",
       "   0.48390841484069824,\n",
       "   0.4245564937591553,\n",
       "   0.45154380798339844,\n",
       "   0.3876638114452362,\n",
       "   0.5100435018539429,\n",
       "   0.38136765360832214,\n",
       "   0.3629095256328583,\n",
       "   0.49055325984954834,\n",
       "   0.4207795560359955,\n",
       "   0.560520589351654,\n",
       "   0.443952739238739,\n",
       "   0.4752286970615387,\n",
       "   0.5149013996124268,\n",
       "   0.47028517723083496,\n",
       "   0.559863805770874,\n",
       "   0.47367745637893677,\n",
       "   0.4212225079536438,\n",
       "   0.4781174063682556,\n",
       "   0.46269139647483826,\n",
       "   0.5645525455474854,\n",
       "   0.43157070875167847,\n",
       "   0.4631011188030243,\n",
       "   0.4694931209087372,\n",
       "   0.4693576991558075,\n",
       "   0.4007229208946228,\n",
       "   0.5006445050239563,\n",
       "   0.4943769872188568,\n",
       "   0.46248161792755127,\n",
       "   0.3956778645515442,\n",
       "   0.613170862197876,\n",
       "   0.5150775909423828,\n",
       "   0.4482553005218506,\n",
       "   0.5536212921142578,\n",
       "   0.4896470308303833,\n",
       "   0.4972327947616577,\n",
       "   0.4893909990787506,\n",
       "   0.46955183148384094,\n",
       "   0.4970998167991638,\n",
       "   0.47572144865989685,\n",
       "   0.510647177696228,\n",
       "   0.43585336208343506,\n",
       "   0.4908267557621002,\n",
       "   0.48729878664016724,\n",
       "   0.48906686902046204,\n",
       "   0.49585551023483276,\n",
       "   0.5762742757797241,\n",
       "   0.409260094165802,\n",
       "   0.40984392166137695,\n",
       "   0.4607619643211365,\n",
       "   0.4863501787185669,\n",
       "   0.4134550094604492,\n",
       "   0.4653246998786926,\n",
       "   0.4106256365776062,\n",
       "   0.5109419822692871,\n",
       "   0.4555087387561798,\n",
       "   0.5729318261146545,\n",
       "   0.4392941892147064,\n",
       "   0.4596937298774719,\n",
       "   0.4951375126838684,\n",
       "   0.4914388656616211,\n",
       "   0.5544403791427612,\n",
       "   0.48952528834342957,\n",
       "   0.5380988717079163,\n",
       "   0.4518270194530487,\n",
       "   0.41869810223579407,\n",
       "   0.4433731734752655,\n",
       "   0.5594251155853271,\n",
       "   0.46453672647476196,\n",
       "   0.5225232839584351,\n",
       "   0.44647571444511414,\n",
       "   0.43305715918540955,\n",
       "   0.42104974389076233,\n",
       "   0.4061901271343231,\n",
       "   0.4925963580608368,\n",
       "   0.4678426682949066,\n",
       "   0.41435667872428894,\n",
       "   0.46838778257369995,\n",
       "   0.5750136375427246,\n",
       "   0.46224546432495117,\n",
       "   0.4637156128883362,\n",
       "   0.4300464987754822,\n",
       "   0.4412841796875,\n",
       "   0.48474782705307007,\n",
       "   0.5084217190742493,\n",
       "   0.4889838695526123,\n",
       "   0.4185469448566437,\n",
       "   0.534982442855835,\n",
       "   0.47986775636672974,\n",
       "   0.41229909658432007,\n",
       "   0.4537000060081482,\n",
       "   0.5351336598396301,\n",
       "   0.46226370334625244,\n",
       "   0.42200589179992676,\n",
       "   0.4151557683944702,\n",
       "   0.4433411955833435,\n",
       "   0.47971364855766296,\n",
       "   0.4862924814224243,\n",
       "   0.4697505235671997,\n",
       "   0.5566526055335999,\n",
       "   0.4477440118789673,\n",
       "   0.4838835895061493,\n",
       "   0.5019991397857666,\n",
       "   0.44764432311058044,\n",
       "   0.5714085698127747,\n",
       "   0.4110344648361206,\n",
       "   0.46858519315719604,\n",
       "   0.4755672514438629,\n",
       "   0.4898800253868103,\n",
       "   0.48738738894462585,\n",
       "   0.4685245156288147,\n",
       "   0.49889451265335083,\n",
       "   0.5042293667793274,\n",
       "   0.39631277322769165,\n",
       "   0.4762176275253296,\n",
       "   0.46402713656425476,\n",
       "   0.48728084564208984,\n",
       "   0.45246976613998413,\n",
       "   0.48447728157043457,\n",
       "   0.509228527545929,\n",
       "   0.4137595295906067,\n",
       "   0.49515488743782043,\n",
       "   0.5129886865615845,\n",
       "   0.5278017520904541,\n",
       "   0.5260509252548218,\n",
       "   0.46086573600769043,\n",
       "   0.4432666301727295,\n",
       "   0.47843873500823975,\n",
       "   0.39114731550216675,\n",
       "   0.49484983086586,\n",
       "   0.4699697196483612,\n",
       "   0.4276208281517029,\n",
       "   0.42224884033203125,\n",
       "   0.38689032196998596,\n",
       "   0.5272800922393799,\n",
       "   0.4971277415752411,\n",
       "   0.4600009024143219,\n",
       "   0.4126475751399994,\n",
       "   0.4514695703983307,\n",
       "   0.4555165767669678,\n",
       "   0.4992275834083557,\n",
       "   0.407564252614975,\n",
       "   0.4395999312400818,\n",
       "   0.45096564292907715,\n",
       "   0.486911803483963,\n",
       "   0.4369822144508362,\n",
       "   0.4413771331310272,\n",
       "   0.46014639735221863,\n",
       "   0.4359756410121918,\n",
       "   0.433510422706604,\n",
       "   0.5994650721549988,\n",
       "   0.481325626373291,\n",
       "   0.44821327924728394,\n",
       "   0.49552860856056213,\n",
       "   0.4757126271724701,\n",
       "   0.393322616815567,\n",
       "   0.5363710522651672,\n",
       "   0.40857964754104614,\n",
       "   0.4771173596382141,\n",
       "   0.5013156533241272,\n",
       "   0.4246830642223358,\n",
       "   0.47375744581222534,\n",
       "   0.5627323389053345,\n",
       "   0.41034966707229614,\n",
       "   0.44290798902511597,\n",
       "   0.39853227138519287,\n",
       "   0.48334643244743347,\n",
       "   0.4729592502117157,\n",
       "   0.541728138923645,\n",
       "   0.43668317794799805,\n",
       "   0.5811662673950195,\n",
       "   0.42398324608802795,\n",
       "   0.48363980650901794,\n",
       "   0.4313565194606781,\n",
       "   0.517256498336792,\n",
       "   0.4811875522136688,\n",
       "   0.4408203661441803,\n",
       "   0.5285186171531677,\n",
       "   0.483243465423584,\n",
       "   0.39624959230422974,\n",
       "   0.5029734969139099,\n",
       "   0.4751090407371521,\n",
       "   0.5128815174102783,\n",
       "   0.43334904313087463,\n",
       "   0.484537810087204,\n",
       "   0.4985142946243286],\n",
       "  'train_ff_loss': [0.3959232568740845,\n",
       "   0.35991615056991577,\n",
       "   0.4014438986778259,\n",
       "   0.4087599217891693,\n",
       "   0.4314974248409271,\n",
       "   0.4011450707912445,\n",
       "   0.37528881430625916,\n",
       "   0.3865806460380554,\n",
       "   0.4108811020851135,\n",
       "   0.36387336254119873,\n",
       "   0.4029316306114197,\n",
       "   0.339324951171875,\n",
       "   0.4332285225391388,\n",
       "   0.3310723304748535,\n",
       "   0.4013848602771759,\n",
       "   0.38300445675849915,\n",
       "   0.35021886229515076,\n",
       "   0.4465956389904022,\n",
       "   0.3320077061653137,\n",
       "   0.365558385848999,\n",
       "   0.32629531621932983,\n",
       "   0.4474046528339386,\n",
       "   0.40979209542274475,\n",
       "   0.4207942485809326,\n",
       "   0.3505244553089142,\n",
       "   0.3611774146556854,\n",
       "   0.38734665513038635,\n",
       "   0.4692036211490631,\n",
       "   0.36322951316833496,\n",
       "   0.3782137334346771,\n",
       "   0.346662312746048,\n",
       "   0.4108361005783081,\n",
       "   0.3749423623085022,\n",
       "   0.33297601342201233,\n",
       "   0.32960066199302673,\n",
       "   0.35769718885421753,\n",
       "   0.3921480178833008,\n",
       "   0.3909241557121277,\n",
       "   0.40597087144851685,\n",
       "   0.40265312790870667,\n",
       "   0.34603068232536316,\n",
       "   0.33428674936294556,\n",
       "   0.3862820267677307,\n",
       "   0.3598988354206085,\n",
       "   0.36142396926879883,\n",
       "   0.3854225277900696,\n",
       "   0.3660214841365814,\n",
       "   0.34960922598838806,\n",
       "   0.28979557752609253,\n",
       "   0.39994940161705017,\n",
       "   0.28971683979034424,\n",
       "   0.3637251853942871,\n",
       "   0.5092678070068359,\n",
       "   0.3631274998188019,\n",
       "   0.4539078176021576,\n",
       "   0.32815638184547424,\n",
       "   0.4640817940235138,\n",
       "   0.38349783420562744,\n",
       "   0.37502768635749817,\n",
       "   0.37991786003112793,\n",
       "   0.5368456244468689,\n",
       "   0.37835070490837097,\n",
       "   0.4730079174041748,\n",
       "   0.3951064348220825,\n",
       "   0.3263894319534302,\n",
       "   0.38028064370155334,\n",
       "   0.3369397819042206,\n",
       "   0.3781188428401947,\n",
       "   0.40753060579299927,\n",
       "   0.48145464062690735,\n",
       "   0.482211709022522,\n",
       "   0.4572623372077942,\n",
       "   0.3015092611312866,\n",
       "   0.37322792410850525,\n",
       "   0.36566591262817383,\n",
       "   0.4560234844684601,\n",
       "   0.30743494629859924,\n",
       "   0.33608707785606384,\n",
       "   0.3407973349094391,\n",
       "   0.4129333198070526,\n",
       "   0.40989798307418823,\n",
       "   0.3621077239513397,\n",
       "   0.38376858830451965,\n",
       "   0.3580431342124939,\n",
       "   0.45783984661102295,\n",
       "   0.3469335436820984,\n",
       "   0.4144706726074219,\n",
       "   0.39267247915267944,\n",
       "   0.4026497006416321,\n",
       "   0.36123737692832947,\n",
       "   0.3073616623878479,\n",
       "   0.3486276865005493,\n",
       "   0.3565121293067932,\n",
       "   0.419655442237854,\n",
       "   0.35744261741638184,\n",
       "   0.3574851155281067,\n",
       "   0.35083043575286865,\n",
       "   0.3597263991832733,\n",
       "   0.407736212015152,\n",
       "   0.42262178659439087,\n",
       "   0.3821592330932617,\n",
       "   0.34047508239746094,\n",
       "   0.4400412440299988,\n",
       "   0.42956990003585815,\n",
       "   0.35736584663391113,\n",
       "   0.3366803228855133,\n",
       "   0.37663373351097107,\n",
       "   0.4446759521961212,\n",
       "   0.3460560441017151,\n",
       "   0.3258214592933655,\n",
       "   0.39530301094055176,\n",
       "   0.40759333968162537,\n",
       "   0.32972609996795654,\n",
       "   0.3279322683811188,\n",
       "   0.34926870465278625,\n",
       "   0.42633044719696045,\n",
       "   0.36668661236763,\n",
       "   0.29240041971206665,\n",
       "   0.39772406220436096,\n",
       "   0.31966572999954224,\n",
       "   0.34488120675086975,\n",
       "   0.4323195517063141,\n",
       "   0.4069441854953766,\n",
       "   0.4100287854671478,\n",
       "   0.32760700583457947,\n",
       "   0.3311326205730438,\n",
       "   0.3759307563304901,\n",
       "   0.4083912968635559,\n",
       "   0.36363399028778076,\n",
       "   0.3294806480407715,\n",
       "   0.3730885088443756,\n",
       "   0.3831792175769806,\n",
       "   0.2872670888900757,\n",
       "   0.3130266070365906,\n",
       "   0.40939080715179443,\n",
       "   0.3673223853111267,\n",
       "   0.3960883319377899,\n",
       "   0.42892441153526306,\n",
       "   0.32767313718795776,\n",
       "   0.41660425066947937,\n",
       "   0.37884101271629333,\n",
       "   0.3864634037017822,\n",
       "   0.3968735337257385,\n",
       "   0.4436756670475006,\n",
       "   0.37190374732017517,\n",
       "   0.38951900601387024,\n",
       "   0.40256303548812866,\n",
       "   0.39049625396728516,\n",
       "   0.34804767370224,\n",
       "   0.4310949742794037,\n",
       "   0.3810184597969055,\n",
       "   0.4544834494590759,\n",
       "   0.40683287382125854,\n",
       "   0.42766231298446655,\n",
       "   0.41295596957206726,\n",
       "   0.33228975534439087,\n",
       "   0.4195755422115326,\n",
       "   0.3924314081668854,\n",
       "   0.40069541335105896,\n",
       "   0.3432644009590149,\n",
       "   0.34680864214897156,\n",
       "   0.34669724106788635,\n",
       "   0.3463853895664215,\n",
       "   0.42229363322257996,\n",
       "   0.41652461886405945,\n",
       "   0.3879758417606354,\n",
       "   0.34314054250717163,\n",
       "   0.2771449089050293,\n",
       "   0.38473621010780334,\n",
       "   0.4014343321323395,\n",
       "   0.3613019287586212,\n",
       "   0.3264572024345398,\n",
       "   0.3965797424316406,\n",
       "   0.406596302986145,\n",
       "   0.3932463526725769,\n",
       "   0.3743963837623596,\n",
       "   0.3326934278011322,\n",
       "   0.2971617877483368,\n",
       "   0.3681960701942444,\n",
       "   0.32609567046165466,\n",
       "   0.39277252554893494,\n",
       "   0.35623279213905334,\n",
       "   0.37117311358451843,\n",
       "   0.3345731794834137,\n",
       "   0.4116269052028656,\n",
       "   0.3930375576019287,\n",
       "   0.3205206096172333,\n",
       "   0.294218510389328,\n",
       "   0.31331726908683777,\n",
       "   0.3591908812522888,\n",
       "   0.39294886589050293,\n",
       "   0.3507344722747803,\n",
       "   0.3556469678878784,\n",
       "   0.3731327950954437,\n",
       "   0.4754718244075775,\n",
       "   0.32614371180534363,\n",
       "   0.36138981580734253,\n",
       "   0.49257344007492065,\n",
       "   0.3432399332523346,\n",
       "   0.3676704168319702,\n",
       "   0.38367289304733276,\n",
       "   0.349882572889328,\n",
       "   0.43332627415657043,\n",
       "   0.38631606101989746,\n",
       "   0.29738664627075195,\n",
       "   0.31078484654426575,\n",
       "   0.36886662244796753,\n",
       "   0.35077691078186035,\n",
       "   0.36943501234054565,\n",
       "   0.43883323669433594,\n",
       "   0.3681045472621918,\n",
       "   0.4291258752346039,\n",
       "   0.3421768248081207,\n",
       "   0.3550322949886322,\n",
       "   0.40157026052474976,\n",
       "   0.3618569076061249,\n",
       "   0.34876635670661926,\n",
       "   0.38305819034576416,\n",
       "   0.3717474043369293,\n",
       "   0.33756595849990845,\n",
       "   0.4429801106452942,\n",
       "   0.40446916222572327,\n",
       "   0.3444564640522003,\n",
       "   0.34250709414482117,\n",
       "   0.40246346592903137,\n",
       "   0.3671249449253082,\n",
       "   0.32949191331863403,\n",
       "   0.3947312533855438,\n",
       "   0.3468198776245117,\n",
       "   0.3676663935184479,\n",
       "   0.33133628964424133,\n",
       "   0.4068853557109833,\n",
       "   0.3649667203426361,\n",
       "   0.3637763261795044,\n",
       "   0.369682252407074,\n",
       "   0.3615250289440155,\n",
       "   0.36897480487823486,\n",
       "   0.37077054381370544,\n",
       "   0.3608124852180481,\n",
       "   0.3570203483104706,\n",
       "   0.34703329205513,\n",
       "   0.4220101833343506,\n",
       "   0.3614111840724945,\n",
       "   0.3462437391281128,\n",
       "   0.43606996536254883,\n",
       "   0.38806334137916565,\n",
       "   0.35241836309432983,\n",
       "   0.4015285074710846,\n",
       "   0.37040233612060547,\n",
       "   0.3664414584636688,\n",
       "   0.409200519323349,\n",
       "   0.3362443149089813,\n",
       "   0.367928147315979,\n",
       "   0.3759819567203522,\n",
       "   0.3283904790878296,\n",
       "   0.35804983973503113,\n",
       "   0.43911948800086975,\n",
       "   0.30853593349456787,\n",
       "   0.36066654324531555,\n",
       "   0.41449642181396484,\n",
       "   0.33643463253974915,\n",
       "   0.26561209559440613,\n",
       "   0.30549356341362,\n",
       "   0.3794640302658081,\n",
       "   0.373484343290329,\n",
       "   0.3645719289779663,\n",
       "   0.3885135054588318,\n",
       "   0.4356911778450012,\n",
       "   0.3254188001155853,\n",
       "   0.3288496434688568,\n",
       "   0.4333012104034424,\n",
       "   0.4141489863395691,\n",
       "   0.383003294467926,\n",
       "   0.3906359076499939,\n",
       "   0.40679362416267395,\n",
       "   0.3871808350086212,\n",
       "   0.3520262837409973,\n",
       "   0.4236617386341095,\n",
       "   0.35423609614372253,\n",
       "   0.3696480393409729,\n",
       "   0.3605848550796509,\n",
       "   0.33395150303840637,\n",
       "   0.3722858130931854,\n",
       "   0.3876066505908966,\n",
       "   0.38677096366882324,\n",
       "   0.4086907207965851,\n",
       "   0.4033636450767517,\n",
       "   0.32170701026916504,\n",
       "   0.35238170623779297,\n",
       "   0.3641047179698944,\n",
       "   0.3500252366065979,\n",
       "   0.33790117502212524,\n",
       "   0.4146222770214081,\n",
       "   0.3289104402065277,\n",
       "   0.37886133790016174,\n",
       "   0.36599183082580566,\n",
       "   0.38452035188674927,\n",
       "   0.3735564351081848,\n",
       "   0.41157495975494385,\n",
       "   0.2796405255794525,\n",
       "   0.34125661849975586,\n",
       "   0.31688690185546875,\n",
       "   0.36087316274642944,\n",
       "   0.33413010835647583,\n",
       "   0.3203667104244232,\n",
       "   0.3630642294883728,\n",
       "   0.3406722843647003,\n",
       "   0.3087054491043091,\n",
       "   0.3351433277130127,\n",
       "   0.35310181975364685,\n",
       "   0.35315269231796265,\n",
       "   0.3235597014427185,\n",
       "   0.4363636374473572,\n",
       "   0.3842213451862335,\n",
       "   0.26725122332572937,\n",
       "   0.4245535433292389,\n",
       "   0.3856305181980133,\n",
       "   0.28389498591423035,\n",
       "   0.43001770973205566,\n",
       "   0.37418022751808167,\n",
       "   0.4089435040950775,\n",
       "   0.4022347033023834,\n",
       "   0.3858175277709961,\n",
       "   0.3198155164718628,\n",
       "   0.3936634659767151,\n",
       "   0.4194546341896057,\n",
       "   0.35428228974342346,\n",
       "   0.3708229064941406,\n",
       "   0.2801949381828308,\n",
       "   0.38783571124076843,\n",
       "   0.36554795503616333,\n",
       "   0.4179571270942688,\n",
       "   0.36319634318351746,\n",
       "   0.2882080078125,\n",
       "   0.31704655289649963,\n",
       "   0.37098896503448486,\n",
       "   0.3105258047580719,\n",
       "   0.396375447511673,\n",
       "   0.4459267854690552,\n",
       "   0.4236968159675598,\n",
       "   0.29360631108283997,\n",
       "   0.32661309838294983,\n",
       "   0.4230319559574127,\n",
       "   0.3853321373462677,\n",
       "   0.4304981529712677,\n",
       "   0.33156296610832214,\n",
       "   0.3368825614452362,\n",
       "   0.43185287714004517,\n",
       "   0.32016661763191223,\n",
       "   0.33003032207489014,\n",
       "   0.37963515520095825,\n",
       "   0.4274980127811432,\n",
       "   0.40679535269737244,\n",
       "   0.37742725014686584,\n",
       "   0.37901562452316284,\n",
       "   0.3540065884590149,\n",
       "   0.3618660867214203,\n",
       "   0.3259042799472809,\n",
       "   0.3693566918373108,\n",
       "   0.3420713245868683,\n",
       "   0.4130091667175293,\n",
       "   0.34315571188926697,\n",
       "   0.34473973512649536,\n",
       "   0.34839922189712524,\n",
       "   0.3375323712825775,\n",
       "   0.3544367849826813,\n",
       "   0.42291054129600525,\n",
       "   0.3571007251739502,\n",
       "   0.29586854577064514,\n",
       "   0.3115794062614441,\n",
       "   0.38176771998405457,\n",
       "   0.4470818042755127,\n",
       "   0.3428214490413666,\n",
       "   0.33393052220344543,\n",
       "   0.39755120873451233,\n",
       "   0.41893476247787476,\n",
       "   0.4976212680339813,\n",
       "   0.3973003625869751,\n",
       "   0.36790359020233154,\n",
       "   0.36258962750434875,\n",
       "   0.3527062237262726,\n",
       "   0.3888228237628937,\n",
       "   0.3853347897529602,\n",
       "   0.3370601236820221,\n",
       "   0.33395248651504517,\n",
       "   0.3104593753814697,\n",
       "   0.4111935794353485,\n",
       "   0.34804683923721313,\n",
       "   0.37625542283058167,\n",
       "   0.3849763572216034,\n",
       "   0.28684335947036743,\n",
       "   0.3137180507183075,\n",
       "   0.31397852301597595,\n",
       "   0.3588207960128784,\n",
       "   0.36989736557006836,\n",
       "   0.31983840465545654,\n",
       "   0.3148650825023651,\n",
       "   0.3675880432128906,\n",
       "   0.37982961535453796,\n",
       "   0.3679640591144562,\n",
       "   0.39095571637153625,\n",
       "   0.42242002487182617,\n",
       "   0.3615042269229889,\n",
       "   0.29413285851478577,\n",
       "   0.4236970841884613,\n",
       "   0.3802482485771179,\n",
       "   0.4291503429412842,\n",
       "   0.34025683999061584,\n",
       "   0.3508032560348511,\n",
       "   0.40556037425994873,\n",
       "   0.3804396688938141,\n",
       "   0.3511331081390381,\n",
       "   0.4132812023162842,\n",
       "   0.3319583237171173,\n",
       "   0.3905620872974396,\n",
       "   0.5181139707565308,\n",
       "   0.313514769077301,\n",
       "   0.3487720787525177,\n",
       "   0.3208765387535095,\n",
       "   0.345578670501709,\n",
       "   0.2878079116344452,\n",
       "   0.3709719777107239,\n",
       "   0.3757762312889099,\n",
       "   0.3700065612792969,\n",
       "   0.3653202950954437,\n",
       "   0.44120490550994873,\n",
       "   0.37513020634651184,\n",
       "   0.2879006862640381,\n",
       "   0.38467010855674744,\n",
       "   0.3606261909008026,\n",
       "   0.35567569732666016,\n",
       "   0.3609839677810669,\n",
       "   0.40591615438461304,\n",
       "   0.37185513973236084,\n",
       "   0.4210233688354492,\n",
       "   0.3757016360759735,\n",
       "   0.35658082365989685,\n",
       "   0.31491538882255554,\n",
       "   0.42914852499961853,\n",
       "   0.4950929582118988,\n",
       "   0.4027799367904663,\n",
       "   0.34717848896980286,\n",
       "   0.3612746596336365,\n",
       "   0.30847814679145813,\n",
       "   0.3935735821723938,\n",
       "   0.44146928191185,\n",
       "   0.31549617648124695,\n",
       "   0.34581658244132996,\n",
       "   0.3673270642757416,\n",
       "   0.3229760527610779,\n",
       "   0.3681420087814331,\n",
       "   0.43944939970970154,\n",
       "   0.340261310338974,\n",
       "   0.35925406217575073,\n",
       "   0.3638319671154022,\n",
       "   0.3615061640739441,\n",
       "   0.3272167444229126,\n",
       "   0.3205941319465637,\n",
       "   0.3517621159553528,\n",
       "   0.28201255202293396,\n",
       "   0.3377808928489685,\n",
       "   0.3497905433177948,\n",
       "   0.2752666771411896,\n",
       "   0.4249705970287323,\n",
       "   0.3391781747341156,\n",
       "   0.3252008259296417,\n",
       "   0.38763895630836487,\n",
       "   0.32189303636550903,\n",
       "   0.409676194190979,\n",
       "   0.41791704297065735,\n",
       "   0.3088914155960083,\n",
       "   0.2661367356777191,\n",
       "   0.2917025089263916,\n",
       "   0.365610271692276,\n",
       "   0.4166810214519501,\n",
       "   0.42594113945961,\n",
       "   0.35896140336990356,\n",
       "   0.35005393624305725,\n",
       "   0.320818692445755,\n",
       "   0.27199649810791016,\n",
       "   0.46115097403526306,\n",
       "   0.32548195123672485,\n",
       "   0.334127277135849,\n",
       "   0.32889875769615173,\n",
       "   0.41697952151298523,\n",
       "   0.3331012427806854,\n",
       "   0.37758561968803406,\n",
       "   0.3713938891887665,\n",
       "   0.34623149037361145,\n",
       "   0.3039748966693878,\n",
       "   0.3075488209724426,\n",
       "   0.3442313075065613,\n",
       "   0.3853563666343689,\n",
       "   0.3213927447795868,\n",
       "   0.32204166054725647,\n",
       "   0.4849463403224945,\n",
       "   0.286702424287796,\n",
       "   0.33728477358818054,\n",
       "   0.43698298931121826,\n",
       "   0.33754462003707886]},\n",
       " 2: {'lr': 1e-06,\n",
       "  'best_loss_epoch': 482,\n",
       "  'best_acc_epoch': 0,\n",
       "  'best_r2_epoch': 424,\n",
       "  'pce_loss': [34.45597457885742,\n",
       "   23.615278244018555,\n",
       "   16.673017501831055,\n",
       "   12.44780158996582,\n",
       "   10.539545059204102,\n",
       "   8.74811840057373,\n",
       "   7.090530872344971,\n",
       "   5.903122901916504,\n",
       "   5.118913173675537,\n",
       "   4.457103252410889,\n",
       "   3.9343338012695312,\n",
       "   3.6320996284484863,\n",
       "   3.2846732139587402,\n",
       "   2.780595302581787,\n",
       "   2.4550225734710693,\n",
       "   2.237196445465088,\n",
       "   2.089566469192505,\n",
       "   2.0062153339385986,\n",
       "   1.9355742931365967,\n",
       "   1.9558659791946411,\n",
       "   1.8557450771331787,\n",
       "   1.8112348318099976,\n",
       "   1.728551983833313,\n",
       "   1.6928637027740479,\n",
       "   1.6416857242584229,\n",
       "   1.6152077913284302,\n",
       "   1.5890387296676636,\n",
       "   1.560178518295288,\n",
       "   1.5883502960205078,\n",
       "   1.591181993484497,\n",
       "   1.432857871055603,\n",
       "   1.4018843173980713,\n",
       "   1.2989124059677124,\n",
       "   1.3403319120407104,\n",
       "   1.3791592121124268,\n",
       "   1.334275484085083,\n",
       "   1.2853997945785522,\n",
       "   1.3174153566360474,\n",
       "   1.3026666641235352,\n",
       "   1.2967941761016846,\n",
       "   1.3133057355880737,\n",
       "   1.2411367893218994,\n",
       "   1.2019765377044678,\n",
       "   1.2007713317871094,\n",
       "   1.220433235168457,\n",
       "   1.158753752708435,\n",
       "   1.1799325942993164,\n",
       "   1.1432154178619385,\n",
       "   1.1682498455047607,\n",
       "   1.141290307044983,\n",
       "   1.1219078302383423,\n",
       "   1.0524414777755737,\n",
       "   1.0461645126342773,\n",
       "   1.063855528831482,\n",
       "   1.0606153011322021,\n",
       "   1.0139392614364624,\n",
       "   1.0313525199890137,\n",
       "   0.9465140104293823,\n",
       "   1.022215723991394,\n",
       "   1.0290056467056274,\n",
       "   0.9910436272621155,\n",
       "   1.0442572832107544,\n",
       "   1.1104377508163452,\n",
       "   1.1448888778686523,\n",
       "   1.1327893733978271,\n",
       "   1.1731871366500854,\n",
       "   1.1898345947265625,\n",
       "   1.2392834424972534,\n",
       "   1.1936568021774292,\n",
       "   1.1560312509536743,\n",
       "   1.1768702268600464,\n",
       "   1.1886484622955322,\n",
       "   1.2457820177078247,\n",
       "   1.1781061887741089,\n",
       "   1.1729613542556763,\n",
       "   1.1657615900039673,\n",
       "   1.1537381410598755,\n",
       "   1.20144784450531,\n",
       "   1.16444993019104,\n",
       "   1.189205288887024,\n",
       "   1.1750144958496094,\n",
       "   1.169872522354126,\n",
       "   1.2488222122192383,\n",
       "   1.2155243158340454,\n",
       "   1.1740922927856445,\n",
       "   1.1502273082733154,\n",
       "   1.1232421398162842,\n",
       "   1.1204441785812378,\n",
       "   1.144112467765808,\n",
       "   1.109761118888855,\n",
       "   1.141865611076355,\n",
       "   1.1409848928451538,\n",
       "   1.0802055597305298,\n",
       "   1.0781590938568115,\n",
       "   1.1000745296478271,\n",
       "   1.0644346475601196,\n",
       "   1.0071594715118408,\n",
       "   1.0300087928771973,\n",
       "   1.0446265935897827,\n",
       "   1.008717656135559,\n",
       "   1.0465320348739624,\n",
       "   1.0431692600250244,\n",
       "   1.0459133386611938,\n",
       "   1.061959147453308,\n",
       "   1.1219284534454346,\n",
       "   1.1475918292999268,\n",
       "   1.1320137977600098,\n",
       "   1.1541082859039307,\n",
       "   1.1606805324554443,\n",
       "   1.1517384052276611,\n",
       "   1.14192533493042,\n",
       "   1.158141016960144,\n",
       "   1.1429418325424194,\n",
       "   1.0940741300582886,\n",
       "   1.1048498153686523,\n",
       "   1.1141201257705688,\n",
       "   1.0977681875228882,\n",
       "   1.0836635828018188,\n",
       "   1.100201964378357,\n",
       "   1.1475101709365845,\n",
       "   1.075358271598816,\n",
       "   1.0643894672393799,\n",
       "   1.1224899291992188,\n",
       "   1.1637412309646606,\n",
       "   1.1578153371810913,\n",
       "   1.1606733798980713,\n",
       "   1.1329201459884644,\n",
       "   1.141923189163208,\n",
       "   1.0894972085952759,\n",
       "   1.1313201189041138,\n",
       "   1.183351755142212,\n",
       "   1.160835862159729,\n",
       "   1.1304751634597778,\n",
       "   1.076369285583496,\n",
       "   1.122802495956421,\n",
       "   1.1299294233322144,\n",
       "   1.2119245529174805,\n",
       "   1.2420028448104858,\n",
       "   1.2389090061187744,\n",
       "   1.233826994895935,\n",
       "   1.1750366687774658,\n",
       "   1.1368622779846191,\n",
       "   1.1478060483932495,\n",
       "   1.1269176006317139,\n",
       "   1.1693569421768188,\n",
       "   1.1026451587677002,\n",
       "   1.0911282300949097,\n",
       "   1.0117497444152832,\n",
       "   0.9971231818199158,\n",
       "   0.9677320122718811,\n",
       "   1.0213078260421753,\n",
       "   1.0133373737335205,\n",
       "   1.0234675407409668,\n",
       "   0.9804111123085022,\n",
       "   1.0045671463012695,\n",
       "   1.0121442079544067,\n",
       "   1.0420502424240112,\n",
       "   1.0557149648666382,\n",
       "   1.1132742166519165,\n",
       "   1.040767788887024,\n",
       "   1.0299854278564453,\n",
       "   1.0176869630813599,\n",
       "   1.0146775245666504,\n",
       "   0.9760623574256897,\n",
       "   1.0334231853485107,\n",
       "   1.0402891635894775,\n",
       "   1.0862985849380493,\n",
       "   1.0889323949813843,\n",
       "   1.0865347385406494,\n",
       "   1.0640106201171875,\n",
       "   1.0761151313781738,\n",
       "   1.0619802474975586,\n",
       "   1.0477827787399292,\n",
       "   1.0019844770431519,\n",
       "   1.002963662147522,\n",
       "   0.9324299097061157,\n",
       "   0.9442630410194397,\n",
       "   0.954193651676178,\n",
       "   0.9307199716567993,\n",
       "   0.9835442900657654,\n",
       "   1.0516091585159302,\n",
       "   1.1355599164962769,\n",
       "   1.138879418373108,\n",
       "   1.0676069259643555,\n",
       "   1.027316927909851,\n",
       "   1.0267510414123535,\n",
       "   1.087073564529419,\n",
       "   1.0474132299423218,\n",
       "   1.0009244680404663,\n",
       "   1.002155065536499,\n",
       "   1.0458407402038574,\n",
       "   1.0094224214553833,\n",
       "   1.0303956270217896,\n",
       "   1.0006952285766602,\n",
       "   0.997324526309967,\n",
       "   0.9709383249282837,\n",
       "   0.9641066789627075,\n",
       "   1.0094205141067505,\n",
       "   0.995286762714386,\n",
       "   0.9542969465255737,\n",
       "   1.0073224306106567,\n",
       "   1.0481339693069458,\n",
       "   1.0578601360321045,\n",
       "   1.094896912574768,\n",
       "   1.1460046768188477,\n",
       "   1.077404260635376,\n",
       "   1.0952316522598267,\n",
       "   1.107378602027893,\n",
       "   1.1021455526351929,\n",
       "   1.049782633781433,\n",
       "   1.117038369178772,\n",
       "   1.08725905418396,\n",
       "   1.1243785619735718,\n",
       "   1.182076334953308,\n",
       "   1.1374008655548096,\n",
       "   1.0851390361785889,\n",
       "   1.0633453130722046,\n",
       "   1.0753343105316162,\n",
       "   1.088182806968689,\n",
       "   1.0693809986114502,\n",
       "   1.0353244543075562,\n",
       "   0.9964879751205444,\n",
       "   1.0111156702041626,\n",
       "   1.0879610776901245,\n",
       "   1.1315079927444458,\n",
       "   1.1598504781723022,\n",
       "   1.1543614864349365,\n",
       "   1.121226191520691,\n",
       "   1.0685771703720093,\n",
       "   1.1243709325790405,\n",
       "   1.0459613800048828,\n",
       "   1.0132501125335693,\n",
       "   0.997367799282074,\n",
       "   0.9944290518760681,\n",
       "   0.9929057955741882,\n",
       "   0.9099448323249817,\n",
       "   0.9022733569145203,\n",
       "   0.8852198719978333,\n",
       "   0.91701740026474,\n",
       "   0.8849973082542419,\n",
       "   0.8513041138648987,\n",
       "   0.888584315776825,\n",
       "   0.8730975985527039,\n",
       "   0.8723158240318298,\n",
       "   0.9152818918228149,\n",
       "   0.9346322417259216,\n",
       "   0.9172886610031128,\n",
       "   0.8965012431144714,\n",
       "   0.9423097372055054,\n",
       "   0.9535382986068726,\n",
       "   0.9779670834541321,\n",
       "   1.0143227577209473,\n",
       "   1.0395092964172363,\n",
       "   0.9850801229476929,\n",
       "   0.9565693736076355,\n",
       "   0.945201575756073,\n",
       "   0.9764686226844788,\n",
       "   0.9754534959793091,\n",
       "   0.9456800222396851,\n",
       "   0.9755001068115234,\n",
       "   0.9666587114334106,\n",
       "   1.0289852619171143,\n",
       "   1.0280181169509888,\n",
       "   1.03183114528656,\n",
       "   1.0490950345993042,\n",
       "   1.0099741220474243,\n",
       "   0.995341956615448,\n",
       "   0.9823706746101379,\n",
       "   0.9898737668991089,\n",
       "   1.0061002969741821,\n",
       "   0.9401311278343201,\n",
       "   0.9437695741653442,\n",
       "   0.919495701789856,\n",
       "   0.9254847764968872,\n",
       "   0.932642936706543,\n",
       "   0.9125159382820129,\n",
       "   0.9606912732124329,\n",
       "   0.9878478646278381,\n",
       "   0.9832063913345337,\n",
       "   0.974254310131073,\n",
       "   0.9438129663467407,\n",
       "   0.9752054214477539,\n",
       "   0.9858568906784058,\n",
       "   0.956295907497406,\n",
       "   0.987575888633728,\n",
       "   0.9428440928459167,\n",
       "   0.9020568132400513,\n",
       "   0.9019090533256531,\n",
       "   0.8719111680984497,\n",
       "   0.9185601472854614,\n",
       "   0.9039903283119202,\n",
       "   0.9422191977500916,\n",
       "   0.9533929824829102,\n",
       "   0.9798292517662048,\n",
       "   1.0168418884277344,\n",
       "   1.0605554580688477,\n",
       "   1.060944676399231,\n",
       "   1.0909088850021362,\n",
       "   1.0220953226089478,\n",
       "   1.0096070766448975,\n",
       "   1.0102301836013794,\n",
       "   1.0189858675003052,\n",
       "   1.0104554891586304,\n",
       "   1.0206083059310913,\n",
       "   1.0146394968032837,\n",
       "   0.97770756483078,\n",
       "   0.9611244797706604,\n",
       "   0.9144791960716248,\n",
       "   0.9050474166870117,\n",
       "   0.9629241228103638,\n",
       "   1.0184663534164429,\n",
       "   1.008378028869629,\n",
       "   0.9511985778808594,\n",
       "   0.9620598554611206,\n",
       "   1.0127192735671997,\n",
       "   1.0007902383804321,\n",
       "   0.9153417348861694,\n",
       "   0.8958702683448792,\n",
       "   0.911824107170105,\n",
       "   0.878955066204071,\n",
       "   0.8977247476577759,\n",
       "   0.9243292212486267,\n",
       "   0.9606440663337708,\n",
       "   0.9436572194099426,\n",
       "   0.9859657287597656,\n",
       "   0.9265455603599548,\n",
       "   0.9519757628440857,\n",
       "   0.8962591290473938,\n",
       "   0.9044580459594727,\n",
       "   0.9524559378623962,\n",
       "   0.9820157885551453,\n",
       "   0.9805120825767517,\n",
       "   1.0042335987091064,\n",
       "   1.0336438417434692,\n",
       "   0.9976283311843872,\n",
       "   0.9765332937240601,\n",
       "   0.950633704662323,\n",
       "   0.9639118909835815,\n",
       "   0.9561501741409302,\n",
       "   0.9802168011665344,\n",
       "   0.9407294988632202,\n",
       "   0.9091801643371582,\n",
       "   0.9262550473213196,\n",
       "   0.9357473254203796,\n",
       "   0.9123033881187439,\n",
       "   0.841408908367157,\n",
       "   0.8484281897544861,\n",
       "   0.9026286005973816,\n",
       "   0.8902570605278015,\n",
       "   0.9034523963928223,\n",
       "   0.9080638885498047,\n",
       "   0.8785791397094727,\n",
       "   0.8585187792778015,\n",
       "   0.8691473007202148,\n",
       "   0.8800548315048218,\n",
       "   0.8571213483810425,\n",
       "   0.9171733856201172,\n",
       "   0.9315174221992493,\n",
       "   0.9094375967979431,\n",
       "   0.8874815106391907,\n",
       "   0.8599066138267517,\n",
       "   0.8477139472961426,\n",
       "   0.8030989766120911,\n",
       "   0.8426818251609802,\n",
       "   0.8292288184165955,\n",
       "   0.8290532231330872,\n",
       "   0.8107396364212036,\n",
       "   0.7875848412513733,\n",
       "   0.8319632411003113,\n",
       "   0.8918958306312561,\n",
       "   0.9004252552986145,\n",
       "   0.909299910068512,\n",
       "   0.8944708704948425,\n",
       "   0.9098685383796692,\n",
       "   1.0032004117965698,\n",
       "   0.984492301940918,\n",
       "   0.9897285103797913,\n",
       "   1.0007073879241943,\n",
       "   0.9693579077720642,\n",
       "   0.9711652994155884,\n",
       "   0.968147873878479,\n",
       "   0.9448051452636719,\n",
       "   0.91205894947052,\n",
       "   0.9139673113822937,\n",
       "   0.924424946308136,\n",
       "   0.9061632752418518,\n",
       "   0.8850326538085938,\n",
       "   0.8713144063949585,\n",
       "   0.8929315209388733,\n",
       "   0.8502978086471558,\n",
       "   0.8839016556739807,\n",
       "   0.8970180749893188,\n",
       "   0.8843247294425964,\n",
       "   0.9801982641220093,\n",
       "   0.944803774356842,\n",
       "   0.9679031372070312,\n",
       "   0.9474959969520569,\n",
       "   0.9344539642333984,\n",
       "   0.9095069169998169,\n",
       "   0.9192127585411072,\n",
       "   0.897830605506897,\n",
       "   0.9082449674606323,\n",
       "   0.9258615374565125,\n",
       "   0.923294723033905,\n",
       "   0.9031299352645874,\n",
       "   0.8638727068901062,\n",
       "   0.8490123748779297,\n",
       "   0.8623282313346863,\n",
       "   0.8724392056465149,\n",
       "   0.865407407283783,\n",
       "   0.8769655823707581,\n",
       "   0.9091581702232361,\n",
       "   0.8979662656784058,\n",
       "   0.83897465467453,\n",
       "   0.8272553086280823,\n",
       "   0.8403148055076599,\n",
       "   0.8592939972877502,\n",
       "   0.8273354172706604,\n",
       "   0.8087771534919739,\n",
       "   0.8009978532791138,\n",
       "   0.7639895677566528,\n",
       "   0.8096826672554016,\n",
       "   0.8292924165725708,\n",
       "   0.8503193259239197,\n",
       "   0.8380113244056702,\n",
       "   0.7972890734672546,\n",
       "   0.7841421961784363,\n",
       "   0.7895373106002808,\n",
       "   0.7370404005050659,\n",
       "   0.7978840470314026,\n",
       "   0.7706073522567749,\n",
       "   0.75274258852005,\n",
       "   0.7610992789268494,\n",
       "   0.7512598037719727,\n",
       "   0.7473363280296326,\n",
       "   0.7739198803901672,\n",
       "   0.7682914733886719,\n",
       "   0.7628422975540161,\n",
       "   0.7652996182441711,\n",
       "   0.7603253126144409,\n",
       "   0.7681760787963867,\n",
       "   0.7713185548782349,\n",
       "   0.7730597257614136,\n",
       "   0.7603110671043396,\n",
       "   0.7575564384460449,\n",
       "   0.8080540895462036,\n",
       "   0.8139470219612122,\n",
       "   0.7834253907203674,\n",
       "   0.8106732368469238,\n",
       "   0.8538134694099426,\n",
       "   0.8792462348937988,\n",
       "   0.8860874772071838,\n",
       "   0.8670457005500793,\n",
       "   0.8734250664710999,\n",
       "   0.8663134574890137,\n",
       "   0.8991146683692932,\n",
       "   0.812466561794281,\n",
       "   0.8323764801025391,\n",
       "   0.799397349357605,\n",
       "   0.7910948991775513,\n",
       "   0.7896203994750977,\n",
       "   0.8143707513809204,\n",
       "   0.8173661231994629,\n",
       "   0.8218836188316345,\n",
       "   0.7759324312210083,\n",
       "   0.8274821043014526,\n",
       "   0.8496370315551758,\n",
       "   0.8515041470527649,\n",
       "   0.8613511919975281,\n",
       "   0.8565901517868042,\n",
       "   0.8478866219520569,\n",
       "   0.8252986669540405,\n",
       "   0.8218414187431335,\n",
       "   0.8348016142845154,\n",
       "   0.8139923214912415,\n",
       "   0.8323511481285095,\n",
       "   0.8551352620124817,\n",
       "   0.8339608311653137,\n",
       "   0.8989731669425964,\n",
       "   0.8949701189994812,\n",
       "   0.8477460741996765,\n",
       "   0.7871630787849426,\n",
       "   0.7404696345329285,\n",
       "   0.7730662226676941,\n",
       "   0.7935318350791931,\n",
       "   0.7820301651954651,\n",
       "   0.7425580620765686,\n",
       "   0.7642249464988708,\n",
       "   0.7728506922721863,\n",
       "   0.7751864790916443,\n",
       "   0.8423553705215454,\n",
       "   0.8647670745849609,\n",
       "   0.8817237019538879,\n",
       "   0.8794293999671936,\n",
       "   0.8789615035057068,\n",
       "   0.8536484837532043,\n",
       "   0.832905650138855,\n",
       "   0.8569253087043762,\n",
       "   0.8635357618331909,\n",
       "   0.9051401019096375],\n",
       "  'voc_loss': [0.41758331656455994,\n",
       "   0.3613945543766022,\n",
       "   0.1657157689332962,\n",
       "   0.1976018249988556,\n",
       "   0.23430359363555908,\n",
       "   0.20276501774787903,\n",
       "   0.14379654824733734,\n",
       "   0.1843690425157547,\n",
       "   0.17164480686187744,\n",
       "   0.14339792728424072,\n",
       "   0.17471157014369965,\n",
       "   0.16291287541389465,\n",
       "   0.16593776643276215,\n",
       "   0.13857761025428772,\n",
       "   0.15348823368549347,\n",
       "   0.13823018968105316,\n",
       "   0.1315392553806305,\n",
       "   0.1432357132434845,\n",
       "   0.1296406090259552,\n",
       "   0.12769843637943268,\n",
       "   0.10687656700611115,\n",
       "   0.09809552133083344,\n",
       "   0.09572769701480865,\n",
       "   0.10742353647947311,\n",
       "   0.09556417167186737,\n",
       "   0.09641510248184204,\n",
       "   0.09799505770206451,\n",
       "   0.10955215990543365,\n",
       "   0.09774845838546753,\n",
       "   0.0986352190375328,\n",
       "   0.09507276117801666,\n",
       "   0.11173070967197418,\n",
       "   0.10104259103536606,\n",
       "   0.09979336708784103,\n",
       "   0.08942732959985733,\n",
       "   0.08591176569461823,\n",
       "   0.08707604557275772,\n",
       "   0.08773630112409592,\n",
       "   0.09073180705308914,\n",
       "   0.08198720216751099,\n",
       "   0.07514669001102448,\n",
       "   0.07265985757112503,\n",
       "   0.06240992620587349,\n",
       "   0.06330503523349762,\n",
       "   0.06736743450164795,\n",
       "   0.07899061590433121,\n",
       "   0.07611171156167984,\n",
       "   0.07993616908788681,\n",
       "   0.07878583669662476,\n",
       "   0.08914179354906082,\n",
       "   0.09945450723171234,\n",
       "   0.10617309808731079,\n",
       "   0.08425432443618774,\n",
       "   0.09550689905881882,\n",
       "   0.11952590942382812,\n",
       "   0.10908953845500946,\n",
       "   0.0999840572476387,\n",
       "   0.08046980202198029,\n",
       "   0.08192290365695953,\n",
       "   0.09285319596529007,\n",
       "   0.10078353434801102,\n",
       "   0.08711317926645279,\n",
       "   0.07418694347143173,\n",
       "   0.08688990026712418,\n",
       "   0.09615517407655716,\n",
       "   0.08938367664813995,\n",
       "   0.08777674287557602,\n",
       "   0.0920243188738823,\n",
       "   0.09474824368953705,\n",
       "   0.10298728197813034,\n",
       "   0.09127578884363174,\n",
       "   0.09710212051868439,\n",
       "   0.10259648412466049,\n",
       "   0.10344361513853073,\n",
       "   0.09041551500558853,\n",
       "   0.08548172563314438,\n",
       "   0.10169755667448044,\n",
       "   0.09987293183803558,\n",
       "   0.09970492869615555,\n",
       "   0.09703726321458817,\n",
       "   0.11986400187015533,\n",
       "   0.12951241433620453,\n",
       "   0.17161543667316437,\n",
       "   0.17617417871952057,\n",
       "   0.15513502061367035,\n",
       "   0.16929037868976593,\n",
       "   0.18631115555763245,\n",
       "   0.17422862350940704,\n",
       "   0.1647493690252304,\n",
       "   0.15727266669273376,\n",
       "   0.13609181344509125,\n",
       "   0.12669065594673157,\n",
       "   0.11406819522380829,\n",
       "   0.12283723801374435,\n",
       "   0.11077793687582016,\n",
       "   0.12247515469789505,\n",
       "   0.10981641709804535,\n",
       "   0.11747581511735916,\n",
       "   0.12165151536464691,\n",
       "   0.12521448731422424,\n",
       "   0.11909397691488266,\n",
       "   0.1181318610906601,\n",
       "   0.12153643369674683,\n",
       "   0.12249471247196198,\n",
       "   0.10959870368242264,\n",
       "   0.08649156987667084,\n",
       "   0.06861373782157898,\n",
       "   0.08097464591264725,\n",
       "   0.07019386440515518,\n",
       "   0.07018819451332092,\n",
       "   0.06861881911754608,\n",
       "   0.06910476833581924,\n",
       "   0.07228151708841324,\n",
       "   0.07472200691699982,\n",
       "   0.07532297819852829,\n",
       "   0.07421319931745529,\n",
       "   0.07849276810884476,\n",
       "   0.07819807529449463,\n",
       "   0.09156627207994461,\n",
       "   0.07959158718585968,\n",
       "   0.072507344186306,\n",
       "   0.07447903603315353,\n",
       "   0.082013800740242,\n",
       "   0.09315717965364456,\n",
       "   0.09594631940126419,\n",
       "   0.10745322704315186,\n",
       "   0.08477555215358734,\n",
       "   0.08227206766605377,\n",
       "   0.08393929898738861,\n",
       "   0.0779908150434494,\n",
       "   0.08270956575870514,\n",
       "   0.09477872401475906,\n",
       "   0.10758012533187866,\n",
       "   0.10398571938276291,\n",
       "   0.0999152734875679,\n",
       "   0.08762888610363007,\n",
       "   0.08401963859796524,\n",
       "   0.08553259819746017,\n",
       "   0.09792548418045044,\n",
       "   0.1150176003575325,\n",
       "   0.09325040131807327,\n",
       "   0.10594457387924194,\n",
       "   0.09808789193630219,\n",
       "   0.10965985804796219,\n",
       "   0.10189632326364517,\n",
       "   0.09720462560653687,\n",
       "   0.09570765495300293,\n",
       "   0.09129947423934937,\n",
       "   0.09745033830404282,\n",
       "   0.08310303837060928,\n",
       "   0.06935740262269974,\n",
       "   0.060819752514362335,\n",
       "   0.07409494370222092,\n",
       "   0.08006500452756882,\n",
       "   0.08780165016651154,\n",
       "   0.09240590780973434,\n",
       "   0.08147867769002914,\n",
       "   0.08190583437681198,\n",
       "   0.08900357782840729,\n",
       "   0.0941050797700882,\n",
       "   0.07488732784986496,\n",
       "   0.06869720667600632,\n",
       "   0.06869422644376755,\n",
       "   0.07017983496189117,\n",
       "   0.07959777861833572,\n",
       "   0.07784106582403183,\n",
       "   0.07384747266769409,\n",
       "   0.08042667806148529,\n",
       "   0.07493972033262253,\n",
       "   0.06748976558446884,\n",
       "   0.06662578880786896,\n",
       "   0.062410373240709305,\n",
       "   0.06294328719377518,\n",
       "   0.06892166286706924,\n",
       "   0.06671176105737686,\n",
       "   0.06443314999341965,\n",
       "   0.06806538254022598,\n",
       "   0.052779871970415115,\n",
       "   0.04855186864733696,\n",
       "   0.04733297973871231,\n",
       "   0.05158691480755806,\n",
       "   0.05285447835922241,\n",
       "   0.04799233004450798,\n",
       "   0.049924563616514206,\n",
       "   0.05035042390227318,\n",
       "   0.04606190696358681,\n",
       "   0.05619199946522713,\n",
       "   0.05821966007351875,\n",
       "   0.05645976588129997,\n",
       "   0.05535000190138817,\n",
       "   0.044364627450704575,\n",
       "   0.042062561959028244,\n",
       "   0.0417756550014019,\n",
       "   0.045768242329359055,\n",
       "   0.045520782470703125,\n",
       "   0.04586247354745865,\n",
       "   0.04390229657292366,\n",
       "   0.04934222623705864,\n",
       "   0.04747198894619942,\n",
       "   0.05054115504026413,\n",
       "   0.049011677503585815,\n",
       "   0.054259296506643295,\n",
       "   0.053469426929950714,\n",
       "   0.05659867450594902,\n",
       "   0.05278199166059494,\n",
       "   0.06581024080514908,\n",
       "   0.05918484553694725,\n",
       "   0.058678142726421356,\n",
       "   0.05697154253721237,\n",
       "   0.05952073261141777,\n",
       "   0.0562455989420414,\n",
       "   0.05278753489255905,\n",
       "   0.05606680363416672,\n",
       "   0.05486222356557846,\n",
       "   0.059956666082143784,\n",
       "   0.05510678142309189,\n",
       "   0.05576575547456741,\n",
       "   0.05378780514001846,\n",
       "   0.05374172329902649,\n",
       "   0.05164402350783348,\n",
       "   0.0603841133415699,\n",
       "   0.06502790749073029,\n",
       "   0.05646253377199173,\n",
       "   0.061730362474918365,\n",
       "   0.0648948922753334,\n",
       "   0.05952996388077736,\n",
       "   0.05519562587141991,\n",
       "   0.05044212564826012,\n",
       "   0.05089295282959938,\n",
       "   0.057979363948106766,\n",
       "   0.06054345518350601,\n",
       "   0.05995852127671242,\n",
       "   0.07023178040981293,\n",
       "   0.07066963613033295,\n",
       "   0.07257809489965439,\n",
       "   0.06788692623376846,\n",
       "   0.06369519978761673,\n",
       "   0.05349283292889595,\n",
       "   0.05833091959357262,\n",
       "   0.0617067776620388,\n",
       "   0.05645623058080673,\n",
       "   0.052447885274887085,\n",
       "   0.05239596217870712,\n",
       "   0.05145011469721794,\n",
       "   0.05494916811585426,\n",
       "   0.04824215546250343,\n",
       "   0.0509580634534359,\n",
       "   0.049182068556547165,\n",
       "   0.04939814284443855,\n",
       "   0.04814339801669121,\n",
       "   0.04871446266770363,\n",
       "   0.05221431329846382,\n",
       "   0.05585045367479324,\n",
       "   0.05386127904057503,\n",
       "   0.05034070461988449,\n",
       "   0.046069007366895676,\n",
       "   0.05264325067400932,\n",
       "   0.04824056103825569,\n",
       "   0.05008595064282417,\n",
       "   0.06164843961596489,\n",
       "   0.061344098299741745,\n",
       "   0.06528861820697784,\n",
       "   0.06796260178089142,\n",
       "   0.06496898084878922,\n",
       "   0.05648799613118172,\n",
       "   0.04642579331994057,\n",
       "   0.04751405119895935,\n",
       "   0.046579502522945404,\n",
       "   0.048590514808893204,\n",
       "   0.04396388679742813,\n",
       "   0.045445192605257034,\n",
       "   0.042661722749471664,\n",
       "   0.04695456475019455,\n",
       "   0.04528621956706047,\n",
       "   0.0401831790804863,\n",
       "   0.04247770458459854,\n",
       "   0.043033305555582047,\n",
       "   0.04389854148030281,\n",
       "   0.04495400935411453,\n",
       "   0.04766199737787247,\n",
       "   0.04959346726536751,\n",
       "   0.05248181149363518,\n",
       "   0.0525500662624836,\n",
       "   0.05206102877855301,\n",
       "   0.05149387568235397,\n",
       "   0.05264309048652649,\n",
       "   0.05378567799925804,\n",
       "   0.07061981409788132,\n",
       "   0.06659441441297531,\n",
       "   0.07173721492290497,\n",
       "   0.06730960309505463,\n",
       "   0.06573040038347244,\n",
       "   0.08096972852945328,\n",
       "   0.06998185068368912,\n",
       "   0.07169748842716217,\n",
       "   0.061965975910425186,\n",
       "   0.0548294298350811,\n",
       "   0.04644414409995079,\n",
       "   0.05171356722712517,\n",
       "   0.053485799580812454,\n",
       "   0.054297927767038345,\n",
       "   0.04865304008126259,\n",
       "   0.043905582278966904,\n",
       "   0.0463867112994194,\n",
       "   0.0553114078938961,\n",
       "   0.05655226111412048,\n",
       "   0.05642760917544365,\n",
       "   0.06192533299326897,\n",
       "   0.05716323480010033,\n",
       "   0.055293016135692596,\n",
       "   0.05015019699931145,\n",
       "   0.05236789211630821,\n",
       "   0.0556880347430706,\n",
       "   0.05540761351585388,\n",
       "   0.057298097759485245,\n",
       "   0.05841958522796631,\n",
       "   0.06027701869606972,\n",
       "   0.06648200005292892,\n",
       "   0.0705687552690506,\n",
       "   0.06855785846710205,\n",
       "   0.0629265084862709,\n",
       "   0.052039798349142075,\n",
       "   0.05356865003705025,\n",
       "   0.05352746322751045,\n",
       "   0.05333072319626808,\n",
       "   0.05966004729270935,\n",
       "   0.056532301008701324,\n",
       "   0.055854786187410355,\n",
       "   0.05767449364066124,\n",
       "   0.05506221204996109,\n",
       "   0.05301240086555481,\n",
       "   0.051547884941101074,\n",
       "   0.050692226737737656,\n",
       "   0.053072817623615265,\n",
       "   0.04871438071131706,\n",
       "   0.04918210953474045,\n",
       "   0.05214754492044449,\n",
       "   0.05995677039027214,\n",
       "   0.06492279469966888,\n",
       "   0.054959140717983246,\n",
       "   0.05734055116772652,\n",
       "   0.05611419305205345,\n",
       "   0.04939671978354454,\n",
       "   0.049438778311014175,\n",
       "   0.04602637514472008,\n",
       "   0.04620569199323654,\n",
       "   0.04933161288499832,\n",
       "   0.053897757083177567,\n",
       "   0.04703465849161148,\n",
       "   0.04889215901494026,\n",
       "   0.05167155712842941,\n",
       "   0.06082068011164665,\n",
       "   0.06194505840539932,\n",
       "   0.0736740380525589,\n",
       "   0.06512374430894852,\n",
       "   0.06734458357095718,\n",
       "   0.059620436280965805,\n",
       "   0.057622719556093216,\n",
       "   0.05503815412521362,\n",
       "   0.06227842718362808,\n",
       "   0.0802808552980423,\n",
       "   0.07254700362682343,\n",
       "   0.08489330857992172,\n",
       "   0.08183109760284424,\n",
       "   0.07629547268152237,\n",
       "   0.0761093944311142,\n",
       "   0.07644890993833542,\n",
       "   0.07470707595348358,\n",
       "   0.06379067897796631,\n",
       "   0.06690074503421783,\n",
       "   0.07684217393398285,\n",
       "   0.07481129467487335,\n",
       "   0.06275537610054016,\n",
       "   0.059993259608745575,\n",
       "   0.06056180223822594,\n",
       "   0.06181737408041954,\n",
       "   0.06250438839197159,\n",
       "   0.05829450860619545,\n",
       "   0.057484496384859085,\n",
       "   0.06542100757360458,\n",
       "   0.07147636264562607,\n",
       "   0.0660300925374031,\n",
       "   0.06240984424948692,\n",
       "   0.06946653872728348,\n",
       "   0.0805215984582901,\n",
       "   0.07911479473114014,\n",
       "   0.07927422225475311,\n",
       "   0.09125224500894547,\n",
       "   0.09148300439119339,\n",
       "   0.08132894337177277,\n",
       "   0.08274243026971817,\n",
       "   0.0806409940123558,\n",
       "   0.0953494980931282,\n",
       "   0.09752815961837769,\n",
       "   0.09363479912281036,\n",
       "   0.0764927938580513,\n",
       "   0.07643108069896698,\n",
       "   0.08154525607824326,\n",
       "   0.07481086254119873,\n",
       "   0.07002134621143341,\n",
       "   0.06512816250324249,\n",
       "   0.06453227251768112,\n",
       "   0.058285634964704514,\n",
       "   0.055266644805669785,\n",
       "   0.052445974200963974,\n",
       "   0.05436615273356438,\n",
       "   0.05246693268418312,\n",
       "   0.05411536619067192,\n",
       "   0.056427001953125,\n",
       "   0.05746079236268997,\n",
       "   0.04918491840362549,\n",
       "   0.051149021834135056,\n",
       "   0.05120532587170601,\n",
       "   0.047420941293239594,\n",
       "   0.04897341504693031,\n",
       "   0.04682827740907669,\n",
       "   0.04779580608010292,\n",
       "   0.04906722903251648,\n",
       "   0.051126349717378616,\n",
       "   0.05666043236851692,\n",
       "   0.052089013159275055,\n",
       "   0.04929304122924805,\n",
       "   0.049240563064813614,\n",
       "   0.051789648830890656,\n",
       "   0.048230864107608795,\n",
       "   0.05252717807888985,\n",
       "   0.05826398357748985,\n",
       "   0.05170664191246033,\n",
       "   0.05305679887533188,\n",
       "   0.05285409092903137,\n",
       "   0.05195596441626549,\n",
       "   0.04845428466796875,\n",
       "   0.04875432327389717,\n",
       "   0.050432901829481125,\n",
       "   0.05431944131851196,\n",
       "   0.05943772569298744,\n",
       "   0.0615810789167881,\n",
       "   0.06048184260725975,\n",
       "   0.05844302102923393,\n",
       "   0.05389079824090004,\n",
       "   0.05538726598024368,\n",
       "   0.05102949216961861,\n",
       "   0.049884431064128876,\n",
       "   0.05233145132660866,\n",
       "   0.054014116525650024,\n",
       "   0.05298382416367531,\n",
       "   0.05959982052445412,\n",
       "   0.05780453234910965,\n",
       "   0.06282774358987808,\n",
       "   0.06151355057954788,\n",
       "   0.06556016951799393,\n",
       "   0.06129683181643486,\n",
       "   0.05733940005302429,\n",
       "   0.052892278879880905,\n",
       "   0.05129917338490486,\n",
       "   0.050803929567337036,\n",
       "   0.04853542521595955,\n",
       "   0.04551476240158081,\n",
       "   0.04435885325074196,\n",
       "   0.04617580771446228,\n",
       "   0.04739762470126152,\n",
       "   0.04927120357751846,\n",
       "   0.04918491467833519,\n",
       "   0.050766270607709885,\n",
       "   0.055020976811647415,\n",
       "   0.05496871471405029,\n",
       "   0.05865641310811043,\n",
       "   0.05636245384812355,\n",
       "   0.061838384717702866,\n",
       "   0.06972881406545639,\n",
       "   0.0681706964969635,\n",
       "   0.069361113011837,\n",
       "   0.07231450080871582,\n",
       "   0.07337628304958344,\n",
       "   0.06544872373342514,\n",
       "   0.07448265701532364,\n",
       "   0.0674443170428276,\n",
       "   0.06582897156476974,\n",
       "   0.05655748024582863,\n",
       "   0.05171152949333191,\n",
       "   0.05514496937394142,\n",
       "   0.04831072688102722,\n",
       "   0.04794630780816078,\n",
       "   0.046941906213760376,\n",
       "   0.0478169247508049,\n",
       "   0.049044329673051834,\n",
       "   0.05005231499671936,\n",
       "   0.04949357360601425,\n",
       "   0.048755381256341934,\n",
       "   0.04792625829577446,\n",
       "   0.04834435507655144,\n",
       "   0.046821605414152145,\n",
       "   0.046428777277469635,\n",
       "   0.04834394529461861,\n",
       "   0.05249200016260147,\n",
       "   0.05367549508810043,\n",
       "   0.05476844310760498,\n",
       "   0.05833740159869194,\n",
       "   0.05641915649175644,\n",
       "   0.04840272292494774],\n",
       "  'jsc_loss': [0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.44451358914375305,\n",
       "   0.4443166255950928,\n",
       "   0.4260512888431549,\n",
       "   0.400834321975708,\n",
       "   0.3783762454986572,\n",
       "   0.37581589818000793,\n",
       "   0.3227621018886566,\n",
       "   0.3116696774959564,\n",
       "   0.3112059533596039,\n",
       "   0.2817448079586029,\n",
       "   0.27110719680786133,\n",
       "   0.2264946550130844,\n",
       "   0.22072577476501465,\n",
       "   0.2386064976453781,\n",
       "   0.24330390989780426,\n",
       "   0.22211113572120667,\n",
       "   0.21783779561519623,\n",
       "   0.2127767950296402,\n",
       "   0.18406473100185394,\n",
       "   0.18070587515830994,\n",
       "   0.1595771610736847,\n",
       "   0.1677398979663849,\n",
       "   0.16201530396938324,\n",
       "   0.16465802490711212,\n",
       "   0.17583677172660828,\n",
       "   0.16494183242321014,\n",
       "   0.13569268584251404,\n",
       "   0.13890165090560913,\n",
       "   0.1267407089471817,\n",
       "   0.12778978049755096,\n",
       "   0.11231187731027603,\n",
       "   0.12600202858448029,\n",
       "   0.12874022126197815,\n",
       "   0.1413147747516632,\n",
       "   0.1435817927122116,\n",
       "   0.1416885107755661,\n",
       "   0.1456901878118515,\n",
       "   0.16940708458423615,\n",
       "   0.16923955082893372,\n",
       "   0.1703035533428192,\n",
       "   0.1751164197921753,\n",
       "   0.16006574034690857,\n",
       "   0.1499936580657959,\n",
       "   0.1367778331041336,\n",
       "   0.15139690041542053,\n",
       "   0.15267349779605865,\n",
       "   0.14205504953861237,\n",
       "   0.122818224132061,\n",
       "   0.13059774041175842,\n",
       "   0.12687599658966064,\n",
       "   0.1375359743833542,\n",
       "   0.11534716933965683,\n",
       "   0.10667545348405838,\n",
       "   0.10968226939439774,\n",
       "   0.10689150542020798,\n",
       "   0.1147785410284996,\n",
       "   0.1041797623038292,\n",
       "   0.1018730029463768,\n",
       "   0.10418552905321121,\n",
       "   0.12117429822683334,\n",
       "   0.12084776908159256,\n",
       "   0.12656868994235992,\n",
       "   0.12647578120231628,\n",
       "   0.13138823211193085,\n",
       "   0.11375222355127335,\n",
       "   0.11976681649684906,\n",
       "   0.1254880130290985,\n",
       "   0.13671822845935822,\n",
       "   0.12828971445560455,\n",
       "   0.1449519544839859,\n",
       "   0.13660673797130585,\n",
       "   0.12648575007915497,\n",
       "   0.11976360529661179,\n",
       "   0.12317711859941483,\n",
       "   0.11495582014322281,\n",
       "   0.13258233666419983,\n",
       "   0.12865345180034637,\n",
       "   0.13658539950847626,\n",
       "   0.13871583342552185,\n",
       "   0.14271412789821625,\n",
       "   0.13870146870613098,\n",
       "   0.14355674386024475,\n",
       "   0.13697801530361176,\n",
       "   0.1374402940273285,\n",
       "   0.13976061344146729,\n",
       "   0.1496276557445526,\n",
       "   0.14725038409233093,\n",
       "   0.13500317931175232,\n",
       "   0.1389838308095932,\n",
       "   0.11364497989416122,\n",
       "   0.11547781527042389,\n",
       "   0.12338750064373016,\n",
       "   0.11119650304317474,\n",
       "   0.12272437661886215,\n",
       "   0.13709016144275665,\n",
       "   0.1421027034521103,\n",
       "   0.13578881323337555,\n",
       "   0.14263124763965607,\n",
       "   0.12372415512800217,\n",
       "   0.11680205911397934,\n",
       "   0.12512458860874176,\n",
       "   0.13840678334236145,\n",
       "   0.11284048855304718,\n",
       "   0.10694871842861176,\n",
       "   0.11910723149776459,\n",
       "   0.10303068161010742,\n",
       "   0.1092725545167923,\n",
       "   0.10647447407245636,\n",
       "   0.10408183187246323,\n",
       "   0.1079147458076477,\n",
       "   0.11036362498998642,\n",
       "   0.10343468189239502,\n",
       "   0.11108125001192093,\n",
       "   0.11611094325780869,\n",
       "   0.10774701088666916,\n",
       "   0.12321224063634872,\n",
       "   0.12285424768924713,\n",
       "   0.11893429607152939,\n",
       "   0.1264777034521103,\n",
       "   0.11495472490787506,\n",
       "   0.1273372918367386,\n",
       "   0.112751804292202,\n",
       "   0.11635494977235794,\n",
       "   0.11408126354217529,\n",
       "   0.10298135131597519,\n",
       "   0.10795395076274872,\n",
       "   0.1173507496714592,\n",
       "   0.11723769456148148,\n",
       "   0.1342538595199585,\n",
       "   0.12737861275672913,\n",
       "   0.13625402748584747,\n",
       "   0.1383817046880722,\n",
       "   0.13399933278560638,\n",
       "   0.13386908173561096,\n",
       "   0.1282275766134262,\n",
       "   0.11951613426208496,\n",
       "   0.12438557296991348,\n",
       "   0.1407242715358734,\n",
       "   0.1516796201467514,\n",
       "   0.16020707786083221,\n",
       "   0.15109312534332275,\n",
       "   0.14980581402778625,\n",
       "   0.16077838838100433,\n",
       "   0.1401028037071228,\n",
       "   0.13598330318927765,\n",
       "   0.12758567929267883,\n",
       "   0.15008509159088135,\n",
       "   0.12054356187582016,\n",
       "   0.1274377554655075,\n",
       "   0.13375023007392883,\n",
       "   0.12021343410015106,\n",
       "   0.12872371077537537,\n",
       "   0.13751952350139618,\n",
       "   0.14968366920948029,\n",
       "   0.14593301713466644,\n",
       "   0.13627000153064728,\n",
       "   0.12317615002393723,\n",
       "   0.1374693363904953,\n",
       "   0.13322177529335022,\n",
       "   0.131270632147789,\n",
       "   0.13950946927070618,\n",
       "   0.14385299384593964,\n",
       "   0.15534159541130066,\n",
       "   0.166224867105484,\n",
       "   0.17329159379005432,\n",
       "   0.1570873111486435,\n",
       "   0.1628473699092865,\n",
       "   0.1730172336101532,\n",
       "   0.13774390518665314,\n",
       "   0.13987413048744202,\n",
       "   0.13975940644741058,\n",
       "   0.1532425582408905,\n",
       "   0.15622077882289886,\n",
       "   0.15467695891857147,\n",
       "   0.15843798220157623,\n",
       "   0.16977153718471527,\n",
       "   0.160503551363945,\n",
       "   0.15017209947109222,\n",
       "   0.1453658789396286,\n",
       "   0.15223988890647888,\n",
       "   0.1555517464876175,\n",
       "   0.15925554931163788,\n",
       "   0.15239110589027405,\n",
       "   0.1406758427619934,\n",
       "   0.13655216991901398,\n",
       "   0.12771354615688324,\n",
       "   0.14447999000549316,\n",
       "   0.14929017424583435,\n",
       "   0.15157566964626312,\n",
       "   0.14193323254585266,\n",
       "   0.140109583735466,\n",
       "   0.1286296397447586,\n",
       "   0.1274222582578659,\n",
       "   0.14748981595039368,\n",
       "   0.14211100339889526,\n",
       "   0.16335272789001465,\n",
       "   0.1514672189950943,\n",
       "   0.1579044610261917,\n",
       "   0.14766891300678253,\n",
       "   0.15354813635349274,\n",
       "   0.16661624610424042,\n",
       "   0.1623009592294693,\n",
       "   0.15283574163913727,\n",
       "   0.15492138266563416,\n",
       "   0.1548987776041031,\n",
       "   0.15623734891414642,\n",
       "   0.17266742885112762,\n",
       "   0.19718769192695618,\n",
       "   0.18622024357318878,\n",
       "   0.19499696791172028,\n",
       "   0.20340362191200256,\n",
       "   0.20678816735744476,\n",
       "   0.19877885282039642,\n",
       "   0.1976599544286728,\n",
       "   0.20435820519924164,\n",
       "   0.17627570033073425,\n",
       "   0.16662967205047607,\n",
       "   0.16996373236179352,\n",
       "   0.1677272617816925,\n",
       "   0.1726737916469574,\n",
       "   0.15540869534015656,\n",
       "   0.15072038769721985,\n",
       "   0.14215818047523499,\n",
       "   0.14327499270439148,\n",
       "   0.13664095103740692,\n",
       "   0.15812793374061584,\n",
       "   0.14918097853660583,\n",
       "   0.12819364666938782,\n",
       "   0.11942432820796967,\n",
       "   0.12578393518924713,\n",
       "   0.12264550477266312,\n",
       "   0.132097989320755,\n",
       "   0.12763120234012604,\n",
       "   0.12310035526752472,\n",
       "   0.12179010361433029,\n",
       "   0.11075789481401443,\n",
       "   0.128448948264122,\n",
       "   0.13533394038677216,\n",
       "   0.12787148356437683,\n",
       "   0.11789686232805252,\n",
       "   0.12402189522981644,\n",
       "   0.12826918065547943,\n",
       "   0.11438518762588501,\n",
       "   0.11617115885019302,\n",
       "   0.11657489836215973,\n",
       "   0.1094607338309288,\n",
       "   0.10900863260030746,\n",
       "   0.1259000450372696,\n",
       "   0.11604131758213043,\n",
       "   0.11229155957698822,\n",
       "   0.11434843391180038,\n",
       "   0.1002512127161026,\n",
       "   0.10252876579761505,\n",
       "   0.10639633238315582,\n",
       "   0.10339613258838654,\n",
       "   0.1063765287399292,\n",
       "   0.11918188631534576,\n",
       "   0.11859742552042007,\n",
       "   0.12285474687814713,\n",
       "   0.12434516847133636,\n",
       "   0.12108677625656128,\n",
       "   0.11047786474227905,\n",
       "   0.11301977932453156,\n",
       "   0.13293759524822235,\n",
       "   0.1527002900838852,\n",
       "   0.1493796855211258,\n",
       "   0.1449403315782547,\n",
       "   0.14613081514835358,\n",
       "   0.1414767950773239,\n",
       "   0.13311506807804108,\n",
       "   0.12394636869430542,\n",
       "   0.13033618032932281,\n",
       "   0.1336357742547989,\n",
       "   0.1415138989686966,\n",
       "   0.15002873539924622,\n",
       "   0.13785436749458313,\n",
       "   0.14626166224479675,\n",
       "   0.1609656810760498,\n",
       "   0.16100089251995087,\n",
       "   0.16601960361003876,\n",
       "   0.16303467750549316,\n",
       "   0.17500324547290802,\n",
       "   0.19312740862369537,\n",
       "   0.20227646827697754,\n",
       "   0.19056487083435059,\n",
       "   0.1917295753955841,\n",
       "   0.20570173859596252,\n",
       "   0.19425736367702484,\n",
       "   0.1907847374677658,\n",
       "   0.2153475135564804,\n",
       "   0.21597063541412354,\n",
       "   0.21477988362312317,\n",
       "   0.1988607943058014,\n",
       "   0.19362202286720276,\n",
       "   0.1865675151348114,\n",
       "   0.18173964321613312,\n",
       "   0.20392325520515442,\n",
       "   0.20609362423419952,\n",
       "   0.1874624788761139,\n",
       "   0.2088157683610916,\n",
       "   0.2082575261592865,\n",
       "   0.20500031113624573,\n",
       "   0.22549499571323395,\n",
       "   0.21446457505226135,\n",
       "   0.24105218052864075,\n",
       "   0.2158544957637787,\n",
       "   0.22424305975437164,\n",
       "   0.22820384800434113,\n",
       "   0.25554513931274414,\n",
       "   0.2482265681028366,\n",
       "   0.24075503647327423,\n",
       "   0.2340608686208725,\n",
       "   0.20774948596954346,\n",
       "   0.18878987431526184,\n",
       "   0.17949868738651276,\n",
       "   0.16710300743579865,\n",
       "   0.15032191574573517,\n",
       "   0.14457082748413086,\n",
       "   0.15516257286071777,\n",
       "   0.15592531859874725,\n",
       "   0.148422509431839,\n",
       "   0.14204682409763336,\n",
       "   0.15808868408203125,\n",
       "   0.14003001153469086,\n",
       "   0.14276017248630524,\n",
       "   0.16007840633392334,\n",
       "   0.17385852336883545,\n",
       "   0.17808368802070618,\n",
       "   0.18334278464317322,\n",
       "   0.17985539138317108,\n",
       "   0.16410674154758453,\n",
       "   0.1593102663755417,\n",
       "   0.16101320087909698,\n",
       "   0.15031446516513824,\n",
       "   0.14456023275852203,\n",
       "   0.15271104872226715,\n",
       "   0.15308108925819397,\n",
       "   0.158630833029747,\n",
       "   0.1579647958278656,\n",
       "   0.16110537946224213,\n",
       "   0.15435777604579926,\n",
       "   0.1466662734746933,\n",
       "   0.1404152661561966,\n",
       "   0.14178447425365448,\n",
       "   0.1370382159948349,\n",
       "   0.15671025216579437,\n",
       "   0.1561681032180786,\n",
       "   0.17763230204582214,\n",
       "   0.16589847207069397,\n",
       "   0.16657552123069763,\n",
       "   0.16533233225345612,\n",
       "   0.17499814927577972,\n",
       "   0.18212927877902985,\n",
       "   0.18142546713352203,\n",
       "   0.17435196042060852,\n",
       "   0.1713947206735611,\n",
       "   0.16386368870735168,\n",
       "   0.17027899622917175,\n",
       "   0.18399643898010254,\n",
       "   0.19084475934505463,\n",
       "   0.1799144297838211,\n",
       "   0.1655070036649704,\n",
       "   0.1772463321685791,\n",
       "   0.15774044394493103,\n",
       "   0.1589030772447586,\n",
       "   0.14315584301948547,\n",
       "   0.13989311456680298,\n",
       "   0.12905524671077728,\n",
       "   0.1333971917629242,\n",
       "   0.13404570519924164,\n",
       "   0.1342744529247284,\n",
       "   0.13279078900814056,\n",
       "   0.13329748809337616,\n",
       "   0.1583959311246872,\n",
       "   0.1502288430929184,\n",
       "   0.15824158489704132,\n",
       "   0.15414230525493622,\n",
       "   0.16333036124706268,\n",
       "   0.1519646942615509,\n",
       "   0.16041335463523865,\n",
       "   0.1585782766342163,\n",
       "   0.16405674815177917,\n",
       "   0.15120670199394226,\n",
       "   0.1482381671667099,\n",
       "   0.14745751023292542,\n",
       "   0.14729629456996918,\n",
       "   0.1445665806531906,\n",
       "   0.14420661330223083,\n",
       "   0.14971554279327393,\n",
       "   0.1526843011379242,\n",
       "   0.16901448369026184,\n",
       "   0.1578216850757599,\n",
       "   0.1512724608182907,\n",
       "   0.15703627467155457,\n",
       "   0.16955241560935974,\n",
       "   0.1638425886631012,\n",
       "   0.155419260263443,\n",
       "   0.13705158233642578,\n",
       "   0.14514680206775665,\n",
       "   0.1428331583738327,\n",
       "   0.1416586935520172,\n",
       "   0.14602422714233398,\n",
       "   0.1424565464258194,\n",
       "   0.14242461323738098,\n",
       "   0.14665916562080383,\n",
       "   0.1566740721464157,\n",
       "   0.1500159054994583,\n",
       "   0.1524949073791504,\n",
       "   0.17195601761341095,\n",
       "   0.15016572177410126,\n",
       "   0.15483848750591278,\n",
       "   0.144551083445549,\n",
       "   0.1397242397069931,\n",
       "   0.12641529738903046,\n",
       "   0.14076487720012665,\n",
       "   0.13058151304721832,\n",
       "   0.14831259846687317,\n",
       "   0.1659451425075531,\n",
       "   0.16179928183555603,\n",
       "   0.15634985268115997,\n",
       "   0.15382318198680878,\n",
       "   0.1444328874349594,\n",
       "   0.1595783233642578,\n",
       "   0.16178689897060394,\n",
       "   0.15345990657806396,\n",
       "   0.14232490956783295,\n",
       "   0.14281204342842102,\n",
       "   0.14035113155841827,\n",
       "   0.14532028138637543,\n",
       "   0.14639762043952942,\n",
       "   0.14726796746253967,\n",
       "   0.13842079043388367,\n",
       "   0.1357504427433014,\n",
       "   0.1312256008386612,\n",
       "   0.1306825876235962,\n",
       "   0.14039158821105957,\n",
       "   0.15755872428417206,\n",
       "   0.16905002295970917,\n",
       "   0.15992248058319092,\n",
       "   0.17352399230003357,\n",
       "   0.15519320964813232,\n",
       "   0.14727282524108887,\n",
       "   0.15581853687763214,\n",
       "   0.14773795008659363,\n",
       "   0.1417146474123001,\n",
       "   0.13924819231033325,\n",
       "   0.13166825473308563,\n",
       "   0.14650249481201172,\n",
       "   0.1579054743051529,\n",
       "   0.15954746305942535,\n",
       "   0.15105964243412018,\n",
       "   0.15764406323432922,\n",
       "   0.15921397507190704,\n",
       "   0.14970722794532776,\n",
       "   0.1571977585554123,\n",
       "   0.15219032764434814,\n",
       "   0.1386364996433258,\n",
       "   0.14135132730007172,\n",
       "   0.1604250818490982,\n",
       "   0.15292327105998993,\n",
       "   0.1422005444765091,\n",
       "   0.15015406906604767,\n",
       "   0.16447313129901886,\n",
       "   0.16557998955249786,\n",
       "   0.16308820247650146,\n",
       "   0.1690501868724823,\n",
       "   0.1491483449935913,\n",
       "   0.14602850377559662,\n",
       "   0.15660516917705536,\n",
       "   0.15993349254131317,\n",
       "   0.15463779866695404,\n",
       "   0.1474372148513794,\n",
       "   0.14501309394836426,\n",
       "   0.15261970460414886,\n",
       "   0.16871872544288635,\n",
       "   0.1842229664325714,\n",
       "   0.16683217883110046,\n",
       "   0.1656033843755722,\n",
       "   0.17602156102657318,\n",
       "   0.16920694708824158,\n",
       "   0.15290644764900208,\n",
       "   0.15568234026432037,\n",
       "   0.1504199206829071,\n",
       "   0.16564004123210907,\n",
       "   0.16747896373271942,\n",
       "   0.16821224987506866,\n",
       "   0.15130889415740967,\n",
       "   0.1514744609594345,\n",
       "   0.16366513073444366,\n",
       "   0.17687757313251495,\n",
       "   0.16993263363838196],\n",
       "  'ff_loss': [0.442303329706192,\n",
       "   0.442303329706192,\n",
       "   0.442303329706192,\n",
       "   0.442303329706192,\n",
       "   0.442303329706192,\n",
       "   0.442303329706192,\n",
       "   0.44290241599082947,\n",
       "   0.4453820288181305,\n",
       "   0.45071932673454285,\n",
       "   0.45836132764816284,\n",
       "   0.45801547169685364,\n",
       "   0.45868611335754395,\n",
       "   0.46589046716690063,\n",
       "   0.4800228774547577,\n",
       "   0.49677181243896484,\n",
       "   0.5030324459075928,\n",
       "   0.5008808970451355,\n",
       "   0.4911794364452362,\n",
       "   0.4825681746006012,\n",
       "   0.4789511263370514,\n",
       "   0.4713955521583557,\n",
       "   0.4628949761390686,\n",
       "   0.46783795952796936,\n",
       "   0.4539642930030823,\n",
       "   0.4495425522327423,\n",
       "   0.45032206177711487,\n",
       "   0.4517998993396759,\n",
       "   0.44913193583488464,\n",
       "   0.4432676434516907,\n",
       "   0.4453478157520294,\n",
       "   0.445676326751709,\n",
       "   0.4452130198478699,\n",
       "   0.44916218519210815,\n",
       "   0.4528560936450958,\n",
       "   0.4511888027191162,\n",
       "   0.4511171877384186,\n",
       "   0.45209455490112305,\n",
       "   0.45743227005004883,\n",
       "   0.4640585482120514,\n",
       "   0.4531809389591217,\n",
       "   0.44711971282958984,\n",
       "   0.45043182373046875,\n",
       "   0.44961872696876526,\n",
       "   0.4500661790370941,\n",
       "   0.456371009349823,\n",
       "   0.45276451110839844,\n",
       "   0.44661226868629456,\n",
       "   0.4431299865245819,\n",
       "   0.4394697844982147,\n",
       "   0.4409081041812897,\n",
       "   0.4379121661186218,\n",
       "   0.43376845121383667,\n",
       "   0.4345853328704834,\n",
       "   0.43944573402404785,\n",
       "   0.4353538453578949,\n",
       "   0.44090574979782104,\n",
       "   0.44434410333633423,\n",
       "   0.4431156814098358,\n",
       "   0.441152423620224,\n",
       "   0.4471797049045563,\n",
       "   0.4455104172229767,\n",
       "   0.44485536217689514,\n",
       "   0.440430223941803,\n",
       "   0.4426126778125763,\n",
       "   0.44303563237190247,\n",
       "   0.44010525941848755,\n",
       "   0.4449504017829895,\n",
       "   0.4454306662082672,\n",
       "   0.4498456120491028,\n",
       "   0.4496708810329437,\n",
       "   0.4511391818523407,\n",
       "   0.45010024309158325,\n",
       "   0.4451257586479187,\n",
       "   0.44317731261253357,\n",
       "   0.43841877579689026,\n",
       "   0.4444134831428528,\n",
       "   0.4500756561756134,\n",
       "   0.4466472566127777,\n",
       "   0.4467439651489258,\n",
       "   0.45237264037132263,\n",
       "   0.4466070234775543,\n",
       "   0.4488300681114197,\n",
       "   0.4476281702518463,\n",
       "   0.443247526884079,\n",
       "   0.44432657957077026,\n",
       "   0.43505582213401794,\n",
       "   0.434219628572464,\n",
       "   0.4377293288707733,\n",
       "   0.43154340982437134,\n",
       "   0.43181130290031433,\n",
       "   0.4364476799964905,\n",
       "   0.4368416368961334,\n",
       "   0.4322161078453064,\n",
       "   0.4301099479198456,\n",
       "   0.4315503239631653,\n",
       "   0.4375453293323517,\n",
       "   0.4350976347923279,\n",
       "   0.430217444896698,\n",
       "   0.42833876609802246,\n",
       "   0.4348185956478119,\n",
       "   0.43772175908088684,\n",
       "   0.43799731135368347,\n",
       "   0.4384453594684601,\n",
       "   0.4403882622718811,\n",
       "   0.44021230936050415,\n",
       "   0.44022801518440247,\n",
       "   0.4418519139289856,\n",
       "   0.4422001540660858,\n",
       "   0.4395729899406433,\n",
       "   0.44289490580558777,\n",
       "   0.4449145793914795,\n",
       "   0.4465881288051605,\n",
       "   0.4480016231536865,\n",
       "   0.447614848613739,\n",
       "   0.4473547637462616,\n",
       "   0.44360408186912537,\n",
       "   0.4466327726840973,\n",
       "   0.44679948687553406,\n",
       "   0.4528743624687195,\n",
       "   0.456104040145874,\n",
       "   0.45843788981437683,\n",
       "   0.4625510573387146,\n",
       "   0.4616228938102722,\n",
       "   0.4672636389732361,\n",
       "   0.4708382487297058,\n",
       "   0.46665531396865845,\n",
       "   0.4673008620738983,\n",
       "   0.4712359607219696,\n",
       "   0.46658414602279663,\n",
       "   0.4637596011161804,\n",
       "   0.46755173802375793,\n",
       "   0.47170963883399963,\n",
       "   0.4675692021846771,\n",
       "   0.46239545941352844,\n",
       "   0.45826035737991333,\n",
       "   0.45826491713523865,\n",
       "   0.4593425691127777,\n",
       "   0.46128422021865845,\n",
       "   0.456208199262619,\n",
       "   0.4580480456352234,\n",
       "   0.449967622756958,\n",
       "   0.4483526051044464,\n",
       "   0.44708916544914246,\n",
       "   0.4422605335712433,\n",
       "   0.44225549697875977,\n",
       "   0.4408550560474396,\n",
       "   0.4371434152126312,\n",
       "   0.43873101472854614,\n",
       "   0.44334521889686584,\n",
       "   0.4443337917327881,\n",
       "   0.4464282691478729,\n",
       "   0.4395330250263214,\n",
       "   0.4349084496498108,\n",
       "   0.43993523716926575,\n",
       "   0.4418269991874695,\n",
       "   0.4402382969856262,\n",
       "   0.4451664090156555,\n",
       "   0.44494757056236267,\n",
       "   0.453388512134552,\n",
       "   0.45213204622268677,\n",
       "   0.4581008553504944,\n",
       "   0.45904406905174255,\n",
       "   0.4598183333873749,\n",
       "   0.4592459797859192,\n",
       "   0.45581284165382385,\n",
       "   0.45857858657836914,\n",
       "   0.44854310154914856,\n",
       "   0.45534226298332214,\n",
       "   0.454413503408432,\n",
       "   0.4546909034252167,\n",
       "   0.45147332549095154,\n",
       "   0.4586654007434845,\n",
       "   0.46052971482276917,\n",
       "   0.45865124464035034,\n",
       "   0.45553162693977356,\n",
       "   0.4507414996623993,\n",
       "   0.45032382011413574,\n",
       "   0.44765177369117737,\n",
       "   0.4435107707977295,\n",
       "   0.4463494122028351,\n",
       "   0.44827932119369507,\n",
       "   0.4450905919075012,\n",
       "   0.449280709028244,\n",
       "   0.44657793641090393,\n",
       "   0.4356050193309784,\n",
       "   0.4275045394897461,\n",
       "   0.4294722378253937,\n",
       "   0.43434938788414,\n",
       "   0.4269864559173584,\n",
       "   0.42472904920578003,\n",
       "   0.42624956369400024,\n",
       "   0.42818284034729004,\n",
       "   0.4260447025299072,\n",
       "   0.4231717586517334,\n",
       "   0.4201681613922119,\n",
       "   0.42248573899269104,\n",
       "   0.42501547932624817,\n",
       "   0.43044543266296387,\n",
       "   0.43000349402427673,\n",
       "   0.42586514353752136,\n",
       "   0.4272735118865967,\n",
       "   0.42636576294898987,\n",
       "   0.42970913648605347,\n",
       "   0.42868080735206604,\n",
       "   0.4323990046977997,\n",
       "   0.4337814450263977,\n",
       "   0.43302470445632935,\n",
       "   0.4346919059753418,\n",
       "   0.43118757009506226,\n",
       "   0.42888039350509644,\n",
       "   0.42889323830604553,\n",
       "   0.4287863075733185,\n",
       "   0.4270201623439789,\n",
       "   0.43010789155960083,\n",
       "   0.43431514501571655,\n",
       "   0.42968541383743286,\n",
       "   0.42628243565559387,\n",
       "   0.4194236099720001,\n",
       "   0.4203526973724365,\n",
       "   0.41313284635543823,\n",
       "   0.41382908821105957,\n",
       "   0.41367655992507935,\n",
       "   0.41396456956863403,\n",
       "   0.41526922583580017,\n",
       "   0.41760119795799255,\n",
       "   0.41506466269493103,\n",
       "   0.4110625386238098,\n",
       "   0.4169240891933441,\n",
       "   0.4205227196216583,\n",
       "   0.42162227630615234,\n",
       "   0.4226047992706299,\n",
       "   0.42007067799568176,\n",
       "   0.4235312342643738,\n",
       "   0.4281551241874695,\n",
       "   0.42771458625793457,\n",
       "   0.4305485188961029,\n",
       "   0.426464319229126,\n",
       "   0.42294982075691223,\n",
       "   0.4208950102329254,\n",
       "   0.4247048497200012,\n",
       "   0.4246439039707184,\n",
       "   0.41815468668937683,\n",
       "   0.4188293516635895,\n",
       "   0.41900578141212463,\n",
       "   0.42045512795448303,\n",
       "   0.4264375567436218,\n",
       "   0.4258711040019989,\n",
       "   0.42413613200187683,\n",
       "   0.4186505377292633,\n",
       "   0.4100545048713684,\n",
       "   0.41144388914108276,\n",
       "   0.40936002135276794,\n",
       "   0.4109048545360565,\n",
       "   0.41163328289985657,\n",
       "   0.41095802187919617,\n",
       "   0.411792516708374,\n",
       "   0.41112977266311646,\n",
       "   0.40863513946533203,\n",
       "   0.4045052230358124,\n",
       "   0.4055755138397217,\n",
       "   0.40630072355270386,\n",
       "   0.405238538980484,\n",
       "   0.4020467698574066,\n",
       "   0.4016343951225281,\n",
       "   0.40339985489845276,\n",
       "   0.40120407938957214,\n",
       "   0.40149161219596863,\n",
       "   0.4071299433708191,\n",
       "   0.4106699824333191,\n",
       "   0.40781787037849426,\n",
       "   0.4060981571674347,\n",
       "   0.4048535227775574,\n",
       "   0.41048336029052734,\n",
       "   0.41484591364860535,\n",
       "   0.40908387303352356,\n",
       "   0.4109521806240082,\n",
       "   0.41147303581237793,\n",
       "   0.4134862422943115,\n",
       "   0.41815561056137085,\n",
       "   0.4232519268989563,\n",
       "   0.4240012764930725,\n",
       "   0.41783013939857483,\n",
       "   0.4202209413051605,\n",
       "   0.41891127824783325,\n",
       "   0.4248756468296051,\n",
       "   0.425697386264801,\n",
       "   0.4250873625278473,\n",
       "   0.4268074333667755,\n",
       "   0.4236309230327606,\n",
       "   0.4169674217700958,\n",
       "   0.41853395104408264,\n",
       "   0.4173653721809387,\n",
       "   0.4172372817993164,\n",
       "   0.416848748922348,\n",
       "   0.42115214467048645,\n",
       "   0.4222605228424072,\n",
       "   0.423939049243927,\n",
       "   0.4225892126560211,\n",
       "   0.42286133766174316,\n",
       "   0.4204353988170624,\n",
       "   0.41305121779441833,\n",
       "   0.4068900942802429,\n",
       "   0.40551215410232544,\n",
       "   0.40165549516677856,\n",
       "   0.4073319733142853,\n",
       "   0.40214329957962036,\n",
       "   0.4036024212837219,\n",
       "   0.40964460372924805,\n",
       "   0.41035929322242737,\n",
       "   0.4102803170681,\n",
       "   0.4064904749393463,\n",
       "   0.4129740297794342,\n",
       "   0.4105450510978699,\n",
       "   0.4083527624607086,\n",
       "   0.4136177599430084,\n",
       "   0.420287162065506,\n",
       "   0.4159245789051056,\n",
       "   0.41706666350364685,\n",
       "   0.4215397536754608,\n",
       "   0.4206824004650116,\n",
       "   0.42100152373313904,\n",
       "   0.4228566586971283,\n",
       "   0.4174189865589142,\n",
       "   0.41476184129714966,\n",
       "   0.41697806119918823,\n",
       "   0.40872836112976074,\n",
       "   0.40850216150283813,\n",
       "   0.39950722455978394,\n",
       "   0.39987441897392273,\n",
       "   0.40261778235435486,\n",
       "   0.4123344123363495,\n",
       "   0.4099476933479309,\n",
       "   0.40948066115379333,\n",
       "   0.4119853973388672,\n",
       "   0.41103219985961914,\n",
       "   0.4097341001033783,\n",
       "   0.4095667004585266,\n",
       "   0.4050218462944031,\n",
       "   0.40816792845726013,\n",
       "   0.4051080644130707,\n",
       "   0.4061606526374817,\n",
       "   0.41164660453796387,\n",
       "   0.41593894362449646,\n",
       "   0.417845219373703,\n",
       "   0.4145839214324951,\n",
       "   0.40884941816329956,\n",
       "   0.40536195039749146,\n",
       "   0.4088578522205353,\n",
       "   0.4072123169898987,\n",
       "   0.4089594781398773,\n",
       "   0.40623778104782104,\n",
       "   0.40348246693611145,\n",
       "   0.4048125445842743,\n",
       "   0.41019707918167114,\n",
       "   0.40852850675582886,\n",
       "   0.41049450635910034,\n",
       "   0.42291226983070374,\n",
       "   0.4298461973667145,\n",
       "   0.42499369382858276,\n",
       "   0.4220063090324402,\n",
       "   0.414480060338974,\n",
       "   0.4193900227546692,\n",
       "   0.41958117485046387,\n",
       "   0.4228746294975281,\n",
       "   0.4249705970287323,\n",
       "   0.4202766716480255,\n",
       "   0.42297491431236267,\n",
       "   0.4259468913078308,\n",
       "   0.4275542199611664,\n",
       "   0.42742255330085754,\n",
       "   0.4148753583431244,\n",
       "   0.4138873219490051,\n",
       "   0.4168100953102112,\n",
       "   0.4151747226715088,\n",
       "   0.41676464676856995,\n",
       "   0.41716334223747253,\n",
       "   0.41980263590812683,\n",
       "   0.4212248921394348,\n",
       "   0.42468923330307007,\n",
       "   0.42432406544685364,\n",
       "   0.4234371781349182,\n",
       "   0.4250955283641815,\n",
       "   0.42448046803474426,\n",
       "   0.4187365174293518,\n",
       "   0.42087021470069885,\n",
       "   0.42024537920951843,\n",
       "   0.41898301243782043,\n",
       "   0.420363187789917,\n",
       "   0.4152257442474365,\n",
       "   0.4216201603412628,\n",
       "   0.4137100577354431,\n",
       "   0.40786173939704895,\n",
       "   0.41120466589927673,\n",
       "   0.40774840116500854,\n",
       "   0.4107477366924286,\n",
       "   0.4196391701698303,\n",
       "   0.4188653528690338,\n",
       "   0.41789957880973816,\n",
       "   0.41927891969680786,\n",
       "   0.41774576902389526,\n",
       "   0.4170726239681244,\n",
       "   0.4163994789123535,\n",
       "   0.41774269938468933,\n",
       "   0.4153392016887665,\n",
       "   0.40961071848869324,\n",
       "   0.40862318873405457,\n",
       "   0.40377551317214966,\n",
       "   0.40395572781562805,\n",
       "   0.39904728531837463,\n",
       "   0.39768078923225403,\n",
       "   0.39694929122924805,\n",
       "   0.400814026594162,\n",
       "   0.40091317892074585,\n",
       "   0.40463975071907043,\n",
       "   0.40441465377807617,\n",
       "   0.40274474024772644,\n",
       "   0.40163174271583557,\n",
       "   0.40258339047431946,\n",
       "   0.4068484604358673,\n",
       "   0.4140385687351227,\n",
       "   0.4147188663482666,\n",
       "   0.41270026564598083,\n",
       "   0.4141746461391449,\n",
       "   0.41244053840637207,\n",
       "   0.4132114350795746,\n",
       "   0.4119245409965515,\n",
       "   0.4083959758281708,\n",
       "   0.41071662306785583,\n",
       "   0.4142005145549774,\n",
       "   0.4047169089317322,\n",
       "   0.4015023708343506,\n",
       "   0.4072916805744171,\n",
       "   0.4137668013572693,\n",
       "   0.4128325283527374,\n",
       "   0.41234704852104187,\n",
       "   0.405505508184433,\n",
       "   0.409901887178421,\n",
       "   0.4073585569858551,\n",
       "   0.4043058454990387,\n",
       "   0.4085101783275604,\n",
       "   0.414921373128891,\n",
       "   0.41061434149742126,\n",
       "   0.41761907935142517,\n",
       "   0.4176032245159149,\n",
       "   0.4194156527519226,\n",
       "   0.4188465476036072,\n",
       "   0.4135494530200958,\n",
       "   0.41662272810935974,\n",
       "   0.4164563715457916,\n",
       "   0.4147866666316986,\n",
       "   0.41021934151649475,\n",
       "   0.40754151344299316,\n",
       "   0.410282701253891,\n",
       "   0.4012654423713684,\n",
       "   0.4005299508571625,\n",
       "   0.4010915160179138,\n",
       "   0.3975440561771393,\n",
       "   0.39680054783821106,\n",
       "   0.3958132565021515,\n",
       "   0.39595404267311096,\n",
       "   0.399522602558136,\n",
       "   0.39760202169418335,\n",
       "   0.3985738754272461,\n",
       "   0.39897528290748596,\n",
       "   0.4093356430530548,\n",
       "   0.4093676209449768,\n",
       "   0.4052433669567108,\n",
       "   0.4045991599559784,\n",
       "   0.40962472558021545,\n",
       "   0.4057888686656952,\n",
       "   0.40323400497436523,\n",
       "   0.39806559681892395,\n",
       "   0.39894795417785645,\n",
       "   0.39584681391716003,\n",
       "   0.3949538767337799,\n",
       "   0.39857134222984314,\n",
       "   0.39883020520210266,\n",
       "   0.4006182551383972,\n",
       "   0.40466421842575073,\n",
       "   0.4047781825065613,\n",
       "   0.40873679518699646,\n",
       "   0.40780261158943176,\n",
       "   0.4063650965690613,\n",
       "   0.40843865275382996,\n",
       "   0.4047246277332306,\n",
       "   0.4022487998008728,\n",
       "   0.3990895450115204,\n",
       "   0.4000110328197479,\n",
       "   0.4046662747859955,\n",
       "   0.40288853645324707,\n",
       "   0.40577414631843567,\n",
       "   0.40223458409309387,\n",
       "   0.40193480253219604,\n",
       "   0.4019724726676941,\n",
       "   0.4012490212917328,\n",
       "   0.39476463198661804,\n",
       "   0.3972596824169159,\n",
       "   0.3982867896556854,\n",
       "   0.3944627642631531,\n",
       "   0.3957703411579132],\n",
       "  'test_losses': [35.76037481427193,\n",
       "   24.863489717245102,\n",
       "   17.725550189614296,\n",
       "   13.532220333814621,\n",
       "   11.660665571689606,\n",
       "   9.837700337171555,\n",
       "   8.12174342572689,\n",
       "   6.977387562394142,\n",
       "   6.1857908964157104,\n",
       "   5.503376096487045,\n",
       "   5.011377468705177,\n",
       "   4.67974990606308,\n",
       "   4.317335769534111,\n",
       "   3.7775720357894897,\n",
       "   3.4810985177755356,\n",
       "   3.2012211829423904,\n",
       "   3.0336562991142273,\n",
       "   2.951836436986923,\n",
       "   2.829527884721756,\n",
       "   2.8336227387189865,\n",
       "   2.66051185131073,\n",
       "   2.5929511040449142,\n",
       "   2.530724138021469,\n",
       "   2.4975554421544075,\n",
       "   2.408903583884239,\n",
       "   2.3797827512025833,\n",
       "   2.351610481739044,\n",
       "   2.3029273450374603,\n",
       "   2.310072273015976,\n",
       "   2.294742189347744,\n",
       "   2.1413468569517136,\n",
       "   2.1208433508872986,\n",
       "   2.0137752071022987,\n",
       "   2.0688181445002556,\n",
       "   2.0847171768546104,\n",
       "   2.006997123360634,\n",
       "   1.9634720459580421,\n",
       "   1.9893246367573738,\n",
       "   1.9852467998862267,\n",
       "   1.9442741945385933,\n",
       "   1.9615741670131683,\n",
       "   1.8929686918854713,\n",
       "   1.8553199656307697,\n",
       "   1.8577243387699127,\n",
       "   1.885860189795494,\n",
       "   1.8361990675330162,\n",
       "   1.872063659131527,\n",
       "   1.835521124303341,\n",
       "   1.8568090200424194,\n",
       "   1.8464566245675087,\n",
       "   1.819340243935585,\n",
       "   1.742376685142517,\n",
       "   1.701782003045082,\n",
       "   1.7502050623297691,\n",
       "   1.7681685537099838,\n",
       "   1.7059895992279053,\n",
       "   1.6984989047050476,\n",
       "   1.6006972342729568,\n",
       "   1.6721670478582382,\n",
       "   1.706574521958828,\n",
       "   1.65268474817276,\n",
       "   1.6829012781381607,\n",
       "   1.7347371876239777,\n",
       "   1.7812829613685608,\n",
       "   1.7867587208747864,\n",
       "   1.8068558350205421,\n",
       "   1.8244347423315048,\n",
       "   1.8809239566326141,\n",
       "   1.8594249561429024,\n",
       "   1.829537183046341,\n",
       "   1.8458538874983788,\n",
       "   1.8623266071081161,\n",
       "   1.9248924925923347,\n",
       "   1.8384793400764465,\n",
       "   1.8215624615550041,\n",
       "   1.821144811809063,\n",
       "   1.8422295823693275,\n",
       "   1.876257747411728,\n",
       "   1.8558507785201073,\n",
       "   1.8752219304442406,\n",
       "   1.867971271276474,\n",
       "   1.867978610098362,\n",
       "   1.9912429377436638,\n",
       "   1.9499018415808678,\n",
       "   1.906136229634285,\n",
       "   1.8832269608974457,\n",
       "   1.8803583234548569,\n",
       "   1.87111796438694,\n",
       "   1.883119374513626,\n",
       "   1.837546557188034,\n",
       "   1.8579618483781815,\n",
       "   1.8414952009916306,\n",
       "   1.763930156826973,\n",
       "   1.7708668932318687,\n",
       "   1.7920304462313652,\n",
       "   1.7717055156826973,\n",
       "   1.6870767027139664,\n",
       "   1.7166858837008476,\n",
       "   1.7082618549466133,\n",
       "   1.684228554368019,\n",
       "   1.726735271513462,\n",
       "   1.7104949355125427,\n",
       "   1.728619508445263,\n",
       "   1.7619322836399078,\n",
       "   1.8138421699404716,\n",
       "   1.8101002275943756,\n",
       "   1.7851106971502304,\n",
       "   1.801007241010666,\n",
       "   1.7872494459152222,\n",
       "   1.7899460941553116,\n",
       "   1.793865516781807,\n",
       "   1.786674402654171,\n",
       "   1.770173691213131,\n",
       "   1.735518217086792,\n",
       "   1.7305582389235497,\n",
       "   1.7412099614739418,\n",
       "   1.7293682023882866,\n",
       "   1.7127429768443108,\n",
       "   1.7525573447346687,\n",
       "   1.7935694232583046,\n",
       "   1.7097381874918938,\n",
       "   1.712500810623169,\n",
       "   1.7822375670075417,\n",
       "   1.8319090604782104,\n",
       "   1.84781214594841,\n",
       "   1.8576361685991287,\n",
       "   1.8039308562874794,\n",
       "   1.8219089210033417,\n",
       "   1.7549753785133362,\n",
       "   1.8004078269004822,\n",
       "   1.846364863216877,\n",
       "   1.8436791747808456,\n",
       "   1.819705754518509,\n",
       "   1.7457318156957626,\n",
       "   1.7889320775866508,\n",
       "   1.7931739762425423,\n",
       "   1.872524455189705,\n",
       "   1.923073522746563,\n",
       "   1.920421302318573,\n",
       "   1.9431466683745384,\n",
       "   1.8566363975405693,\n",
       "   1.8251587897539139,\n",
       "   1.8268521875143051,\n",
       "   1.8070655688643456,\n",
       "   1.8330248966813087,\n",
       "   1.7650904133915901,\n",
       "   1.7647035717964172,\n",
       "   1.6934598535299301,\n",
       "   1.6981258168816566,\n",
       "   1.6462619677186012,\n",
       "   1.6868993118405342,\n",
       "   1.6744685396552086,\n",
       "   1.6725737378001213,\n",
       "   1.6363946571946144,\n",
       "   1.6617814749479294,\n",
       "   1.6948735043406487,\n",
       "   1.689238891005516,\n",
       "   1.7100061252713203,\n",
       "   1.7894165366888046,\n",
       "   1.70721834897995,\n",
       "   1.69169732183218,\n",
       "   1.682947762310505,\n",
       "   1.692873753607273,\n",
       "   1.6514211893081665,\n",
       "   1.7051038071513176,\n",
       "   1.6998849660158157,\n",
       "   1.7461584955453873,\n",
       "   1.757923111319542,\n",
       "   1.747158594429493,\n",
       "   1.7257007583975792,\n",
       "   1.738067239522934,\n",
       "   1.738397616893053,\n",
       "   1.7374806478619576,\n",
       "   1.7028489783406258,\n",
       "   1.6822943612933159,\n",
       "   1.6104519292712212,\n",
       "   1.6356694772839546,\n",
       "   1.5923692025244236,\n",
       "   1.5626567415893078,\n",
       "   1.6169860884547234,\n",
       "   1.7047179527580738,\n",
       "   1.7897257655858994,\n",
       "   1.7908294163644314,\n",
       "   1.7225474081933498,\n",
       "   1.683043908327818,\n",
       "   1.6608210392296314,\n",
       "   1.722909901291132,\n",
       "   1.6853481568396091,\n",
       "   1.6366105787456036,\n",
       "   1.6377858631312847,\n",
       "   1.6757104806602001,\n",
       "   1.6320589296519756,\n",
       "   1.638891827315092,\n",
       "   1.6061873994767666,\n",
       "   1.5907270163297653,\n",
       "   1.5837665274739265,\n",
       "   1.5823146291077137,\n",
       "   1.6407838426530361,\n",
       "   1.6146954782307148,\n",
       "   1.5708128288388252,\n",
       "   1.6122372597455978,\n",
       "   1.6561812870204449,\n",
       "   1.6885285153985023,\n",
       "   1.7222873978316784,\n",
       "   1.794538401067257,\n",
       "   1.728463165462017,\n",
       "   1.745345663279295,\n",
       "   1.7484175637364388,\n",
       "   1.7438528016209602,\n",
       "   1.7048000060021877,\n",
       "   1.7644781656563282,\n",
       "   1.7216686382889748,\n",
       "   1.7623869106173515,\n",
       "   1.8219452276825905,\n",
       "   1.7879100255668163,\n",
       "   1.7425986602902412,\n",
       "   1.742581196129322,\n",
       "   1.7347659692168236,\n",
       "   1.7572741955518723,\n",
       "   1.7375614903867245,\n",
       "   1.7163258232176304,\n",
       "   1.6739712953567505,\n",
       "   1.6792027279734612,\n",
       "   1.7693188712000847,\n",
       "   1.790279783308506,\n",
       "   1.8010747767984867,\n",
       "   1.7905833832919598,\n",
       "   1.7563196681439877,\n",
       "   1.7126666344702244,\n",
       "   1.7593812681734562,\n",
       "   1.6798300221562386,\n",
       "   1.6354374922811985,\n",
       "   1.6344058066606522,\n",
       "   1.6298947632312775,\n",
       "   1.651326410472393,\n",
       "   1.557561255991459,\n",
       "   1.5206265226006508,\n",
       "   1.481086853891611,\n",
       "   1.5220272652804852,\n",
       "   1.494054440408945,\n",
       "   1.4645022377371788,\n",
       "   1.486818090081215,\n",
       "   1.4674232676625252,\n",
       "   1.4645618237555027,\n",
       "   1.5014440827071667,\n",
       "   1.537760902196169,\n",
       "   1.5294517688453197,\n",
       "   1.4976909272372723,\n",
       "   1.5282552801072598,\n",
       "   1.5357580967247486,\n",
       "   1.566394615918398,\n",
       "   1.590282279998064,\n",
       "   1.6224357634782791,\n",
       "   1.5671495832502842,\n",
       "   1.527328833937645,\n",
       "   1.5120717324316502,\n",
       "   1.5661416910588741,\n",
       "   1.5483705140650272,\n",
       "   1.5125627554953098,\n",
       "   1.5570724941790104,\n",
       "   1.5345547460019588,\n",
       "   1.6020411849021912,\n",
       "   1.6044238209724426,\n",
       "   1.6018306538462639,\n",
       "   1.6153594143688679,\n",
       "   1.5767858810722828,\n",
       "   1.562945045530796,\n",
       "   1.5589348673820496,\n",
       "   1.5734794326126575,\n",
       "   1.5789688304066658,\n",
       "   1.5021523423492908,\n",
       "   1.5043045990169048,\n",
       "   1.5098712220788002,\n",
       "   1.5383171997964382,\n",
       "   1.5312896743416786,\n",
       "   1.5108861550688744,\n",
       "   1.5613284297287464,\n",
       "   1.5867094434797764,\n",
       "   1.5794310793280602,\n",
       "   1.5691146031022072,\n",
       "   1.5477438904345036,\n",
       "   1.5791531465947628,\n",
       "   1.6001417972147465,\n",
       "   1.5772969499230385,\n",
       "   1.6017997786402702,\n",
       "   1.567446231842041,\n",
       "   1.5418955348432064,\n",
       "   1.5603371933102608,\n",
       "   1.5281561091542244,\n",
       "   1.5702994614839554,\n",
       "   1.5648371279239655,\n",
       "   1.618442378938198,\n",
       "   1.6538764610886574,\n",
       "   1.6572247222065926,\n",
       "   1.701421096920967,\n",
       "   1.7504836954176426,\n",
       "   1.733970519155264,\n",
       "   1.750726979225874,\n",
       "   1.7120177410542965,\n",
       "   1.6994989104568958,\n",
       "   1.6923592127859592,\n",
       "   1.673389796167612,\n",
       "   1.6534952484071255,\n",
       "   1.6552180275321007,\n",
       "   1.6590225212275982,\n",
       "   1.6403263807296753,\n",
       "   1.6272481344640255,\n",
       "   1.5735116116702557,\n",
       "   1.581385713070631,\n",
       "   1.6367549821734428,\n",
       "   1.6801073364913464,\n",
       "   1.6992149464786053,\n",
       "   1.6318962387740612,\n",
       "   1.6668724119663239,\n",
       "   1.699489627033472,\n",
       "   1.703740045428276,\n",
       "   1.6197471804916859,\n",
       "   1.634964071214199,\n",
       "   1.652159184217453,\n",
       "   1.608950361609459,\n",
       "   1.6157136484980583,\n",
       "   1.6069751642644405,\n",
       "   1.620421577244997,\n",
       "   1.5914452113211155,\n",
       "   1.6233775205910206,\n",
       "   1.54525588452816,\n",
       "   1.561581052839756,\n",
       "   1.5067837126553059,\n",
       "   1.5179322771728039,\n",
       "   1.5585584416985512,\n",
       "   1.589409425854683,\n",
       "   1.600096344947815,\n",
       "   1.6044364981353283,\n",
       "   1.641462229192257,\n",
       "   1.6174533180892467,\n",
       "   1.6093080267310143,\n",
       "   1.5904316380620003,\n",
       "   1.61223329231143,\n",
       "   1.6090962886810303,\n",
       "   1.6043907478451729,\n",
       "   1.56354096904397,\n",
       "   1.5379541628062725,\n",
       "   1.5419051758944988,\n",
       "   1.5475915558636189,\n",
       "   1.5256247334182262,\n",
       "   1.449545107781887,\n",
       "   1.4617525860667229,\n",
       "   1.52334900572896,\n",
       "   1.5056094154715538,\n",
       "   1.515661809593439,\n",
       "   1.5126395002007484,\n",
       "   1.4832975529134274,\n",
       "   1.4670608565211296,\n",
       "   1.4900566339492798,\n",
       "   1.5104173347353935,\n",
       "   1.4911285415291786,\n",
       "   1.5773383937776089,\n",
       "   1.584884811192751,\n",
       "   1.5560449659824371,\n",
       "   1.537098579108715,\n",
       "   1.5296656787395477,\n",
       "   1.521780252456665,\n",
       "   1.4889989271759987,\n",
       "   1.521739512681961,\n",
       "   1.5018896088004112,\n",
       "   1.4893029779195786,\n",
       "   1.4804424569010735,\n",
       "   1.4722352474927902,\n",
       "   1.5141528993844986,\n",
       "   1.5661335587501526,\n",
       "   1.5576497912406921,\n",
       "   1.5752448588609695,\n",
       "   1.531776785850525,\n",
       "   1.5439395979046822,\n",
       "   1.6236827038228512,\n",
       "   1.603366132825613,\n",
       "   1.601090781390667,\n",
       "   1.6136239804327488,\n",
       "   1.585577342659235,\n",
       "   1.595184825360775,\n",
       "   1.5958522036671638,\n",
       "   1.5692282542586327,\n",
       "   1.5573451928794384,\n",
       "   1.5523992106318474,\n",
       "   1.5840583443641663,\n",
       "   1.5596657544374466,\n",
       "   1.54662024974823,\n",
       "   1.5348945334553719,\n",
       "   1.5600536242127419,\n",
       "   1.5118251889944077,\n",
       "   1.5444108918309212,\n",
       "   1.5367275103926659,\n",
       "   1.5391170606017113,\n",
       "   1.632932335138321,\n",
       "   1.5964826047420502,\n",
       "   1.6086016818881035,\n",
       "   1.5869990438222885,\n",
       "   1.5836143419146538,\n",
       "   1.5562810003757477,\n",
       "   1.5759943574666977,\n",
       "   1.5378530770540237,\n",
       "   1.5404491797089577,\n",
       "   1.5589261464774609,\n",
       "   1.563452985137701,\n",
       "   1.5290292166173458,\n",
       "   1.4822813086211681,\n",
       "   1.4423064030706882,\n",
       "   1.465546127408743,\n",
       "   1.4707466512918472,\n",
       "   1.4622076824307442,\n",
       "   1.4691240191459656,\n",
       "   1.5035777650773525,\n",
       "   1.4925093837082386,\n",
       "   1.437694512307644,\n",
       "   1.4373174495995045,\n",
       "   1.4399037286639214,\n",
       "   1.4612164534628391,\n",
       "   1.4509420543909073,\n",
       "   1.416917685419321,\n",
       "   1.4265353418886662,\n",
       "   1.3753485307097435,\n",
       "   1.4114002138376236,\n",
       "   1.4191229231655598,\n",
       "   1.455314390361309,\n",
       "   1.4300351366400719,\n",
       "   1.4100533910095692,\n",
       "   1.41674729809165,\n",
       "   1.413759857416153,\n",
       "   1.3606475666165352,\n",
       "   1.409278228878975,\n",
       "   1.3684985749423504,\n",
       "   1.3680668771266937,\n",
       "   1.3854073025286198,\n",
       "   1.3679851405322552,\n",
       "   1.3563277274370193,\n",
       "   1.3816751576960087,\n",
       "   1.3801255710422993,\n",
       "   1.3760029785335064,\n",
       "   1.3744461052119732,\n",
       "   1.369994256645441,\n",
       "   1.376905508339405,\n",
       "   1.3687128312885761,\n",
       "   1.3717888370156288,\n",
       "   1.3609283305704594,\n",
       "   1.3713777959346771,\n",
       "   1.4374431855976582,\n",
       "   1.4561463184654713,\n",
       "   1.4177751317620277,\n",
       "   1.463481344282627,\n",
       "   1.4853068962693214,\n",
       "   1.5022985711693764,\n",
       "   1.510744359344244,\n",
       "   1.4824057519435883,\n",
       "   1.4692974351346493,\n",
       "   1.4573907740414143,\n",
       "   1.4826783686876297,\n",
       "   1.4050485379993916,\n",
       "   1.4325972646474838,\n",
       "   1.3991169221699238,\n",
       "   1.3842843919992447,\n",
       "   1.3941846899688244,\n",
       "   1.4204579517245293,\n",
       "   1.414832141250372,\n",
       "   1.4288229309022427,\n",
       "   1.3924793787300587,\n",
       "   1.4304549396038055,\n",
       "   1.4548881389200687,\n",
       "   1.472890842705965,\n",
       "   1.4857375733554363,\n",
       "   1.4743083789944649,\n",
       "   1.4694453924894333,\n",
       "   1.4571985080838203,\n",
       "   1.4586838632822037,\n",
       "   1.4671129137277603,\n",
       "   1.4434451088309288,\n",
       "   1.4545534923672676,\n",
       "   1.4674382880330086,\n",
       "   1.457013227045536,\n",
       "   1.520128358155489,\n",
       "   1.5060976296663284,\n",
       "   1.4590650536119938,\n",
       "   1.3882895112037659,\n",
       "   1.3474007435142994,\n",
       "   1.3971655070781708,\n",
       "   1.4302963539958,\n",
       "   1.4001554735004902,\n",
       "   1.3573033064603806,\n",
       "   1.3897511139512062,\n",
       "   1.3954792954027653,\n",
       "   1.378907721489668,\n",
       "   1.452156212180853,\n",
       "   1.464243184775114,\n",
       "   1.4957273229956627,\n",
       "   1.4972247816622257,\n",
       "   1.5009147748351097,\n",
       "   1.4533975049853325,\n",
       "   1.4364082366228104,\n",
       "   1.4772146306931973,\n",
       "   1.4912952557206154,\n",
       "   1.5192457996308804],\n",
       "  'pce_acc': [43263.625,\n",
       "   43187.21484375,\n",
       "   43129.72265625,\n",
       "   43087.92578125,\n",
       "   43066.58203125,\n",
       "   43045.125,\n",
       "   43022.84375,\n",
       "   43004.9296875,\n",
       "   42991.63671875,\n",
       "   42980.18359375,\n",
       "   42969.703125,\n",
       "   42963.36328125,\n",
       "   42956.37109375,\n",
       "   42945.2890625,\n",
       "   42936.5078125,\n",
       "   42932.1484375,\n",
       "   42926.8515625,\n",
       "   42924.0625,\n",
       "   42921.17578125,\n",
       "   42920.14453125,\n",
       "   42916.34375,\n",
       "   42914.22265625,\n",
       "   42911.08984375,\n",
       "   42909.90625,\n",
       "   42907.53515625,\n",
       "   42906.3046875,\n",
       "   42904.92578125,\n",
       "   42903.5,\n",
       "   42904.3046875,\n",
       "   42904.77734375,\n",
       "   42899.89453125,\n",
       "   42898.33203125,\n",
       "   42894.9296875,\n",
       "   42896.28125,\n",
       "   42897.109375,\n",
       "   42896.45703125,\n",
       "   42894.3828125,\n",
       "   42895.27734375,\n",
       "   42895.4140625,\n",
       "   42895.75,\n",
       "   42895.75,\n",
       "   42892.8984375,\n",
       "   42889.578125,\n",
       "   42889.796875,\n",
       "   42890.2578125,\n",
       "   42888.45703125,\n",
       "   42889.30078125,\n",
       "   42889.1328125,\n",
       "   42889.10546875,\n",
       "   42888.203125,\n",
       "   42888.11328125,\n",
       "   42886.6796875,\n",
       "   42886.84375,\n",
       "   42887.546875,\n",
       "   42887.30078125,\n",
       "   42886.52734375,\n",
       "   42886.55859375,\n",
       "   42886.05859375,\n",
       "   42886.546875,\n",
       "   42886.80859375,\n",
       "   42886.41015625,\n",
       "   42887.0078125,\n",
       "   42888.3125,\n",
       "   42887.93359375,\n",
       "   42887.92578125,\n",
       "   42888.7890625,\n",
       "   42889.03515625,\n",
       "   42890.31640625,\n",
       "   42889.94140625,\n",
       "   42889.25390625,\n",
       "   42889.4296875,\n",
       "   42889.9375,\n",
       "   42891.84375,\n",
       "   42889.2578125,\n",
       "   42889.94140625,\n",
       "   42889.46875,\n",
       "   42889.05859375,\n",
       "   42889.6328125,\n",
       "   42888.8046875,\n",
       "   42888.90625,\n",
       "   42888.90234375,\n",
       "   42888.77734375,\n",
       "   42889.75390625,\n",
       "   42889.30859375,\n",
       "   42888.2734375,\n",
       "   42886.86328125,\n",
       "   42886.6484375,\n",
       "   42886.80859375,\n",
       "   42887.296875,\n",
       "   42886.86328125,\n",
       "   42887.296875,\n",
       "   42887.9609375,\n",
       "   42886.9296875,\n",
       "   42887.2265625,\n",
       "   42887.23046875,\n",
       "   42886.6796875,\n",
       "   42886.30078125,\n",
       "   42887.03125,\n",
       "   42886.83203125,\n",
       "   42886.46484375,\n",
       "   42886.7265625,\n",
       "   42887.328125,\n",
       "   42887.5703125,\n",
       "   42887.3046875,\n",
       "   42889.21484375,\n",
       "   42889.59765625,\n",
       "   42889.046875,\n",
       "   42889.45703125,\n",
       "   42889.15625,\n",
       "   42888.03125,\n",
       "   42888.6640625,\n",
       "   42889.328125,\n",
       "   42889.1328125,\n",
       "   42887.78515625,\n",
       "   42888.22265625,\n",
       "   42888.0859375,\n",
       "   42886.921875,\n",
       "   42886.65625,\n",
       "   42887.18359375,\n",
       "   42888.6796875,\n",
       "   42887.73828125,\n",
       "   42887.3125,\n",
       "   42888.5234375,\n",
       "   42889.453125,\n",
       "   42888.73046875,\n",
       "   42889.21875,\n",
       "   42888.97265625,\n",
       "   42888.984375,\n",
       "   42888.02734375,\n",
       "   42888.8515625,\n",
       "   42889.8046875,\n",
       "   42889.42578125,\n",
       "   42887.8984375,\n",
       "   42886.6796875,\n",
       "   42887.5078125,\n",
       "   42887.26171875,\n",
       "   42889.21484375,\n",
       "   42890.31640625,\n",
       "   42890.30859375,\n",
       "   42890.203125,\n",
       "   42888.42578125,\n",
       "   42887.51953125,\n",
       "   42888.39453125,\n",
       "   42888.49609375,\n",
       "   42889.60546875,\n",
       "   42888.26171875,\n",
       "   42887.36328125,\n",
       "   42885.984375,\n",
       "   42886.1796875,\n",
       "   42885.953125,\n",
       "   42887.5078125,\n",
       "   42887.421875,\n",
       "   42887.265625,\n",
       "   42885.984375,\n",
       "   42886.3046875,\n",
       "   42886.08984375,\n",
       "   42886.34375,\n",
       "   42886.1015625,\n",
       "   42887.3203125,\n",
       "   42886.234375,\n",
       "   42886.2109375,\n",
       "   42886.02734375,\n",
       "   42886.015625,\n",
       "   42885.54296875,\n",
       "   42886.5390625,\n",
       "   42886.94921875,\n",
       "   42887.5,\n",
       "   42887.9921875,\n",
       "   42887.8203125,\n",
       "   42887.20703125,\n",
       "   42887.234375,\n",
       "   42886.0546875,\n",
       "   42886.08203125,\n",
       "   42885.89453125,\n",
       "   42886.109375,\n",
       "   42885.63671875,\n",
       "   42885.7890625,\n",
       "   42885.46875,\n",
       "   42885.359375,\n",
       "   42885.72265625,\n",
       "   42887.1875,\n",
       "   42889.11328125,\n",
       "   42889.296875,\n",
       "   42887.5546875,\n",
       "   42886.47265625,\n",
       "   42887.0546875,\n",
       "   42887.58203125,\n",
       "   42886.15234375,\n",
       "   42885.44140625,\n",
       "   42885.56640625,\n",
       "   42886.23828125,\n",
       "   42885.7890625,\n",
       "   42886.5546875,\n",
       "   42885.8203125,\n",
       "   42885.3046875,\n",
       "   42885.109375,\n",
       "   42885.00390625,\n",
       "   42885.40625,\n",
       "   42885.50390625,\n",
       "   42885.06640625,\n",
       "   42885.56640625,\n",
       "   42886.42578125,\n",
       "   42886.44140625,\n",
       "   42887.66015625,\n",
       "   42889.25,\n",
       "   42887.03125,\n",
       "   42887.01171875,\n",
       "   42887.484375,\n",
       "   42887.578125,\n",
       "   42886.5859375,\n",
       "   42887.44921875,\n",
       "   42886.734375,\n",
       "   42887.46484375,\n",
       "   42888.7265625,\n",
       "   42887.3046875,\n",
       "   42886.1796875,\n",
       "   42886.015625,\n",
       "   42886.35546875,\n",
       "   42886.1875,\n",
       "   42885.5390625,\n",
       "   42885.5625,\n",
       "   42885.46875,\n",
       "   42885.734375,\n",
       "   42887.21484375,\n",
       "   42888.06640625,\n",
       "   42888.69921875,\n",
       "   42888.3984375,\n",
       "   42886.28515625,\n",
       "   42885.3203125,\n",
       "   42885.70703125,\n",
       "   42884.99609375,\n",
       "   42884.81640625,\n",
       "   42884.69140625,\n",
       "   42884.58984375,\n",
       "   42884.92578125,\n",
       "   42884.30078125,\n",
       "   42883.75,\n",
       "   42883.296875,\n",
       "   42883.53515625,\n",
       "   42883.3671875,\n",
       "   42883.12890625,\n",
       "   42883.73046875,\n",
       "   42883.8359375,\n",
       "   42883.6484375,\n",
       "   42884.46484375,\n",
       "   42884.80078125,\n",
       "   42884.83203125,\n",
       "   42884.89453125,\n",
       "   42885.21484375,\n",
       "   42885.23046875,\n",
       "   42885.0390625,\n",
       "   42885.30859375,\n",
       "   42885.46875,\n",
       "   42885.203125,\n",
       "   42884.73046875,\n",
       "   42884.33203125,\n",
       "   42884.78515625,\n",
       "   42884.66015625,\n",
       "   42884.6875,\n",
       "   42884.98046875,\n",
       "   42884.8125,\n",
       "   42885.95703125,\n",
       "   42884.94140625,\n",
       "   42884.8046875,\n",
       "   42885.796875,\n",
       "   42884.6875,\n",
       "   42884.6015625,\n",
       "   42885.0078125,\n",
       "   42885.09765625,\n",
       "   42885.890625,\n",
       "   42884.8125,\n",
       "   42884.96875,\n",
       "   42884.91015625,\n",
       "   42885.10546875,\n",
       "   42885.4765625,\n",
       "   42885.32421875,\n",
       "   42885.55078125,\n",
       "   42885.875,\n",
       "   42885.91015625,\n",
       "   42885.5546875,\n",
       "   42885.25390625,\n",
       "   42885.46875,\n",
       "   42885.8125,\n",
       "   42885.43359375,\n",
       "   42886.40234375,\n",
       "   42885.46875,\n",
       "   42885.05078125,\n",
       "   42884.609375,\n",
       "   42884.26953125,\n",
       "   42884.58984375,\n",
       "   42884.578125,\n",
       "   42885.06640625,\n",
       "   42884.70703125,\n",
       "   42884.96484375,\n",
       "   42885.234375,\n",
       "   42885.33203125,\n",
       "   42885.34765625,\n",
       "   42886.61328125,\n",
       "   42885.21484375,\n",
       "   42885.63671875,\n",
       "   42885.33203125,\n",
       "   42885.62890625,\n",
       "   42885.5,\n",
       "   42885.515625,\n",
       "   42885.65234375,\n",
       "   42885.1484375,\n",
       "   42884.96875,\n",
       "   42884.21875,\n",
       "   42884.078125,\n",
       "   42884.5625,\n",
       "   42885.234375,\n",
       "   42885.46484375,\n",
       "   42885.10546875,\n",
       "   42885.23046875,\n",
       "   42885.484375,\n",
       "   42885.4296875,\n",
       "   42884.52734375,\n",
       "   42884.46484375,\n",
       "   42885.0234375,\n",
       "   42884.44140625,\n",
       "   42884.6328125,\n",
       "   42885.1796875,\n",
       "   42885.3515625,\n",
       "   42885.30859375,\n",
       "   42886.20703125,\n",
       "   42885.9609375,\n",
       "   42886.48828125,\n",
       "   42885.7109375,\n",
       "   42885.93359375,\n",
       "   42887.109375,\n",
       "   42888.2734375,\n",
       "   42888.0390625,\n",
       "   42888.453125,\n",
       "   42889.47265625,\n",
       "   42887.94140625,\n",
       "   42886.8984375,\n",
       "   42885.8828125,\n",
       "   42885.8203125,\n",
       "   42885.67578125,\n",
       "   42885.70703125,\n",
       "   42885.54296875,\n",
       "   42885.17578125,\n",
       "   42885.22265625,\n",
       "   42885.26953125,\n",
       "   42885.25390625,\n",
       "   42884.5078125,\n",
       "   42884.6484375,\n",
       "   42885.453125,\n",
       "   42885.55078125,\n",
       "   42885.01953125,\n",
       "   42885.30859375,\n",
       "   42884.84375,\n",
       "   42884.81640625,\n",
       "   42884.56640625,\n",
       "   42884.3515625,\n",
       "   42883.80859375,\n",
       "   42884.296875,\n",
       "   42884.59765625,\n",
       "   42884.359375,\n",
       "   42883.796875,\n",
       "   42883.61328125,\n",
       "   42883.6875,\n",
       "   42883.30859375,\n",
       "   42883.96484375,\n",
       "   42883.671875,\n",
       "   42883.640625,\n",
       "   42883.3828125,\n",
       "   42883.2421875,\n",
       "   42883.84375,\n",
       "   42884.06640625,\n",
       "   42884.1640625,\n",
       "   42884.1328125,\n",
       "   42884.08203125,\n",
       "   42884.03125,\n",
       "   42884.953125,\n",
       "   42884.80078125,\n",
       "   42884.7109375,\n",
       "   42885.20703125,\n",
       "   42885.05078125,\n",
       "   42885.4296875,\n",
       "   42885.15234375,\n",
       "   42885.17578125,\n",
       "   42885.0546875,\n",
       "   42884.6015625,\n",
       "   42884.33203125,\n",
       "   42883.9921875,\n",
       "   42883.95703125,\n",
       "   42883.98046875,\n",
       "   42884.12109375,\n",
       "   42883.953125,\n",
       "   42884.703125,\n",
       "   42884.53125,\n",
       "   42884.421875,\n",
       "   42885.5390625,\n",
       "   42885.05859375,\n",
       "   42885.0234375,\n",
       "   42884.7734375,\n",
       "   42884.53125,\n",
       "   42884.00390625,\n",
       "   42883.89453125,\n",
       "   42883.4765625,\n",
       "   42883.62890625,\n",
       "   42883.83203125,\n",
       "   42883.5859375,\n",
       "   42883.890625,\n",
       "   42883.71484375,\n",
       "   42883.80859375,\n",
       "   42883.59765625,\n",
       "   42883.94140625,\n",
       "   42883.6328125,\n",
       "   42883.6015625,\n",
       "   42883.68359375,\n",
       "   42883.93359375,\n",
       "   42883.57421875,\n",
       "   42883.921875,\n",
       "   42884.05859375,\n",
       "   42884.21484375,\n",
       "   42883.83984375,\n",
       "   42883.37890625,\n",
       "   42883.1015625,\n",
       "   42882.4140625,\n",
       "   42883.02734375,\n",
       "   42883.05078125,\n",
       "   42883.06640625,\n",
       "   42883.046875,\n",
       "   42882.6953125,\n",
       "   42882.8125,\n",
       "   42882.7890625,\n",
       "   42881.953125,\n",
       "   42882.765625,\n",
       "   42882.7890625,\n",
       "   42882.21875,\n",
       "   42882.34375,\n",
       "   42882.55078125,\n",
       "   42882.78515625,\n",
       "   42883.4921875,\n",
       "   42883.9765625,\n",
       "   42883.59375,\n",
       "   42882.84765625,\n",
       "   42883.02734375,\n",
       "   42883.12890625,\n",
       "   42883.02734375,\n",
       "   42882.81640625,\n",
       "   42882.9375,\n",
       "   42882.875,\n",
       "   42883.18359375,\n",
       "   42883.29296875,\n",
       "   42883.2109375,\n",
       "   42884.02734375,\n",
       "   42884.53125,\n",
       "   42884.7265625,\n",
       "   42884.3671875,\n",
       "   42883.9375,\n",
       "   42884.40234375,\n",
       "   42884.984375,\n",
       "   42884.328125,\n",
       "   42883.76171875,\n",
       "   42883.8828125,\n",
       "   42883.37109375,\n",
       "   42883.515625,\n",
       "   42883.4765625,\n",
       "   42883.76171875,\n",
       "   42883.54296875,\n",
       "   42883.140625,\n",
       "   42882.70703125,\n",
       "   42883.1484375,\n",
       "   42882.8203125,\n",
       "   42882.8203125,\n",
       "   42882.78125,\n",
       "   42882.890625,\n",
       "   42883.015625,\n",
       "   42882.8828125,\n",
       "   42882.6953125,\n",
       "   42883.046875,\n",
       "   42882.73828125,\n",
       "   42883.1875,\n",
       "   42883.49609375,\n",
       "   42883.140625,\n",
       "   42884.23828125,\n",
       "   42884.578125,\n",
       "   42884.3359375,\n",
       "   42883.6875,\n",
       "   42883.3671875,\n",
       "   42883.28515625,\n",
       "   42883.59375,\n",
       "   42883.62890625,\n",
       "   42883.2265625,\n",
       "   42883.4375,\n",
       "   42883.6484375,\n",
       "   42883.8828125,\n",
       "   42884.44140625,\n",
       "   42884.60546875,\n",
       "   42884.84375,\n",
       "   42884.5546875,\n",
       "   42884.34375,\n",
       "   42883.84375,\n",
       "   42883.5546875,\n",
       "   42883.76953125,\n",
       "   42883.9765625,\n",
       "   42884.4765625],\n",
       "  'voc_acc': [42898.9921875,\n",
       "   42894.9140625,\n",
       "   42880.40234375,\n",
       "   42882.33203125,\n",
       "   42885.34765625,\n",
       "   42881.765625,\n",
       "   42876.3203125,\n",
       "   42879.90234375,\n",
       "   42878.73828125,\n",
       "   42875.56640625,\n",
       "   42877.36328125,\n",
       "   42875.82421875,\n",
       "   42875.5546875,\n",
       "   42872.5234375,\n",
       "   42873.7890625,\n",
       "   42872.0625,\n",
       "   42871.1171875,\n",
       "   42871.93359375,\n",
       "   42870.125,\n",
       "   42869.1484375,\n",
       "   42867.68359375,\n",
       "   42867.58203125,\n",
       "   42867.50390625,\n",
       "   42867.3984375,\n",
       "   42867.6484375,\n",
       "   42867.5546875,\n",
       "   42867.65234375,\n",
       "   42867.79296875,\n",
       "   42867.62890625,\n",
       "   42867.36328125,\n",
       "   42867.47265625,\n",
       "   42868.14453125,\n",
       "   42867.5546875,\n",
       "   42867.44921875,\n",
       "   42867.55078125,\n",
       "   42867.62890625,\n",
       "   42867.55859375,\n",
       "   42867.65625,\n",
       "   42867.65625,\n",
       "   42867.6015625,\n",
       "   42867.58203125,\n",
       "   42867.625,\n",
       "   42867.79296875,\n",
       "   42867.9375,\n",
       "   42867.1953125,\n",
       "   42867.1328125,\n",
       "   42867.265625,\n",
       "   42867.4296875,\n",
       "   42867.14453125,\n",
       "   42867.30859375,\n",
       "   42867.75,\n",
       "   42868.46484375,\n",
       "   42867.5,\n",
       "   42867.50390625,\n",
       "   42869.92578125,\n",
       "   42868.91015625,\n",
       "   42867.46484375,\n",
       "   42867.234375,\n",
       "   42866.62109375,\n",
       "   42866.38671875,\n",
       "   42866.2578125,\n",
       "   42866.16015625,\n",
       "   42866.16796875,\n",
       "   42866.109375,\n",
       "   42866.21484375,\n",
       "   42866.31640625,\n",
       "   42866.140625,\n",
       "   42865.69921875,\n",
       "   42865.83984375,\n",
       "   42865.7890625,\n",
       "   42866.0078125,\n",
       "   42866.16015625,\n",
       "   42866.3125,\n",
       "   42866.0390625,\n",
       "   42865.828125,\n",
       "   42865.703125,\n",
       "   42865.78515625,\n",
       "   42865.73046875,\n",
       "   42865.6640625,\n",
       "   42865.62890625,\n",
       "   42865.7890625,\n",
       "   42866.87109375,\n",
       "   42871.13671875,\n",
       "   42871.6328125,\n",
       "   42869.4140625,\n",
       "   42870.96875,\n",
       "   42873.34765625,\n",
       "   42872.359375,\n",
       "   42871.609375,\n",
       "   42870.9453125,\n",
       "   42868.828125,\n",
       "   42867.375,\n",
       "   42866.4765625,\n",
       "   42867.40625,\n",
       "   42866.7734375,\n",
       "   42867.921875,\n",
       "   42866.80078125,\n",
       "   42867.33984375,\n",
       "   42868.125,\n",
       "   42868.72265625,\n",
       "   42867.390625,\n",
       "   42866.9140625,\n",
       "   42867.2109375,\n",
       "   42866.8203125,\n",
       "   42866.25,\n",
       "   42866.19140625,\n",
       "   42865.6640625,\n",
       "   42865.71875,\n",
       "   42865.90234375,\n",
       "   42865.96484375,\n",
       "   42865.8828125,\n",
       "   42865.93359375,\n",
       "   42865.9921875,\n",
       "   42866.26953125,\n",
       "   42866.0234375,\n",
       "   42865.91015625,\n",
       "   42866.0859375,\n",
       "   42865.98046875,\n",
       "   42865.9375,\n",
       "   42865.875,\n",
       "   42865.96875,\n",
       "   42865.92578125,\n",
       "   42866.0390625,\n",
       "   42865.890625,\n",
       "   42866.015625,\n",
       "   42866.10546875,\n",
       "   42865.79296875,\n",
       "   42865.44921875,\n",
       "   42865.33984375,\n",
       "   42865.16796875,\n",
       "   42865.37890625,\n",
       "   42865.36328125,\n",
       "   42865.53515625,\n",
       "   42865.640625,\n",
       "   42865.60546875,\n",
       "   42865.58984375,\n",
       "   42865.453125,\n",
       "   42865.4765625,\n",
       "   42865.55859375,\n",
       "   42866.0078125,\n",
       "   42865.48046875,\n",
       "   42865.390625,\n",
       "   42865.28125,\n",
       "   42865.421875,\n",
       "   42865.28125,\n",
       "   42865.390625,\n",
       "   42865.44921875,\n",
       "   42865.62109375,\n",
       "   42865.62109375,\n",
       "   42865.640625,\n",
       "   42865.4453125,\n",
       "   42865.421875,\n",
       "   42865.3359375,\n",
       "   42865.48828125,\n",
       "   42865.69921875,\n",
       "   42865.4375,\n",
       "   42865.3984375,\n",
       "   42865.48828125,\n",
       "   42865.36328125,\n",
       "   42865.578125,\n",
       "   42865.578125,\n",
       "   42865.6015625,\n",
       "   42865.640625,\n",
       "   42865.671875,\n",
       "   42865.6328125,\n",
       "   42865.7890625,\n",
       "   42865.4375,\n",
       "   42865.03125,\n",
       "   42864.70703125,\n",
       "   42864.92578125,\n",
       "   42864.9609375,\n",
       "   42864.796875,\n",
       "   42864.83984375,\n",
       "   42864.640625,\n",
       "   42864.80859375,\n",
       "   42865.01171875,\n",
       "   42864.84765625,\n",
       "   42864.6875,\n",
       "   42864.99609375,\n",
       "   42864.93359375,\n",
       "   42865.05859375,\n",
       "   42865.046875,\n",
       "   42865.19140625,\n",
       "   42865.29296875,\n",
       "   42865.3515625,\n",
       "   42865.421875,\n",
       "   42865.60546875,\n",
       "   42865.6328125,\n",
       "   42865.55859375,\n",
       "   42865.62109375,\n",
       "   42865.3828125,\n",
       "   42865.3515625,\n",
       "   42865.15625,\n",
       "   42865.36328125,\n",
       "   42865.4375,\n",
       "   42865.12109375,\n",
       "   42865.08984375,\n",
       "   42865.25,\n",
       "   42865.1875,\n",
       "   42865.37109375,\n",
       "   42865.37109375,\n",
       "   42865.44921875,\n",
       "   42865.484375,\n",
       "   42865.74609375,\n",
       "   42865.7265625,\n",
       "   42865.98828125,\n",
       "   42865.64453125,\n",
       "   42865.48046875,\n",
       "   42865.53125,\n",
       "   42865.43359375,\n",
       "   42865.80078125,\n",
       "   42865.7421875,\n",
       "   42865.4140625,\n",
       "   42865.3671875,\n",
       "   42865.4765625,\n",
       "   42865.41015625,\n",
       "   42865.4296875,\n",
       "   42865.625,\n",
       "   42865.47265625,\n",
       "   42865.38671875,\n",
       "   42865.32421875,\n",
       "   42865.3515625,\n",
       "   42865.546875,\n",
       "   42865.671875,\n",
       "   42865.890625,\n",
       "   42865.80078125,\n",
       "   42865.8359375,\n",
       "   42865.640625,\n",
       "   42865.5703125,\n",
       "   42865.66796875,\n",
       "   42865.32421875,\n",
       "   42865.21484375,\n",
       "   42865.1015625,\n",
       "   42865.1796875,\n",
       "   42865.41015625,\n",
       "   42865.578125,\n",
       "   42865.65234375,\n",
       "   42865.44921875,\n",
       "   42865.3125,\n",
       "   42865.10546875,\n",
       "   42864.86328125,\n",
       "   42865.12109375,\n",
       "   42864.890625,\n",
       "   42864.58984375,\n",
       "   42864.8046875,\n",
       "   42864.94140625,\n",
       "   42865.19140625,\n",
       "   42864.984375,\n",
       "   42865.07421875,\n",
       "   42864.97265625,\n",
       "   42864.828125,\n",
       "   42864.921875,\n",
       "   42864.8515625,\n",
       "   42864.73828125,\n",
       "   42864.58203125,\n",
       "   42864.4375,\n",
       "   42864.61328125,\n",
       "   42864.77734375,\n",
       "   42864.71875,\n",
       "   42864.58984375,\n",
       "   42864.8359375,\n",
       "   42864.66796875,\n",
       "   42864.59375,\n",
       "   42864.5,\n",
       "   42864.5234375,\n",
       "   42864.79296875,\n",
       "   42864.515625,\n",
       "   42864.4609375,\n",
       "   42864.48046875,\n",
       "   42864.5,\n",
       "   42864.29296875,\n",
       "   42864.6796875,\n",
       "   42864.234375,\n",
       "   42864.54296875,\n",
       "   42866.44921875,\n",
       "   42864.4609375,\n",
       "   42865.828125,\n",
       "   42865.73046875,\n",
       "   42867.15234375,\n",
       "   42865.37890625,\n",
       "   42864.82421875,\n",
       "   42864.43359375,\n",
       "   42864.06640625,\n",
       "   42864.23828125,\n",
       "   42864.2734375,\n",
       "   42864.296875,\n",
       "   42864.3515625,\n",
       "   42864.29296875,\n",
       "   42864.33203125,\n",
       "   42864.59765625,\n",
       "   42864.30859375,\n",
       "   42864.48828125,\n",
       "   42864.890625,\n",
       "   42864.6328125,\n",
       "   42864.8359375,\n",
       "   42864.94921875,\n",
       "   42864.6171875,\n",
       "   42864.6171875,\n",
       "   42864.43359375,\n",
       "   42864.5,\n",
       "   42864.4140625,\n",
       "   42864.4921875,\n",
       "   42865.5234375,\n",
       "   42864.98828125,\n",
       "   42864.0546875,\n",
       "   42864.1171875,\n",
       "   42864.3359375,\n",
       "   42864.2734375,\n",
       "   42864.3359375,\n",
       "   42864.11328125,\n",
       "   42864.12890625,\n",
       "   42864.30859375,\n",
       "   42864.2578125,\n",
       "   42864.3515625,\n",
       "   42864.25390625,\n",
       "   42864.3515625,\n",
       "   42864.57421875,\n",
       "   42864.58984375,\n",
       "   42864.59375,\n",
       "   42864.57421875,\n",
       "   42864.36328125,\n",
       "   42864.47265625,\n",
       "   42864.015625,\n",
       "   42864.08984375,\n",
       "   42864.10546875,\n",
       "   42863.57421875,\n",
       "   42864.359375,\n",
       "   42864.6171875,\n",
       "   42864.6875,\n",
       "   42866.97265625,\n",
       "   42867.234375,\n",
       "   42866.0234375,\n",
       "   42867.53125,\n",
       "   42866.3515625,\n",
       "   42868.671875,\n",
       "   42865.62109375,\n",
       "   42864.42578125,\n",
       "   42863.4296875,\n",
       "   42863.62890625,\n",
       "   42863.859375,\n",
       "   42863.9453125,\n",
       "   42863.94140625,\n",
       "   42864.19140625,\n",
       "   42864.1875,\n",
       "   42865.04296875,\n",
       "   42865.56640625,\n",
       "   42864.17578125,\n",
       "   42863.95703125,\n",
       "   42864.62109375,\n",
       "   42864.4296875,\n",
       "   42864.53125,\n",
       "   42863.57421875,\n",
       "   42863.45703125,\n",
       "   42863.40625,\n",
       "   42863.37890625,\n",
       "   42863.40234375,\n",
       "   42863.76953125,\n",
       "   42863.82421875,\n",
       "   42863.984375,\n",
       "   42863.4609375,\n",
       "   42863.69921875,\n",
       "   42863.69140625,\n",
       "   42863.74609375,\n",
       "   42863.84765625,\n",
       "   42863.87109375,\n",
       "   42863.796875,\n",
       "   42863.98828125,\n",
       "   42863.94140625,\n",
       "   42863.9296875,\n",
       "   42863.8359375,\n",
       "   42863.94921875,\n",
       "   42864.12109375,\n",
       "   42864.01171875,\n",
       "   42863.9609375,\n",
       "   42864.1015625,\n",
       "   42864.0546875,\n",
       "   42864.0625,\n",
       "   42863.8984375,\n",
       "   42863.90234375,\n",
       "   42864.13671875,\n",
       "   42864.21875,\n",
       "   42864.30078125,\n",
       "   42864.35546875,\n",
       "   42864.19140625,\n",
       "   42864.27734375,\n",
       "   42864.1796875,\n",
       "   42864.2734375,\n",
       "   42864.4609375,\n",
       "   42864.55859375,\n",
       "   42864.73046875,\n",
       "   42864.4609375,\n",
       "   42864.421875,\n",
       "   42864.69921875,\n",
       "   42864.81640625,\n",
       "   42864.6953125,\n",
       "   42864.62109375,\n",
       "   42864.54296875,\n",
       "   42864.62890625,\n",
       "   42864.21875,\n",
       "   42864.03515625,\n",
       "   42863.90625,\n",
       "   42863.88671875,\n",
       "   42863.77734375,\n",
       "   42864.1015625,\n",
       "   42864.53515625,\n",
       "   42864.1953125,\n",
       "   42864.39453125,\n",
       "   42864.37109375,\n",
       "   42864.18359375,\n",
       "   42863.5546875,\n",
       "   42867.60546875,\n",
       "   42864.8828125,\n",
       "   42865.859375,\n",
       "   42867.97265625,\n",
       "   42866.2890625,\n",
       "   42867.1796875,\n",
       "   42864.48046875,\n",
       "   42864.1171875,\n",
       "   42863.88671875,\n",
       "   42863.94921875,\n",
       "   42864.921875,\n",
       "   42866.1640625,\n",
       "   42864.9765625,\n",
       "   42864.40625,\n",
       "   42865.37109375,\n",
       "   42864.03515625,\n",
       "   42863.67578125,\n",
       "   42864.3046875,\n",
       "   42864.109375,\n",
       "   42864.0390625,\n",
       "   42864.44921875,\n",
       "   42864.43359375,\n",
       "   42864.359375,\n",
       "   42864.49609375,\n",
       "   42863.7421875,\n",
       "   42863.453125,\n",
       "   42863.70703125,\n",
       "   42863.71875,\n",
       "   42863.8125,\n",
       "   42863.7734375,\n",
       "   42863.640625,\n",
       "   42863.9140625,\n",
       "   42864.13671875,\n",
       "   42863.984375,\n",
       "   42864.109375,\n",
       "   42863.91796875,\n",
       "   42863.50390625,\n",
       "   42864.14453125,\n",
       "   42863.64453125,\n",
       "   42864.1171875,\n",
       "   42863.4140625,\n",
       "   42864.1171875,\n",
       "   42864.921875,\n",
       "   42865.390625,\n",
       "   42865.46875,\n",
       "   42865.80078125,\n",
       "   42865.828125,\n",
       "   42867.40234375,\n",
       "   42866.6953125,\n",
       "   42866.72265625,\n",
       "   42867.9296875,\n",
       "   42867.94140625,\n",
       "   42866.0390625,\n",
       "   42867.0546875,\n",
       "   42866.0078125,\n",
       "   42865.87890625,\n",
       "   42864.27734375,\n",
       "   42864.6640625,\n",
       "   42863.71875,\n",
       "   42863.234375,\n",
       "   42863.08203125,\n",
       "   42863.21875,\n",
       "   42863.1484375,\n",
       "   42863.2578125,\n",
       "   42863.30078125,\n",
       "   42863.51171875,\n",
       "   42863.40625,\n",
       "   42863.3984375,\n",
       "   42864.06640625,\n",
       "   42864.546875,\n",
       "   42864.4765625,\n",
       "   42868.73046875,\n",
       "   42870.515625,\n",
       "   42867.66796875,\n",
       "   42869.109375,\n",
       "   42866.57421875,\n",
       "   42869.3671875,\n",
       "   42870.68359375,\n",
       "   42870.51171875,\n",
       "   42868.4375,\n",
       "   42869.68359375,\n",
       "   42868.1171875,\n",
       "   42866.1953125,\n",
       "   42865.2734375,\n",
       "   42864.08203125,\n",
       "   42863.76171875,\n",
       "   42863.75,\n",
       "   42863.6640625,\n",
       "   42863.70703125,\n",
       "   42864.12890625],\n",
       "  'jsc_acc': [nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42882.37890625,\n",
       "   42883.15625,\n",
       "   nan,\n",
       "   42882.0234375,\n",
       "   42881.9296875,\n",
       "   nan,\n",
       "   42880.9765625,\n",
       "   42880.5859375,\n",
       "   nan,\n",
       "   42881.7890625,\n",
       "   42881.328125,\n",
       "   42881.640625,\n",
       "   42878.7421875,\n",
       "   42879.625,\n",
       "   42880.53515625,\n",
       "   42879.08203125,\n",
       "   42879.94921875,\n",
       "   42881.8359375,\n",
       "   42881.10546875,\n",
       "   42882.21875,\n",
       "   42882.45703125,\n",
       "   42881.7734375,\n",
       "   42880.01171875,\n",
       "   42880.86328125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42885.765625,\n",
       "   42884.6640625,\n",
       "   42882.51171875,\n",
       "   42883.76953125,\n",
       "   42884.26171875,\n",
       "   42885.296875,\n",
       "   nan,\n",
       "   42884.80078125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42886.08203125,\n",
       "   nan,\n",
       "   nan,\n",
       "   42886.74609375,\n",
       "   nan,\n",
       "   nan,\n",
       "   42885.7421875,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42885.02734375,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42885.49609375,\n",
       "   42884.9375,\n",
       "   42883.11328125,\n",
       "   42883.3359375,\n",
       "   42883.0859375,\n",
       "   42882.90625,\n",
       "   42882.6171875,\n",
       "   42882.69921875,\n",
       "   42886.83984375,\n",
       "   42885.56640625,\n",
       "   42886.765625,\n",
       "   42886.2734375,\n",
       "   42887.5390625,\n",
       "   42885.6875,\n",
       "   42887.18359375,\n",
       "   42886.8828125,\n",
       "   42887.69140625,\n",
       "   42885.8828125,\n",
       "   42885.5546875,\n",
       "   42885.40234375,\n",
       "   42885.421875,\n",
       "   42885.2734375,\n",
       "   42885.30859375,\n",
       "   42885.69921875,\n",
       "   42886.23828125,\n",
       "   nan,\n",
       "   42887.1640625,\n",
       "   42886.3359375,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42884.28515625,\n",
       "   42885.80859375,\n",
       "   42885.2578125,\n",
       "   42885.140625,\n",
       "   42885.33203125,\n",
       "   42884.609375,\n",
       "   42884.67578125,\n",
       "   42884.9140625,\n",
       "   42887.015625,\n",
       "   42886.2734375,\n",
       "   nan,\n",
       "   nan,\n",
       "   42886.53125,\n",
       "   42886.9921875,\n",
       "   42885.71484375,\n",
       "   42884.76171875,\n",
       "   42882.87109375,\n",
       "   42885.203125,\n",
       "   42883.44140625,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42886.89453125,\n",
       "   42885.26953125,\n",
       "   nan,\n",
       "   nan,\n",
       "   42886.5234375,\n",
       "   42884.48828125,\n",
       "   42885.06640625,\n",
       "   42884.6484375,\n",
       "   42885.328125,\n",
       "   42885.5625,\n",
       "   42886.0859375,\n",
       "   42885.0078125,\n",
       "   42884.6171875,\n",
       "   42883.14453125,\n",
       "   42883.2109375,\n",
       "   42884.5625,\n",
       "   nan,\n",
       "   nan,\n",
       "   42887.60546875,\n",
       "   42889.3046875,\n",
       "   42886.57421875,\n",
       "   42884.85546875,\n",
       "   42886.3125,\n",
       "   42885.5390625,\n",
       "   42884.7265625,\n",
       "   42884.94140625,\n",
       "   42883.87890625,\n",
       "   42885.91015625,\n",
       "   42887.60546875,\n",
       "   42887.66015625,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42887.27734375,\n",
       "   42885.26171875,\n",
       "   42885.76953125,\n",
       "   nan,\n",
       "   42886.91796875,\n",
       "   42885.45703125,\n",
       "   42886.16015625,\n",
       "   42887.98828125,\n",
       "   42887.7265625,\n",
       "   42887.46875,\n",
       "   42888.12109375,\n",
       "   42885.390625,\n",
       "   42884.8125,\n",
       "   42886.61328125,\n",
       "   42886.90625,\n",
       "   42886.11328125,\n",
       "   42884.5625,\n",
       "   42883.578125,\n",
       "   42885.34765625,\n",
       "   42888.203125,\n",
       "   nan,\n",
       "   42888.078125,\n",
       "   42888.015625,\n",
       "   nan,\n",
       "   nan,\n",
       "   42886.40234375,\n",
       "   42887.1015625,\n",
       "   42886.51953125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42886.796875,\n",
       "   42886.75,\n",
       "   nan,\n",
       "   nan,\n",
       "   42888.625],\n",
       "  'ff_acc': [nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   42914.28125,\n",
       "   42914.28125,\n",
       "   42914.28515625,\n",
       "   42913.7890625,\n",
       "   42912.25390625,\n",
       "   42911.78125,\n",
       "   42911.88671875,\n",
       "   42911.7109375,\n",
       "   42910.7578125,\n",
       "   42911.25,\n",
       "   42909.515625,\n",
       "   42909.21484375,\n",
       "   42909.3203125,\n",
       "   42908.34765625,\n",
       "   42907.72265625,\n",
       "   42907.64453125,\n",
       "   42906.97265625,\n",
       "   42906.4921875,\n",
       "   42906.5625,\n",
       "   42906.74609375,\n",
       "   42907.19140625,\n",
       "   42906.37109375,\n",
       "   42906.1015625,\n",
       "   42906.7578125,\n",
       "   42907.45703125,\n",
       "   42907.6640625,\n",
       "   42907.125,\n",
       "   42906.9375,\n",
       "   42906.2109375,\n",
       "   42906.02734375,\n",
       "   42906.8359375,\n",
       "   42907.25,\n",
       "   42907.00390625,\n",
       "   42906.95703125,\n",
       "   42906.60546875,\n",
       "   42906.62109375,\n",
       "   42906.56640625,\n",
       "   42906.2578125,\n",
       "   42906.0234375,\n",
       "   42905.41796875,\n",
       "   42904.83984375,\n",
       "   42905.01171875,\n",
       "   42905.83203125,\n",
       "   42906.2421875,\n",
       "   42905.71484375,\n",
       "   42905.5390625,\n",
       "   42906.6328125,\n",
       "   42906.73046875,\n",
       "   42906.5625,\n",
       "   42905.94140625,\n",
       "   42906.80078125,\n",
       "   42907.44921875,\n",
       "   42907.26953125,\n",
       "   42906.421875,\n",
       "   42905.29296875,\n",
       "   42905.40234375,\n",
       "   42905.0,\n",
       "   42905.015625,\n",
       "   42904.26171875,\n",
       "   42904.09375,\n",
       "   42904.12109375,\n",
       "   42903.9453125,\n",
       "   42903.9375,\n",
       "   42903.3515625,\n",
       "   42903.45703125,\n",
       "   42903.2734375,\n",
       "   42902.953125,\n",
       "   42903.0546875,\n",
       "   42902.76953125,\n",
       "   42903.546875,\n",
       "   42903.8828125,\n",
       "   42904.1953125,\n",
       "   42904.8359375,\n",
       "   42904.34375,\n",
       "   42903.61328125,\n",
       "   42903.1484375,\n",
       "   42903.35546875,\n",
       "   42902.81640625,\n",
       "   42902.67578125,\n",
       "   42903.515625,\n",
       "   42903.63671875,\n",
       "   42903.5078125,\n",
       "   42903.4453125,\n",
       "   42903.41015625,\n",
       "   42904.08203125,\n",
       "   42904.79296875,\n",
       "   42905.29296875,\n",
       "   42906.00390625,\n",
       "   42905.25,\n",
       "   42904.98046875,\n",
       "   42904.73828125,\n",
       "   42905.6484375,\n",
       "   42905.6796875,\n",
       "   42905.21484375,\n",
       "   42905.2578125,\n",
       "   42904.80078125,\n",
       "   42905.4765625,\n",
       "   42905.87890625,\n",
       "   42904.5390625,\n",
       "   42904.3515625,\n",
       "   42903.57421875,\n",
       "   42903.53125,\n",
       "   42903.73828125,\n",
       "   42903.54296875,\n",
       "   42903.67578125,\n",
       "   42903.88671875,\n",
       "   42904.3046875,\n",
       "   42904.83203125,\n",
       "   42904.59765625,\n",
       "   42904.27734375,\n",
       "   42904.6328125,\n",
       "   42905.796875,\n",
       "   42905.8359375,\n",
       "   42905.890625,\n",
       "   42906.0703125,\n",
       "   42906.05078125,\n",
       "   42906.3828125,\n",
       "   42906.60546875,\n",
       "   42907.1875,\n",
       "   42906.8203125,\n",
       "   42907.13671875,\n",
       "   42906.80078125,\n",
       "   42907.109375,\n",
       "   42907.98828125,\n",
       "   42907.6640625,\n",
       "   42908.17578125,\n",
       "   42907.25390625,\n",
       "   42906.484375,\n",
       "   42906.6015625,\n",
       "   42906.24609375,\n",
       "   42906.08984375,\n",
       "   42906.25,\n",
       "   42906.3984375,\n",
       "   42906.6015625,\n",
       "   42906.2421875,\n",
       "   42906.2421875,\n",
       "   42906.51953125,\n",
       "   42906.671875,\n",
       "   42906.19140625,\n",
       "   42905.8515625,\n",
       "   42906.55078125,\n",
       "   42907.38671875,\n",
       "   42907.6796875,\n",
       "   42906.9921875,\n",
       "   42906.7890625,\n",
       "   42906.76171875,\n",
       "   42905.9609375,\n",
       "   42906.23046875,\n",
       "   42907.27734375,\n",
       "   42907.44140625,\n",
       "   42907.52734375,\n",
       "   42906.80859375,\n",
       "   42906.66796875,\n",
       "   42906.2890625,\n",
       "   42905.2265625,\n",
       "   42905.25,\n",
       "   42905.55859375,\n",
       "   42905.53125,\n",
       "   42905.68359375,\n",
       "   42906.2890625,\n",
       "   42906.34765625,\n",
       "   42906.0234375,\n",
       "   42904.6796875,\n",
       "   42905.26953125,\n",
       "   42904.1640625,\n",
       "   42904.1015625,\n",
       "   42903.5390625,\n",
       "   42904.40234375,\n",
       "   42904.06640625,\n",
       "   42903.38671875,\n",
       "   42902.984375,\n",
       "   42903.9140625,\n",
       "   42903.9453125,\n",
       "   42904.18359375,\n",
       "   42905.0078125,\n",
       "   42905.29296875,\n",
       "   42905.421875,\n",
       "   42905.88671875,\n",
       "   42905.5390625,\n",
       "   42906.16015625,\n",
       "   42906.6328125,\n",
       "   42906.47265625,\n",
       "   42905.83984375,\n",
       "   42905.83203125,\n",
       "   42906.53515625,\n",
       "   42905.9765625,\n",
       "   42905.4609375,\n",
       "   42905.7421875,\n",
       "   42905.984375,\n",
       "   42905.7109375,\n",
       "   42905.0859375,\n",
       "   42904.6953125,\n",
       "   42903.8515625,\n",
       "   42903.05859375,\n",
       "   42903.73046875,\n",
       "   42904.375,\n",
       "   42905.015625,\n",
       "   42905.14453125,\n",
       "   42904.98828125,\n",
       "   42904.96484375,\n",
       "   42905.0390625,\n",
       "   42905.28125,\n",
       "   42905.12890625,\n",
       "   42904.01953125,\n",
       "   42903.55859375,\n",
       "   42903.55078125,\n",
       "   42903.6328125,\n",
       "   42903.81640625,\n",
       "   42903.20703125,\n",
       "   42902.3359375,\n",
       "   42903.41796875,\n",
       "   42903.87109375,\n",
       "   42904.109375,\n",
       "   42904.171875,\n",
       "   42904.72265625,\n",
       "   42905.09765625,\n",
       "   42905.92578125,\n",
       "   42906.953125,\n",
       "   42907.1796875,\n",
       "   42907.11328125,\n",
       "   42907.328125,\n",
       "   42906.765625,\n",
       "   42906.1796875,\n",
       "   42905.94921875,\n",
       "   42906.34765625,\n",
       "   42906.41015625,\n",
       "   42906.87109375,\n",
       "   42906.6640625,\n",
       "   42906.203125,\n",
       "   42906.6171875,\n",
       "   42906.2421875,\n",
       "   42905.984375,\n",
       "   42905.890625,\n",
       "   42904.71875,\n",
       "   42904.58203125,\n",
       "   42904.8359375,\n",
       "   42904.4453125,\n",
       "   42904.15234375,\n",
       "   42904.34765625,\n",
       "   42904.86328125,\n",
       "   42904.4453125,\n",
       "   42904.0625,\n",
       "   42903.86328125,\n",
       "   42904.4453125,\n",
       "   42905.09375,\n",
       "   42904.45703125,\n",
       "   42904.70703125,\n",
       "   42904.38671875,\n",
       "   42903.765625,\n",
       "   42903.375,\n",
       "   42902.92578125,\n",
       "   42902.91015625,\n",
       "   42903.48046875,\n",
       "   42903.3828125,\n",
       "   42904.40234375,\n",
       "   42904.421875,\n",
       "   42904.46875,\n",
       "   42904.37109375,\n",
       "   42903.93359375,\n",
       "   42904.24609375,\n",
       "   42905.6171875,\n",
       "   42904.92578125,\n",
       "   42904.875,\n",
       "   42904.58984375,\n",
       "   42904.1640625,\n",
       "   42903.33984375,\n",
       "   42902.85546875,\n",
       "   42903.73046875,\n",
       "   42903.16015625,\n",
       "   42903.84765625,\n",
       "   42902.5546875,\n",
       "   42901.84765625,\n",
       "   42901.68359375,\n",
       "   42901.8359375,\n",
       "   42902.328125,\n",
       "   42903.1171875,\n",
       "   42903.56640625,\n",
       "   42903.9140625,\n",
       "   42904.33203125,\n",
       "   42904.2890625,\n",
       "   42903.88671875,\n",
       "   42903.7890625,\n",
       "   42902.76953125,\n",
       "   42902.95703125,\n",
       "   42903.73828125,\n",
       "   42903.4375,\n",
       "   42903.4453125,\n",
       "   42903.77734375,\n",
       "   42904.05859375,\n",
       "   42904.08203125,\n",
       "   42903.91015625,\n",
       "   42903.640625,\n",
       "   42903.69140625,\n",
       "   42903.45703125,\n",
       "   42902.125,\n",
       "   42902.8359375,\n",
       "   42902.1171875,\n",
       "   42902.04296875,\n",
       "   42901.52734375,\n",
       "   42901.984375,\n",
       "   42902.0703125,\n",
       "   42902.26953125,\n",
       "   42902.765625,\n",
       "   42902.91015625,\n",
       "   42903.046875,\n",
       "   42903.046875,\n",
       "   42903.31640625,\n",
       "   42903.42578125,\n",
       "   42902.8046875,\n",
       "   42903.41015625,\n",
       "   42903.3515625,\n",
       "   42903.45703125,\n",
       "   42903.8515625,\n",
       "   42903.5,\n",
       "   42903.1171875,\n",
       "   42903.14453125,\n",
       "   42904.26953125,\n",
       "   42904.05078125,\n",
       "   42904.4453125,\n",
       "   42904.66796875,\n",
       "   42905.27734375,\n",
       "   42905.3515625,\n",
       "   42905.04296875,\n",
       "   42904.734375,\n",
       "   42905.21484375,\n",
       "   42904.58203125,\n",
       "   42905.234375,\n",
       "   42904.91796875,\n",
       "   42905.33984375,\n",
       "   42905.671875,\n",
       "   42905.96484375,\n",
       "   42905.59375,\n",
       "   42904.5703125,\n",
       "   42904.453125,\n",
       "   42905.15625,\n",
       "   42904.2734375,\n",
       "   42904.12890625,\n",
       "   42904.21484375,\n",
       "   42904.23046875,\n",
       "   42903.546875,\n",
       "   42901.66015625,\n",
       "   42902.28125,\n",
       "   42902.30859375,\n",
       "   42902.48828125,\n",
       "   42901.890625,\n",
       "   42902.125,\n",
       "   42902.3046875,\n",
       "   42902.64453125,\n",
       "   42902.03125,\n",
       "   42900.37109375,\n",
       "   42900.2421875,\n",
       "   42899.94140625,\n",
       "   42900.53125,\n",
       "   42901.73828125,\n",
       "   42901.5703125,\n",
       "   42901.4453125,\n",
       "   42901.84765625,\n",
       "   42901.86328125,\n",
       "   42903.23046875,\n",
       "   42902.9140625,\n",
       "   42902.88671875,\n",
       "   42902.39453125,\n",
       "   42901.87109375,\n",
       "   42901.91796875,\n",
       "   42902.32421875,\n",
       "   42901.45703125,\n",
       "   42901.359375,\n",
       "   42901.39453125,\n",
       "   42901.50390625,\n",
       "   42901.875,\n",
       "   42902.28125,\n",
       "   42901.90625,\n",
       "   42901.90625,\n",
       "   42902.41796875,\n",
       "   42901.98828125,\n",
       "   42902.37890625,\n",
       "   42902.19921875,\n",
       "   42902.3671875,\n",
       "   42902.421875,\n",
       "   42901.84765625,\n",
       "   42902.40625,\n",
       "   42902.87890625,\n",
       "   42902.0234375,\n",
       "   42901.85546875,\n",
       "   42902.4609375,\n",
       "   42901.953125,\n",
       "   42902.6328125,\n",
       "   42904.21875,\n",
       "   42904.64453125,\n",
       "   42903.60546875,\n",
       "   42903.7265625,\n",
       "   42903.96484375,\n",
       "   42904.6640625,\n",
       "   42904.08203125,\n",
       "   42904.6875,\n",
       "   42904.609375,\n",
       "   42904.38671875,\n",
       "   42902.8828125,\n",
       "   42903.32421875,\n",
       "   42902.51171875,\n",
       "   42901.484375,\n",
       "   42900.80078125,\n",
       "   42900.34375,\n",
       "   42900.67578125,\n",
       "   42900.2890625,\n",
       "   42900.60546875,\n",
       "   42900.9609375,\n",
       "   42900.77734375,\n",
       "   42902.43359375,\n",
       "   42901.515625,\n",
       "   42901.22265625,\n",
       "   42900.4609375,\n",
       "   42901.01953125,\n",
       "   42901.54296875,\n",
       "   42901.47265625,\n",
       "   42902.12890625,\n",
       "   42902.1640625,\n",
       "   42901.77734375,\n",
       "   42901.45703125,\n",
       "   42900.60546875,\n",
       "   42900.71875,\n",
       "   42901.1015625,\n",
       "   42901.6171875,\n",
       "   42901.00390625,\n",
       "   42900.7734375,\n",
       "   42901.60546875,\n",
       "   42901.09375,\n",
       "   42901.47265625,\n",
       "   42901.7109375,\n",
       "   42901.1953125,\n",
       "   42901.4140625,\n",
       "   42900.7109375,\n",
       "   42900.921875,\n",
       "   42901.3125,\n",
       "   42901.203125,\n",
       "   42901.05859375,\n",
       "   42900.75390625,\n",
       "   42900.5703125,\n",
       "   42901.33203125,\n",
       "   42901.66796875,\n",
       "   42902.296875,\n",
       "   42902.37109375,\n",
       "   42901.83984375,\n",
       "   42901.9375,\n",
       "   42902.4453125,\n",
       "   42902.65625,\n",
       "   42902.9140625,\n",
       "   42902.65625,\n",
       "   42903.3984375,\n",
       "   42903.41015625,\n",
       "   42902.8828125,\n",
       "   42902.55859375,\n",
       "   42902.75,\n",
       "   42902.6328125,\n",
       "   42903.359375,\n",
       "   42903.546875,\n",
       "   42903.265625,\n",
       "   42903.22265625,\n",
       "   42903.0,\n",
       "   42903.65234375,\n",
       "   42901.90625,\n",
       "   42901.6796875,\n",
       "   42901.9140625,\n",
       "   42901.71875,\n",
       "   42901.359375,\n",
       "   42900.859375,\n",
       "   42900.0,\n",
       "   42900.7890625,\n",
       "   42901.1640625,\n",
       "   42901.68359375,\n",
       "   42900.46875,\n",
       "   42900.66796875,\n",
       "   42900.5390625,\n",
       "   42900.2265625,\n",
       "   42900.23046875,\n",
       "   42900.69921875,\n",
       "   42900.45703125,\n",
       "   42900.80859375,\n",
       "   42901.9765625,\n",
       "   42902.34765625,\n",
       "   42902.23046875,\n",
       "   42902.65234375,\n",
       "   42902.671875,\n",
       "   42902.171875,\n",
       "   42901.79296875,\n",
       "   42901.5703125,\n",
       "   42901.58203125],\n",
       "  'test_accs': [nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171537.50390625,\n",
       "   171537.8046875,\n",
       "   nan,\n",
       "   171536.6015625,\n",
       "   171537.11328125,\n",
       "   nan,\n",
       "   171534.3515625,\n",
       "   171533.21875,\n",
       "   nan,\n",
       "   171536.3203125,\n",
       "   171535.19140625,\n",
       "   171535.91796875,\n",
       "   171532.77734375,\n",
       "   171534.015625,\n",
       "   171533.4453125,\n",
       "   171531.3125,\n",
       "   171533.1796875,\n",
       "   171534.796875,\n",
       "   171533.60546875,\n",
       "   171536.08984375,\n",
       "   171536.45703125,\n",
       "   171536.6328125,\n",
       "   171533.48828125,\n",
       "   171534.4453125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171541.53515625,\n",
       "   171541.06640625,\n",
       "   171536.30078125,\n",
       "   171537.578125,\n",
       "   171537.32421875,\n",
       "   171539.0234375,\n",
       "   nan,\n",
       "   171537.32421875,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171540.4296875,\n",
       "   nan,\n",
       "   nan,\n",
       "   171545.09765625,\n",
       "   nan,\n",
       "   nan,\n",
       "   171546.39453125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171534.66015625,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171536.9453125,\n",
       "   171535.6640625,\n",
       "   171533.8046875,\n",
       "   171534.765625,\n",
       "   171533.49609375,\n",
       "   171533.83203125,\n",
       "   171533.3828125,\n",
       "   171533.6796875,\n",
       "   171538.125,\n",
       "   171536.640625,\n",
       "   171537.28125,\n",
       "   171536.3515625,\n",
       "   171538.1875,\n",
       "   171536.1171875,\n",
       "   171538.2421875,\n",
       "   171537.765625,\n",
       "   171539.22265625,\n",
       "   171537.2578125,\n",
       "   171536.5234375,\n",
       "   171538.1640625,\n",
       "   171538.0546875,\n",
       "   171536.94140625,\n",
       "   171536.48046875,\n",
       "   171537.3203125,\n",
       "   171536.4140625,\n",
       "   nan,\n",
       "   171538.765625,\n",
       "   171538.49609375,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171536.5703125,\n",
       "   171538.46484375,\n",
       "   171537.9921875,\n",
       "   171536.71484375,\n",
       "   171539.421875,\n",
       "   171536.5,\n",
       "   171536.98046875,\n",
       "   171537.9453125,\n",
       "   171538.02734375,\n",
       "   171537.85546875,\n",
       "   nan,\n",
       "   nan,\n",
       "   171534.40234375,\n",
       "   171535.00390625,\n",
       "   171533.828125,\n",
       "   171536.38671875,\n",
       "   171532.4140625,\n",
       "   171533.8984375,\n",
       "   171532.3203125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171535.86328125,\n",
       "   171534.28515625,\n",
       "   nan,\n",
       "   nan,\n",
       "   171534.2890625,\n",
       "   171532.1171875,\n",
       "   171533.62890625,\n",
       "   171533.3359375,\n",
       "   171533.4140625,\n",
       "   171533.828125,\n",
       "   171533.98046875,\n",
       "   171533.25,\n",
       "   171533.26953125,\n",
       "   171531.29296875,\n",
       "   171531.546875,\n",
       "   171532.2578125,\n",
       "   nan,\n",
       "   nan,\n",
       "   171536.1640625,\n",
       "   171538.03515625,\n",
       "   171535.9765625,\n",
       "   171533.56640625,\n",
       "   171536.12890625,\n",
       "   171536.06640625,\n",
       "   171536.81640625,\n",
       "   171537.765625,\n",
       "   171535.84765625,\n",
       "   171537.4375,\n",
       "   171541.3359375,\n",
       "   171540.3828125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171538.55078125,\n",
       "   171537.0390625,\n",
       "   171535.5,\n",
       "   nan,\n",
       "   171536.96484375,\n",
       "   171534.84765625,\n",
       "   171535.48046875,\n",
       "   171537.08984375,\n",
       "   171537.22265625,\n",
       "   171535.6796875,\n",
       "   171535.83984375,\n",
       "   171534.00390625,\n",
       "   171533.43359375,\n",
       "   171534.51171875,\n",
       "   171536.0703125,\n",
       "   171535.23828125,\n",
       "   171534.1640625,\n",
       "   171537.16015625,\n",
       "   171540.9140625,\n",
       "   171539.625,\n",
       "   nan,\n",
       "   171538.8203125,\n",
       "   171540.8359375,\n",
       "   nan,\n",
       "   nan,\n",
       "   171539.1796875,\n",
       "   171542.03515625,\n",
       "   171541.21875,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   171537.07421875,\n",
       "   171536.2265625,\n",
       "   nan,\n",
       "   nan,\n",
       "   171538.8125],\n",
       "  'pce_r2': [-213.91381113280704,\n",
       "   -187.02891832570344,\n",
       "   -161.03846983449273,\n",
       "   -137.83605817959557,\n",
       "   -121.76053172372708,\n",
       "   -107.51746802898147,\n",
       "   -97.72650136720061,\n",
       "   -90.06984236986757,\n",
       "   -83.4225308948721,\n",
       "   -76.33857948128616,\n",
       "   -71.1493406193137,\n",
       "   -67.57742408118973,\n",
       "   -61.624147325886995,\n",
       "   -55.39941102352836,\n",
       "   -49.1376636383709,\n",
       "   -47.75360705627191,\n",
       "   -42.251166710071814,\n",
       "   -40.0383565200402,\n",
       "   -36.91889541107357,\n",
       "   -33.75836574521288,\n",
       "   -30.882731291569044,\n",
       "   -29.047916637305736,\n",
       "   -26.900097332993138,\n",
       "   -26.258881805827738,\n",
       "   -24.473455089116783,\n",
       "   -23.67071768929305,\n",
       "   -22.44328264749266,\n",
       "   -21.460445993423303,\n",
       "   -21.141102267631645,\n",
       "   -21.16332298718117,\n",
       "   -19.746854060348515,\n",
       "   -19.15201654483824,\n",
       "   -18.471094550928886,\n",
       "   -19.0504838384032,\n",
       "   -18.961543970925735,\n",
       "   -19.166408188190246,\n",
       "   -18.43716455176525,\n",
       "   -18.58509725173663,\n",
       "   -19.075176980394346,\n",
       "   -19.612353246862188,\n",
       "   -19.009616023984854,\n",
       "   -18.47966108233135,\n",
       "   -16.679449113465488,\n",
       "   -17.019073928728538,\n",
       "   -17.000373844093364,\n",
       "   -16.151509311538767,\n",
       "   -16.672262427514347,\n",
       "   -17.248411230894956,\n",
       "   -16.61063684628906,\n",
       "   -16.054123224953376,\n",
       "   -16.51312792315435,\n",
       "   -16.084356279831145,\n",
       "   -16.229374548534665,\n",
       "   -16.5759495690539,\n",
       "   -16.293780493840792,\n",
       "   -16.549984476424235,\n",
       "   -15.888796257764085,\n",
       "   -15.745464393418246,\n",
       "   -16.197147338677222,\n",
       "   -16.49793347327707,\n",
       "   -16.78154535847299,\n",
       "   -16.45204720184547,\n",
       "   -16.907018187156403,\n",
       "   -16.244585947559198,\n",
       "   -16.395303299121316,\n",
       "   -16.360838976065953,\n",
       "   -16.224038377816044,\n",
       "   -16.36751403764563,\n",
       "   -17.08466975798114,\n",
       "   -17.104123878808984,\n",
       "   -16.868938858094015,\n",
       "   -17.146907375160108,\n",
       "   -17.48656219014193,\n",
       "   -16.74686212575627,\n",
       "   -17.76463125172572,\n",
       "   -17.479741786814003,\n",
       "   -17.082175867309385,\n",
       "   -16.82851680418325,\n",
       "   -16.49626493244513,\n",
       "   -16.107628445844306,\n",
       "   -16.057620345977636,\n",
       "   -16.101091578813513,\n",
       "   -15.808135046248307,\n",
       "   -15.666080122067719,\n",
       "   -15.43288232528365,\n",
       "   -14.272693446191488,\n",
       "   -14.320738181319218,\n",
       "   -14.373744610359216,\n",
       "   -14.533024400499135,\n",
       "   -14.780461277989989,\n",
       "   -14.836164945320219,\n",
       "   -15.527131717534058,\n",
       "   -15.672545955739388,\n",
       "   -15.922546409038642,\n",
       "   -15.877997993825971,\n",
       "   -16.00218450406298,\n",
       "   -16.665860844459576,\n",
       "   -16.968079852957636,\n",
       "   -16.366520318055198,\n",
       "   -16.069832784657276,\n",
       "   -16.236180027821483,\n",
       "   -16.84896659509108,\n",
       "   -16.838310378630368,\n",
       "   -16.14807346381857,\n",
       "   -17.063860606605754,\n",
       "   -17.298030480995703,\n",
       "   -16.797110334439367,\n",
       "   -17.154410732137404,\n",
       "   -16.778074327853624,\n",
       "   -16.3704870682637,\n",
       "   -17.529708431552805,\n",
       "   -18.07312380692853,\n",
       "   -18.265622559599002,\n",
       "   -17.775713973321885,\n",
       "   -18.08149292941445,\n",
       "   -17.363599773390963,\n",
       "   -16.078759371811454,\n",
       "   -15.869371500273285,\n",
       "   -16.3104215830167,\n",
       "   -17.16398171214745,\n",
       "   -17.65569221902456,\n",
       "   -17.32393589693118,\n",
       "   -17.61950893872607,\n",
       "   -17.909694341361018,\n",
       "   -17.272367161148317,\n",
       "   -17.55914697494869,\n",
       "   -17.714801690311123,\n",
       "   -17.466488429137797,\n",
       "   -16.986607200777545,\n",
       "   -16.862178738470853,\n",
       "   -16.932107637563632,\n",
       "   -16.908235030283024,\n",
       "   -15.548104628277986,\n",
       "   -15.133446137985263,\n",
       "   -15.393460063970192,\n",
       "   -15.015406726509994,\n",
       "   -15.558232286960376,\n",
       "   -16.20756006417727,\n",
       "   -16.397394329575047,\n",
       "   -16.442848727709436,\n",
       "   -15.722580966252362,\n",
       "   -15.714028358459966,\n",
       "   -16.276905278916416,\n",
       "   -16.72938205638423,\n",
       "   -17.333992016360458,\n",
       "   -17.23744474786343,\n",
       "   -16.516360774290717,\n",
       "   -15.99989167494714,\n",
       "   -16.117878893774307,\n",
       "   -17.125959218400137,\n",
       "   -17.85882355151524,\n",
       "   -17.837532623664167,\n",
       "   -17.370874801722852,\n",
       "   -16.650287294205206,\n",
       "   -16.618760035837283,\n",
       "   -16.121404214100576,\n",
       "   -16.387518259164146,\n",
       "   -16.10292130073075,\n",
       "   -16.34791048239195,\n",
       "   -16.65947501409317,\n",
       "   -16.708168265858653,\n",
       "   -16.32488620925527,\n",
       "   -15.969423382108936,\n",
       "   -15.445252381146968,\n",
       "   -16.392189215252117,\n",
       "   -16.44037005046629,\n",
       "   -16.130297333973303,\n",
       "   -16.523715544432203,\n",
       "   -16.565949748242463,\n",
       "   -16.282837010028835,\n",
       "   -16.201024919512772,\n",
       "   -15.607028410866427,\n",
       "   -15.594454542316694,\n",
       "   -15.608248254640628,\n",
       "   -15.374863966544506,\n",
       "   -15.519653077557482,\n",
       "   -15.752622937044553,\n",
       "   -15.660907341483266,\n",
       "   -15.760801589882107,\n",
       "   -15.969176460395119,\n",
       "   -17.405160914563396,\n",
       "   -17.968411392424432,\n",
       "   -18.011654169967798,\n",
       "   -17.119920530824494,\n",
       "   -16.924502709538853,\n",
       "   -17.686475483082738,\n",
       "   -17.50554117712142,\n",
       "   -16.698556338288522,\n",
       "   -16.801841028170266,\n",
       "   -16.81297272128917,\n",
       "   -16.63083136729385,\n",
       "   -16.495364068661424,\n",
       "   -16.69004665705846,\n",
       "   -16.309532905519625,\n",
       "   -16.65055009034981,\n",
       "   -16.236488916092352,\n",
       "   -15.84886546683326,\n",
       "   -16.425531030083782,\n",
       "   -16.19248498653842,\n",
       "   -15.404696680531455,\n",
       "   -16.00083670519296,\n",
       "   -16.816343203319306,\n",
       "   -16.899864960813186,\n",
       "   -17.605034341271736,\n",
       "   -18.97239839588272,\n",
       "   -17.49526501503039,\n",
       "   -17.33649716046221,\n",
       "   -17.831028984386652,\n",
       "   -17.938810119653134,\n",
       "   -17.300357502707037,\n",
       "   -17.03566453008504,\n",
       "   -16.372143492652345,\n",
       "   -16.51406469283836,\n",
       "   -16.7987641563019,\n",
       "   -16.196214159747225,\n",
       "   -15.607834945034153,\n",
       "   -15.668385173919784,\n",
       "   -15.627908448849603,\n",
       "   -15.242228811141786,\n",
       "   -15.132916744071352,\n",
       "   -14.755519857390214,\n",
       "   -14.614041237075199,\n",
       "   -14.756502919695624,\n",
       "   -15.669464270247246,\n",
       "   -15.63003783391132,\n",
       "   -16.406496338401116,\n",
       "   -16.049580127643253,\n",
       "   -14.576701949109601,\n",
       "   -13.85408306862232,\n",
       "   -14.36061349510098,\n",
       "   -13.925188496295753,\n",
       "   -14.113876418604539,\n",
       "   -14.110086781328041,\n",
       "   -14.429946043449736,\n",
       "   -14.739829285564793,\n",
       "   -14.252179954830762,\n",
       "   -13.573152485556934,\n",
       "   -13.030404376358337,\n",
       "   -12.809697924987573,\n",
       "   -12.639867148011758,\n",
       "   -12.996647095284413,\n",
       "   -13.508439952125599,\n",
       "   -13.525609986403222,\n",
       "   -13.662721160798785,\n",
       "   -14.778215982515858,\n",
       "   -15.46608214793227,\n",
       "   -15.815953825174198,\n",
       "   -15.98152823407041,\n",
       "   -16.066402751472797,\n",
       "   -15.996874985231056,\n",
       "   -15.740603078569851,\n",
       "   -15.88945348105543,\n",
       "   -16.012309662455397,\n",
       "   -16.094700878721667,\n",
       "   -15.670110252739079,\n",
       "   -15.280113124903625,\n",
       "   -15.743237408058544,\n",
       "   -15.684174319417274,\n",
       "   -16.535908405603408,\n",
       "   -16.659393890608513,\n",
       "   -17.18441105216241,\n",
       "   -17.794156684897416,\n",
       "   -16.44458926369285,\n",
       "   -15.621527991559581,\n",
       "   -17.034399877461947,\n",
       "   -16.75384178564653,\n",
       "   -16.670210731765096,\n",
       "   -17.60969828857298,\n",
       "   -17.623909286163514,\n",
       "   -17.808401102684076,\n",
       "   -17.204432170626312,\n",
       "   -16.851331468930688,\n",
       "   -16.823184015297254,\n",
       "   -17.342902850500636,\n",
       "   -17.52852241530209,\n",
       "   -17.207083217728144,\n",
       "   -16.730290668609104,\n",
       "   -17.185708872798926,\n",
       "   -16.7010004678592,\n",
       "   -16.204334784754423,\n",
       "   -15.534870250745893,\n",
       "   -15.727685802229395,\n",
       "   -16.18964378630565,\n",
       "   -15.477598732590522,\n",
       "   -17.018057313389054,\n",
       "   -16.404322350541662,\n",
       "   -16.551832580961847,\n",
       "   -15.720250383095411,\n",
       "   -15.663223048164099,\n",
       "   -15.740402123245683,\n",
       "   -16.69470059902333,\n",
       "   -17.11546007328669,\n",
       "   -16.81619084187009,\n",
       "   -16.448666271917734,\n",
       "   -16.727668975862358,\n",
       "   -16.21124768570408,\n",
       "   -16.382481337689665,\n",
       "   -16.976937509091183,\n",
       "   -16.20030696294588,\n",
       "   -16.73260031039053,\n",
       "   -16.367130084039648,\n",
       "   -16.047378339649818,\n",
       "   -16.18228595782584,\n",
       "   -15.975147298872777,\n",
       "   -16.382510888390527,\n",
       "   -15.886962002451003,\n",
       "   -16.312661984986825,\n",
       "   -15.410312052389813,\n",
       "   -15.394346326381154,\n",
       "   -15.602062446376497,\n",
       "   -16.216643592525617,\n",
       "   -16.408923244681326,\n",
       "   -16.77774800290558,\n",
       "   -16.17058790928121,\n",
       "   -16.581846065570677,\n",
       "   -16.71696180587294,\n",
       "   -17.116359306648853,\n",
       "   -17.429111263175646,\n",
       "   -17.778554093665818,\n",
       "   -17.349708887499038,\n",
       "   -17.54444787500387,\n",
       "   -17.73926265929205,\n",
       "   -17.6356895811037,\n",
       "   -17.602231168813724,\n",
       "   -17.863901274191445,\n",
       "   -18.476057928789377,\n",
       "   -19.09183560273136,\n",
       "   -18.896452785899584,\n",
       "   -19.270344889413977,\n",
       "   -19.986006722454622,\n",
       "   -20.7468835602508,\n",
       "   -20.83271980896476,\n",
       "   -21.165887398698057,\n",
       "   -21.50127032296961,\n",
       "   -20.245025101471025,\n",
       "   -19.8414430678371,\n",
       "   -18.68731415449517,\n",
       "   -18.14766739236275,\n",
       "   -17.683364113040195,\n",
       "   -17.5266881743952,\n",
       "   -16.933889467305185,\n",
       "   -17.22639166799104,\n",
       "   -16.764348464267375,\n",
       "   -16.697621553085423,\n",
       "   -16.869101447723907,\n",
       "   -16.019471147993944,\n",
       "   -16.662774909001485,\n",
       "   -17.47186655604956,\n",
       "   -17.835515127426174,\n",
       "   -16.83299513414687,\n",
       "   -16.6384437190588,\n",
       "   -16.64373341577408,\n",
       "   -17.277602665331177,\n",
       "   -16.93501867213694,\n",
       "   -16.99452914913127,\n",
       "   -16.68757112280543,\n",
       "   -17.749545284451987,\n",
       "   -18.159149979285573,\n",
       "   -18.22499802560914,\n",
       "   -17.14272867042002,\n",
       "   -17.29605626630758,\n",
       "   -17.47695723122201,\n",
       "   -17.287191274445757,\n",
       "   -17.577260898388026,\n",
       "   -16.726502539866807,\n",
       "   -17.233037649398664,\n",
       "   -17.90596491107252,\n",
       "   -17.366308631088767,\n",
       "   -18.00033440188331,\n",
       "   -18.786446441830964,\n",
       "   -18.398835008058622,\n",
       "   -17.980938365410704,\n",
       "   -18.031243743494922,\n",
       "   -17.54058700493432,\n",
       "   -18.160207241001668,\n",
       "   -18.385683078917932,\n",
       "   -17.62274434724598,\n",
       "   -18.32734868173143,\n",
       "   -18.67834835442526,\n",
       "   -18.84294424802069,\n",
       "   -18.551828396241845,\n",
       "   -19.010606601928135,\n",
       "   -18.637114424412484,\n",
       "   -18.06842068502469,\n",
       "   -16.922333882950493,\n",
       "   -16.63872630534688,\n",
       "   -16.557345378077212,\n",
       "   -16.620282544480666,\n",
       "   -17.035050734818224,\n",
       "   -16.37059784066464,\n",
       "   -17.49123983954025,\n",
       "   -17.08245611160822,\n",
       "   -16.784037253080154,\n",
       "   -18.10954708321087,\n",
       "   -17.77899604265299,\n",
       "   -17.681435048210027,\n",
       "   -17.802236070780666,\n",
       "   -17.760060043738616,\n",
       "   -16.788313652020932,\n",
       "   -16.961298108411164,\n",
       "   -15.88326674733193,\n",
       "   -16.033439112194184,\n",
       "   -15.752590208015157,\n",
       "   -15.84456881564138,\n",
       "   -16.3943873949365,\n",
       "   -16.444619349859405,\n",
       "   -16.649448941542992,\n",
       "   -16.04206539776024,\n",
       "   -16.43738803111859,\n",
       "   -15.668820348106411,\n",
       "   -15.295786744293391,\n",
       "   -15.126382874959933,\n",
       "   -15.564035405971186,\n",
       "   -15.498678559314023,\n",
       "   -16.285279229888534,\n",
       "   -16.098201232837496,\n",
       "   -16.869032051350107,\n",
       "   -16.317338326591294,\n",
       "   -15.4387847222267,\n",
       "   -14.965133878212626,\n",
       "   -14.011447205784787,\n",
       "   -14.492803132925083,\n",
       "   -14.318204372957174,\n",
       "   -14.3756106189192,\n",
       "   -14.328937482280239,\n",
       "   -14.278375085295362,\n",
       "   -14.37621280590558,\n",
       "   -14.812055712169084,\n",
       "   -14.592617257191865,\n",
       "   -15.799148930026863,\n",
       "   -16.07551368235417,\n",
       "   -15.553017265405604,\n",
       "   -15.283326074918026,\n",
       "   -15.986427346600998,\n",
       "   -16.45892782463653,\n",
       "   -17.51606322987929,\n",
       "   -18.486181526867508,\n",
       "   -18.316875645102293,\n",
       "   -17.113632993687187,\n",
       "   -17.21786421037707,\n",
       "   -16.928221290644878,\n",
       "   -17.12116991093781,\n",
       "   -16.295414592249504,\n",
       "   -16.370530630230085,\n",
       "   -16.265618957590814,\n",
       "   -16.675609450436962,\n",
       "   -16.401838165203138,\n",
       "   -16.91999172860177,\n",
       "   -18.347053155593166,\n",
       "   -17.665853386542956,\n",
       "   -18.62237124614149,\n",
       "   -18.985821881028368,\n",
       "   -18.630828021234198,\n",
       "   -19.984798482449275,\n",
       "   -21.397220478828096,\n",
       "   -20.34900562624304,\n",
       "   -20.08250611307714,\n",
       "   -19.83350896015602,\n",
       "   -19.714859672143522,\n",
       "   -19.190288145637616,\n",
       "   -18.05158610280186,\n",
       "   -18.514628031150707,\n",
       "   -17.924437194779983,\n",
       "   -16.732921879630144,\n",
       "   -16.76192263631729,\n",
       "   -16.88121859840824,\n",
       "   -15.618941334055485,\n",
       "   -15.916370608766535,\n",
       "   -15.92401060919612,\n",
       "   -16.531363915764935,\n",
       "   -16.731143868778073,\n",
       "   -17.058004224077973,\n",
       "   -16.588190208024084,\n",
       "   -17.642401634332185,\n",
       "   -17.37440545858309,\n",
       "   -17.91061443999627,\n",
       "   -18.39631463924075,\n",
       "   -17.992588074625967,\n",
       "   -19.53483776357719,\n",
       "   -20.64874270227328,\n",
       "   -19.583204106623143,\n",
       "   -19.012779390531353,\n",
       "   -18.5288035590525,\n",
       "   -18.322342048083094,\n",
       "   -19.01896870174558,\n",
       "   -19.019798393865898,\n",
       "   -18.770481995172688,\n",
       "   -18.916832713597323,\n",
       "   -18.890346270471635,\n",
       "   -19.882632505565628,\n",
       "   -19.593960499874672,\n",
       "   -19.740825452358255,\n",
       "   -19.32978480134396,\n",
       "   -18.32098270623752,\n",
       "   -18.058432989807113,\n",
       "   -17.309387991070594,\n",
       "   -17.225732832089264,\n",
       "   -17.15553322463436,\n",
       "   -17.004636743126785,\n",
       "   -17.816535510601845],\n",
       "  'voc_r2': [-1.4134067253725195,\n",
       "   -1.2665351029651157,\n",
       "   -0.09693133461138959,\n",
       "   -0.4092900521793523,\n",
       "   -0.6442388086498885,\n",
       "   -0.5677810169030528,\n",
       "   -0.13619375978049209,\n",
       "   -0.46436851928702816,\n",
       "   -0.36323399851165483,\n",
       "   -0.18783412742429562,\n",
       "   -0.5860510142121804,\n",
       "   -0.5350760286978491,\n",
       "   -0.6481936863222735,\n",
       "   -0.4252756935918711,\n",
       "   -0.6134667525350384,\n",
       "   -0.4608672189809937,\n",
       "   -0.43025481912267916,\n",
       "   -0.6056279275044296,\n",
       "   -0.5192607970161398,\n",
       "   -0.5846855788205934,\n",
       "   -0.34865733468572513,\n",
       "   -0.26068349388080403,\n",
       "   -0.22175819080759474,\n",
       "   -0.3583032226024434,\n",
       "   -0.16251770858852832,\n",
       "   -0.16869813997386207,\n",
       "   -0.20842167622973284,\n",
       "   -0.3373078429110197,\n",
       "   -0.20229617357643903,\n",
       "   -0.246773064762708,\n",
       "   -0.16115278592693838,\n",
       "   -0.32182145906474635,\n",
       "   -0.1693843932954875,\n",
       "   -0.15207382344041642,\n",
       "   -0.017880562875597805,\n",
       "   0.03261824354989018,\n",
       "   0.013817913436274032,\n",
       "   0.0029498090787142273,\n",
       "   -0.05002540912916209,\n",
       "   0.048846681795767566,\n",
       "   0.14100212419458336,\n",
       "   0.17748844849014178,\n",
       "   0.3116384686656969,\n",
       "   0.3186154965761472,\n",
       "   0.1608624381911562,\n",
       "   0.01350887131062517,\n",
       "   0.07414505680809746,\n",
       "   0.07004048703375643,\n",
       "   0.030833491028761317,\n",
       "   -0.031659976297826686,\n",
       "   -0.09776150372711423,\n",
       "   -0.166567201953991,\n",
       "   0.08213558211751315,\n",
       "   -0.0506012874402999,\n",
       "   -0.3405494866717842,\n",
       "   -0.20649114056068774,\n",
       "   -0.1472899688465925,\n",
       "   0.04671890947827484,\n",
       "   -0.09889423159922095,\n",
       "   -0.3105540125487183,\n",
       "   -0.44952338775036793,\n",
       "   -0.25571832323784127,\n",
       "   -0.04590787968370691,\n",
       "   -0.26815416219530896,\n",
       "   -0.3882199513707545,\n",
       "   -0.26304622545832945,\n",
       "   -0.289213802930514,\n",
       "   -0.5163648288042635,\n",
       "   -0.5102760419484902,\n",
       "   -0.6717565156190555,\n",
       "   -0.40346995299076194,\n",
       "   -0.4540540797264072,\n",
       "   -0.5407028078600693,\n",
       "   -0.6867830136205477,\n",
       "   -0.5305758328512711,\n",
       "   -0.49342368079213683,\n",
       "   -0.781953895636706,\n",
       "   -0.7516178779484721,\n",
       "   -0.7537572397047525,\n",
       "   -0.6943627927081486,\n",
       "   -1.0215464052391119,\n",
       "   -1.1076570714396068,\n",
       "   -1.8573116124297973,\n",
       "   -1.910884387287834,\n",
       "   -1.5994016875930117,\n",
       "   -1.7940153746756673,\n",
       "   -1.8632947591141193,\n",
       "   -1.6204341470709207,\n",
       "   -1.425168327473664,\n",
       "   -1.324200055571894,\n",
       "   -0.9797281407517144,\n",
       "   -0.9143630107882057,\n",
       "   -0.656678716226532,\n",
       "   -0.7659455666407209,\n",
       "   -0.5382031091620256,\n",
       "   -0.6702000739278553,\n",
       "   -0.5106932442926233,\n",
       "   -0.5954202708415126,\n",
       "   -0.6223340842219802,\n",
       "   -0.6503248156060073,\n",
       "   -0.6622603390835369,\n",
       "   -0.7037362858072616,\n",
       "   -0.752325119904572,\n",
       "   -0.8582224771109321,\n",
       "   -0.6341349820364017,\n",
       "   -0.2624792910681222,\n",
       "   -0.07837930734879839,\n",
       "   -0.2536352269127229,\n",
       "   -0.07894336064211638,\n",
       "   -0.06276144467703881,\n",
       "   -0.02281081657200379,\n",
       "   -0.038010813567781465,\n",
       "   -0.06013738164747817,\n",
       "   -0.04445641490529395,\n",
       "   -0.0810265552680498,\n",
       "   -0.08617043033246308,\n",
       "   -0.1216042030737221,\n",
       "   -0.144649757236198,\n",
       "   -0.35076655610172036,\n",
       "   -0.20416727437312776,\n",
       "   -0.05168958288730696,\n",
       "   -0.08282053595800187,\n",
       "   -0.1664454028292799,\n",
       "   -0.38394600942311463,\n",
       "   -0.38180724612316963,\n",
       "   -0.5453737666671015,\n",
       "   -0.31044192711407725,\n",
       "   -0.35033761249745554,\n",
       "   -0.4312052820871155,\n",
       "   -0.3479705852646735,\n",
       "   -0.4126064006421837,\n",
       "   -0.6611110768726263,\n",
       "   -0.8562330174096184,\n",
       "   -0.7437787700570273,\n",
       "   -0.662877869877279,\n",
       "   -0.4504176829527633,\n",
       "   -0.4320175608941237,\n",
       "   -0.47261271462202137,\n",
       "   -0.6211566364338132,\n",
       "   -0.7830537343375183,\n",
       "   -0.5321122351310221,\n",
       "   -0.7271259403204042,\n",
       "   -0.6143697710890905,\n",
       "   -0.793384271354374,\n",
       "   -0.6908928616266619,\n",
       "   -0.5846559167401082,\n",
       "   -0.5124099495101051,\n",
       "   -0.42903858460435496,\n",
       "   -0.5582894138989618,\n",
       "   -0.34765436637907476,\n",
       "   -0.16558087094061258,\n",
       "   -0.04559245050358607,\n",
       "   -0.2962458405491257,\n",
       "   -0.3466791144405279,\n",
       "   -0.402365664162023,\n",
       "   -0.5452850567976086,\n",
       "   -0.36149642072820076,\n",
       "   -0.3280308490149355,\n",
       "   -0.5036954717076616,\n",
       "   -0.544667063772031,\n",
       "   -0.2460764785189249,\n",
       "   -0.11374586872898473,\n",
       "   -0.07649982100541042,\n",
       "   -0.08928680871053585,\n",
       "   -0.22024066684947607,\n",
       "   -0.15933443663091995,\n",
       "   -0.17901268229101053,\n",
       "   -0.3810985036839514,\n",
       "   -0.38009585772181875,\n",
       "   -0.19230140120440287,\n",
       "   -0.15361413333302498,\n",
       "   -0.11020144415843025,\n",
       "   -0.13466003163579243,\n",
       "   -0.3200270468600379,\n",
       "   -0.22866955752859708,\n",
       "   -0.13234445178808718,\n",
       "   -0.2355555966232692,\n",
       "   0.005007136725742489,\n",
       "   0.14627486655788657,\n",
       "   0.17251138633940089,\n",
       "   0.12660510056764362,\n",
       "   0.1013647790042036,\n",
       "   0.2114036339371883,\n",
       "   0.18501433610057416,\n",
       "   0.18672500212442056,\n",
       "   0.2862029996301817,\n",
       "   0.16949278731681705,\n",
       "   0.15061098298090347,\n",
       "   0.19622185780365764,\n",
       "   0.2396401959569291,\n",
       "   0.38288353096426475,\n",
       "   0.4027089653968132,\n",
       "   0.37785383759396507,\n",
       "   0.3412613348836182,\n",
       "   0.35682894879607885,\n",
       "   0.30098006642776753,\n",
       "   0.30292443546750114,\n",
       "   0.24067364425513083,\n",
       "   0.271508712245741,\n",
       "   0.22735292418200548,\n",
       "   0.2674475139169228,\n",
       "   0.20541151482376951,\n",
       "   0.21490272074415162,\n",
       "   0.20369762074095454,\n",
       "   0.2559682693795329,\n",
       "   0.10108374870271564,\n",
       "   0.13225162084798703,\n",
       "   0.12663208649010738,\n",
       "   0.14533268249459663,\n",
       "   0.08451489194354866,\n",
       "   0.19876755507385813,\n",
       "   0.227469408751664,\n",
       "   0.11704177597165111,\n",
       "   0.12917123026548438,\n",
       "   0.09115650930795882,\n",
       "   0.18601103890994275,\n",
       "   0.20172739798777217,\n",
       "   0.24663384482068518,\n",
       "   0.2296114992356577,\n",
       "   0.25509065018016586,\n",
       "   0.0824356008760957,\n",
       "   0.037768680880658434,\n",
       "   0.1974265677493081,\n",
       "   0.11466880589931727,\n",
       "   0.14542169190948362,\n",
       "   0.2105061049465422,\n",
       "   0.27085335184135373,\n",
       "   0.30965239165234804,\n",
       "   0.2833812333302823,\n",
       "   0.19092179351994,\n",
       "   0.0621804251492547,\n",
       "   0.041527342300691994,\n",
       "   -0.1434703930572201,\n",
       "   -0.147461652129542,\n",
       "   -0.12717568746170893,\n",
       "   -0.030008258335384452,\n",
       "   0.027565680289926564,\n",
       "   0.14264816243151401,\n",
       "   0.06485297117909405,\n",
       "   0.001349061485976022,\n",
       "   0.060199925260880915,\n",
       "   0.1801586360144687,\n",
       "   0.13411797225729793,\n",
       "   0.11485454901706027,\n",
       "   0.08323226391349903,\n",
       "   0.2161073156117912,\n",
       "   0.19417667030572439,\n",
       "   0.18951979276765196,\n",
       "   0.21900308856945672,\n",
       "   0.19326690299485683,\n",
       "   0.1597506242053608,\n",
       "   0.08036290092511456,\n",
       "   0.007577151340250277,\n",
       "   0.044818076854110545,\n",
       "   0.07011404237981822,\n",
       "   0.1740590369746564,\n",
       "   0.06912566948051979,\n",
       "   0.18871090044889682,\n",
       "   0.1211830406749923,\n",
       "   -0.14233440309713163,\n",
       "   -0.09903746862603091,\n",
       "   -0.19443924150341108,\n",
       "   -0.3071564084311249,\n",
       "   -0.242313239354069,\n",
       "   -0.0636143171295509,\n",
       "   0.1486368461295362,\n",
       "   0.1251929919137138,\n",
       "   0.1495916158454127,\n",
       "   0.1633824279921806,\n",
       "   0.2549392146539897,\n",
       "   0.18746418607820192,\n",
       "   0.2605235574070677,\n",
       "   0.13853844433285167,\n",
       "   0.16624651023995263,\n",
       "   0.2653048601976642,\n",
       "   0.26244074195676115,\n",
       "   0.17962581055874616,\n",
       "   0.14629325482506117,\n",
       "   0.072612084563489,\n",
       "   0.009934271827183538,\n",
       "   -0.04552468809601673,\n",
       "   -0.12900468335680193,\n",
       "   -0.09278197340630889,\n",
       "   -0.06652414505006132,\n",
       "   -0.03853284214404695,\n",
       "   -0.025524399553640498,\n",
       "   -0.004993141751143737,\n",
       "   -0.39893461066211744,\n",
       "   -0.29016208908656593,\n",
       "   -0.3386091094330037,\n",
       "   -0.3184932182802689,\n",
       "   -0.24338328242480478,\n",
       "   -0.481698335022299,\n",
       "   -0.32140046300260106,\n",
       "   -0.289569526803501,\n",
       "   -0.06632380004459848,\n",
       "   -0.016799143370084035,\n",
       "   0.1694356568551204,\n",
       "   0.016983766374650155,\n",
       "   0.019620235958619725,\n",
       "   -0.004603347899384325,\n",
       "   0.1013145115293902,\n",
       "   0.17428510501314043,\n",
       "   0.10791315585580885,\n",
       "   -0.11854862713001668,\n",
       "   -0.13639905922976792,\n",
       "   -0.0906020638644649,\n",
       "   -0.17838761167624195,\n",
       "   -0.042419890744954,\n",
       "   -0.00990424884441854,\n",
       "   0.08204500969120831,\n",
       "   0.04174407020388582,\n",
       "   -0.03304811546133113,\n",
       "   -0.0008461461378030943,\n",
       "   -0.043162081757990745,\n",
       "   -0.048649684712705366,\n",
       "   -0.04833438849874594,\n",
       "   -0.14357515211034477,\n",
       "   -0.23987513845623631,\n",
       "   -0.21992857438415303,\n",
       "   -0.1689836176012638,\n",
       "   0.061457834875256356,\n",
       "   -0.0524722503965207,\n",
       "   -0.13345337977994354,\n",
       "   -0.1247488437133526,\n",
       "   -0.31995926575680445,\n",
       "   -0.31744377154008063,\n",
       "   -0.32070984136894,\n",
       "   -0.4175858132440069,\n",
       "   -0.3991613517741033,\n",
       "   -0.3123255659113815,\n",
       "   -0.20420345461163492,\n",
       "   -0.2063330462640398,\n",
       "   -0.2854259193668871,\n",
       "   -0.144264680076168,\n",
       "   -0.08419969198061317,\n",
       "   -0.15633209717439556,\n",
       "   -0.2895976487385272,\n",
       "   -0.2826340923427544,\n",
       "   -0.04625468816727207,\n",
       "   -0.096083009372957,\n",
       "   -0.09166500184536552,\n",
       "   0.0812177271157305,\n",
       "   0.07496679278982088,\n",
       "   0.10609989654061025,\n",
       "   0.07309614506028639,\n",
       "   0.04535648952334159,\n",
       "   -0.0919760466488233,\n",
       "   0.08127531507782904,\n",
       "   0.034362583155046034,\n",
       "   -0.10124165372270433,\n",
       "   -0.33940563145740654,\n",
       "   -0.4641526961271283,\n",
       "   -0.6906056693826033,\n",
       "   -0.5143546056414658,\n",
       "   -0.5664372903763919,\n",
       "   -0.4110504253816669,\n",
       "   -0.31116533914266564,\n",
       "   -0.19931979905078379,\n",
       "   -0.4332837641435483,\n",
       "   -0.8466207761349482,\n",
       "   -0.600391095208344,\n",
       "   -0.8582325684386061,\n",
       "   -0.7931846367976605,\n",
       "   -0.6463224408409796,\n",
       "   -0.646712877797103,\n",
       "   -0.5928400878127815,\n",
       "   -0.5604593223012053,\n",
       "   -0.31643848977324285,\n",
       "   -0.4321105362401887,\n",
       "   -0.592914245114267,\n",
       "   -0.4679947654965655,\n",
       "   -0.2534967163827655,\n",
       "   -0.21512995069382246,\n",
       "   -0.1786545924971008,\n",
       "   -0.1826036517765519,\n",
       "   -0.16905337155538325,\n",
       "   -0.11130706946282132,\n",
       "   -0.14556275900176652,\n",
       "   -0.2704339387248973,\n",
       "   -0.35679712989373646,\n",
       "   -0.2069365440069928,\n",
       "   -0.1191005724048062,\n",
       "   -0.32723966383341274,\n",
       "   -0.5440133723779927,\n",
       "   -0.46593350244052645,\n",
       "   -0.4411689683331659,\n",
       "   -0.6168253032799076,\n",
       "   -0.5829005041312807,\n",
       "   -0.35222368841321794,\n",
       "   -0.49679672783997675,\n",
       "   -0.48112763661948765,\n",
       "   -0.6450114331547436,\n",
       "   -0.6317744709306008,\n",
       "   -0.552780675645735,\n",
       "   -0.2832818357016724,\n",
       "   -0.3546691179794925,\n",
       "   -0.443779192814993,\n",
       "   -0.4563595039197146,\n",
       "   -0.40605195971165076,\n",
       "   -0.3352260640002107,\n",
       "   -0.35978557030641856,\n",
       "   -0.21482859627153084,\n",
       "   -0.20982017066864644,\n",
       "   -0.12963179826427162,\n",
       "   -0.16169093392120115,\n",
       "   -0.10512051829315361,\n",
       "   -0.17886218062025994,\n",
       "   -0.2864524287328325,\n",
       "   -0.2439392205467601,\n",
       "   -0.14026553238490957,\n",
       "   -0.11770654357839527,\n",
       "   -0.1688008007396331,\n",
       "   -0.05646865402585144,\n",
       "   -0.08856764420194718,\n",
       "   -0.015118142197628304,\n",
       "   0.03419361190169756,\n",
       "   0.024440134219531062,\n",
       "   -0.014906118386365907,\n",
       "   -0.267826953864464,\n",
       "   -0.1623309861373452,\n",
       "   -0.08650203001376067,\n",
       "   -0.03200472275003108,\n",
       "   -0.09046435293512567,\n",
       "   -0.0044902121263277195,\n",
       "   -0.08440961938011182,\n",
       "   -0.2696204141248053,\n",
       "   -0.09350631388157327,\n",
       "   -0.13230343658900767,\n",
       "   -0.12495772814693029,\n",
       "   -0.12376018744920203,\n",
       "   0.014370507516872744,\n",
       "   0.014095633307602773,\n",
       "   -0.0676722956346969,\n",
       "   -0.15704668977826008,\n",
       "   -0.27935284833220964,\n",
       "   -0.272655165595606,\n",
       "   -0.1916502293159692,\n",
       "   -0.13616378140065244,\n",
       "   -0.05459155118153114,\n",
       "   -0.13792447452457335,\n",
       "   -0.02079423049226614,\n",
       "   0.0022215663103467564,\n",
       "   -0.010902958515780714,\n",
       "   -0.015504483900690591,\n",
       "   -0.025170768421928003,\n",
       "   -0.3176128288943536,\n",
       "   -0.33050333903440277,\n",
       "   -0.44359808051536254,\n",
       "   -0.5122247351424294,\n",
       "   -0.6126499342554035,\n",
       "   -0.5100594997040198,\n",
       "   -0.42513366245824846,\n",
       "   -0.2230562116508341,\n",
       "   -0.14012131596210775,\n",
       "   -0.1385741486673282,\n",
       "   -0.054830506181210215,\n",
       "   0.039286901374268934,\n",
       "   0.10528843727840997,\n",
       "   0.0397652180443101,\n",
       "   -0.05385656743722067,\n",
       "   -0.13753316489073852,\n",
       "   -0.06601212758033337,\n",
       "   -0.18975908165243394,\n",
       "   -0.36630450804522785,\n",
       "   -0.3441294274962601,\n",
       "   -0.4129157457788355,\n",
       "   -0.34292172545674515,\n",
       "   -0.5086926365278308,\n",
       "   -0.6420364265969061,\n",
       "   -0.7369506698903008,\n",
       "   -0.855736743896736,\n",
       "   -0.856111111350867,\n",
       "   -0.8052766765310926,\n",
       "   -0.6029878380988634,\n",
       "   -0.7677634691236501,\n",
       "   -0.5474217809584687,\n",
       "   -0.5167405254200703,\n",
       "   -0.29055816095905174,\n",
       "   -0.13168234756576247,\n",
       "   -0.28028817771531656,\n",
       "   -0.11454931688229952,\n",
       "   -0.11167270824217335,\n",
       "   -0.02201660110546788,\n",
       "   -0.09237061256674872,\n",
       "   -0.08834248398806599,\n",
       "   -0.20111391352384111,\n",
       "   -0.1731612697503211,\n",
       "   -0.13782132840531758,\n",
       "   -0.0757516510445444,\n",
       "   -0.10421085220451842,\n",
       "   -0.018477168660303134,\n",
       "   0.0439486542170443,\n",
       "   -0.006139240995301831,\n",
       "   -0.0990174497284606,\n",
       "   -0.11231058056170484,\n",
       "   -0.12785773197116734,\n",
       "   -0.22715691687111628,\n",
       "   -0.18108366033101242,\n",
       "   0.05342797971208857],\n",
       "  'jsc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -5299202.137875704,\n",
       "   -1253.5467823038418,\n",
       "   -196.50747338946124,\n",
       "   -74.48440061811266,\n",
       "   -67.87337466765048,\n",
       "   -31.967503715625583,\n",
       "   -26.66318830970082,\n",
       "   -25.744916657408343,\n",
       "   -17.40102950079208,\n",
       "   -15.670267844039277,\n",
       "   -8.53300072067005,\n",
       "   -7.829147342745266,\n",
       "   -10.351478885319256,\n",
       "   -11.087029635013728,\n",
       "   -8.190766175699713,\n",
       "   -7.878767987916827,\n",
       "   -7.33049852810402,\n",
       "   -4.7676619356784045,\n",
       "   -4.416831706596922,\n",
       "   -2.9329036113393405,\n",
       "   -3.552687846264135,\n",
       "   -3.3591221594575726,\n",
       "   -3.4115386062898496,\n",
       "   -4.160612280991338,\n",
       "   -3.5212804515224354,\n",
       "   -2.1485748936747564,\n",
       "   -2.387944512984664,\n",
       "   -1.838102522480615,\n",
       "   -1.9085852526576708,\n",
       "   -1.2067771830214236,\n",
       "   -1.7194130726104864,\n",
       "   -1.8456536239674581,\n",
       "   -2.4974711612331393,\n",
       "   -2.5542372263460025,\n",
       "   -2.3943038606780713,\n",
       "   -2.574858344936902,\n",
       "   -4.117644551339014,\n",
       "   -4.156259351242937,\n",
       "   -4.393521110881911,\n",
       "   -4.916373963336823,\n",
       "   -3.781641823678405,\n",
       "   -2.964757966162347,\n",
       "   -2.2989522635722603,\n",
       "   -3.117380531454655,\n",
       "   -3.115365936405027,\n",
       "   -2.6243012933645673,\n",
       "   -1.6187932286438205,\n",
       "   -1.9751130399951116,\n",
       "   -1.8227256203357234,\n",
       "   -2.361943736187161,\n",
       "   -1.3249235659374876,\n",
       "   -0.9802133525727963,\n",
       "   -1.0874544716529568,\n",
       "   -0.9676281346772941,\n",
       "   -1.263103401198718,\n",
       "   -0.867843225298196,\n",
       "   -0.7741156951467614,\n",
       "   -0.8604023896226203,\n",
       "   -1.4303280478179854,\n",
       "   -1.472209287432055,\n",
       "   -1.6910838939890689,\n",
       "   -1.7395273459049205,\n",
       "   -1.93819841312625,\n",
       "   -1.2412406004781182,\n",
       "   -1.4510395314434494,\n",
       "   -1.6940601274460128,\n",
       "   -2.195031487208687,\n",
       "   -1.836372524817289,\n",
       "   -2.7346179574845624,\n",
       "   -2.3186604594064812,\n",
       "   -1.8332956488486176,\n",
       "   -1.5321338072857773,\n",
       "   -1.6348561092146756,\n",
       "   -1.2964922057100177,\n",
       "   -2.024252578786944,\n",
       "   -1.8505570382174001,\n",
       "   -2.278921864828474,\n",
       "   -2.3841951480043333,\n",
       "   -2.6252981638569115,\n",
       "   -2.4307942224039616,\n",
       "   -2.6803057941365696,\n",
       "   -2.324559474621528,\n",
       "   -2.3837177654472774,\n",
       "   -2.4513107895513895,\n",
       "   -2.932581358242401,\n",
       "   -2.8093087560837757,\n",
       "   -2.2087193035566175,\n",
       "   -2.335482108884759,\n",
       "   -1.2593019738944813,\n",
       "   -1.3545457203831792,\n",
       "   -1.7251529970322577,\n",
       "   -1.1959044042427807,\n",
       "   -1.6288121835677059,\n",
       "   -2.2839207650197575,\n",
       "   -2.5165328037775025,\n",
       "   -2.189863362571213,\n",
       "   -2.5550955085936176,\n",
       "   -1.652709303843666,\n",
       "   -1.3131629792709218,\n",
       "   -1.712900345833544,\n",
       "   -2.3325556755664874,\n",
       "   -1.2045900410081298,\n",
       "   -0.9941285837331917,\n",
       "   -1.4663345056922483,\n",
       "   -0.858134240263297,\n",
       "   -1.0953570538257171,\n",
       "   -0.9992244849280985,\n",
       "   -0.9197263142271832,\n",
       "   -1.0424677543404357,\n",
       "   -1.1237135121703345,\n",
       "   -0.8692060121612843,\n",
       "   -1.163186625308259,\n",
       "   -1.371928974999192,\n",
       "   -1.050456045836477,\n",
       "   -1.694046623813465,\n",
       "   -1.6814734543233012,\n",
       "   -1.4918768527259059,\n",
       "   -1.8554455432369745,\n",
       "   -1.387734466686458,\n",
       "   -1.909179511887921,\n",
       "   -1.2163914012489467,\n",
       "   -1.360381795066929,\n",
       "   -1.2442814741631403,\n",
       "   -0.9069013702176003,\n",
       "   -1.0414712758254057,\n",
       "   -1.4183568886156936,\n",
       "   -1.4006070159836677,\n",
       "   -2.2594168534068975,\n",
       "   -1.8959328213514626,\n",
       "   -2.350409878620582,\n",
       "   -2.3745542043414276,\n",
       "   -2.1269817441601484,\n",
       "   -2.0922617922360898,\n",
       "   -1.8704538075049695,\n",
       "   -1.453620286804155,\n",
       "   -1.6065004252808044,\n",
       "   -2.3134511983673716,\n",
       "   -2.9495271175449993,\n",
       "   -3.475592799832814,\n",
       "   -2.9202739533922255,\n",
       "   -3.0179382766890637,\n",
       "   -3.5751569240234415,\n",
       "   -2.397250763054666,\n",
       "   -2.102452922190439,\n",
       "   -1.7559566473898296,\n",
       "   -2.8110528105151587,\n",
       "   -1.404012687234995,\n",
       "   -1.7099332346928193,\n",
       "   -2.0389048753681744,\n",
       "   -1.4142957334408384,\n",
       "   -1.7502047364843683,\n",
       "   -2.25126298840927,\n",
       "   -2.9214227807565356,\n",
       "   -2.685202266765876,\n",
       "   -2.134114025880637,\n",
       "   -1.5117858390332986,\n",
       "   -2.127375367620893,\n",
       "   -1.9415309999025432,\n",
       "   -1.8429964538879369,\n",
       "   -2.2599214612557375,\n",
       "   -2.5509890464245024,\n",
       "   -3.1149364973640523,\n",
       "   -3.85223010469117,\n",
       "   -4.339765375171419,\n",
       "   -3.212871943627208,\n",
       "   -3.6696142241779945,\n",
       "   -4.379730205402568,\n",
       "   -2.230684725936829,\n",
       "   -2.4272007590845694,\n",
       "   -2.3067199730239745,\n",
       "   -3.080803505016922,\n",
       "   -3.2151813466646955,\n",
       "   -3.113026559480537,\n",
       "   -3.311858146917287,\n",
       "   -4.118721335790112,\n",
       "   -3.4169264817826006,\n",
       "   -2.8923842789319876,\n",
       "   -2.602613056416741,\n",
       "   -2.816899176534587,\n",
       "   -2.9793820193640146,\n",
       "   -3.206077493327804,\n",
       "   -2.659045440635348,\n",
       "   -2.0362290095029674,\n",
       "   -1.891861910113279,\n",
       "   -1.5272293712262575,\n",
       "   -2.354891169222879,\n",
       "   -2.633818235541769,\n",
       "   -2.7788088051798594,\n",
       "   -2.2985164052106146,\n",
       "   -2.3399437326426624,\n",
       "   -1.7396470414507936,\n",
       "   -1.7215595444222918,\n",
       "   -2.7445864623124265,\n",
       "   -2.433073325391931,\n",
       "   -3.5724597257673416,\n",
       "   -2.8657175173078295,\n",
       "   -3.2522399801928774,\n",
       "   -2.6847978653105717,\n",
       "   -3.0152549272051736,\n",
       "   -3.862484228915373,\n",
       "   -3.675942236932282,\n",
       "   -3.074767900124664,\n",
       "   -3.2016830296269827,\n",
       "   -3.23884803536313,\n",
       "   -3.3326409994003745,\n",
       "   -4.264512763079096,\n",
       "   -6.216396348573438,\n",
       "   -5.4963453715934705,\n",
       "   -6.142815524916675,\n",
       "   -7.1368790506388695,\n",
       "   -7.595952883916782,\n",
       "   -6.626099488735563,\n",
       "   -6.6762034385269,\n",
       "   -7.5036960663306385,\n",
       "   -4.809918228451119,\n",
       "   -4.026227748012618,\n",
       "   -4.248827649049253,\n",
       "   -4.066175166500349,\n",
       "   -4.529959939163637,\n",
       "   -3.26694575997945,\n",
       "   -2.8426089299522137,\n",
       "   -2.4000328697850177,\n",
       "   -2.481932855860298,\n",
       "   -2.0967624257471473,\n",
       "   -3.2238111358455006,\n",
       "   -2.876754158280674,\n",
       "   -1.8188878549920013,\n",
       "   -1.4029479804660299,\n",
       "   -1.636325714513207,\n",
       "   -1.54656178594138,\n",
       "   -2.038047974368938,\n",
       "   -1.8836665132224373,\n",
       "   -1.6669439089612217,\n",
       "   -1.5957602560940316,\n",
       "   -1.1784665447546776,\n",
       "   -1.9262866543774066,\n",
       "   -2.196434404447928,\n",
       "   -1.8366008530226487,\n",
       "   -1.4845424731200607,\n",
       "   -1.6725009411413359,\n",
       "   -1.763429428196675,\n",
       "   -1.3828316906813805,\n",
       "   -1.366047381818671,\n",
       "   -1.1286904301894687,\n",
       "   -0.9968101857815217,\n",
       "   -1.0362097385473845,\n",
       "   -1.549884792933132,\n",
       "   -1.2580914495327122,\n",
       "   -1.1197318351514505,\n",
       "   -1.2568442011746188,\n",
       "   -0.9410814515820538,\n",
       "   -0.7829894261375081,\n",
       "   -0.8865045538490619,\n",
       "   -1.0356554715501356,\n",
       "   -1.106718580414956,\n",
       "   -1.4756044700133608,\n",
       "   -1.7842811024154859,\n",
       "   -1.9566377163543889,\n",
       "   -1.959521942007505,\n",
       "   -1.910347277437371,\n",
       "   -1.4538808625805824,\n",
       "   -1.3581843864892837,\n",
       "   -1.8618475042009544,\n",
       "   -2.9999308321781695,\n",
       "   -2.796259943711317,\n",
       "   -2.6366103827106286,\n",
       "   -2.743791455939286,\n",
       "   -2.8454707655156164,\n",
       "   -2.4685005729064398,\n",
       "   -2.207098213246178,\n",
       "   -2.265915418940807,\n",
       "   -2.2006874104146252,\n",
       "   -2.4078116678071106,\n",
       "   -2.8639153957451375,\n",
       "   -2.301467441385076,\n",
       "   -2.44553728723611,\n",
       "   -3.3498646647220847,\n",
       "   -3.2052769627452067,\n",
       "   -3.4331000500746898,\n",
       "   -3.3105496523367783,\n",
       "   -4.22966183620178,\n",
       "   -5.862498739055903,\n",
       "   -6.726624766327147,\n",
       "   -5.685424869609365,\n",
       "   -6.023170105202734,\n",
       "   -7.618659089815173,\n",
       "   -6.168491679754805,\n",
       "   -5.62098509618872,\n",
       "   -8.139349216867815,\n",
       "   -8.330202179629804,\n",
       "   -8.230450920524767,\n",
       "   -6.6471634509705035,\n",
       "   -6.071824853982681,\n",
       "   -5.553223488061973,\n",
       "   -5.406547344460392,\n",
       "   -7.22382954964781,\n",
       "   -7.356201572374159,\n",
       "   -5.358553227070788,\n",
       "   -7.515628402154107,\n",
       "   -7.50736327883074,\n",
       "   -7.326374371893117,\n",
       "   -10.016422123890498,\n",
       "   -8.335634529046633,\n",
       "   -12.596043861706299,\n",
       "   -8.93217430860274,\n",
       "   -9.980049106131574,\n",
       "   -10.386830260843196,\n",
       "   -15.487834916797315,\n",
       "   -13.37230478903662,\n",
       "   -11.93558852034623,\n",
       "   -11.448227915715513,\n",
       "   -7.835505508000692,\n",
       "   -6.018748573981179,\n",
       "   -5.133571560045512,\n",
       "   -4.281324405141238,\n",
       "   -3.11082412316431,\n",
       "   -3.1293140902061065,\n",
       "   -3.479635530494292,\n",
       "   -3.57959475599774,\n",
       "   -3.13385105222686,\n",
       "   -2.650360763519148,\n",
       "   -3.5918241096346044,\n",
       "   -2.812834171756998,\n",
       "   -2.7646201328663733,\n",
       "   -3.828761415427712,\n",
       "   -4.914360152998761,\n",
       "   -5.419546004418518,\n",
       "   -5.96719006641723,\n",
       "   -5.711191618918876,\n",
       "   -4.323127301549478,\n",
       "   -3.936604016282809,\n",
       "   -4.178316363938361,\n",
       "   -3.302085573013011,\n",
       "   -2.899733617316742,\n",
       "   -3.341382566119915,\n",
       "   -3.308093241828101,\n",
       "   -3.6286865444748564,\n",
       "   -3.4821507597170474,\n",
       "   -3.840570168646189,\n",
       "   -3.439772390214493,\n",
       "   -2.8733180750406992,\n",
       "   -2.5105858852462593,\n",
       "   -2.578617306100749,\n",
       "   -2.4504336926262034,\n",
       "   -3.4465048859838516,\n",
       "   -3.3127387297885003,\n",
       "   -5.1159525864351885,\n",
       "   -4.055807772323133,\n",
       "   -4.046611645245393,\n",
       "   -4.044139409987536,\n",
       "   -4.865569457645186,\n",
       "   -5.4349464589345065,\n",
       "   -5.450776988556515,\n",
       "   -4.669065500139899,\n",
       "   -4.447866240520656,\n",
       "   -3.9821324124879647,\n",
       "   -4.419703336904569,\n",
       "   -5.558890932819626,\n",
       "   -6.310662692220835,\n",
       "   -5.1215854340699,\n",
       "   -3.952824985581578,\n",
       "   -4.981134113328565,\n",
       "   -3.3273656466885058,\n",
       "   -3.407850977434058,\n",
       "   -2.6819408224379138,\n",
       "   -2.764840388265571,\n",
       "   -2.334653947204867,\n",
       "   -2.7935879911794776,\n",
       "   -2.5731789854858333,\n",
       "   -2.668050172929565,\n",
       "   -2.689655912524512,\n",
       "   -2.6894476515145467,\n",
       "   -3.655705815383916,\n",
       "   -3.4082292252190722,\n",
       "   -4.012182671632059,\n",
       "   -3.71535978604662,\n",
       "   -4.191141934863755,\n",
       "   -4.10712046310054,\n",
       "   -4.602151799233045,\n",
       "   -4.701120457580077,\n",
       "   -4.161108659438915,\n",
       "   -3.410450323177896,\n",
       "   -2.992086421250664,\n",
       "   -3.165973263200799,\n",
       "   -3.0260537767054636,\n",
       "   -2.615340341285656,\n",
       "   -2.6302428703176797,\n",
       "   -3.361290487088497,\n",
       "   -3.203079570997949,\n",
       "   -3.925079871901972,\n",
       "   -3.3391968768045013,\n",
       "   -3.0486395861578117,\n",
       "   -3.3797364817815865,\n",
       "   -4.237385977147727,\n",
       "   -4.02502009424252,\n",
       "   -3.458231857733014,\n",
       "   -3.202851793775551,\n",
       "   -3.690627858774402,\n",
       "   -3.385250890858253,\n",
       "   -3.0248347163519274,\n",
       "   -3.4325594437776745,\n",
       "   -3.368085808123004,\n",
       "   -3.3833909507265068,\n",
       "   -3.7333294047031993,\n",
       "   -3.6819320396002784,\n",
       "   -3.0438103966454824,\n",
       "   -3.191241593994235,\n",
       "   -4.596955371912025,\n",
       "   -3.0500577598952097,\n",
       "   -3.5679706145031043,\n",
       "   -3.2257091112425886,\n",
       "   -3.0029509934844336,\n",
       "   -2.230046971957591,\n",
       "   -2.536827304727445,\n",
       "   -2.1429282439878437,\n",
       "   -2.78707656451185,\n",
       "   -3.919287317478121,\n",
       "   -3.65251647623543,\n",
       "   -3.271305401578516,\n",
       "   -3.1349481266104506,\n",
       "   -2.922501028979268,\n",
       "   -3.3529706846877882,\n",
       "   -3.637351194363366,\n",
       "   -3.764878876543861,\n",
       "   -3.3857625197854464,\n",
       "   -3.3924738363802103,\n",
       "   -3.3854068930044967,\n",
       "   -3.4172940364294986,\n",
       "   -3.4927580145908648,\n",
       "   -3.3472813812343185,\n",
       "   -2.567937143152076,\n",
       "   -2.5711467664915606,\n",
       "   -2.682868428385749,\n",
       "   -2.456601983370145,\n",
       "   -2.7827059949084068,\n",
       "   -3.3980528929799014,\n",
       "   -4.195702975628064,\n",
       "   -3.715970348850167,\n",
       "   -4.556968284166671,\n",
       "   -3.7825080016485897,\n",
       "   -3.441208405453901,\n",
       "   -4.114192621517234,\n",
       "   -3.508537219021264,\n",
       "   -3.213129300176943,\n",
       "   -2.780490576036493,\n",
       "   -2.389114045076995,\n",
       "   -3.2925244292296636,\n",
       "   -3.8510074290996705,\n",
       "   -3.9555861802604078,\n",
       "   -3.143094621636868,\n",
       "   -3.8449527216631223,\n",
       "   -3.895113693460889,\n",
       "   -3.2108577693737494,\n",
       "   -3.7803298945544297,\n",
       "   -3.46539740583916,\n",
       "   -2.7120696628390952,\n",
       "   -2.760898645149892,\n",
       "   -3.8066254691058505,\n",
       "   -3.3031978156425366,\n",
       "   -2.992096960734842,\n",
       "   -3.691911274595653,\n",
       "   -4.601781299404139,\n",
       "   -4.753302532465479,\n",
       "   -4.223143134025031,\n",
       "   -4.563088750779922,\n",
       "   -3.4074240379805687,\n",
       "   -3.3182388945466306,\n",
       "   -3.5426367735750928,\n",
       "   -4.056318491769697,\n",
       "   -3.9391975754826154,\n",
       "   -3.737928568667508,\n",
       "   -3.786326721198682,\n",
       "   -3.884011892237478,\n",
       "   -4.027264125666883,\n",
       "   -4.900814011703014,\n",
       "   -4.213720793959103,\n",
       "   -4.150643124060444,\n",
       "   -4.835477360406551,\n",
       "   -4.221730113075517,\n",
       "   -3.9856087761252867,\n",
       "   -3.847180522611164,\n",
       "   -3.270335766092127,\n",
       "   -4.244416352231314,\n",
       "   -4.530270340822448,\n",
       "   -4.434944958142379,\n",
       "   -3.38419465811112,\n",
       "   -3.4858335565359813,\n",
       "   -4.051278286361344,\n",
       "   -5.139464843816258,\n",
       "   -4.803854500530196],\n",
       "  'ff_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -861.4692933550637,\n",
       "   -167.7753080064858,\n",
       "   -61.48099583586759,\n",
       "   -32.30141473189731,\n",
       "   -33.00877340972446,\n",
       "   -31.66434104984443,\n",
       "   -22.043863231402444,\n",
       "   -15.968784680177656,\n",
       "   -12.242530327168472,\n",
       "   -11.246713405359376,\n",
       "   -10.426335552285225,\n",
       "   -9.005078086292409,\n",
       "   -9.127342324913345,\n",
       "   -9.61520167601634,\n",
       "   -10.039063657482645,\n",
       "   -9.403891724205629,\n",
       "   -9.65343202405856,\n",
       "   -8.53546745853049,\n",
       "   -8.492127596172482,\n",
       "   -8.600270076526906,\n",
       "   -7.559894577299993,\n",
       "   -7.202681721580124,\n",
       "   -7.414009860664901,\n",
       "   -6.820308787340582,\n",
       "   -6.50872059427534,\n",
       "   -6.600683245332826,\n",
       "   -6.532313953086011,\n",
       "   -6.687032077519802,\n",
       "   -6.2585221920980585,\n",
       "   -6.100750401269322,\n",
       "   -6.444380588513313,\n",
       "   -6.662432233233857,\n",
       "   -6.588642656788453,\n",
       "   -6.6781043882086575,\n",
       "   -6.7792594186279365,\n",
       "   -6.231388057562877,\n",
       "   -6.125026012843545,\n",
       "   -6.55886987676065,\n",
       "   -6.597491897269911,\n",
       "   -6.5869914637947975,\n",
       "   -6.811861010407732,\n",
       "   -6.722788266033136,\n",
       "   -6.901301011356432,\n",
       "   -6.8050594158258875,\n",
       "   -6.741002805527581,\n",
       "   -6.751213499100698,\n",
       "   -6.303551364626891,\n",
       "   -5.819226071244149,\n",
       "   -6.004159946129594,\n",
       "   -6.260650391950093,\n",
       "   -6.392429249904658,\n",
       "   -6.177271355925102,\n",
       "   -6.14680241871043,\n",
       "   -6.5527561032465185,\n",
       "   -6.656950304630315,\n",
       "   -6.590517287041624,\n",
       "   -6.402449557200794,\n",
       "   -6.826927016396603,\n",
       "   -7.269444854939646,\n",
       "   -7.2826845309411485,\n",
       "   -6.510952265762353,\n",
       "   -5.84338208521388,\n",
       "   -5.76859448140103,\n",
       "   -5.574585546040109,\n",
       "   -5.570415554720342,\n",
       "   -5.303923939273404,\n",
       "   -5.310306147187469,\n",
       "   -5.40298419102883,\n",
       "   -5.441709472451079,\n",
       "   -5.302137872555023,\n",
       "   -4.958915572879192,\n",
       "   -5.0880446178826695,\n",
       "   -5.080763727633197,\n",
       "   -4.840598514982328,\n",
       "   -4.9953927791970765,\n",
       "   -4.874648192877399,\n",
       "   -5.174144886014108,\n",
       "   -5.414925118277218,\n",
       "   -5.539391666295495,\n",
       "   -6.068730441180045,\n",
       "   -5.848863391326885,\n",
       "   -5.413182389464675,\n",
       "   -5.3227650045938075,\n",
       "   -5.402985645683208,\n",
       "   -5.082820852712801,\n",
       "   -5.005376386835357,\n",
       "   -5.442285700867219,\n",
       "   -5.534697220646681,\n",
       "   -5.478372382516797,\n",
       "   -5.331945003941528,\n",
       "   -5.372679703588409,\n",
       "   -5.798338435092755,\n",
       "   -6.201019837457181,\n",
       "   -6.262327489656719,\n",
       "   -6.562490863945016,\n",
       "   -6.118622955189047,\n",
       "   -5.927268898839575,\n",
       "   -5.812712482864869,\n",
       "   -6.279978520619608,\n",
       "   -6.265417576882402,\n",
       "   -5.96969302653376,\n",
       "   -5.958105475655798,\n",
       "   -5.815444062705851,\n",
       "   -6.05829483563682,\n",
       "   -6.208440497702108,\n",
       "   -5.513676710500099,\n",
       "   -5.393368312682507,\n",
       "   -5.157568227004471,\n",
       "   -5.150797047871107,\n",
       "   -5.270387274760781,\n",
       "   -5.120118883097665,\n",
       "   -5.1885797334789014,\n",
       "   -5.192567252508082,\n",
       "   -5.274599038161033,\n",
       "   -5.417045505310159,\n",
       "   -5.233511607095192,\n",
       "   -5.11931593088552,\n",
       "   -5.170664498652543,\n",
       "   -5.517018606616943,\n",
       "   -5.594841320300268,\n",
       "   -5.60927981116316,\n",
       "   -5.615348476267896,\n",
       "   -5.730830045005532,\n",
       "   -5.943156096420666,\n",
       "   -5.962656318371814,\n",
       "   -6.070395009557297,\n",
       "   -6.041794072510003,\n",
       "   -6.353143482201548,\n",
       "   -6.305720336091703,\n",
       "   -6.398649512391438,\n",
       "   -6.93692757990356,\n",
       "   -6.646556034872911,\n",
       "   -7.266295686294864,\n",
       "   -6.5545410966787,\n",
       "   -6.401867337776621,\n",
       "   -6.506696170520506,\n",
       "   -6.326738791573735,\n",
       "   -6.44591873992402,\n",
       "   -6.500005992679926,\n",
       "   -6.749185329375571,\n",
       "   -7.041767170446544,\n",
       "   -6.746054528471757,\n",
       "   -6.520114605748079,\n",
       "   -6.6643475980594955,\n",
       "   -6.612238444828313,\n",
       "   -6.58599587299512,\n",
       "   -6.535082670102189,\n",
       "   -6.790638414381779,\n",
       "   -7.344660339255608,\n",
       "   -7.712741401103267,\n",
       "   -6.929435238422448,\n",
       "   -6.772129562700337,\n",
       "   -6.398102632048178,\n",
       "   -6.006750156348805,\n",
       "   -5.960992975088558,\n",
       "   -6.468303938379288,\n",
       "   -6.554974307986741,\n",
       "   -6.6532848548907255,\n",
       "   -6.346535306747364,\n",
       "   -6.179981534742131,\n",
       "   -6.311315447627798,\n",
       "   -5.636320079941125,\n",
       "   -5.698669170563709,\n",
       "   -5.811384152010201,\n",
       "   -5.836579299782804,\n",
       "   -5.673197472451128,\n",
       "   -5.887692225825695,\n",
       "   -5.985890923629653,\n",
       "   -5.976587234256375,\n",
       "   -5.564747933946581,\n",
       "   -5.791185075696447,\n",
       "   -5.420676938919488,\n",
       "   -5.490744270174673,\n",
       "   -5.177514795920916,\n",
       "   -5.4990223932203826,\n",
       "   -5.37474471007507,\n",
       "   -4.9881288606621474,\n",
       "   -4.921772507157464,\n",
       "   -5.4672634974911345,\n",
       "   -5.74925968602969,\n",
       "   -5.765015919970015,\n",
       "   -5.99707100285982,\n",
       "   -6.470018537177694,\n",
       "   -6.678691838106272,\n",
       "   -6.982115644527993,\n",
       "   -6.585850369245984,\n",
       "   -7.125870289627317,\n",
       "   -7.612033478956771,\n",
       "   -7.588791616904187,\n",
       "   -6.944973675243379,\n",
       "   -6.881052854610924,\n",
       "   -7.153081410260462,\n",
       "   -6.724252455523767,\n",
       "   -6.559364658085315,\n",
       "   -6.749108172382152,\n",
       "   -6.977936624292864,\n",
       "   -6.641472223201705,\n",
       "   -6.303692638299798,\n",
       "   -5.9778134747446225,\n",
       "   -5.552936682043288,\n",
       "   -5.237397828687491,\n",
       "   -5.5043934807084085,\n",
       "   -5.883968191028303,\n",
       "   -6.296679229959111,\n",
       "   -6.406968554345678,\n",
       "   -6.308541106062323,\n",
       "   -6.371659073909482,\n",
       "   -6.299247541532986,\n",
       "   -6.298478341582213,\n",
       "   -6.32137909048597,\n",
       "   -5.861317885192643,\n",
       "   -5.8105592500866985,\n",
       "   -5.785342605843156,\n",
       "   -6.001183784435979,\n",
       "   -6.027519705190517,\n",
       "   -5.643393125778257,\n",
       "   -5.264497750384499,\n",
       "   -5.747053013414633,\n",
       "   -5.914006421897903,\n",
       "   -6.204107493103684,\n",
       "   -6.388624612518651,\n",
       "   -6.498828246475138,\n",
       "   -6.578384005346558,\n",
       "   -7.178837032206342,\n",
       "   -7.997816459738084,\n",
       "   -8.45982080705536,\n",
       "   -8.137185551682927,\n",
       "   -8.077628967454691,\n",
       "   -7.6614503534119525,\n",
       "   -7.044087745950295,\n",
       "   -7.006136634214355,\n",
       "   -7.472529577573766,\n",
       "   -7.607062272489703,\n",
       "   -7.785522018952216,\n",
       "   -7.593590499222987,\n",
       "   -7.472507335981,\n",
       "   -7.788364249378365,\n",
       "   -7.4881172422359565,\n",
       "   -7.281873165184736,\n",
       "   -6.939798586843572,\n",
       "   -6.12101129346722,\n",
       "   -6.126058709228039,\n",
       "   -6.465257625557577,\n",
       "   -6.524812313887507,\n",
       "   -6.307077513238878,\n",
       "   -6.493269713768713,\n",
       "   -6.783943951713543,\n",
       "   -6.483011612301715,\n",
       "   -6.222173951682794,\n",
       "   -6.081025619673927,\n",
       "   -6.453236916807795,\n",
       "   -7.022173025558482,\n",
       "   -6.724741595190879,\n",
       "   -6.914754702938553,\n",
       "   -6.695624190756315,\n",
       "   -6.318374043201486,\n",
       "   -6.216567390614146,\n",
       "   -6.0098642898217856,\n",
       "   -5.947171050715136,\n",
       "   -6.295173662744519,\n",
       "   -6.219649171622941,\n",
       "   -6.705215764425135,\n",
       "   -6.5461711699124,\n",
       "   -6.688025370072597,\n",
       "   -6.650404009350988,\n",
       "   -6.421679946258617,\n",
       "   -6.444349224013034,\n",
       "   -7.283716665190658,\n",
       "   -6.9678387646517175,\n",
       "   -6.851752001457995,\n",
       "   -6.63613667522791,\n",
       "   -6.285915524278334,\n",
       "   -5.659930144598161,\n",
       "   -5.301629852378186,\n",
       "   -5.683492348654759,\n",
       "   -5.5334677592445605,\n",
       "   -5.827988556017563,\n",
       "   -5.243894704127597,\n",
       "   -4.8708525309267,\n",
       "   -4.793848782415746,\n",
       "   -4.862739731391613,\n",
       "   -5.037163732086067,\n",
       "   -5.3793392401144615,\n",
       "   -5.772642526318241,\n",
       "   -5.923681307832433,\n",
       "   -6.205021654613048,\n",
       "   -6.2073313169231294,\n",
       "   -5.97412039703781,\n",
       "   -5.851416248717257,\n",
       "   -5.370641192235679,\n",
       "   -5.40928009286581,\n",
       "   -5.819620639488087,\n",
       "   -5.63019193688285,\n",
       "   -5.703992500950826,\n",
       "   -6.106955905903508,\n",
       "   -6.428379127436396,\n",
       "   -6.496446698182913,\n",
       "   -6.515471128261728,\n",
       "   -6.159419438360473,\n",
       "   -6.324580290046698,\n",
       "   -6.163482655963207,\n",
       "   -5.300899257525387,\n",
       "   -5.630372311137082,\n",
       "   -5.283797935130142,\n",
       "   -5.296370386273369,\n",
       "   -4.957159368820946,\n",
       "   -5.199542474141672,\n",
       "   -5.276315964016399,\n",
       "   -5.22401901841804,\n",
       "   -5.272357016197297,\n",
       "   -5.412988638663082,\n",
       "   -5.470666170442605,\n",
       "   -5.3625946423674185,\n",
       "   -5.491460623903559,\n",
       "   -5.510701116727395,\n",
       "   -5.213652379028652,\n",
       "   -5.625932914966761,\n",
       "   -5.709823095267642,\n",
       "   -5.684924455919389,\n",
       "   -6.120162368729988,\n",
       "   -5.902708004289354,\n",
       "   -5.909655837584124,\n",
       "   -5.9118123612216325,\n",
       "   -6.601108372981863,\n",
       "   -6.053313229083078,\n",
       "   -6.393030359581756,\n",
       "   -6.5057021238651505,\n",
       "   -6.897241352910703,\n",
       "   -7.01774190332795,\n",
       "   -6.853172511866898,\n",
       "   -6.689168082221395,\n",
       "   -7.301687989611652,\n",
       "   -6.675496100566554,\n",
       "   -7.318256219881496,\n",
       "   -7.024962096048586,\n",
       "   -7.10547987083522,\n",
       "   -7.174304250915098,\n",
       "   -7.3304604779169065,\n",
       "   -7.168419870736901,\n",
       "   -6.622636641017069,\n",
       "   -6.643009739706287,\n",
       "   -7.0353451641813525,\n",
       "   -6.395598633286725,\n",
       "   -6.250120335486326,\n",
       "   -6.401111127819343,\n",
       "   -6.47709525448871,\n",
       "   -5.998532401589949,\n",
       "   -5.009842076114249,\n",
       "   -5.286798356836929,\n",
       "   -5.301698148245902,\n",
       "   -5.133585438153247,\n",
       "   -4.785254238525906,\n",
       "   -4.960866132711843,\n",
       "   -5.043019657973861,\n",
       "   -5.351203281393767,\n",
       "   -5.070436181495729,\n",
       "   -4.522060172871289,\n",
       "   -4.4544783689068765,\n",
       "   -4.298912564671982,\n",
       "   -4.532964858397286,\n",
       "   -4.8907213062335835,\n",
       "   -4.7906261218120685,\n",
       "   -4.745305359992259,\n",
       "   -4.901809275493308,\n",
       "   -5.051514871501287,\n",
       "   -5.699596734403181,\n",
       "   -5.541492127574108,\n",
       "   -5.568916429826887,\n",
       "   -5.315758030714823,\n",
       "   -5.12396414110847,\n",
       "   -5.113025940623353,\n",
       "   -5.251534846093383,\n",
       "   -4.86521867214341,\n",
       "   -4.83071563147024,\n",
       "   -4.844990881825126,\n",
       "   -4.869950496491356,\n",
       "   -4.990708432745622,\n",
       "   -5.263128690603488,\n",
       "   -5.058044338649432,\n",
       "   -5.089816490496409,\n",
       "   -5.269825799905375,\n",
       "   -5.083713339982359,\n",
       "   -5.365892085002148,\n",
       "   -5.150791132871073,\n",
       "   -5.326564054365307,\n",
       "   -5.473206534681698,\n",
       "   -5.110312862306165,\n",
       "   -5.447268391093972,\n",
       "   -5.569251787802918,\n",
       "   -5.094256208078869,\n",
       "   -5.121282987351131,\n",
       "   -5.3636708665191914,\n",
       "   -5.108128845532658,\n",
       "   -5.411616253848821,\n",
       "   -6.187358947227756,\n",
       "   -6.449331315403713,\n",
       "   -5.81464317537265,\n",
       "   -5.9620398940451045,\n",
       "   -6.232162953222657,\n",
       "   -6.716552206268038,\n",
       "   -6.4956370142867375,\n",
       "   -7.006010011023735,\n",
       "   -7.076679154910183,\n",
       "   -6.937176593070553,\n",
       "   -5.987023922530466,\n",
       "   -6.153531180182994,\n",
       "   -5.749484462501662,\n",
       "   -5.143920062625492,\n",
       "   -4.857252222099786,\n",
       "   -4.744789316975364,\n",
       "   -4.8401433465300725,\n",
       "   -4.686766904534732,\n",
       "   -4.752988649951479,\n",
       "   -4.772367411987611,\n",
       "   -4.71630365615452,\n",
       "   -5.406290430507195,\n",
       "   -4.996088955954667,\n",
       "   -4.902673322809342,\n",
       "   -4.63950407223761,\n",
       "   -4.8680054690703685,\n",
       "   -5.063625528873784,\n",
       "   -4.967412298281154,\n",
       "   -5.205114482939893,\n",
       "   -5.495055777547721,\n",
       "   -5.409156696591626,\n",
       "   -5.162971867566773,\n",
       "   -4.701744899230479,\n",
       "   -4.726640945572523,\n",
       "   -4.894680592386268,\n",
       "   -5.188920688209322,\n",
       "   -4.8575393162735265,\n",
       "   -4.800678728328968,\n",
       "   -5.175661807492552,\n",
       "   -4.900548190184024,\n",
       "   -4.917631053362328,\n",
       "   -5.102983605078002,\n",
       "   -4.814670154853821,\n",
       "   -4.895618277089208,\n",
       "   -4.610019441686342,\n",
       "   -4.653561532053077,\n",
       "   -4.905778897871343,\n",
       "   -4.779730259741184,\n",
       "   -4.746867060457341,\n",
       "   -4.666357064909768,\n",
       "   -4.669082120176743,\n",
       "   -4.968605083754557,\n",
       "   -5.085526368234109,\n",
       "   -5.56239261755118,\n",
       "   -5.640106553482563,\n",
       "   -5.327697674730495,\n",
       "   -5.503069369583524,\n",
       "   -5.830846745192515,\n",
       "   -5.995229738692193,\n",
       "   -6.090470258353639,\n",
       "   -5.77330492370718,\n",
       "   -6.290656420598502,\n",
       "   -6.169735993883348,\n",
       "   -5.944662229912663,\n",
       "   -5.479598486367422,\n",
       "   -5.530378492051184,\n",
       "   -5.633667099359954,\n",
       "   -6.050817331717327,\n",
       "   -5.987085297446598,\n",
       "   -5.96451420178733,\n",
       "   -6.008598069793209,\n",
       "   -6.0381635225917,\n",
       "   -6.399341768958716,\n",
       "   -5.469953895055276,\n",
       "   -5.2553791795284726,\n",
       "   -5.3168574702652585,\n",
       "   -5.223185636797161,\n",
       "   -5.05895676516263,\n",
       "   -4.776613410815419,\n",
       "   -4.455893721236069,\n",
       "   -4.687222026164436,\n",
       "   -4.887855266788231,\n",
       "   -5.116296493030092,\n",
       "   -4.611254833762346,\n",
       "   -4.726312178706532,\n",
       "   -4.732541766168206,\n",
       "   -4.652848998209933,\n",
       "   -4.597080970207555,\n",
       "   -4.693334845403873,\n",
       "   -4.640996287042603,\n",
       "   -4.7059126925645325,\n",
       "   -5.264535908487774,\n",
       "   -5.441531315508004,\n",
       "   -5.413159098335064,\n",
       "   -5.571182112119006,\n",
       "   -5.7343497410808375,\n",
       "   -5.370185558843515,\n",
       "   -5.141605495250285,\n",
       "   -5.1352402693109385,\n",
       "   -5.129101188399864],\n",
       "  'test_r2s': [-215.32721785817955,\n",
       "   -188.29545342866857,\n",
       "   -161.1354011691041,\n",
       "   -138.24534823177493,\n",
       "   -122.40477053237697,\n",
       "   -108.08524904588452,\n",
       "   -959.3319884820447,\n",
       "   -258.3095188956404,\n",
       "   -145.26676072925136,\n",
       "   -108.82782834060777,\n",
       "   -5299306.882040747,\n",
       "   -1353.3236234635738,\n",
       "   -280.8236776330729,\n",
       "   -146.27787201541057,\n",
       "   -129.8670353857249,\n",
       "   -91.42869139623787,\n",
       "   -79.77094539118053,\n",
       "   -75.39397919124539,\n",
       "   -63.96652803379513,\n",
       "   -59.62852084408908,\n",
       "   -49.80345300440746,\n",
       "   -46.54163919813744,\n",
       "   -47.126766433178545,\n",
       "   -46.2396821219744,\n",
       "   -41.3188665695775,\n",
       "   -40.31845389371065,\n",
       "   -37.5420974291264,\n",
       "   -33.76809749359285,\n",
       "   -33.17424000846991,\n",
       "   -31.163308450623802,\n",
       "   -29.96941528681493,\n",
       "   -29.433643408693385,\n",
       "   -28.584331503600232,\n",
       "   -30.050202020354757,\n",
       "   -28.759227177421828,\n",
       "   -27.383115239584434,\n",
       "   -27.25567173982695,\n",
       "   -27.08268219837239,\n",
       "   -27.622430298969633,\n",
       "   -27.4483881362965,\n",
       "   -27.367286391028696,\n",
       "   -26.379214315371545,\n",
       "   -24.990307818876474,\n",
       "   -25.813565535259045,\n",
       "   -25.83130716385019,\n",
       "   -25.299850248959842,\n",
       "   -27.527622932452992,\n",
       "   -28.057418361137273,\n",
       "   -27.87462547749864,\n",
       "   -27.807216580413915,\n",
       "   -27.13353405608745,\n",
       "   -25.96689494704818,\n",
       "   -24.749742594616304,\n",
       "   -25.563157459193004,\n",
       "   -25.753855863047196,\n",
       "   -25.641427302299583,\n",
       "   -24.047308705159157,\n",
       "   -23.85112987986018,\n",
       "   -24.265569609322597,\n",
       "   -25.723187325259467,\n",
       "   -25.21294261679116,\n",
       "   -24.27849616469773,\n",
       "   -24.44283009569386,\n",
       "   -24.307295260828404,\n",
       "   -25.316071506630433,\n",
       "   -24.774412957763623,\n",
       "   -23.798320141655672,\n",
       "   -23.587663341286394,\n",
       "   -24.793868329148644,\n",
       "   -24.822675227900202,\n",
       "   -24.533908259794188,\n",
       "   -24.64441274006484,\n",
       "   -25.27576955831572,\n",
       "   -24.077869930883764,\n",
       "   -25.187956088471516,\n",
       "   -24.969363467607174,\n",
       "   -25.018076823033972,\n",
       "   -24.50455182483168,\n",
       "   -25.065403857267643,\n",
       "   -23.961250212941263,\n",
       "   -23.90785517926244,\n",
       "   -23.615530650416297,\n",
       "   -24.47444765390689,\n",
       "   -24.288381833342786,\n",
       "   -24.595928257959102,\n",
       "   -23.9859963002646,\n",
       "   -24.311818196588696,\n",
       "   -23.791556294899145,\n",
       "   -23.90625589642352,\n",
       "   -23.938441201649052,\n",
       "   -23.579019732921303,\n",
       "   -23.771430589779143,\n",
       "   -24.155228138280417,\n",
       "   -24.674499985877432,\n",
       "   -24.827154843747195,\n",
       "   -24.81363833801614,\n",
       "   -24.75795309589722,\n",
       "   -25.697320667776662,\n",
       "   -24.44917621362884,\n",
       "   -24.337030810303183,\n",
       "   -25.186084227882294,\n",
       "   -24.867230240330173,\n",
       "   -25.146716580942222,\n",
       "   -25.10292918881413,\n",
       "   -26.494506913039263,\n",
       "   -26.01579071151744,\n",
       "   -25.400278176915542,\n",
       "   -25.01886073854959,\n",
       "   -23.985624730472512,\n",
       "   -24.204443694411104,\n",
       "   -26.093515421393406,\n",
       "   -24.82940137200454,\n",
       "   -24.713256837662183,\n",
       "   -24.4440731209239,\n",
       "   -24.171450772816904,\n",
       "   -23.81551453230992,\n",
       "   -22.31970694291094,\n",
       "   -22.12232730521557,\n",
       "   -22.896223145966943,\n",
       "   -23.766461536851942,\n",
       "   -23.99363331938331,\n",
       "   -23.803454665292634,\n",
       "   -24.277199247440063,\n",
       "   -24.51476089527315,\n",
       "   -24.865239637701897,\n",
       "   -25.380835516239358,\n",
       "   -25.126400281314265,\n",
       "   -25.287620061140125,\n",
       "   -24.53637699455665,\n",
       "   -25.06248493204411,\n",
       "   -24.523761757826577,\n",
       "   -25.000122911779876,\n",
       "   -23.69041319236075,\n",
       "   -23.13726976046144,\n",
       "   -23.403529545764577,\n",
       "   -23.28283081046989,\n",
       "   -24.327784443741727,\n",
       "   -25.586145667079098,\n",
       "   -26.180779473655186,\n",
       "   -26.130853437346236,\n",
       "   -25.031114743501437,\n",
       "   -25.074832213461026,\n",
       "   -25.310275633815333,\n",
       "   -25.839138875167592,\n",
       "   -25.978511157471203,\n",
       "   -26.177786419259913,\n",
       "   -26.383989092614737,\n",
       "   -26.12451190556825,\n",
       "   -26.67187571325416,\n",
       "   -27.058235136230934,\n",
       "   -27.654581143973232,\n",
       "   -28.044277871186317,\n",
       "   -26.59945407542883,\n",
       "   -25.89005774521795,\n",
       "   -26.121742686644744,\n",
       "   -27.190483482516612,\n",
       "   -25.082462605549793,\n",
       "   -24.91301494713884,\n",
       "   -25.288613461515965,\n",
       "   -24.625187967654846,\n",
       "   -24.665442455950505,\n",
       "   -25.15819900477281,\n",
       "   -25.52232029185762,\n",
       "   -24.873026311514106,\n",
       "   -25.093079214729595,\n",
       "   -24.29147186087264,\n",
       "   -24.748000831513004,\n",
       "   -24.482665127959823,\n",
       "   -24.48771123041593,\n",
       "   -24.546444024499177,\n",
       "   -24.742207399053104,\n",
       "   -24.50536382484004,\n",
       "   -25.469036904469352,\n",
       "   -26.25393160030174,\n",
       "   -24.79299270195669,\n",
       "   -24.886359687470144,\n",
       "   -26.159093814766837,\n",
       "   -23.307261869613843,\n",
       "   -23.53247175258346,\n",
       "   -23.280899843000608,\n",
       "   -25.858381712233058,\n",
       "   -26.456972670159995,\n",
       "   -25.901405956173296,\n",
       "   -25.16853684879867,\n",
       "   -26.323762540695682,\n",
       "   -26.566458651264846,\n",
       "   -25.993448588706606,\n",
       "   -25.147629414584177,\n",
       "   -25.89253688407889,\n",
       "   -26.231406382802525,\n",
       "   -26.436140974185385,\n",
       "   -25.337550913145947,\n",
       "   -25.474292118594782,\n",
       "   -25.472166959706055,\n",
       "   -25.409742129684176,\n",
       "   -25.235373694130843,\n",
       "   -25.060812121518452,\n",
       "   -26.116747601268973,\n",
       "   -24.943745135027065,\n",
       "   -24.076652147077425,\n",
       "   -24.22214440510898,\n",
       "   -25.31042785721069,\n",
       "   -26.071020925583166,\n",
       "   -26.13810268422251,\n",
       "   -28.26670332701515,\n",
       "   -25.81283546567879,\n",
       "   -25.693883348494587,\n",
       "   -25.893588243915524,\n",
       "   -26.692700555392012,\n",
       "   -27.375006069637973,\n",
       "   -26.919807766289143,\n",
       "   -25.527983090087666,\n",
       "   -25.97036502040318,\n",
       "   -26.20768850293253,\n",
       "   -25.736176991421857,\n",
       "   -26.007715759689276,\n",
       "   -27.544372009698094,\n",
       "   -26.688179225709085,\n",
       "   -26.94077544266596,\n",
       "   -28.015888928966035,\n",
       "   -28.296556845621417,\n",
       "   -26.84576517070836,\n",
       "   -26.499777540857718,\n",
       "   -28.8055445440932,\n",
       "   -26.20854079235086,\n",
       "   -26.426325474570874,\n",
       "   -26.416179037369805,\n",
       "   -24.83205297043274,\n",
       "   -24.67904577980223,\n",
       "   -24.61547449376683,\n",
       "   -24.7034334608368,\n",
       "   -24.932202753144225,\n",
       "   -24.872675581928483,\n",
       "   -24.751799088781112,\n",
       "   -25.752266462283956,\n",
       "   -24.203030117397113,\n",
       "   -22.370611294473363,\n",
       "   -21.76323377196662,\n",
       "   -21.988232940811386,\n",
       "   -21.970601891419378,\n",
       "   -22.568085643615458,\n",
       "   -22.684455165314567,\n",
       "   -22.84680017248551,\n",
       "   -22.63174411011171,\n",
       "   -23.15532342854177,\n",
       "   -24.116060073541455,\n",
       "   -23.939222852783622,\n",
       "   -23.754668003553448,\n",
       "   -23.797199761580977,\n",
       "   -24.00092133726504,\n",
       "   -23.651359395800043,\n",
       "   -23.685191984580406,\n",
       "   -24.15472384464736,\n",
       "   -23.661584844358742,\n",
       "   -22.818980347823576,\n",
       "   -22.22328944615028,\n",
       "   -23.67723344831895,\n",
       "   -23.77572789405957,\n",
       "   -24.259198795270745,\n",
       "   -24.973327197818815,\n",
       "   -24.92015416312681,\n",
       "   -25.08995939573982,\n",
       "   -23.854817616587184,\n",
       "   -22.90936099228557,\n",
       "   -24.15190382572159,\n",
       "   -24.37598307227487,\n",
       "   -24.548948013889806,\n",
       "   -26.12196015350709,\n",
       "   -25.966219970091238,\n",
       "   -26.151834535540054,\n",
       "   -25.121252856479682,\n",
       "   -24.37067224427152,\n",
       "   -24.990842299178393,\n",
       "   -27.46030383762951,\n",
       "   -27.02731626346746,\n",
       "   -26.433004859940006,\n",
       "   -25.93059298921755,\n",
       "   -26.170801907767817,\n",
       "   -24.75681910080031,\n",
       "   -23.7031285785516,\n",
       "   -23.529802706437476,\n",
       "   -23.59084565524538,\n",
       "   -24.518225983536627,\n",
       "   -23.65193297751332,\n",
       "   -24.228910127844877,\n",
       "   -23.669232819747158,\n",
       "   -24.769430118826687,\n",
       "   -24.361625688588802,\n",
       "   -24.765824427439817,\n",
       "   -25.162203411333707,\n",
       "   -27.166536961337812,\n",
       "   -29.42636374938045,\n",
       "   -30.231845260142663,\n",
       "   -28.429612001567506,\n",
       "   -28.89182485658585,\n",
       "   -29.266871767799536,\n",
       "   -27.977052253680363,\n",
       "   -28.24810758791287,\n",
       "   -29.952864350321896,\n",
       "   -30.74717475501254,\n",
       "   -30.709140258367306,\n",
       "   -29.02160640652733,\n",
       "   -28.576272404978297,\n",
       "   -27.93592875934067,\n",
       "   -28.06702629834141,\n",
       "   -29.571770901375277,\n",
       "   -29.92294827718866,\n",
       "   -26.24815214866223,\n",
       "   -28.582766930417293,\n",
       "   -28.403127909181794,\n",
       "   -28.757343341000897,\n",
       "   -31.34076066718888,\n",
       "   -30.345973121555215,\n",
       "   -34.04379388114171,\n",
       "   -30.781201474349448,\n",
       "   -32.01801761291452,\n",
       "   -32.964512594653876,\n",
       "   -38.53118750252591,\n",
       "   -36.75332866352609,\n",
       "   -34.996686606132975,\n",
       "   -34.67236052504804,\n",
       "   -30.726962711446134,\n",
       "   -29.33284332044816,\n",
       "   -28.57907920390682,\n",
       "   -27.954898978965424,\n",
       "   -28.02700368644048,\n",
       "   -28.441301468766905,\n",
       "   -28.60645399534694,\n",
       "   -29.179337819877354,\n",
       "   -30.120127499437448,\n",
       "   -29.76288311876441,\n",
       "   -31.021777732792756,\n",
       "   -30.69075674058424,\n",
       "   -31.44855772811357,\n",
       "   -31.235793100302857,\n",
       "   -31.69317542468337,\n",
       "   -30.952360338309475,\n",
       "   -31.70614309713016,\n",
       "   -30.35268592486838,\n",
       "   -29.214326383993452,\n",
       "   -27.99153858900954,\n",
       "   -28.60185290460999,\n",
       "   -27.159520561079752,\n",
       "   -26.852848855529253,\n",
       "   -27.272803988040113,\n",
       "   -25.877104885778827,\n",
       "   -26.889114703659285,\n",
       "   -28.08133852659678,\n",
       "   -27.990408614281257,\n",
       "   -26.48852527669264,\n",
       "   -26.014114575641543,\n",
       "   -25.970820186966456,\n",
       "   -26.318905069149004,\n",
       "   -25.085900110259995,\n",
       "   -26.242186997593517,\n",
       "   -25.868445291216222,\n",
       "   -28.41013373442209,\n",
       "   -27.311377329277274,\n",
       "   -27.431795602617157,\n",
       "   -26.663171502524968,\n",
       "   -28.35944978148148,\n",
       "   -28.582730966860588,\n",
       "   -28.118261004312167,\n",
       "   -27.493989404232465,\n",
       "   -26.119603785900424,\n",
       "   -26.394847798081017,\n",
       "   -27.809229642023453,\n",
       "   -28.276285008021667,\n",
       "   -29.372740943869648,\n",
       "   -29.241951687634362,\n",
       "   -27.996089110255756,\n",
       "   -29.129663978639016,\n",
       "   -27.153598234140304,\n",
       "   -26.73248436288909,\n",
       "   -26.336560686651502,\n",
       "   -26.457091260068527,\n",
       "   -25.23947760662958,\n",
       "   -26.48377858846711,\n",
       "   -26.26230877105627,\n",
       "   -26.61214399114539,\n",
       "   -26.443272320485217,\n",
       "   -26.77694129394103,\n",
       "   -27.40262924494683,\n",
       "   -27.067018264680662,\n",
       "   -26.53657426560998,\n",
       "   -25.909836084330436,\n",
       "   -26.45948208117951,\n",
       "   -26.427941650843472,\n",
       "   -27.5859951231847,\n",
       "   -26.57473311952901,\n",
       "   -27.47570928118445,\n",
       "   -26.447240606087302,\n",
       "   -25.531447969791728,\n",
       "   -27.35456320843624,\n",
       "   -26.92708228280711,\n",
       "   -25.674313433276225,\n",
       "   -25.908431046428973,\n",
       "   -26.928800590161295,\n",
       "   -25.555881572471254,\n",
       "   -26.704046193873605,\n",
       "   -25.745048635364398,\n",
       "   -25.891195584062125,\n",
       "   -25.161798461440924,\n",
       "   -26.25381485750286,\n",
       "   -26.781202240665948,\n",
       "   -26.781094347781657,\n",
       "   -26.453058267898435,\n",
       "   -26.917565448178635,\n",
       "   -27.185770505619857,\n",
       "   -25.87477087807565,\n",
       "   -24.85563564298644,\n",
       "   -24.765706406844327,\n",
       "   -24.86571161993899,\n",
       "   -24.432396680668568,\n",
       "   -24.913031135790543,\n",
       "   -23.901919088655973,\n",
       "   -24.866223379972716,\n",
       "   -25.57662046881852,\n",
       "   -23.256737250459754,\n",
       "   -23.573298858567803,\n",
       "   -22.115790959319238,\n",
       "   -22.98854658693047,\n",
       "   -21.57634502361946,\n",
       "   -21.905575599391113,\n",
       "   -21.11586001063202,\n",
       "   -22.017866738257695,\n",
       "   -23.62874606638229,\n",
       "   -23.52549080056724,\n",
       "   -23.201340578299284,\n",
       "   -24.554110562331964,\n",
       "   -24.53093159537427,\n",
       "   -24.054589310143292,\n",
       "   -23.60832653520427,\n",
       "   -24.54561946435208,\n",
       "   -24.8964176265865,\n",
       "   -26.37681060280103,\n",
       "   -27.001782901741137,\n",
       "   -26.72649863917673,\n",
       "   -25.918216597171256,\n",
       "   -25.52028533297694,\n",
       "   -24.551713961683856,\n",
       "   -24.81609451299964,\n",
       "   -23.790731609178728,\n",
       "   -23.733653849205222,\n",
       "   -23.673848878086254,\n",
       "   -24.75239464389187,\n",
       "   -25.820932867596902,\n",
       "   -25.746195676227522,\n",
       "   -28.09448658073254,\n",
       "   -26.626943188243743,\n",
       "   -27.34531170602754,\n",
       "   -28.57867908600418,\n",
       "   -27.65002527094782,\n",
       "   -28.98337661182823,\n",
       "   -29.95793892430926,\n",
       "   -28.204391494717857,\n",
       "   -28.93293041807154,\n",
       "   -29.47607623307394,\n",
       "   -29.56038715381771,\n",
       "   -28.384087807583818,\n",
       "   -27.723700315609385,\n",
       "   -28.837931310100835,\n",
       "   -27.371043085617416,\n",
       "   -26.64767308574967,\n",
       "   -26.0732230365691,\n",
       "   -25.467796180794778,\n",
       "   -24.426422824344165,\n",
       "   -26.116735135046454,\n",
       "   -25.722986358813085,\n",
       "   -26.130011504884013,\n",
       "   -27.168603883057237,\n",
       "   -28.553685789970544,\n",
       "   -28.596945620799147,\n",
       "   -28.140775339943588,\n",
       "   -27.795861226990347,\n",
       "   -27.402659417365747,\n",
       "   -27.485160951543012,\n",
       "   -27.11092213878376,\n",
       "   -28.658327827121358,\n",
       "   -29.175516346557732,\n",
       "   -28.288642879170403,\n",
       "   -27.801510695400566,\n",
       "   -27.640784652562242,\n",
       "   -26.982877608617788,\n",
       "   -28.738465504721876,\n",
       "   -28.05440343798127,\n",
       "   -27.775088030966906,\n",
       "   -28.522552313961747,\n",
       "   -27.94323255735634,\n",
       "   -28.584989219778063,\n",
       "   -28.251264567254886,\n",
       "   -28.29417429559846,\n",
       "   -28.97178381486623,\n",
       "   -28.270551386390334,\n",
       "   -28.16357750979696,\n",
       "   -26.540242970824256,\n",
       "   -26.20960967943993,\n",
       "   -26.57557392311711,\n",
       "   -27.460425516584994,\n",
       "   -27.696063219819816],\n",
       "  'train_pce_loss': [0.8211109638214111,\n",
       "   0.9855216145515442,\n",
       "   0.8078866600990295,\n",
       "   0.8844578862190247,\n",
       "   0.7893542051315308,\n",
       "   0.8868177533149719,\n",
       "   0.8553139567375183,\n",
       "   1.054389476776123,\n",
       "   1.01982843875885,\n",
       "   0.909996747970581,\n",
       "   0.6202008724212646,\n",
       "   0.9277610778808594,\n",
       "   0.762861430644989,\n",
       "   0.9702140688896179,\n",
       "   0.9549850821495056,\n",
       "   0.9222673177719116,\n",
       "   0.8748716711997986,\n",
       "   1.3311809301376343,\n",
       "   0.7231097221374512,\n",
       "   1.014474868774414,\n",
       "   0.7608639001846313,\n",
       "   0.7680947780609131,\n",
       "   1.0768073797225952,\n",
       "   0.7437095046043396,\n",
       "   0.8512970209121704,\n",
       "   0.8200691342353821,\n",
       "   0.848661482334137,\n",
       "   0.7356378436088562,\n",
       "   1.2673598527908325,\n",
       "   1.0748077630996704,\n",
       "   1.0188860893249512,\n",
       "   1.0742876529693604,\n",
       "   0.7670956254005432,\n",
       "   1.2225526571273804,\n",
       "   0.7441198229789734,\n",
       "   1.0040311813354492,\n",
       "   0.8172707557678223,\n",
       "   0.640553891658783,\n",
       "   0.6921743154525757,\n",
       "   0.8527721166610718,\n",
       "   1.0353320837020874,\n",
       "   0.9990123510360718,\n",
       "   0.892146110534668,\n",
       "   0.7188307642936707,\n",
       "   0.8436296582221985,\n",
       "   0.882583737373352,\n",
       "   0.786263644695282,\n",
       "   0.6664674282073975,\n",
       "   0.8816171884536743,\n",
       "   0.8872987031936646,\n",
       "   0.9508604407310486,\n",
       "   1.1561954021453857,\n",
       "   0.9622821807861328,\n",
       "   0.8873334527015686,\n",
       "   1.0200144052505493,\n",
       "   0.7709403038024902,\n",
       "   1.0772509574890137,\n",
       "   0.9398984313011169,\n",
       "   0.8832525014877319,\n",
       "   0.6640151143074036,\n",
       "   0.9482652544975281,\n",
       "   0.6094170212745667,\n",
       "   0.6493127942085266,\n",
       "   0.7580751180648804,\n",
       "   0.9026239514350891,\n",
       "   0.9424096941947937,\n",
       "   0.5548830628395081,\n",
       "   0.7343495488166809,\n",
       "   0.9199711680412292,\n",
       "   0.7818160653114319,\n",
       "   0.9066823124885559,\n",
       "   0.692164957523346,\n",
       "   0.90565425157547,\n",
       "   0.7469199299812317,\n",
       "   0.7894790768623352,\n",
       "   0.9228119850158691,\n",
       "   0.6848858594894409,\n",
       "   1.0026203393936157,\n",
       "   0.807011604309082,\n",
       "   0.8323351144790649,\n",
       "   0.7596012353897095,\n",
       "   0.7690193057060242,\n",
       "   0.5618348121643066,\n",
       "   0.938176691532135,\n",
       "   0.6187113523483276,\n",
       "   0.6683142185211182,\n",
       "   0.7090261578559875,\n",
       "   0.963485836982727,\n",
       "   0.7652632594108582,\n",
       "   0.7310446500778198,\n",
       "   0.8221941590309143,\n",
       "   0.7167356014251709,\n",
       "   0.9805751442909241,\n",
       "   0.8966832756996155,\n",
       "   0.9301543235778809,\n",
       "   0.8286203742027283,\n",
       "   0.7070352435112,\n",
       "   0.7551811933517456,\n",
       "   0.9174026250839233,\n",
       "   0.8414252400398254,\n",
       "   0.8277762532234192,\n",
       "   0.7880940437316895,\n",
       "   1.1069632768630981,\n",
       "   0.7389602661132812,\n",
       "   0.9602347016334534,\n",
       "   0.8310849070549011,\n",
       "   0.8329594135284424,\n",
       "   0.7171794772148132,\n",
       "   1.0623834133148193,\n",
       "   0.9269472360610962,\n",
       "   0.7675147652626038,\n",
       "   0.7057081460952759,\n",
       "   0.765692412853241,\n",
       "   0.6328562498092651,\n",
       "   0.715056300163269,\n",
       "   0.810131311416626,\n",
       "   0.7801787853240967,\n",
       "   0.5815308690071106,\n",
       "   0.9151756763458252,\n",
       "   0.7344150543212891,\n",
       "   1.0271357297897339,\n",
       "   0.8921401500701904,\n",
       "   0.6252105832099915,\n",
       "   0.7090546488761902,\n",
       "   1.0238889455795288,\n",
       "   0.620880663394928,\n",
       "   0.5834132432937622,\n",
       "   0.7965101599693298,\n",
       "   0.9441014528274536,\n",
       "   0.7930294871330261,\n",
       "   0.677304744720459,\n",
       "   0.906885027885437,\n",
       "   0.6312002539634705,\n",
       "   0.7015141248703003,\n",
       "   0.6289944052696228,\n",
       "   0.7813858389854431,\n",
       "   0.7415841817855835,\n",
       "   0.9935417175292969,\n",
       "   0.6986008286476135,\n",
       "   0.8446915745735168,\n",
       "   0.5812479257583618,\n",
       "   0.8521387577056885,\n",
       "   0.4912831485271454,\n",
       "   0.5486322641372681,\n",
       "   0.7611261606216431,\n",
       "   1.0472731590270996,\n",
       "   0.6211621761322021,\n",
       "   0.908414900302887,\n",
       "   0.7572925686836243,\n",
       "   0.8322820067405701,\n",
       "   0.7508689165115356,\n",
       "   1.029250979423523,\n",
       "   0.874801516532898,\n",
       "   1.000360369682312,\n",
       "   0.7819611430168152,\n",
       "   0.7804317474365234,\n",
       "   0.8288324475288391,\n",
       "   0.6476896405220032,\n",
       "   0.5986496210098267,\n",
       "   0.5718911290168762,\n",
       "   0.7167051434516907,\n",
       "   0.7078801393508911,\n",
       "   1.0229647159576416,\n",
       "   0.6239928007125854,\n",
       "   0.8503942489624023,\n",
       "   0.7127186059951782,\n",
       "   0.735436737537384,\n",
       "   0.7980330586433411,\n",
       "   0.8555461168289185,\n",
       "   1.0920132398605347,\n",
       "   0.7947307229042053,\n",
       "   0.7950990200042725,\n",
       "   0.8497688174247742,\n",
       "   0.5688251256942749,\n",
       "   0.8761487007141113,\n",
       "   0.712044358253479,\n",
       "   0.6811383962631226,\n",
       "   0.5474236607551575,\n",
       "   0.7899226546287537,\n",
       "   0.7491550445556641,\n",
       "   0.9220371842384338,\n",
       "   0.6278539299964905,\n",
       "   1.0859055519104004,\n",
       "   0.8323528170585632,\n",
       "   1.0193825960159302,\n",
       "   0.6385759115219116,\n",
       "   0.659845769405365,\n",
       "   0.5611099004745483,\n",
       "   0.6689649224281311,\n",
       "   0.46961066126823425,\n",
       "   1.0039317607879639,\n",
       "   0.9061139225959778,\n",
       "   0.6659346222877502,\n",
       "   0.8168472051620483,\n",
       "   0.581864058971405,\n",
       "   0.7890869379043579,\n",
       "   0.5248929262161255,\n",
       "   0.6472399234771729,\n",
       "   0.8666352033615112,\n",
       "   0.7479704022407532,\n",
       "   0.7612388730049133,\n",
       "   0.6830018162727356,\n",
       "   0.7483044862747192,\n",
       "   0.7569205164909363,\n",
       "   0.972937822341919,\n",
       "   0.7001053690910339,\n",
       "   0.7140386700630188,\n",
       "   0.8570767045021057,\n",
       "   0.7536851167678833,\n",
       "   0.6396095156669617,\n",
       "   0.5385895371437073,\n",
       "   0.6396377682685852,\n",
       "   0.803107500076294,\n",
       "   1.0879210233688354,\n",
       "   0.7618587613105774,\n",
       "   0.7139949798583984,\n",
       "   0.7059373259544373,\n",
       "   0.6035405993461609,\n",
       "   0.7971066832542419,\n",
       "   0.6769962906837463,\n",
       "   0.9199479818344116,\n",
       "   0.622838020324707,\n",
       "   0.6736565828323364,\n",
       "   0.796789824962616,\n",
       "   0.8188072443008423,\n",
       "   0.6971262693405151,\n",
       "   0.7549075484275818,\n",
       "   0.6918638348579407,\n",
       "   0.6121099591255188,\n",
       "   0.6961066126823425,\n",
       "   0.7244406938552856,\n",
       "   0.7157058119773865,\n",
       "   0.6095128655433655,\n",
       "   0.5568834543228149,\n",
       "   0.8733539581298828,\n",
       "   0.835272490978241,\n",
       "   0.7305802702903748,\n",
       "   0.833314061164856,\n",
       "   0.8320220112800598,\n",
       "   1.5107669830322266,\n",
       "   0.8172648549079895,\n",
       "   0.72247314453125,\n",
       "   0.8711592555046082,\n",
       "   0.7390550374984741,\n",
       "   0.5504658222198486,\n",
       "   0.9254661798477173,\n",
       "   0.6677643656730652,\n",
       "   1.0283552408218384,\n",
       "   0.798984169960022,\n",
       "   0.5381742715835571,\n",
       "   0.6935535669326782,\n",
       "   0.6969237327575684,\n",
       "   0.6346511840820312,\n",
       "   0.8069925308227539,\n",
       "   1.044523000717163,\n",
       "   0.9484332203865051,\n",
       "   0.8561333417892456,\n",
       "   1.0230460166931152,\n",
       "   0.9786730408668518,\n",
       "   0.759232759475708,\n",
       "   0.5739331841468811,\n",
       "   0.8361039161682129,\n",
       "   0.5811413526535034,\n",
       "   0.7099213004112244,\n",
       "   1.0399768352508545,\n",
       "   0.8723767995834351,\n",
       "   0.7359098792076111,\n",
       "   0.9866287112236023,\n",
       "   0.9356406331062317,\n",
       "   0.8203611969947815,\n",
       "   0.9190757870674133,\n",
       "   0.8419016599655151,\n",
       "   0.565851628780365,\n",
       "   0.7025455832481384,\n",
       "   0.8294867873191833,\n",
       "   0.8492205142974854,\n",
       "   0.7063674330711365,\n",
       "   0.9252499938011169,\n",
       "   0.686342716217041,\n",
       "   0.8106282949447632,\n",
       "   0.8408737182617188,\n",
       "   0.5167100429534912,\n",
       "   0.9173420667648315,\n",
       "   0.949181079864502,\n",
       "   0.6797264218330383,\n",
       "   0.6707370281219482,\n",
       "   0.7857149243354797,\n",
       "   0.7126541137695312,\n",
       "   0.6334971189498901,\n",
       "   0.6920011639595032,\n",
       "   0.6703248023986816,\n",
       "   0.9492748379707336,\n",
       "   0.4949978291988373,\n",
       "   0.8152312636375427,\n",
       "   0.6655089259147644,\n",
       "   0.6460145711898804,\n",
       "   0.9446708559989929,\n",
       "   0.8303419947624207,\n",
       "   0.49956774711608887,\n",
       "   0.9571080803871155,\n",
       "   1.054307460784912,\n",
       "   0.7287801504135132,\n",
       "   0.5913611054420471,\n",
       "   0.6264275312423706,\n",
       "   0.7542378306388855,\n",
       "   0.7718287706375122,\n",
       "   0.7187595367431641,\n",
       "   0.5687562227249146,\n",
       "   0.5793238282203674,\n",
       "   0.575959324836731,\n",
       "   0.7263524532318115,\n",
       "   0.5100061297416687,\n",
       "   0.662802517414093,\n",
       "   0.5260301232337952,\n",
       "   0.8977347612380981,\n",
       "   0.8804274797439575,\n",
       "   0.7062263488769531,\n",
       "   0.7704692482948303,\n",
       "   0.8029362559318542,\n",
       "   0.674173891544342,\n",
       "   0.9426003694534302,\n",
       "   0.5797895193099976,\n",
       "   0.7268125414848328,\n",
       "   0.6253841519355774,\n",
       "   0.9060404896736145,\n",
       "   0.8904811143875122,\n",
       "   0.7388883233070374,\n",
       "   0.6101261973381042,\n",
       "   0.9047672748565674,\n",
       "   0.623913049697876,\n",
       "   0.7717851996421814,\n",
       "   0.902334451675415,\n",
       "   0.567878782749176,\n",
       "   0.7386882305145264,\n",
       "   0.7793102264404297,\n",
       "   0.7054343819618225,\n",
       "   0.46182146668434143,\n",
       "   0.6697895526885986,\n",
       "   0.7071518898010254,\n",
       "   0.7945874333381653,\n",
       "   0.7165974378585815,\n",
       "   0.7430406808853149,\n",
       "   0.7404564023017883,\n",
       "   1.0166441202163696,\n",
       "   0.783464252948761,\n",
       "   0.8372898697853088,\n",
       "   0.482712060213089,\n",
       "   0.5816851854324341,\n",
       "   0.5386743545532227,\n",
       "   0.7927421927452087,\n",
       "   0.9108984470367432,\n",
       "   0.6108078956604004,\n",
       "   0.5592381954193115,\n",
       "   0.7814261317253113,\n",
       "   0.5267999768257141,\n",
       "   0.6806457042694092,\n",
       "   0.8153457641601562,\n",
       "   0.7527027726173401,\n",
       "   0.8430789709091187,\n",
       "   0.6501351594924927,\n",
       "   0.9351402521133423,\n",
       "   0.7607246041297913,\n",
       "   0.8175492882728577,\n",
       "   0.9173761010169983,\n",
       "   0.7172174453735352,\n",
       "   0.6904034614562988,\n",
       "   0.7123973965644836,\n",
       "   0.5523996949195862,\n",
       "   0.8498831391334534,\n",
       "   0.62969571352005,\n",
       "   0.6603400111198425,\n",
       "   0.8575943112373352,\n",
       "   0.8252623081207275,\n",
       "   0.6453114748001099,\n",
       "   1.0426523685455322,\n",
       "   0.8597981333732605,\n",
       "   0.8029712438583374,\n",
       "   0.9386990070343018,\n",
       "   0.8261546492576599,\n",
       "   0.4529873728752136,\n",
       "   0.6979670524597168,\n",
       "   0.5553010106086731,\n",
       "   0.7313298583030701,\n",
       "   0.6625807285308838,\n",
       "   0.6847634315490723,\n",
       "   0.6671397089958191,\n",
       "   0.41721129417419434,\n",
       "   0.7315798401832581,\n",
       "   0.6294330954551697,\n",
       "   0.6854458451271057,\n",
       "   0.6600244641304016,\n",
       "   0.595751941204071,\n",
       "   0.813667893409729,\n",
       "   0.7667471766471863,\n",
       "   0.7617434859275818,\n",
       "   0.5978952646255493,\n",
       "   0.6314412951469421,\n",
       "   0.7936897873878479,\n",
       "   0.816035807132721,\n",
       "   0.8736485242843628,\n",
       "   0.6389903426170349,\n",
       "   0.8606745004653931,\n",
       "   0.7127613425254822,\n",
       "   0.7404855489730835,\n",
       "   0.666542649269104,\n",
       "   0.7886181473731995,\n",
       "   0.8188709616661072,\n",
       "   0.6668805480003357,\n",
       "   0.6713717579841614,\n",
       "   0.768994927406311,\n",
       "   0.7358774542808533,\n",
       "   0.6033449172973633,\n",
       "   0.7741823196411133,\n",
       "   0.6150838732719421,\n",
       "   0.7867575287818909,\n",
       "   0.5519677996635437,\n",
       "   0.9174203872680664,\n",
       "   0.6936718821525574,\n",
       "   0.5195229649543762,\n",
       "   0.5365873575210571,\n",
       "   0.7915712594985962,\n",
       "   0.5834125876426697,\n",
       "   0.578493595123291,\n",
       "   0.7602770328521729,\n",
       "   0.8850807547569275,\n",
       "   0.8564817905426025,\n",
       "   0.8776178359985352,\n",
       "   0.7073217630386353,\n",
       "   1.0300822257995605,\n",
       "   0.5325204133987427,\n",
       "   0.7850255370140076,\n",
       "   0.7649231553077698,\n",
       "   0.7469401359558105,\n",
       "   0.7278940677642822,\n",
       "   0.7103404998779297,\n",
       "   0.5767931938171387,\n",
       "   0.6580302715301514,\n",
       "   0.9768750071525574,\n",
       "   0.9489343762397766,\n",
       "   0.4816568195819855,\n",
       "   0.6199564337730408,\n",
       "   1.079023003578186,\n",
       "   0.6606417894363403,\n",
       "   0.6171686053276062,\n",
       "   0.8417330384254456,\n",
       "   0.7462553977966309,\n",
       "   0.5262320041656494,\n",
       "   0.616138219833374,\n",
       "   0.6579636931419373,\n",
       "   0.6091408133506775,\n",
       "   0.6665303111076355,\n",
       "   0.8420314192771912,\n",
       "   0.8164896965026855,\n",
       "   0.6680781245231628,\n",
       "   1.0411251783370972,\n",
       "   0.6723289489746094,\n",
       "   0.8015784025192261,\n",
       "   0.7736000418663025,\n",
       "   0.6177238821983337,\n",
       "   0.5895652174949646,\n",
       "   0.7993287444114685,\n",
       "   0.5789231061935425,\n",
       "   0.7078609466552734,\n",
       "   0.6837546229362488,\n",
       "   0.7136950492858887,\n",
       "   0.7963441014289856,\n",
       "   0.766240119934082,\n",
       "   0.6960031986236572,\n",
       "   0.8004099130630493,\n",
       "   0.7663630843162537,\n",
       "   0.7209972143173218,\n",
       "   0.6791684627532959,\n",
       "   0.6571875214576721,\n",
       "   0.7906230092048645,\n",
       "   0.8582398891448975,\n",
       "   0.7061733603477478,\n",
       "   1.1768873929977417,\n",
       "   0.937965989112854,\n",
       "   0.9664844274520874,\n",
       "   0.6161018013954163,\n",
       "   0.6913478970527649,\n",
       "   0.7796819806098938,\n",
       "   0.6505459547042847,\n",
       "   0.7572467923164368,\n",
       "   0.5530524253845215,\n",
       "   0.7542750239372253,\n",
       "   0.7834044694900513,\n",
       "   0.7596344351768494,\n",
       "   0.649914026260376,\n",
       "   0.5914740562438965,\n",
       "   0.8216387033462524,\n",
       "   0.5477717518806458,\n",
       "   0.6500273942947388,\n",
       "   0.7836039066314697,\n",
       "   0.6281071901321411,\n",
       "   0.5502872467041016,\n",
       "   0.9887216091156006,\n",
       "   0.6690263152122498,\n",
       "   0.8563874959945679,\n",
       "   0.6578695178031921],\n",
       "  'train_voc_loss': [1.3843327760696411,\n",
       "   1.7450913190841675,\n",
       "   1.2674059867858887,\n",
       "   1.350084900856018,\n",
       "   1.6396338939666748,\n",
       "   1.3505620956420898,\n",
       "   1.2643288373947144,\n",
       "   1.0554372072219849,\n",
       "   1.4333570003509521,\n",
       "   1.2898560762405396,\n",
       "   1.8286060094833374,\n",
       "   0.9694847464561462,\n",
       "   0.8066548705101013,\n",
       "   1.1102184057235718,\n",
       "   1.5617356300354004,\n",
       "   1.8986480236053467,\n",
       "   1.4151118993759155,\n",
       "   1.4653384685516357,\n",
       "   1.7390481233596802,\n",
       "   1.2970253229141235,\n",
       "   1.540069580078125,\n",
       "   0.8059156537055969,\n",
       "   1.1321038007736206,\n",
       "   1.316243052482605,\n",
       "   1.4197784662246704,\n",
       "   0.8672754764556885,\n",
       "   0.9882017374038696,\n",
       "   1.2449003458023071,\n",
       "   1.448111653327942,\n",
       "   1.3809242248535156,\n",
       "   1.2489415407180786,\n",
       "   1.2007501125335693,\n",
       "   1.0024954080581665,\n",
       "   1.5956453084945679,\n",
       "   0.7105061411857605,\n",
       "   1.9825201034545898,\n",
       "   1.3605937957763672,\n",
       "   1.6256850957870483,\n",
       "   1.1415858268737793,\n",
       "   1.1807966232299805,\n",
       "   1.1506142616271973,\n",
       "   1.1567294597625732,\n",
       "   1.0628770589828491,\n",
       "   1.785164475440979,\n",
       "   0.830193281173706,\n",
       "   1.769974708557129,\n",
       "   0.9949067831039429,\n",
       "   1.8038870096206665,\n",
       "   1.0359737873077393,\n",
       "   1.135789394378662,\n",
       "   1.2816624641418457,\n",
       "   1.0638306140899658,\n",
       "   1.1647003889083862,\n",
       "   1.5408105850219727,\n",
       "   1.0277889966964722,\n",
       "   1.2377136945724487,\n",
       "   1.943885087966919,\n",
       "   1.1348899602890015,\n",
       "   1.2956088781356812,\n",
       "   1.0967682600021362,\n",
       "   1.0096853971481323,\n",
       "   1.5260003805160522,\n",
       "   1.21713125705719,\n",
       "   1.3904117345809937,\n",
       "   1.6280653476715088,\n",
       "   1.067074179649353,\n",
       "   1.226726770401001,\n",
       "   0.7970080971717834,\n",
       "   1.4078437089920044,\n",
       "   1.3981865644454956,\n",
       "   1.1025042533874512,\n",
       "   1.233933448791504,\n",
       "   1.5092190504074097,\n",
       "   1.0208330154418945,\n",
       "   1.2417359352111816,\n",
       "   1.8506311178207397,\n",
       "   1.228464126586914,\n",
       "   0.9999213218688965,\n",
       "   1.0030323266983032,\n",
       "   1.0944888591766357,\n",
       "   1.0198745727539062,\n",
       "   1.319097638130188,\n",
       "   1.2441751956939697,\n",
       "   1.0053138732910156,\n",
       "   1.6130664348602295,\n",
       "   1.6243927478790283,\n",
       "   1.5745608806610107,\n",
       "   1.0267229080200195,\n",
       "   1.1998186111450195,\n",
       "   1.617150902748108,\n",
       "   0.6609177589416504,\n",
       "   1.1915431022644043,\n",
       "   0.7980697154998779,\n",
       "   1.0747451782226562,\n",
       "   0.8949405550956726,\n",
       "   1.955417513847351,\n",
       "   2.302581548690796,\n",
       "   1.1542534828186035,\n",
       "   0.9808610677719116,\n",
       "   1.0539379119873047,\n",
       "   1.2058597803115845,\n",
       "   1.2534958124160767,\n",
       "   1.370501160621643,\n",
       "   1.0434250831604004,\n",
       "   1.101598858833313,\n",
       "   1.3711262941360474,\n",
       "   1.260210633277893,\n",
       "   1.507545828819275,\n",
       "   1.24368155002594,\n",
       "   1.3340685367584229,\n",
       "   0.9830895662307739,\n",
       "   1.143821358680725,\n",
       "   1.5663589239120483,\n",
       "   1.252603530883789,\n",
       "   1.0672703981399536,\n",
       "   1.08518385887146,\n",
       "   1.0905207395553589,\n",
       "   1.2007768154144287,\n",
       "   1.5481131076812744,\n",
       "   1.3777222633361816,\n",
       "   0.7631034255027771,\n",
       "   1.2891532182693481,\n",
       "   0.8968252539634705,\n",
       "   1.995671272277832,\n",
       "   1.8838142156600952,\n",
       "   1.180443286895752,\n",
       "   1.0860704183578491,\n",
       "   1.4799447059631348,\n",
       "   1.2428354024887085,\n",
       "   2.224381446838379,\n",
       "   1.9516183137893677,\n",
       "   1.0245676040649414,\n",
       "   1.1927906274795532,\n",
       "   0.9847683310508728,\n",
       "   1.1712151765823364,\n",
       "   0.9882314205169678,\n",
       "   1.1088725328445435,\n",
       "   1.002269983291626,\n",
       "   1.3489904403686523,\n",
       "   1.1531463861465454,\n",
       "   0.8867611289024353,\n",
       "   1.18965482711792,\n",
       "   1.4906342029571533,\n",
       "   0.8536188006401062,\n",
       "   1.490405559539795,\n",
       "   1.0040944814682007,\n",
       "   1.556885004043579,\n",
       "   1.0042566061019897,\n",
       "   1.350104570388794,\n",
       "   0.8122491836547852,\n",
       "   0.9810451865196228,\n",
       "   2.1549606323242188,\n",
       "   2.233253002166748,\n",
       "   1.293654441833496,\n",
       "   1.1060973405838013,\n",
       "   0.7185478210449219,\n",
       "   0.7016667723655701,\n",
       "   1.4606746435165405,\n",
       "   1.0895020961761475,\n",
       "   1.2758102416992188,\n",
       "   0.8537648320198059,\n",
       "   1.7964942455291748,\n",
       "   1.9339714050292969,\n",
       "   1.2138557434082031,\n",
       "   1.4689412117004395,\n",
       "   0.8887706995010376,\n",
       "   1.4422239065170288,\n",
       "   0.9564606547355652,\n",
       "   0.7890987396240234,\n",
       "   0.8753625750541687,\n",
       "   0.8290154933929443,\n",
       "   1.216797947883606,\n",
       "   1.3329577445983887,\n",
       "   0.99534010887146,\n",
       "   0.8197999596595764,\n",
       "   1.0750244855880737,\n",
       "   1.1845710277557373,\n",
       "   1.0064494609832764,\n",
       "   1.09163498878479,\n",
       "   1.30359947681427,\n",
       "   1.072379231452942,\n",
       "   0.9072595238685608,\n",
       "   1.064208745956421,\n",
       "   1.0181605815887451,\n",
       "   1.124956727027893,\n",
       "   0.8362123966217041,\n",
       "   1.299941897392273,\n",
       "   1.0200936794281006,\n",
       "   1.4456764459609985,\n",
       "   1.0069935321807861,\n",
       "   1.537036657333374,\n",
       "   1.2518264055252075,\n",
       "   1.2279103994369507,\n",
       "   1.1174020767211914,\n",
       "   1.1599456071853638,\n",
       "   1.0701416730880737,\n",
       "   0.8194248676300049,\n",
       "   1.2476967573165894,\n",
       "   0.9785769581794739,\n",
       "   1.1731184720993042,\n",
       "   1.2035484313964844,\n",
       "   1.1315808296203613,\n",
       "   1.0893352031707764,\n",
       "   0.7787646055221558,\n",
       "   0.9697171449661255,\n",
       "   0.9592068791389465,\n",
       "   0.9309417605400085,\n",
       "   1.3769513368606567,\n",
       "   1.010305404663086,\n",
       "   1.230578064918518,\n",
       "   1.0178009271621704,\n",
       "   0.8434105515480042,\n",
       "   0.7893320322036743,\n",
       "   1.1460423469543457,\n",
       "   0.9127771854400635,\n",
       "   0.999789297580719,\n",
       "   1.033420443534851,\n",
       "   1.148531198501587,\n",
       "   1.18442964553833,\n",
       "   0.9023211598396301,\n",
       "   1.227823257446289,\n",
       "   1.6536732912063599,\n",
       "   0.7714564800262451,\n",
       "   1.1579138040542603,\n",
       "   0.7950639724731445,\n",
       "   0.7176091074943542,\n",
       "   1.546658992767334,\n",
       "   0.950964093208313,\n",
       "   0.9601442813873291,\n",
       "   1.1303071975708008,\n",
       "   0.8642236590385437,\n",
       "   1.0105639696121216,\n",
       "   0.8710599541664124,\n",
       "   0.9067249298095703,\n",
       "   0.9174239039421082,\n",
       "   1.0175169706344604,\n",
       "   1.276610016822815,\n",
       "   1.0569405555725098,\n",
       "   1.004223346710205,\n",
       "   1.2344685792922974,\n",
       "   1.0593814849853516,\n",
       "   1.0008817911148071,\n",
       "   1.0235393047332764,\n",
       "   0.7541516423225403,\n",
       "   0.7684640288352966,\n",
       "   1.2005316019058228,\n",
       "   0.627336323261261,\n",
       "   0.62123703956604,\n",
       "   1.4833085536956787,\n",
       "   0.801816463470459,\n",
       "   1.0804212093353271,\n",
       "   1.115010142326355,\n",
       "   1.0557180643081665,\n",
       "   0.8723275065422058,\n",
       "   0.7539588212966919,\n",
       "   1.1993924379348755,\n",
       "   1.1331515312194824,\n",
       "   1.077882170677185,\n",
       "   1.419506549835205,\n",
       "   1.275876760482788,\n",
       "   1.5110008716583252,\n",
       "   0.9789572358131409,\n",
       "   0.827775239944458,\n",
       "   0.6383362412452698,\n",
       "   1.2384876012802124,\n",
       "   0.8200807571411133,\n",
       "   1.172704815864563,\n",
       "   1.1700531244277954,\n",
       "   1.1125963926315308,\n",
       "   1.1461457014083862,\n",
       "   1.460525393486023,\n",
       "   0.9896734952926636,\n",
       "   1.3934260606765747,\n",
       "   1.1280995607376099,\n",
       "   0.9991804957389832,\n",
       "   1.1578714847564697,\n",
       "   0.64182049036026,\n",
       "   0.9539014101028442,\n",
       "   0.8592442870140076,\n",
       "   1.539709448814392,\n",
       "   0.9362231492996216,\n",
       "   1.2516064643859863,\n",
       "   1.1447893381118774,\n",
       "   0.9196205735206604,\n",
       "   1.2449331283569336,\n",
       "   1.102358341217041,\n",
       "   0.6816079020500183,\n",
       "   0.9633678197860718,\n",
       "   0.9908952116966248,\n",
       "   0.7483646869659424,\n",
       "   0.7926316857337952,\n",
       "   0.9300055503845215,\n",
       "   0.8000501394271851,\n",
       "   1.1887034177780151,\n",
       "   1.358019471168518,\n",
       "   0.865615963935852,\n",
       "   0.9196515083312988,\n",
       "   0.9959896802902222,\n",
       "   0.9962860941886902,\n",
       "   0.6800519227981567,\n",
       "   0.9021403789520264,\n",
       "   1.022556185722351,\n",
       "   1.2763211727142334,\n",
       "   1.0994818210601807,\n",
       "   1.2979921102523804,\n",
       "   0.629783034324646,\n",
       "   0.9153117537498474,\n",
       "   0.8953120112419128,\n",
       "   1.3070013523101807,\n",
       "   1.2019861936569214,\n",
       "   0.8672768473625183,\n",
       "   1.0742084980010986,\n",
       "   1.229731559753418,\n",
       "   1.2497599124908447,\n",
       "   1.2548859119415283,\n",
       "   0.9931833744049072,\n",
       "   1.0585509538650513,\n",
       "   0.9960727691650391,\n",
       "   1.3826795816421509,\n",
       "   1.0983164310455322,\n",
       "   0.8839638829231262,\n",
       "   1.0189217329025269,\n",
       "   0.9550884366035461,\n",
       "   0.9516686201095581,\n",
       "   1.1969248056411743,\n",
       "   0.6509982347488403,\n",
       "   0.6641014814376831,\n",
       "   0.8397560715675354,\n",
       "   0.8550535440444946,\n",
       "   1.209997296333313,\n",
       "   1.023261547088623,\n",
       "   0.9729483127593994,\n",
       "   0.6833465099334717,\n",
       "   0.7928476333618164,\n",
       "   1.0015829801559448,\n",
       "   0.9415784478187561,\n",
       "   1.3771852254867554,\n",
       "   0.983150064945221,\n",
       "   0.9146974086761475,\n",
       "   1.3153053522109985,\n",
       "   0.6856809258460999,\n",
       "   0.9516909122467041,\n",
       "   0.7478741407394409,\n",
       "   1.0180926322937012,\n",
       "   1.1747865676879883,\n",
       "   1.130379557609558,\n",
       "   1.2824980020523071,\n",
       "   0.760311484336853,\n",
       "   0.9174084663391113,\n",
       "   1.013033390045166,\n",
       "   0.7677712440490723,\n",
       "   0.9808295965194702,\n",
       "   0.8025814890861511,\n",
       "   0.6503897309303284,\n",
       "   0.6725897789001465,\n",
       "   1.423877239227295,\n",
       "   1.034419298171997,\n",
       "   1.3501652479171753,\n",
       "   0.8642650246620178,\n",
       "   1.0363872051239014,\n",
       "   0.8999848365783691,\n",
       "   1.0013082027435303,\n",
       "   1.112653136253357,\n",
       "   0.6630416512489319,\n",
       "   0.6446450352668762,\n",
       "   0.9720105528831482,\n",
       "   1.13333261013031,\n",
       "   1.3428794145584106,\n",
       "   0.6160175204277039,\n",
       "   0.8422805666923523,\n",
       "   1.1671262979507446,\n",
       "   0.9804010391235352,\n",
       "   1.265501856803894,\n",
       "   0.9088244438171387,\n",
       "   0.777212381362915,\n",
       "   0.7655219435691833,\n",
       "   1.1564933061599731,\n",
       "   1.0249158143997192,\n",
       "   0.712889552116394,\n",
       "   1.0125572681427002,\n",
       "   0.6433097720146179,\n",
       "   1.0378016233444214,\n",
       "   1.1617165803909302,\n",
       "   1.0162063837051392,\n",
       "   1.0062332153320312,\n",
       "   0.8124253749847412,\n",
       "   0.6536220908164978,\n",
       "   1.2058887481689453,\n",
       "   1.101086139678955,\n",
       "   0.6734336614608765,\n",
       "   1.0711075067520142,\n",
       "   0.7436472773551941,\n",
       "   0.9968679547309875,\n",
       "   0.9312708377838135,\n",
       "   0.9115610718727112,\n",
       "   0.8999873399734497,\n",
       "   0.7147967219352722,\n",
       "   0.7070961594581604,\n",
       "   0.5757420659065247,\n",
       "   0.9731591939926147,\n",
       "   0.6727944612503052,\n",
       "   0.8607076406478882,\n",
       "   1.0454524755477905,\n",
       "   0.9404596090316772,\n",
       "   1.4122633934020996,\n",
       "   0.7640765309333801,\n",
       "   1.1186028718948364,\n",
       "   0.6578382849693298,\n",
       "   0.9452148079872131,\n",
       "   1.1749929189682007,\n",
       "   0.9998231530189514,\n",
       "   0.935215950012207,\n",
       "   0.6992406845092773,\n",
       "   0.7835342288017273,\n",
       "   1.246573567390442,\n",
       "   1.123965859413147,\n",
       "   0.9393010139465332,\n",
       "   0.8874041438102722,\n",
       "   0.7686643600463867,\n",
       "   0.6990848779678345,\n",
       "   1.287328839302063,\n",
       "   0.9794586300849915,\n",
       "   1.123961091041565,\n",
       "   1.4076234102249146,\n",
       "   0.43418121337890625,\n",
       "   1.097957968711853,\n",
       "   1.3274037837982178,\n",
       "   0.6882954835891724,\n",
       "   1.0120049715042114,\n",
       "   1.0524965524673462,\n",
       "   0.9858931303024292,\n",
       "   1.4616910219192505,\n",
       "   0.6744188070297241,\n",
       "   0.9471545815467834,\n",
       "   0.9164863228797913,\n",
       "   0.5605680346488953,\n",
       "   0.9145298004150391,\n",
       "   0.7444301843643188,\n",
       "   0.7803840637207031,\n",
       "   0.5940839648246765,\n",
       "   0.8586574792861938,\n",
       "   0.8534045219421387,\n",
       "   1.0759350061416626,\n",
       "   1.214020013809204,\n",
       "   0.9591358304023743,\n",
       "   1.104986548423767,\n",
       "   0.982589840888977,\n",
       "   0.87652188539505,\n",
       "   0.717593789100647,\n",
       "   0.8981682658195496,\n",
       "   0.9891657829284668,\n",
       "   0.6506214141845703,\n",
       "   0.5603875517845154,\n",
       "   0.8019528985023499,\n",
       "   1.1566053628921509,\n",
       "   0.8176738619804382,\n",
       "   1.3279616832733154,\n",
       "   1.3215957880020142,\n",
       "   1.1296886205673218,\n",
       "   0.9990450739860535,\n",
       "   0.6949800252914429,\n",
       "   0.49072265625,\n",
       "   0.7901992201805115,\n",
       "   1.0810859203338623,\n",
       "   1.0378886461257935,\n",
       "   0.914696991443634,\n",
       "   0.5648775696754456,\n",
       "   0.7558019757270813,\n",
       "   0.8635486960411072,\n",
       "   0.8803674578666687,\n",
       "   0.6187187433242798,\n",
       "   0.7280614376068115,\n",
       "   0.9759373664855957,\n",
       "   0.7739842534065247,\n",
       "   0.7926878333091736,\n",
       "   0.8833348751068115,\n",
       "   0.646060585975647,\n",
       "   1.35618257522583,\n",
       "   0.8458516597747803,\n",
       "   0.8090623617172241,\n",
       "   0.6598901748657227,\n",
       "   0.859635591506958,\n",
       "   1.0819159746170044,\n",
       "   0.7253406643867493,\n",
       "   0.999489426612854,\n",
       "   0.9126107096672058,\n",
       "   0.5516858100891113,\n",
       "   0.8463499546051025,\n",
       "   0.683231770992279,\n",
       "   0.763060986995697,\n",
       "   0.8117062449455261,\n",
       "   0.9557034373283386,\n",
       "   1.0444287061691284,\n",
       "   0.7852630615234375,\n",
       "   1.1635076999664307,\n",
       "   1.2266634702682495,\n",
       "   1.1640576124191284,\n",
       "   0.8214336037635803,\n",
       "   0.8878677487373352,\n",
       "   0.9317863583564758],\n",
       "  'train_jsc_loss': [0.9726518392562866,\n",
       "   1.1449506282806396,\n",
       "   1.0168479681015015,\n",
       "   1.0917232036590576,\n",
       "   1.0850335359573364,\n",
       "   1.0830110311508179,\n",
       "   0.8942335247993469,\n",
       "   1.029871940612793,\n",
       "   1.1532615423202515,\n",
       "   0.990327775478363,\n",
       "   0.9971287250518799,\n",
       "   1.649588942527771,\n",
       "   1.213315486907959,\n",
       "   1.089779257774353,\n",
       "   1.3539671897888184,\n",
       "   1.3316599130630493,\n",
       "   1.276375651359558,\n",
       "   1.143357753753662,\n",
       "   1.2595856189727783,\n",
       "   0.9873414635658264,\n",
       "   0.8749270439147949,\n",
       "   0.9600611925125122,\n",
       "   0.9697257876396179,\n",
       "   1.06150484085083,\n",
       "   1.1282719373703003,\n",
       "   0.9448041319847107,\n",
       "   1.0800925493240356,\n",
       "   0.95970219373703,\n",
       "   1.2096543312072754,\n",
       "   0.8794970512390137,\n",
       "   0.9925968647003174,\n",
       "   1.1821645498275757,\n",
       "   1.0623401403427124,\n",
       "   1.0053454637527466,\n",
       "   0.8135943412780762,\n",
       "   0.7948135137557983,\n",
       "   1.0206114053726196,\n",
       "   1.222472071647644,\n",
       "   0.76373690366745,\n",
       "   0.7867825627326965,\n",
       "   1.0555622577667236,\n",
       "   0.9973812699317932,\n",
       "   1.047253131866455,\n",
       "   1.3112152814865112,\n",
       "   1.032284140586853,\n",
       "   1.0194716453552246,\n",
       "   0.9733983278274536,\n",
       "   0.7280591130256653,\n",
       "   0.8268527984619141,\n",
       "   0.9056500196456909,\n",
       "   0.8791293501853943,\n",
       "   0.9662650227546692,\n",
       "   1.1284701824188232,\n",
       "   0.7753809690475464,\n",
       "   0.779839813709259,\n",
       "   0.6924487352371216,\n",
       "   0.8934926986694336,\n",
       "   0.8495192527770996,\n",
       "   0.7220930457115173,\n",
       "   0.7374116778373718,\n",
       "   0.6270911693572998,\n",
       "   0.8427127599716187,\n",
       "   0.589419960975647,\n",
       "   0.7475122809410095,\n",
       "   0.6339488625526428,\n",
       "   0.9697362780570984,\n",
       "   0.8131776452064514,\n",
       "   0.7718696594238281,\n",
       "   0.9684445261955261,\n",
       "   0.7655327320098877,\n",
       "   0.6632259488105774,\n",
       "   0.7034567594528198,\n",
       "   0.6366040706634521,\n",
       "   0.7598797082901001,\n",
       "   0.553372323513031,\n",
       "   0.7063504457473755,\n",
       "   0.9538745284080505,\n",
       "   0.7070479989051819,\n",
       "   0.7950524091720581,\n",
       "   0.6284784078598022,\n",
       "   0.9089469313621521,\n",
       "   0.6366730332374573,\n",
       "   0.40898117423057556,\n",
       "   0.6790793538093567,\n",
       "   0.6530646085739136,\n",
       "   0.7840312719345093,\n",
       "   0.669255256652832,\n",
       "   0.7539886236190796,\n",
       "   0.6801671385765076,\n",
       "   1.0152360200881958,\n",
       "   0.7277188897132874,\n",
       "   0.990269660949707,\n",
       "   0.6637336611747742,\n",
       "   0.7013159394264221,\n",
       "   0.545569658279419,\n",
       "   0.7221292853355408,\n",
       "   0.6537196040153503,\n",
       "   0.7546206116676331,\n",
       "   0.7351350784301758,\n",
       "   0.773719072341919,\n",
       "   0.8245059251785278,\n",
       "   0.7734062075614929,\n",
       "   0.6132429838180542,\n",
       "   0.7079911828041077,\n",
       "   0.9373456835746765,\n",
       "   0.6423032283782959,\n",
       "   0.6939414739608765,\n",
       "   0.7983462810516357,\n",
       "   0.5710955262184143,\n",
       "   0.7662714719772339,\n",
       "   0.8138464689254761,\n",
       "   0.6049742102622986,\n",
       "   0.4851284623146057,\n",
       "   0.41488444805145264,\n",
       "   0.6625502705574036,\n",
       "   0.7181723117828369,\n",
       "   0.8920424580574036,\n",
       "   0.6742491126060486,\n",
       "   0.9428446888923645,\n",
       "   0.7369170784950256,\n",
       "   0.8861316442489624,\n",
       "   0.6932359933853149,\n",
       "   1.0069150924682617,\n",
       "   0.8029213547706604,\n",
       "   0.7114608287811279,\n",
       "   0.803153395652771,\n",
       "   0.6553959250450134,\n",
       "   0.6171813607215881,\n",
       "   0.7167827486991882,\n",
       "   0.5484902858734131,\n",
       "   0.7532342672348022,\n",
       "   0.6655945181846619,\n",
       "   0.6014226675033569,\n",
       "   0.3955795168876648,\n",
       "   0.7603259682655334,\n",
       "   0.5597513318061829,\n",
       "   0.6100631356239319,\n",
       "   0.7957906126976013,\n",
       "   0.7434452772140503,\n",
       "   0.9189496636390686,\n",
       "   0.6952505111694336,\n",
       "   0.9040988087654114,\n",
       "   0.8351584076881409,\n",
       "   1.0339367389678955,\n",
       "   0.5400370359420776,\n",
       "   0.676055371761322,\n",
       "   0.5719540119171143,\n",
       "   0.620378315448761,\n",
       "   0.591277003288269,\n",
       "   0.526229202747345,\n",
       "   0.9154440760612488,\n",
       "   0.5243302583694458,\n",
       "   0.5275074243545532,\n",
       "   0.6950279474258423,\n",
       "   1.0790752172470093,\n",
       "   0.9313618540763855,\n",
       "   0.5511425733566284,\n",
       "   0.6623931527137756,\n",
       "   0.5363803505897522,\n",
       "   0.47508010268211365,\n",
       "   0.6437548398971558,\n",
       "   0.5585029721260071,\n",
       "   0.5749505162239075,\n",
       "   0.6441847085952759,\n",
       "   1.0233635902404785,\n",
       "   0.602773129940033,\n",
       "   0.6016802787780762,\n",
       "   0.8407459855079651,\n",
       "   0.866227924823761,\n",
       "   0.6570817828178406,\n",
       "   0.9201165437698364,\n",
       "   0.6389360427856445,\n",
       "   0.7862135171890259,\n",
       "   0.6026524305343628,\n",
       "   0.5939925909042358,\n",
       "   0.5794248580932617,\n",
       "   0.6498343348503113,\n",
       "   0.8661064505577087,\n",
       "   0.6652778387069702,\n",
       "   0.47731876373291016,\n",
       "   0.6542072296142578,\n",
       "   0.8744738101959229,\n",
       "   0.6285597085952759,\n",
       "   0.6349067091941833,\n",
       "   0.34314849972724915,\n",
       "   0.686989963054657,\n",
       "   0.6530203819274902,\n",
       "   0.5700799226760864,\n",
       "   0.9582646489143372,\n",
       "   0.5834500193595886,\n",
       "   0.8216008543968201,\n",
       "   0.9065458178520203,\n",
       "   0.5763689279556274,\n",
       "   0.6082322597503662,\n",
       "   0.7883553504943848,\n",
       "   0.6400942802429199,\n",
       "   0.5961655378341675,\n",
       "   0.6455175876617432,\n",
       "   0.5158758759498596,\n",
       "   0.5791041254997253,\n",
       "   0.4005303680896759,\n",
       "   0.4158914089202881,\n",
       "   0.6266781687736511,\n",
       "   0.5373708009719849,\n",
       "   0.505108118057251,\n",
       "   0.6676304936408997,\n",
       "   0.6565587520599365,\n",
       "   0.6707361340522766,\n",
       "   0.309919536113739,\n",
       "   0.5450377464294434,\n",
       "   0.5469249486923218,\n",
       "   0.4113363027572632,\n",
       "   0.5439918041229248,\n",
       "   0.49884873628616333,\n",
       "   0.526401698589325,\n",
       "   0.4722718596458435,\n",
       "   0.5712249875068665,\n",
       "   0.4348847270011902,\n",
       "   0.744999885559082,\n",
       "   0.6080201268196106,\n",
       "   0.7528369426727295,\n",
       "   0.6808246970176697,\n",
       "   0.7342005968093872,\n",
       "   0.46468445658683777,\n",
       "   0.5440471768379211,\n",
       "   0.4469631612300873,\n",
       "   0.5702334642410278,\n",
       "   0.4244154989719391,\n",
       "   0.6749414801597595,\n",
       "   0.4340343177318573,\n",
       "   0.867080807685852,\n",
       "   0.7581484913825989,\n",
       "   0.5452866554260254,\n",
       "   0.6400745511054993,\n",
       "   0.5986128449440002,\n",
       "   0.373381108045578,\n",
       "   0.9886453747749329,\n",
       "   0.7212920784950256,\n",
       "   0.4898799955844879,\n",
       "   0.5104261040687561,\n",
       "   0.6337531208992004,\n",
       "   0.6982747912406921,\n",
       "   0.500289261341095,\n",
       "   0.6458240151405334,\n",
       "   0.5457971096038818,\n",
       "   0.5389334559440613,\n",
       "   0.5906312465667725,\n",
       "   0.6612694263458252,\n",
       "   0.60561603307724,\n",
       "   0.6907986402511597,\n",
       "   0.8031222224235535,\n",
       "   0.5846496820449829,\n",
       "   0.6210741400718689,\n",
       "   0.5942748785018921,\n",
       "   0.6190668940544128,\n",
       "   0.9689596891403198,\n",
       "   0.5653601288795471,\n",
       "   0.6230648159980774,\n",
       "   0.5475717782974243,\n",
       "   0.6125110387802124,\n",
       "   0.6133536100387573,\n",
       "   0.5230804085731506,\n",
       "   0.7107226848602295,\n",
       "   0.6397445797920227,\n",
       "   0.5322473049163818,\n",
       "   0.5875493884086609,\n",
       "   0.6291778087615967,\n",
       "   0.5319584012031555,\n",
       "   0.5563828349113464,\n",
       "   0.5950524806976318,\n",
       "   0.359884113073349,\n",
       "   0.5556371212005615,\n",
       "   0.5320736765861511,\n",
       "   0.549602746963501,\n",
       "   0.6488602757453918,\n",
       "   0.4684778153896332,\n",
       "   0.5790321230888367,\n",
       "   0.47220802307128906,\n",
       "   0.8695671558380127,\n",
       "   0.7036339044570923,\n",
       "   0.9363312721252441,\n",
       "   0.6096242070198059,\n",
       "   0.36866775155067444,\n",
       "   0.8237032294273376,\n",
       "   0.416530966758728,\n",
       "   0.7057688236236572,\n",
       "   0.6285244822502136,\n",
       "   0.574488639831543,\n",
       "   0.801924467086792,\n",
       "   0.5518547296524048,\n",
       "   0.7081618905067444,\n",
       "   0.6471988558769226,\n",
       "   0.6180222034454346,\n",
       "   0.6741021871566772,\n",
       "   0.5524076223373413,\n",
       "   0.5617152452468872,\n",
       "   0.5572075247764587,\n",
       "   0.7856764793395996,\n",
       "   0.6408490538597107,\n",
       "   0.7587599754333496,\n",
       "   0.4469446539878845,\n",
       "   0.5768037438392639,\n",
       "   0.5511165261268616,\n",
       "   0.5435558557510376,\n",
       "   0.5384157299995422,\n",
       "   0.45243924856185913,\n",
       "   0.7105439901351929,\n",
       "   0.6048963069915771,\n",
       "   0.5065476894378662,\n",
       "   0.5441917181015015,\n",
       "   0.5389593243598938,\n",
       "   0.746388852596283,\n",
       "   0.5760315656661987,\n",
       "   0.5212276577949524,\n",
       "   0.5765886902809143,\n",
       "   0.8589475154876709,\n",
       "   0.8062657117843628,\n",
       "   0.7690732479095459,\n",
       "   0.6629050970077515,\n",
       "   0.6673656105995178,\n",
       "   0.3819321095943451,\n",
       "   0.5772864818572998,\n",
       "   0.5887797474861145,\n",
       "   0.6265688538551331,\n",
       "   0.5916917324066162,\n",
       "   0.505515456199646,\n",
       "   0.3704967498779297,\n",
       "   0.5299603343009949,\n",
       "   0.5147260427474976,\n",
       "   0.6193017363548279,\n",
       "   0.7001566290855408,\n",
       "   0.4733522832393646,\n",
       "   0.7860637307167053,\n",
       "   0.5398537516593933,\n",
       "   0.4796575903892517,\n",
       "   0.4942251443862915,\n",
       "   0.9783182144165039,\n",
       "   0.5901767015457153,\n",
       "   0.8908620476722717,\n",
       "   0.5953231453895569,\n",
       "   0.48501670360565186,\n",
       "   0.5264988541603088,\n",
       "   0.49100375175476074,\n",
       "   0.6517751216888428,\n",
       "   0.5174440741539001,\n",
       "   0.42352116107940674,\n",
       "   0.704006552696228,\n",
       "   0.8355512022972107,\n",
       "   0.35764217376708984,\n",
       "   0.5729498863220215,\n",
       "   0.5364676117897034,\n",
       "   0.5373898148536682,\n",
       "   0.6116871237754822,\n",
       "   0.6144819855690002,\n",
       "   0.51575767993927,\n",
       "   0.47014570236206055,\n",
       "   0.749770998954773,\n",
       "   0.4754253625869751,\n",
       "   0.5674604177474976,\n",
       "   0.5389426350593567,\n",
       "   0.5096416473388672,\n",
       "   0.475462406873703,\n",
       "   0.8532284498214722,\n",
       "   0.4901147484779358,\n",
       "   0.7472958564758301,\n",
       "   0.6450318694114685,\n",
       "   0.5631995797157288,\n",
       "   0.6293721795082092,\n",
       "   0.42164871096611023,\n",
       "   0.5068991184234619,\n",
       "   0.37443602085113525,\n",
       "   0.8498722910881042,\n",
       "   0.8957773447036743,\n",
       "   0.7982500791549683,\n",
       "   0.5872500538825989,\n",
       "   0.5729246139526367,\n",
       "   0.6146839261054993,\n",
       "   0.631934404373169,\n",
       "   0.4084438979625702,\n",
       "   0.6467941403388977,\n",
       "   0.3561702072620392,\n",
       "   0.40438273549079895,\n",
       "   0.560554027557373,\n",
       "   0.6472464203834534,\n",
       "   0.5333152413368225,\n",
       "   0.5823655128479004,\n",
       "   0.7285100817680359,\n",
       "   0.5718221068382263,\n",
       "   0.5044896602630615,\n",
       "   0.7746842503547668,\n",
       "   0.3773314952850342,\n",
       "   0.568982720375061,\n",
       "   0.3858661949634552,\n",
       "   0.626911997795105,\n",
       "   0.6261768937110901,\n",
       "   0.5241171717643738,\n",
       "   0.4982280433177948,\n",
       "   0.6640335917472839,\n",
       "   0.654259979724884,\n",
       "   0.4002377390861511,\n",
       "   0.5396794676780701,\n",
       "   0.48787862062454224,\n",
       "   0.5822416543960571,\n",
       "   0.44792696833610535,\n",
       "   0.6744406819343567,\n",
       "   0.7016115784645081,\n",
       "   0.678206741809845,\n",
       "   0.5589390993118286,\n",
       "   0.2946102023124695,\n",
       "   0.4418947100639343,\n",
       "   0.5418714880943298,\n",
       "   0.6253300905227661,\n",
       "   0.6352644562721252,\n",
       "   0.6608129143714905,\n",
       "   0.5541499853134155,\n",
       "   0.3111191689968109,\n",
       "   0.5044482946395874,\n",
       "   0.5102735161781311,\n",
       "   0.519771933555603,\n",
       "   0.45535358786582947,\n",
       "   0.5084856748580933,\n",
       "   0.44831833243370056,\n",
       "   0.5587791204452515,\n",
       "   0.43418848514556885,\n",
       "   0.5435404777526855,\n",
       "   0.4457895755767822,\n",
       "   0.5433290600776672,\n",
       "   0.5313571095466614,\n",
       "   0.6479666829109192,\n",
       "   0.5018947124481201,\n",
       "   0.548569917678833,\n",
       "   0.35257476568222046,\n",
       "   0.689456045627594,\n",
       "   0.6277878284454346,\n",
       "   0.5140313506126404,\n",
       "   0.5707493424415588,\n",
       "   0.592363715171814,\n",
       "   0.6279790997505188,\n",
       "   0.5707663297653198,\n",
       "   0.3650280833244324,\n",
       "   0.6029422283172607,\n",
       "   0.5684971213340759,\n",
       "   0.4904650151729584,\n",
       "   0.4072672724723816,\n",
       "   0.6325694918632507,\n",
       "   0.5807396769523621,\n",
       "   0.4415833353996277,\n",
       "   0.6812958717346191,\n",
       "   0.7439001798629761,\n",
       "   0.4787047803401947,\n",
       "   0.5087710022926331,\n",
       "   0.5343500971794128,\n",
       "   0.573862612247467,\n",
       "   0.5061280727386475,\n",
       "   0.6404565572738647,\n",
       "   0.6447733640670776,\n",
       "   0.5951626300811768,\n",
       "   0.48063498735427856,\n",
       "   0.49251067638397217,\n",
       "   0.5592228174209595,\n",
       "   0.5882070064544678,\n",
       "   1.045678973197937,\n",
       "   0.5825058221817017,\n",
       "   0.5687938332557678,\n",
       "   0.9801580309867859,\n",
       "   0.5284972786903381,\n",
       "   0.3649648427963257,\n",
       "   0.6582348346710205,\n",
       "   0.6292795538902283,\n",
       "   0.33341994881629944,\n",
       "   0.6987155675888062,\n",
       "   0.3150102496147156,\n",
       "   0.44971802830696106,\n",
       "   0.5913153886795044,\n",
       "   0.5190181136131287,\n",
       "   0.4076697826385498,\n",
       "   0.5299426317214966,\n",
       "   0.7476774454116821,\n",
       "   0.5513752698898315,\n",
       "   0.46977898478507996,\n",
       "   0.4633314311504364,\n",
       "   0.7620208263397217,\n",
       "   0.8093134760856628,\n",
       "   0.6094053387641907,\n",
       "   0.44496044516563416,\n",
       "   0.4510835111141205,\n",
       "   0.5047346353530884,\n",
       "   0.44758135080337524,\n",
       "   0.6441501379013062,\n",
       "   0.4212234318256378,\n",
       "   0.45738711953163147,\n",
       "   0.5854575037956238,\n",
       "   0.637907087802887,\n",
       "   0.46405696868896484,\n",
       "   0.46342507004737854,\n",
       "   0.7034475207328796,\n",
       "   0.4588254988193512,\n",
       "   0.5789305567741394,\n",
       "   0.6713820099830627,\n",
       "   0.4810922145843506],\n",
       "  'train_ff_loss': [0.5268966555595398,\n",
       "   0.5635579824447632,\n",
       "   0.4959578812122345,\n",
       "   0.7270249724388123,\n",
       "   0.4964791238307953,\n",
       "   0.4603726267814636,\n",
       "   0.5638420581817627,\n",
       "   0.4694138765335083,\n",
       "   0.44096294045448303,\n",
       "   0.372754842042923,\n",
       "   0.464231014251709,\n",
       "   0.537297785282135,\n",
       "   0.45299461483955383,\n",
       "   0.5047661662101746,\n",
       "   0.5275443196296692,\n",
       "   0.6412640810012817,\n",
       "   0.36859533190727234,\n",
       "   0.45530104637145996,\n",
       "   0.4720429480075836,\n",
       "   0.41518011689186096,\n",
       "   0.4413203001022339,\n",
       "   0.44683343172073364,\n",
       "   0.40882086753845215,\n",
       "   0.47206953167915344,\n",
       "   0.49176621437072754,\n",
       "   0.4706062972545624,\n",
       "   0.38663533329963684,\n",
       "   0.4549797773361206,\n",
       "   0.47571972012519836,\n",
       "   0.42559361457824707,\n",
       "   0.4146398901939392,\n",
       "   0.43240854144096375,\n",
       "   0.5623961687088013,\n",
       "   0.47819089889526367,\n",
       "   0.5064194202423096,\n",
       "   0.5224593877792358,\n",
       "   0.48417928814888,\n",
       "   0.3658255934715271,\n",
       "   0.5211772322654724,\n",
       "   0.3408437967300415,\n",
       "   0.4281127154827118,\n",
       "   0.534706711769104,\n",
       "   0.5641903877258301,\n",
       "   0.4791296720504761,\n",
       "   0.4051480293273926,\n",
       "   0.4734048843383789,\n",
       "   0.4053795039653778,\n",
       "   0.5248429179191589,\n",
       "   0.4484751224517822,\n",
       "   0.3901801109313965,\n",
       "   0.48233547806739807,\n",
       "   0.4316695034503937,\n",
       "   0.44862422347068787,\n",
       "   0.34276607632637024,\n",
       "   0.3193765878677368,\n",
       "   0.39409223198890686,\n",
       "   0.43149900436401367,\n",
       "   0.5052457451820374,\n",
       "   0.43743768334388733,\n",
       "   0.38208723068237305,\n",
       "   0.559795081615448,\n",
       "   0.41687124967575073,\n",
       "   0.5337793231010437,\n",
       "   0.35923340916633606,\n",
       "   0.5427371263504028,\n",
       "   0.4832994043827057,\n",
       "   0.3279118239879608,\n",
       "   0.4684985876083374,\n",
       "   0.38700807094573975,\n",
       "   0.39377379417419434,\n",
       "   0.40981370210647583,\n",
       "   0.4542199969291687,\n",
       "   0.5699975490570068,\n",
       "   0.4306742548942566,\n",
       "   0.5227070450782776,\n",
       "   0.34819918870925903,\n",
       "   0.4405769407749176,\n",
       "   0.4560964107513428,\n",
       "   0.42064061760902405,\n",
       "   0.46059125661849976,\n",
       "   0.46389809250831604,\n",
       "   0.4875471889972687,\n",
       "   0.2839186489582062,\n",
       "   0.38721615076065063,\n",
       "   0.5746651887893677,\n",
       "   0.3711530864238739,\n",
       "   0.47096574306488037,\n",
       "   0.4639303982257843,\n",
       "   0.3371637165546417,\n",
       "   0.48626813292503357,\n",
       "   0.3285151720046997,\n",
       "   0.3838755786418915,\n",
       "   0.4619382619857788,\n",
       "   0.3571964502334595,\n",
       "   0.42144137620925903,\n",
       "   0.454922080039978,\n",
       "   0.38009580969810486,\n",
       "   0.4271555244922638,\n",
       "   0.54344242811203,\n",
       "   0.4318518042564392,\n",
       "   0.3767499029636383,\n",
       "   0.3346695601940155,\n",
       "   0.5299119353294373,\n",
       "   0.41629618406295776,\n",
       "   0.4555063247680664,\n",
       "   0.4384438097476959,\n",
       "   0.5778513550758362,\n",
       "   0.3968319594860077,\n",
       "   0.42298269271850586,\n",
       "   0.3667267858982086,\n",
       "   0.463710755109787,\n",
       "   0.36099591851234436,\n",
       "   0.38893285393714905,\n",
       "   0.4760286509990692,\n",
       "   0.5665910840034485,\n",
       "   0.4515446424484253,\n",
       "   0.5023702383041382,\n",
       "   0.5203667879104614,\n",
       "   0.6013641357421875,\n",
       "   0.3781026005744934,\n",
       "   0.2790779173374176,\n",
       "   0.45492023229599,\n",
       "   0.48079946637153625,\n",
       "   0.3313445746898651,\n",
       "   0.4202035963535309,\n",
       "   0.40895962715148926,\n",
       "   0.4377446174621582,\n",
       "   0.29899418354034424,\n",
       "   0.38526013493537903,\n",
       "   0.3694878816604614,\n",
       "   0.36936306953430176,\n",
       "   0.3872777223587036,\n",
       "   0.4015887975692749,\n",
       "   0.436279296875,\n",
       "   0.498189777135849,\n",
       "   0.4088689088821411,\n",
       "   0.3193853199481964,\n",
       "   0.4056936502456665,\n",
       "   0.47791948914527893,\n",
       "   0.35427579283714294,\n",
       "   0.4397186040878296,\n",
       "   0.46308308839797974,\n",
       "   0.36124086380004883,\n",
       "   0.4752156734466553,\n",
       "   0.34697452187538147,\n",
       "   0.4254405200481415,\n",
       "   0.503028154373169,\n",
       "   0.4688015282154083,\n",
       "   0.4715158939361572,\n",
       "   0.443277508020401,\n",
       "   0.5733826756477356,\n",
       "   0.3707490861415863,\n",
       "   0.4319543242454529,\n",
       "   0.46127432584762573,\n",
       "   0.49826210737228394,\n",
       "   0.4907534718513489,\n",
       "   0.43603959679603577,\n",
       "   0.4520310163497925,\n",
       "   0.3503159284591675,\n",
       "   0.35814517736434937,\n",
       "   0.4077993631362915,\n",
       "   0.3145977258682251,\n",
       "   0.4792025685310364,\n",
       "   0.42635318636894226,\n",
       "   0.5851126313209534,\n",
       "   0.3685421645641327,\n",
       "   0.5599876642227173,\n",
       "   0.4152946472167969,\n",
       "   0.4069984257221222,\n",
       "   0.3962826132774353,\n",
       "   0.5383895635604858,\n",
       "   0.4529854357242584,\n",
       "   0.4059411883354187,\n",
       "   0.326081782579422,\n",
       "   0.3249211311340332,\n",
       "   0.3238888680934906,\n",
       "   0.3581449091434479,\n",
       "   0.4424995183944702,\n",
       "   0.4756007492542267,\n",
       "   0.39076587557792664,\n",
       "   0.42258137464523315,\n",
       "   0.4548945724964142,\n",
       "   0.361431360244751,\n",
       "   0.5274792313575745,\n",
       "   0.4152528941631317,\n",
       "   0.4921191930770874,\n",
       "   0.454759418964386,\n",
       "   0.45635199546813965,\n",
       "   0.4431389570236206,\n",
       "   0.47045210003852844,\n",
       "   0.40535154938697815,\n",
       "   0.30207589268684387,\n",
       "   0.3456101715564728,\n",
       "   0.409475177526474,\n",
       "   0.4571918547153473,\n",
       "   0.3741465210914612,\n",
       "   0.3910068869590759,\n",
       "   0.39082643389701843,\n",
       "   0.34519755840301514,\n",
       "   0.3541417717933655,\n",
       "   0.39446550607681274,\n",
       "   0.3953135907649994,\n",
       "   0.45344752073287964,\n",
       "   0.33294641971588135,\n",
       "   0.3506319522857666,\n",
       "   0.4622964859008789,\n",
       "   0.34025079011917114,\n",
       "   0.391551673412323,\n",
       "   0.4181433618068695,\n",
       "   0.28139689564704895,\n",
       "   0.4148275852203369,\n",
       "   0.43088558316230774,\n",
       "   0.4223822355270386,\n",
       "   0.356914222240448,\n",
       "   0.4365978240966797,\n",
       "   0.308098167181015,\n",
       "   0.37357380986213684,\n",
       "   0.4065658748149872,\n",
       "   0.4696018397808075,\n",
       "   0.27118271589279175,\n",
       "   0.42086440324783325,\n",
       "   0.4199734628200531,\n",
       "   0.4009208083152771,\n",
       "   0.5234392881393433,\n",
       "   0.3025865852832794,\n",
       "   0.4043493866920471,\n",
       "   0.3888498544692993,\n",
       "   0.4852808713912964,\n",
       "   0.31143420934677124,\n",
       "   0.4031192362308502,\n",
       "   0.37180396914482117,\n",
       "   0.3998284637928009,\n",
       "   0.4921738803386688,\n",
       "   0.3119009733200073,\n",
       "   0.5090989470481873,\n",
       "   0.5188727974891663,\n",
       "   0.3811549246311188,\n",
       "   0.4724295139312744,\n",
       "   0.37276744842529297,\n",
       "   0.39759689569473267,\n",
       "   0.2959884703159332,\n",
       "   0.3736571669578552,\n",
       "   0.3982815444469452,\n",
       "   0.3827718198299408,\n",
       "   0.4104347825050354,\n",
       "   0.43413040041923523,\n",
       "   0.28969788551330566,\n",
       "   0.2995409667491913,\n",
       "   0.3769058883190155,\n",
       "   0.2512541115283966,\n",
       "   0.30182740092277527,\n",
       "   0.43268129229545593,\n",
       "   0.3476501405239105,\n",
       "   0.3556438982486725,\n",
       "   0.527014970779419,\n",
       "   0.35058197379112244,\n",
       "   0.2554592490196228,\n",
       "   0.35173746943473816,\n",
       "   0.40834182500839233,\n",
       "   0.4796573221683502,\n",
       "   0.4264034032821655,\n",
       "   0.3435565233230591,\n",
       "   0.4851894974708557,\n",
       "   0.44479382038116455,\n",
       "   0.45636653900146484,\n",
       "   0.37215909361839294,\n",
       "   0.49982938170433044,\n",
       "   0.44425010681152344,\n",
       "   0.47752755880355835,\n",
       "   0.4005540907382965,\n",
       "   0.5013843774795532,\n",
       "   0.3475578725337982,\n",
       "   0.434198260307312,\n",
       "   0.4300802946090698,\n",
       "   0.363755464553833,\n",
       "   0.3723396062850952,\n",
       "   0.31740912795066833,\n",
       "   0.49027588963508606,\n",
       "   0.5342261791229248,\n",
       "   0.3268294334411621,\n",
       "   0.28239890933036804,\n",
       "   0.31202054023742676,\n",
       "   0.3757688105106354,\n",
       "   0.3855530619621277,\n",
       "   0.44877710938453674,\n",
       "   0.32349109649658203,\n",
       "   0.3623020052909851,\n",
       "   0.2758639454841614,\n",
       "   0.46109727025032043,\n",
       "   0.3070389926433563,\n",
       "   0.285264790058136,\n",
       "   0.330626904964447,\n",
       "   0.2987748086452484,\n",
       "   0.3735046088695526,\n",
       "   0.32628133893013,\n",
       "   0.3316590487957001,\n",
       "   0.29268279671669006,\n",
       "   0.432985782623291,\n",
       "   0.36745500564575195,\n",
       "   0.4647311866283417,\n",
       "   0.4021804928779602,\n",
       "   0.32747364044189453,\n",
       "   0.26051992177963257,\n",
       "   0.3965340256690979,\n",
       "   0.32963573932647705,\n",
       "   0.5176637768745422,\n",
       "   0.45764458179473877,\n",
       "   0.35398346185684204,\n",
       "   0.33679819107055664,\n",
       "   0.5232299566268921,\n",
       "   0.4410701096057892,\n",
       "   0.37893146276474,\n",
       "   0.29022136330604553,\n",
       "   0.5299161076545715,\n",
       "   0.44578075408935547,\n",
       "   0.39746347069740295,\n",
       "   0.40343937277793884,\n",
       "   0.31193047761917114,\n",
       "   0.4664280414581299,\n",
       "   0.3224789500236511,\n",
       "   0.3971424102783203,\n",
       "   0.42529284954071045,\n",
       "   0.3121773302555084,\n",
       "   0.3879391551017761,\n",
       "   0.4099072217941284,\n",
       "   0.3453371226787567,\n",
       "   0.29731202125549316,\n",
       "   0.3433417081832886,\n",
       "   0.32513079047203064,\n",
       "   0.3517424464225769,\n",
       "   0.3031967580318451,\n",
       "   0.4042004942893982,\n",
       "   0.3429011106491089,\n",
       "   0.35128822922706604,\n",
       "   0.4433706998825073,\n",
       "   0.35201093554496765,\n",
       "   0.4958112835884094,\n",
       "   0.44657886028289795,\n",
       "   0.4733761250972748,\n",
       "   0.34917551279067993,\n",
       "   0.28793174028396606,\n",
       "   0.33666813373565674,\n",
       "   0.37251004576683044,\n",
       "   0.26701265573501587,\n",
       "   0.38932564854621887,\n",
       "   0.33792704343795776,\n",
       "   0.34968301653862,\n",
       "   0.36059436202049255,\n",
       "   0.334824800491333,\n",
       "   0.3782637119293213,\n",
       "   0.3401595652103424,\n",
       "   0.4450512230396271,\n",
       "   0.4250868856906891,\n",
       "   0.40786808729171753,\n",
       "   0.3848668038845062,\n",
       "   0.2309780716896057,\n",
       "   0.38253477215766907,\n",
       "   0.46089786291122437,\n",
       "   0.3713197112083435,\n",
       "   0.4611295461654663,\n",
       "   0.2785565257072449,\n",
       "   0.32551899552345276,\n",
       "   0.32998788356781006,\n",
       "   0.4237377643585205,\n",
       "   0.3180542588233948,\n",
       "   0.2846809923648834,\n",
       "   0.2911774218082428,\n",
       "   0.3798231780529022,\n",
       "   0.3750835359096527,\n",
       "   0.4386930763721466,\n",
       "   0.3238447606563568,\n",
       "   0.43652182817459106,\n",
       "   0.36107927560806274,\n",
       "   0.334302693605423,\n",
       "   0.3220405578613281,\n",
       "   0.39933228492736816,\n",
       "   0.4246440827846527,\n",
       "   0.36442306637763977,\n",
       "   0.3655064105987549,\n",
       "   0.30214351415634155,\n",
       "   0.4292212426662445,\n",
       "   0.36760517954826355,\n",
       "   0.25406306982040405,\n",
       "   0.360615074634552,\n",
       "   0.3480134606361389,\n",
       "   0.3616059720516205,\n",
       "   0.30671584606170654,\n",
       "   0.4090115427970886,\n",
       "   0.33569222688674927,\n",
       "   0.3697216808795929,\n",
       "   0.3402079641819,\n",
       "   0.31839802861213684,\n",
       "   0.39947745203971863,\n",
       "   0.28782907128334045,\n",
       "   0.3391152024269104,\n",
       "   0.2886887192726135,\n",
       "   0.29870733618736267,\n",
       "   0.37843647599220276,\n",
       "   0.3470233976840973,\n",
       "   0.3910151720046997,\n",
       "   0.30826443433761597,\n",
       "   0.2870684564113617,\n",
       "   0.31189265847206116,\n",
       "   0.339154988527298,\n",
       "   0.396242618560791,\n",
       "   0.3721175193786621,\n",
       "   0.30739739537239075,\n",
       "   0.3917199969291687,\n",
       "   0.4380747973918915,\n",
       "   0.36653628945350647,\n",
       "   0.32825297117233276,\n",
       "   0.4285498857498169,\n",
       "   0.2222541719675064,\n",
       "   0.5739173293113708,\n",
       "   0.49015602469444275,\n",
       "   0.33602508902549744,\n",
       "   0.4034274220466614,\n",
       "   0.30623939633369446,\n",
       "   0.3033226430416107,\n",
       "   0.42131632566452026,\n",
       "   0.3355095684528351,\n",
       "   0.4155612587928772,\n",
       "   0.4039173722267151,\n",
       "   0.3938557505607605,\n",
       "   0.38047632575035095,\n",
       "   0.3006495237350464,\n",
       "   0.37770742177963257,\n",
       "   0.4389075040817261,\n",
       "   0.45903995633125305,\n",
       "   0.3392457962036133,\n",
       "   0.36497601866722107,\n",
       "   0.3616950213909149,\n",
       "   0.35405170917510986,\n",
       "   0.276218444108963,\n",
       "   0.3316324055194855,\n",
       "   0.3471531271934509,\n",
       "   0.42950907349586487,\n",
       "   0.34264400601387024,\n",
       "   0.3339468836784363,\n",
       "   0.3425561487674713,\n",
       "   0.3790549635887146,\n",
       "   0.3413328230381012,\n",
       "   0.4150627851486206,\n",
       "   0.40653347969055176,\n",
       "   0.3432108759880066,\n",
       "   0.3189510703086853,\n",
       "   0.28796130418777466,\n",
       "   0.3773408532142639,\n",
       "   0.49117207527160645,\n",
       "   0.298234224319458,\n",
       "   0.3003324568271637,\n",
       "   0.29747921228408813,\n",
       "   0.3078441619873047,\n",
       "   0.3361615240573883,\n",
       "   0.5529487729072571,\n",
       "   0.4129387438297272,\n",
       "   0.304720014333725,\n",
       "   0.39080727100372314,\n",
       "   0.43058598041534424,\n",
       "   0.41721493005752563,\n",
       "   0.395868718624115,\n",
       "   0.38959160447120667,\n",
       "   0.31476640701293945,\n",
       "   0.24197694659233093,\n",
       "   0.4057407081127167,\n",
       "   0.31356221437454224,\n",
       "   0.298125684261322,\n",
       "   0.2959228456020355,\n",
       "   0.4686267673969269,\n",
       "   0.3963099420070648,\n",
       "   0.4185790419578552,\n",
       "   0.4365881383419037,\n",
       "   0.33572250604629517,\n",
       "   0.4288608729839325,\n",
       "   0.39332351088523865,\n",
       "   0.29218602180480957,\n",
       "   0.5700774788856506,\n",
       "   0.3299461007118225,\n",
       "   0.3657692074775696,\n",
       "   0.27842697501182556,\n",
       "   0.33223819732666016,\n",
       "   0.2874318063259125,\n",
       "   0.28656578063964844,\n",
       "   0.43389254808425903,\n",
       "   0.2412971556186676,\n",
       "   0.3717804551124573,\n",
       "   0.43107569217681885,\n",
       "   0.2887659966945648,\n",
       "   0.4216138422489166,\n",
       "   0.41421714425086975,\n",
       "   0.29681673645973206,\n",
       "   0.35152220726013184,\n",
       "   0.3506668210029602,\n",
       "   0.38089805841445923,\n",
       "   0.3379265367984772,\n",
       "   0.3501448631286621,\n",
       "   0.2877000868320465,\n",
       "   0.3339989483356476,\n",
       "   0.3200128674507141,\n",
       "   0.4240739941596985]},\n",
       " 3: {'lr': 1e-06,\n",
       "  'best_loss_epoch': 258,\n",
       "  'best_acc_epoch': 113,\n",
       "  'best_r2_epoch': 23,\n",
       "  'pce_loss': [26.29336929321289,\n",
       "   15.792176246643066,\n",
       "   10.257272720336914,\n",
       "   7.263622760772705,\n",
       "   5.281472206115723,\n",
       "   3.915876626968384,\n",
       "   2.954474687576294,\n",
       "   2.1639328002929688,\n",
       "   1.7043678760528564,\n",
       "   1.3034703731536865,\n",
       "   1.045172929763794,\n",
       "   0.8284965753555298,\n",
       "   0.6383730173110962,\n",
       "   0.524505078792572,\n",
       "   0.4500795304775238,\n",
       "   0.3764840066432953,\n",
       "   0.32179901003837585,\n",
       "   0.2733071446418762,\n",
       "   0.23757432401180267,\n",
       "   0.2227819859981537,\n",
       "   0.20581376552581787,\n",
       "   0.1875838339328766,\n",
       "   0.17980289459228516,\n",
       "   0.16614772379398346,\n",
       "   0.15393564105033875,\n",
       "   0.14898045361042023,\n",
       "   0.14554302394390106,\n",
       "   0.1512610912322998,\n",
       "   0.14515230059623718,\n",
       "   0.13485029339790344,\n",
       "   0.1287110596895218,\n",
       "   0.121683768928051,\n",
       "   0.11903343349695206,\n",
       "   0.12221743166446686,\n",
       "   0.11902814358472824,\n",
       "   0.11947079002857208,\n",
       "   0.12396522611379623,\n",
       "   0.12998202443122864,\n",
       "   0.12439732998609543,\n",
       "   0.1314067542552948,\n",
       "   0.12765838205814362,\n",
       "   0.12254653871059418,\n",
       "   0.12250488251447678,\n",
       "   0.12783020734786987,\n",
       "   0.11946167796850204,\n",
       "   0.11358487606048584,\n",
       "   0.1111382395029068,\n",
       "   0.1091880276799202,\n",
       "   0.11481986194849014,\n",
       "   0.11667709052562714,\n",
       "   0.11911432445049286,\n",
       "   0.11695554107427597,\n",
       "   0.11885492503643036,\n",
       "   0.11171067506074905,\n",
       "   0.11224047094583511,\n",
       "   0.1088271290063858,\n",
       "   0.11848507821559906,\n",
       "   0.10919521749019623,\n",
       "   0.10885648429393768,\n",
       "   0.11029106378555298,\n",
       "   0.10571553558111191,\n",
       "   0.10196048766374588,\n",
       "   0.10344935953617096,\n",
       "   0.10853440314531326,\n",
       "   0.10721973329782486,\n",
       "   0.1130005270242691,\n",
       "   0.11004199832677841,\n",
       "   0.1089233011007309,\n",
       "   0.09948617964982986,\n",
       "   0.10260885208845139,\n",
       "   0.10534662753343582,\n",
       "   0.11009259521961212,\n",
       "   0.108931764960289,\n",
       "   0.10673097521066666,\n",
       "   0.10683189332485199,\n",
       "   0.09728775918483734,\n",
       "   0.09940462559461594,\n",
       "   0.10462494194507599,\n",
       "   0.10311528295278549,\n",
       "   0.09735587984323502,\n",
       "   0.08989274501800537,\n",
       "   0.10136213153600693,\n",
       "   0.10093658417463303,\n",
       "   0.10445113480091095,\n",
       "   0.10060740262269974,\n",
       "   0.09918884932994843,\n",
       "   0.09864963591098785,\n",
       "   0.10378049314022064,\n",
       "   0.1050548106431961,\n",
       "   0.11047406494617462,\n",
       "   0.1065787523984909,\n",
       "   0.10836287587881088,\n",
       "   0.11314407736063004,\n",
       "   0.11504343897104263,\n",
       "   0.12002906948328018,\n",
       "   0.12256424874067307,\n",
       "   0.11684275418519974,\n",
       "   0.11701782792806625,\n",
       "   0.1207844540476799,\n",
       "   0.11736131459474564,\n",
       "   0.11213698238134384,\n",
       "   0.1103290319442749,\n",
       "   0.11014825105667114,\n",
       "   0.10514306277036667,\n",
       "   0.1069493219256401,\n",
       "   0.11055457592010498,\n",
       "   0.10777045041322708,\n",
       "   0.10204173624515533,\n",
       "   0.1025034636259079,\n",
       "   0.09397955983877182,\n",
       "   0.08772033452987671,\n",
       "   0.0845566838979721,\n",
       "   0.08758015185594559,\n",
       "   0.09060419350862503,\n",
       "   0.09177207201719284,\n",
       "   0.09096129983663559,\n",
       "   0.08567667007446289,\n",
       "   0.08507055044174194,\n",
       "   0.08519046008586884,\n",
       "   0.08494018018245697,\n",
       "   0.09180732816457748,\n",
       "   0.08989449590444565,\n",
       "   0.0849623754620552,\n",
       "   0.08276786655187607,\n",
       "   0.08734796196222305,\n",
       "   0.09138291329145432,\n",
       "   0.08640909940004349,\n",
       "   0.09342627972364426,\n",
       "   0.09462101012468338,\n",
       "   0.0965191125869751,\n",
       "   0.09748321026563644,\n",
       "   0.09408503025770187,\n",
       "   0.10097216069698334,\n",
       "   0.09697633981704712,\n",
       "   0.10190325975418091,\n",
       "   0.102605901658535,\n",
       "   0.09278523176908493,\n",
       "   0.0945480465888977,\n",
       "   0.09087622910737991,\n",
       "   0.08652650564908981,\n",
       "   0.09272589534521103,\n",
       "   0.08758926391601562,\n",
       "   0.08256673812866211,\n",
       "   0.07480160146951675,\n",
       "   0.07166792452335358,\n",
       "   0.0788344144821167,\n",
       "   0.07256877422332764,\n",
       "   0.06868160516023636,\n",
       "   0.07162397354841232,\n",
       "   0.08875817805528641,\n",
       "   0.08608066290616989,\n",
       "   0.09021468460559845,\n",
       "   0.09315727651119232,\n",
       "   0.0922638401389122,\n",
       "   0.08500887453556061,\n",
       "   0.08351515233516693,\n",
       "   0.08408509939908981,\n",
       "   0.08350180089473724,\n",
       "   0.08195959031581879,\n",
       "   0.08682035654783249,\n",
       "   0.08356738090515137,\n",
       "   0.08797941356897354,\n",
       "   0.08793877065181732,\n",
       "   0.09689414501190186,\n",
       "   0.09360402077436447,\n",
       "   0.09156394749879837,\n",
       "   0.08492811769247055,\n",
       "   0.08240214735269547,\n",
       "   0.08897232264280319,\n",
       "   0.08327198773622513,\n",
       "   0.0876479223370552,\n",
       "   0.0903998613357544,\n",
       "   0.08451300859451294,\n",
       "   0.07094965875148773,\n",
       "   0.06675310432910919,\n",
       "   0.05727153643965721,\n",
       "   0.05554301664233208,\n",
       "   0.056303203105926514,\n",
       "   0.0587538480758667,\n",
       "   0.06019895523786545,\n",
       "   0.06692221015691757,\n",
       "   0.06358280032873154,\n",
       "   0.06137343868613243,\n",
       "   0.05745392292737961,\n",
       "   0.05580497533082962,\n",
       "   0.06043819710612297,\n",
       "   0.05849277973175049,\n",
       "   0.055030208081007004,\n",
       "   0.05571262910962105,\n",
       "   0.06037371978163719,\n",
       "   0.05477672815322876,\n",
       "   0.06254997849464417,\n",
       "   0.05748334154486656,\n",
       "   0.05853206664323807,\n",
       "   0.06218181177973747,\n",
       "   0.06784327328205109,\n",
       "   0.07219262421131134,\n",
       "   0.07337246090173721,\n",
       "   0.07562372833490372,\n",
       "   0.08349943906068802,\n",
       "   0.07424912601709366,\n",
       "   0.07935003191232681,\n",
       "   0.07889050245285034,\n",
       "   0.07123146206140518,\n",
       "   0.07074164599180222,\n",
       "   0.07454051822423935,\n",
       "   0.0707162544131279,\n",
       "   0.06976278126239777,\n",
       "   0.0716060996055603,\n",
       "   0.07603258639574051,\n",
       "   0.07709992676973343,\n",
       "   0.08500928431749344,\n",
       "   0.07988200336694717,\n",
       "   0.0822710320353508,\n",
       "   0.08151078969240189,\n",
       "   0.08767594397068024,\n",
       "   0.07608173787593842,\n",
       "   0.07990376651287079,\n",
       "   0.08099653571844101,\n",
       "   0.0804215595126152,\n",
       "   0.07853250950574875,\n",
       "   0.07702075690031052,\n",
       "   0.07563506811857224,\n",
       "   0.07495827227830887,\n",
       "   0.07347334921360016,\n",
       "   0.07406795024871826,\n",
       "   0.07259315252304077,\n",
       "   0.07715349644422531,\n",
       "   0.07118253409862518,\n",
       "   0.07165998220443726,\n",
       "   0.07299534231424332,\n",
       "   0.07614276558160782,\n",
       "   0.07894809544086456,\n",
       "   0.08024556934833527,\n",
       "   0.08626695722341537,\n",
       "   0.08947042375802994,\n",
       "   0.08734619617462158,\n",
       "   0.0804317519068718,\n",
       "   0.07701443135738373,\n",
       "   0.07571642100811005,\n",
       "   0.08219470083713531,\n",
       "   0.09189675748348236,\n",
       "   0.08698543161153793,\n",
       "   0.08849496394395828,\n",
       "   0.08371126651763916,\n",
       "   0.07854437828063965,\n",
       "   0.0810314267873764,\n",
       "   0.08272425085306168,\n",
       "   0.0833425521850586,\n",
       "   0.07941966503858566,\n",
       "   0.08336420357227325,\n",
       "   0.0850372314453125,\n",
       "   0.09343291819095612,\n",
       "   0.09458308666944504,\n",
       "   0.09960276633501053,\n",
       "   0.10238460451364517,\n",
       "   0.10065174102783203,\n",
       "   0.10081542283296585,\n",
       "   0.10509104281663895,\n",
       "   0.1044275164604187,\n",
       "   0.09340756386518478,\n",
       "   0.10180412232875824,\n",
       "   0.09127066284418106,\n",
       "   0.10058777034282684,\n",
       "   0.09218541532754898,\n",
       "   0.09913799911737442,\n",
       "   0.09731175005435944,\n",
       "   0.09866707026958466,\n",
       "   0.09188558161258698,\n",
       "   0.09399402141571045,\n",
       "   0.09129142761230469,\n",
       "   0.09377038478851318,\n",
       "   0.08739973604679108,\n",
       "   0.08216558396816254,\n",
       "   0.08919917792081833,\n",
       "   0.09113019704818726,\n",
       "   0.08736244589090347,\n",
       "   0.0915185809135437,\n",
       "   0.09105294197797775,\n",
       "   0.10172466188669205,\n",
       "   0.10561615973711014,\n",
       "   0.09150578081607819,\n",
       "   0.09540621191263199,\n",
       "   0.08798518031835556,\n",
       "   0.08876518905162811,\n",
       "   0.09222599118947983,\n",
       "   0.09416359663009644,\n",
       "   0.08269917964935303,\n",
       "   0.07768416404724121,\n",
       "   0.08077692985534668,\n",
       "   0.08503018319606781,\n",
       "   0.09220423549413681,\n",
       "   0.08989381790161133,\n",
       "   0.09097731858491898,\n",
       "   0.09324660152196884,\n",
       "   0.09172052890062332,\n",
       "   0.09110061824321747,\n",
       "   0.08944537490606308,\n",
       "   0.08885014802217484,\n",
       "   0.08965554088354111,\n",
       "   0.08373437076807022,\n",
       "   0.0930827409029007,\n",
       "   0.10390682518482208,\n",
       "   0.0985320657491684,\n",
       "   0.09783013164997101,\n",
       "   0.1085662692785263,\n",
       "   0.10028516501188278,\n",
       "   0.0961448922753334,\n",
       "   0.09688283503055573,\n",
       "   0.09666363149881363,\n",
       "   0.11126530170440674,\n",
       "   0.13037428259849548,\n",
       "   0.1257847398519516,\n",
       "   0.12298096716403961,\n",
       "   0.11932503432035446,\n",
       "   0.12252265959978104,\n",
       "   0.12757202982902527,\n",
       "   0.12124813348054886,\n",
       "   0.11548250168561935,\n",
       "   0.10643967241048813,\n",
       "   0.11684508621692657,\n",
       "   0.09711558371782303,\n",
       "   0.11978186666965485,\n",
       "   0.12083850800991058,\n",
       "   0.11206109076738358,\n",
       "   0.10026546567678452,\n",
       "   0.09319353103637695,\n",
       "   0.09494786709547043,\n",
       "   0.08520643413066864,\n",
       "   0.09618359059095383,\n",
       "   0.10735249519348145,\n",
       "   0.11128780990839005,\n",
       "   0.11294657737016678,\n",
       "   0.12199123203754425,\n",
       "   0.12175030261278152,\n",
       "   0.13949696719646454,\n",
       "   0.1402452439069748,\n",
       "   0.1575181484222412,\n",
       "   0.15939639508724213,\n",
       "   0.12989145517349243,\n",
       "   0.12004201114177704,\n",
       "   0.11197036504745483,\n",
       "   0.1155666932463646,\n",
       "   0.10375718027353287,\n",
       "   0.09397240728139877,\n",
       "   0.09914735704660416,\n",
       "   0.10322483628988266,\n",
       "   0.10544450581073761,\n",
       "   0.09930287301540375,\n",
       "   0.10599855333566666,\n",
       "   0.1081814169883728,\n",
       "   0.12571829557418823,\n",
       "   0.14206786453723907,\n",
       "   0.17752967774868011,\n",
       "   0.16248629987239838,\n",
       "   0.1455381065607071,\n",
       "   0.1509859263896942,\n",
       "   0.1425294727087021,\n",
       "   0.1368590146303177,\n",
       "   0.15059176087379456,\n",
       "   0.15420496463775635,\n",
       "   0.14311596751213074,\n",
       "   0.13297352194786072,\n",
       "   0.12241638451814651,\n",
       "   0.12955884635448456,\n",
       "   0.13456568121910095,\n",
       "   0.14044062793254852,\n",
       "   0.1603030115365982,\n",
       "   0.17163191735744476,\n",
       "   0.16808785498142242,\n",
       "   0.17083682119846344,\n",
       "   0.17761564254760742,\n",
       "   0.18004284799098969,\n",
       "   0.18565736711025238,\n",
       "   0.18867024779319763,\n",
       "   0.17769719660282135,\n",
       "   0.18548515439033508,\n",
       "   0.17889325320720673,\n",
       "   0.18050436675548553,\n",
       "   0.18097181618213654,\n",
       "   0.2114103138446808,\n",
       "   0.20984168350696564,\n",
       "   0.21257655322551727,\n",
       "   0.20059670507907867,\n",
       "   0.21019324660301208,\n",
       "   0.2004104107618332,\n",
       "   0.1980402022600174,\n",
       "   0.18120484054088593,\n",
       "   0.20206306874752045,\n",
       "   0.18441574275493622,\n",
       "   0.1966084986925125,\n",
       "   0.1984035223722458,\n",
       "   0.18772412836551666,\n",
       "   0.21692559123039246,\n",
       "   0.21802160143852234,\n",
       "   0.18057753145694733,\n",
       "   0.18185201287269592,\n",
       "   0.17703260481357574,\n",
       "   0.16936184465885162,\n",
       "   0.16397225856781006,\n",
       "   0.16829310357570648,\n",
       "   0.19140838086605072,\n",
       "   0.21956084668636322,\n",
       "   0.22084543108940125,\n",
       "   0.21808026731014252,\n",
       "   0.22421108186244965,\n",
       "   0.23434624075889587,\n",
       "   0.23262111842632294,\n",
       "   0.21917562186717987,\n",
       "   0.22583726048469543,\n",
       "   0.22735129296779633,\n",
       "   0.21668612957000732,\n",
       "   0.24810905754566193,\n",
       "   0.26694929599761963,\n",
       "   0.23067687451839447,\n",
       "   0.24879789352416992,\n",
       "   0.28165191411972046,\n",
       "   0.325483500957489,\n",
       "   0.278803288936615,\n",
       "   0.29263418912887573,\n",
       "   0.3142354190349579,\n",
       "   0.36452922224998474,\n",
       "   0.3535076975822449,\n",
       "   0.3520147204399109,\n",
       "   0.3359658420085907,\n",
       "   0.29748958349227905,\n",
       "   0.2642495930194855,\n",
       "   0.2665271461009979,\n",
       "   0.2747627794742584,\n",
       "   0.2629283368587494,\n",
       "   0.26907944679260254,\n",
       "   0.2448214292526245,\n",
       "   0.26574790477752686,\n",
       "   0.27493977546691895,\n",
       "   0.3125929832458496,\n",
       "   0.2939073443412781,\n",
       "   0.2875610291957855,\n",
       "   0.3097444474697113,\n",
       "   0.2782430946826935,\n",
       "   0.3306644558906555,\n",
       "   0.33421850204467773,\n",
       "   0.332129567861557,\n",
       "   0.32577943801879883,\n",
       "   0.25786319375038147,\n",
       "   0.25864940881729126,\n",
       "   0.2880093455314636,\n",
       "   0.31400248408317566,\n",
       "   0.3159159719944,\n",
       "   0.3656167984008789,\n",
       "   0.3554019331932068,\n",
       "   0.3462526202201843,\n",
       "   0.3131789565086365,\n",
       "   0.3131941258907318,\n",
       "   0.33847931027412415,\n",
       "   0.33025026321411133,\n",
       "   0.31480422616004944,\n",
       "   0.27908220887184143,\n",
       "   0.3022069036960602,\n",
       "   0.3301107883453369,\n",
       "   0.32859963178634644,\n",
       "   0.34195375442504883,\n",
       "   0.354888379573822,\n",
       "   0.35782140493392944,\n",
       "   0.3909778594970703,\n",
       "   0.37010741233825684,\n",
       "   0.34723201394081116,\n",
       "   0.3493126928806305,\n",
       "   0.3260796368122101,\n",
       "   0.37111958861351013,\n",
       "   0.3332103192806244,\n",
       "   0.32872146368026733,\n",
       "   0.29131197929382324,\n",
       "   0.28282055258750916,\n",
       "   0.3190150558948517,\n",
       "   0.2987239360809326,\n",
       "   0.28865256905555725,\n",
       "   0.26685526967048645,\n",
       "   0.30331987142562866,\n",
       "   0.2898009717464447,\n",
       "   0.31544581055641174,\n",
       "   0.2977502644062042,\n",
       "   0.2941855490207672,\n",
       "   0.29322686791419983,\n",
       "   0.31828993558883667,\n",
       "   0.3459320366382599,\n",
       "   0.4007948935031891,\n",
       "   0.40920066833496094,\n",
       "   0.38242611289024353,\n",
       "   0.40497878193855286,\n",
       "   0.41154053807258606,\n",
       "   0.4695577025413513,\n",
       "   0.4375280737876892,\n",
       "   0.420845091342926,\n",
       "   0.44056203961372375,\n",
       "   0.4753684103488922,\n",
       "   0.4516298770904541,\n",
       "   0.43439456820487976,\n",
       "   0.41288477182388306,\n",
       "   0.42170450091362,\n",
       "   0.4289734661579132],\n",
       "  'voc_loss': [6.076565265655518,\n",
       "   4.14308500289917,\n",
       "   2.5,\n",
       "   1.7919273376464844,\n",
       "   1.4374438524246216,\n",
       "   1.1172336339950562,\n",
       "   0.9622889757156372,\n",
       "   0.8570572733879089,\n",
       "   0.749082088470459,\n",
       "   0.6518610119819641,\n",
       "   0.5835261940956116,\n",
       "   0.5396407842636108,\n",
       "   0.4912680685520172,\n",
       "   0.46896132826805115,\n",
       "   0.4606369435787201,\n",
       "   0.45922520756721497,\n",
       "   0.4518778622150421,\n",
       "   0.450721800327301,\n",
       "   0.4436839520931244,\n",
       "   0.43277859687805176,\n",
       "   0.4305470585823059,\n",
       "   0.4284193813800812,\n",
       "   0.41958004236221313,\n",
       "   0.4169999659061432,\n",
       "   0.41389432549476624,\n",
       "   0.4145282804965973,\n",
       "   0.4132179319858551,\n",
       "   0.413085401058197,\n",
       "   0.41316866874694824,\n",
       "   0.41434231400489807,\n",
       "   0.4117400348186493,\n",
       "   0.4131998121738434,\n",
       "   0.4129399359226227,\n",
       "   0.41094937920570374,\n",
       "   0.41058704257011414,\n",
       "   0.4104810655117035,\n",
       "   0.41013744473457336,\n",
       "   0.4112301766872406,\n",
       "   0.41095635294914246,\n",
       "   0.41192764043807983,\n",
       "   0.4130113124847412,\n",
       "   0.41365233063697815,\n",
       "   0.4130468964576721,\n",
       "   0.41097429394721985,\n",
       "   0.4093000888824463,\n",
       "   0.4099040627479553,\n",
       "   0.4107028543949127,\n",
       "   0.4101724326610565,\n",
       "   0.410973459482193,\n",
       "   0.4112861454486847,\n",
       "   0.41077473759651184,\n",
       "   0.41113898158073425,\n",
       "   0.4098517596721649,\n",
       "   0.4107411205768585,\n",
       "   0.4099322259426117,\n",
       "   0.40997639298439026,\n",
       "   0.4105287492275238,\n",
       "   0.4108024537563324,\n",
       "   0.41042205691337585,\n",
       "   0.41073960065841675,\n",
       "   0.4090462028980255,\n",
       "   0.40976929664611816,\n",
       "   0.40874770283699036,\n",
       "   0.40802714228630066,\n",
       "   0.4093654155731201,\n",
       "   0.40931573510169983,\n",
       "   0.40938445925712585,\n",
       "   0.4111616015434265,\n",
       "   0.411682665348053,\n",
       "   0.4119553565979004,\n",
       "   0.41220173239707947,\n",
       "   0.41233351826667786,\n",
       "   0.4130513668060303,\n",
       "   0.41290634870529175,\n",
       "   0.4120105803012848,\n",
       "   0.4109818935394287,\n",
       "   0.410654217004776,\n",
       "   0.40983787178993225,\n",
       "   0.41045746207237244,\n",
       "   0.4092694818973541,\n",
       "   0.40800389647483826,\n",
       "   0.41127604246139526,\n",
       "   0.4130057692527771,\n",
       "   0.41324329376220703,\n",
       "   0.4119557738304138,\n",
       "   0.4121139943599701,\n",
       "   0.4118170440196991,\n",
       "   0.4108741283416748,\n",
       "   0.41077741980552673,\n",
       "   0.40836092829704285,\n",
       "   0.4083905816078186,\n",
       "   0.4081299901008606,\n",
       "   0.40879005193710327,\n",
       "   0.40909460186958313,\n",
       "   0.4090215861797333,\n",
       "   0.40907952189445496,\n",
       "   0.41190311312675476,\n",
       "   0.4129289984703064,\n",
       "   0.4135240912437439,\n",
       "   0.41404852271080017,\n",
       "   0.4134054183959961,\n",
       "   0.4117133617401123,\n",
       "   0.4113989770412445,\n",
       "   0.4102255702018738,\n",
       "   0.41235047578811646,\n",
       "   0.41168999671936035,\n",
       "   0.4112643897533417,\n",
       "   0.4114687442779541,\n",
       "   0.4127843677997589,\n",
       "   0.41276293992996216,\n",
       "   0.41223734617233276,\n",
       "   0.4133455157279968,\n",
       "   0.4109594523906708,\n",
       "   0.4103615880012512,\n",
       "   0.41174373030662537,\n",
       "   0.4116602838039398,\n",
       "   0.41182252764701843,\n",
       "   0.4139251410961151,\n",
       "   0.4118521213531494,\n",
       "   0.41187915205955505,\n",
       "   0.4106147587299347,\n",
       "   0.41064903140068054,\n",
       "   0.4111846089363098,\n",
       "   0.4106859862804413,\n",
       "   0.4107837378978729,\n",
       "   0.41069671511650085,\n",
       "   0.4109111428260803,\n",
       "   0.40924617648124695,\n",
       "   0.4086635708808899,\n",
       "   0.40993690490722656,\n",
       "   0.4108329117298126,\n",
       "   0.4100950062274933,\n",
       "   0.41090890765190125,\n",
       "   0.41181740164756775,\n",
       "   0.4114537239074707,\n",
       "   0.4103958308696747,\n",
       "   0.4112630784511566,\n",
       "   0.41060715913772583,\n",
       "   0.41020041704177856,\n",
       "   0.4092239439487457,\n",
       "   0.4093605577945709,\n",
       "   0.4082150161266327,\n",
       "   0.4073854386806488,\n",
       "   0.40769511461257935,\n",
       "   0.4073273837566376,\n",
       "   0.406569242477417,\n",
       "   0.40787166357040405,\n",
       "   0.4077067971229553,\n",
       "   0.40732619166374207,\n",
       "   0.4063505530357361,\n",
       "   0.40614053606987,\n",
       "   0.4063580334186554,\n",
       "   0.4064624011516571,\n",
       "   0.40676894783973694,\n",
       "   0.4089326858520508,\n",
       "   0.4084433913230896,\n",
       "   0.40835800766944885,\n",
       "   0.4071356952190399,\n",
       "   0.4074968695640564,\n",
       "   0.4088124632835388,\n",
       "   0.4079202115535736,\n",
       "   0.4099433422088623,\n",
       "   0.4107191860675812,\n",
       "   0.4086136817932129,\n",
       "   0.4089353680610657,\n",
       "   0.4090164601802826,\n",
       "   0.40945371985435486,\n",
       "   0.40991124510765076,\n",
       "   0.41095098853111267,\n",
       "   0.410769522190094,\n",
       "   0.41202351450920105,\n",
       "   0.411435067653656,\n",
       "   0.40957245230674744,\n",
       "   0.4083215296268463,\n",
       "   0.4083065092563629,\n",
       "   0.40867024660110474,\n",
       "   0.4082373082637787,\n",
       "   0.4077487587928772,\n",
       "   0.4080573618412018,\n",
       "   0.40836477279663086,\n",
       "   0.4081569314002991,\n",
       "   0.40753939747810364,\n",
       "   0.40735670924186707,\n",
       "   0.4064883589744568,\n",
       "   0.4077002704143524,\n",
       "   0.4086323380470276,\n",
       "   0.4100453555583954,\n",
       "   0.4109697639942169,\n",
       "   0.41073837876319885,\n",
       "   0.410874605178833,\n",
       "   0.4097875654697418,\n",
       "   0.4100029468536377,\n",
       "   0.4113285541534424,\n",
       "   0.40958166122436523,\n",
       "   0.40841320157051086,\n",
       "   0.4091470539569855,\n",
       "   0.4081738591194153,\n",
       "   0.4082757532596588,\n",
       "   0.40864044427871704,\n",
       "   0.40867847204208374,\n",
       "   0.40995997190475464,\n",
       "   0.4080262780189514,\n",
       "   0.408340722322464,\n",
       "   0.4082874357700348,\n",
       "   0.4077479839324951,\n",
       "   0.40864309668540955,\n",
       "   0.4097990095615387,\n",
       "   0.409088671207428,\n",
       "   0.40860018134117126,\n",
       "   0.40838027000427246,\n",
       "   0.4071972966194153,\n",
       "   0.4074244201183319,\n",
       "   0.4067617356777191,\n",
       "   0.4086006283760071,\n",
       "   0.40802162885665894,\n",
       "   0.4083327353000641,\n",
       "   0.40611153841018677,\n",
       "   0.4056452810764313,\n",
       "   0.40531978011131287,\n",
       "   0.4060044586658478,\n",
       "   0.4062146842479706,\n",
       "   0.40656009316444397,\n",
       "   0.40579915046691895,\n",
       "   0.40591341257095337,\n",
       "   0.40589791536331177,\n",
       "   0.40740257501602173,\n",
       "   0.407243549823761,\n",
       "   0.4073517918586731,\n",
       "   0.40713194012641907,\n",
       "   0.4069775640964508,\n",
       "   0.4060710072517395,\n",
       "   0.4054538905620575,\n",
       "   0.4054969251155853,\n",
       "   0.40638068318367004,\n",
       "   0.40413549542427063,\n",
       "   0.4058894217014313,\n",
       "   0.40855640172958374,\n",
       "   0.4088422954082489,\n",
       "   0.4092086851596832,\n",
       "   0.4066961705684662,\n",
       "   0.4070684313774109,\n",
       "   0.4067355990409851,\n",
       "   0.40541306138038635,\n",
       "   0.40572863817214966,\n",
       "   0.40557003021240234,\n",
       "   0.4060157835483551,\n",
       "   0.4066753387451172,\n",
       "   0.40813395380973816,\n",
       "   0.40763622522354126,\n",
       "   0.40569519996643066,\n",
       "   0.4072512984275818,\n",
       "   0.4075523316860199,\n",
       "   0.40764909982681274,\n",
       "   0.40653935074806213,\n",
       "   0.4046318233013153,\n",
       "   0.40450340509414673,\n",
       "   0.40535375475883484,\n",
       "   0.4060012698173523,\n",
       "   0.4060244560241699,\n",
       "   0.40582820773124695,\n",
       "   0.4046935737133026,\n",
       "   0.40488722920417786,\n",
       "   0.4032873213291168,\n",
       "   0.4031582176685333,\n",
       "   0.4024161994457245,\n",
       "   0.4032738208770752,\n",
       "   0.40477830171585083,\n",
       "   0.4054601788520813,\n",
       "   0.4077601730823517,\n",
       "   0.4080694317817688,\n",
       "   0.4108123183250427,\n",
       "   0.4094712734222412,\n",
       "   0.4092462360858917,\n",
       "   0.4086712896823883,\n",
       "   0.41039371490478516,\n",
       "   0.4100453853607178,\n",
       "   0.410387247800827,\n",
       "   0.40893760323524475,\n",
       "   0.4089590907096863,\n",
       "   0.4104117453098297,\n",
       "   0.4116457998752594,\n",
       "   0.4127083718776703,\n",
       "   0.412832111120224,\n",
       "   0.4128522574901581,\n",
       "   0.4136834144592285,\n",
       "   0.41157039999961853,\n",
       "   0.4119347035884857,\n",
       "   0.4122186601161957,\n",
       "   0.4139551818370819,\n",
       "   0.4137152135372162,\n",
       "   0.4130091965198517,\n",
       "   0.4118809401988983,\n",
       "   0.4131644368171692,\n",
       "   0.4139326214790344,\n",
       "   0.4146256148815155,\n",
       "   0.4153209328651428,\n",
       "   0.41595157980918884,\n",
       "   0.41647428274154663,\n",
       "   0.41603899002075195,\n",
       "   0.4149303436279297,\n",
       "   0.4139948785305023,\n",
       "   0.41266992688179016,\n",
       "   0.4113370478153229,\n",
       "   0.4108739495277405,\n",
       "   0.4136025309562683,\n",
       "   0.41368594765663147,\n",
       "   0.41005808115005493,\n",
       "   0.4079280197620392,\n",
       "   0.4067728817462921,\n",
       "   0.40679430961608887,\n",
       "   0.4077252447605133,\n",
       "   0.4064004123210907,\n",
       "   0.40556079149246216,\n",
       "   0.40472373366355896,\n",
       "   0.4056311547756195,\n",
       "   0.40535691380500793,\n",
       "   0.40551671385765076,\n",
       "   0.4066784679889679,\n",
       "   0.4066202640533447,\n",
       "   0.4079034924507141,\n",
       "   0.4065532386302948,\n",
       "   0.40660861134529114,\n",
       "   0.4069676101207733,\n",
       "   0.40871497988700867,\n",
       "   0.40796926617622375,\n",
       "   0.40923184156417847,\n",
       "   0.4094560444355011,\n",
       "   0.40898361802101135,\n",
       "   0.4090867042541504,\n",
       "   0.41004374623298645,\n",
       "   0.4088567793369293,\n",
       "   0.40855279564857483,\n",
       "   0.40694481134414673,\n",
       "   0.4073038101196289,\n",
       "   0.4071728587150574,\n",
       "   0.40861040353775024,\n",
       "   0.4108874201774597,\n",
       "   0.4096459448337555,\n",
       "   0.41008779406547546,\n",
       "   0.4101961553096771,\n",
       "   0.40877050161361694,\n",
       "   0.4094087779521942,\n",
       "   0.40803131461143494,\n",
       "   0.406955748796463,\n",
       "   0.4055216908454895,\n",
       "   0.4062919318675995,\n",
       "   0.4064522385597229,\n",
       "   0.40759748220443726,\n",
       "   0.40822169184684753,\n",
       "   0.40772491693496704,\n",
       "   0.40881243348121643,\n",
       "   0.4080965220928192,\n",
       "   0.40747693181037903,\n",
       "   0.4060044586658478,\n",
       "   0.40623927116394043,\n",
       "   0.40602821111679077,\n",
       "   0.4062858521938324,\n",
       "   0.40688496828079224,\n",
       "   0.4075046479701996,\n",
       "   0.4081927239894867,\n",
       "   0.40681198239326477,\n",
       "   0.40741780400276184,\n",
       "   0.40852198004722595,\n",
       "   0.408659964799881,\n",
       "   0.41049203276634216,\n",
       "   0.4099311828613281,\n",
       "   0.4109472930431366,\n",
       "   0.4142025113105774,\n",
       "   0.4140538275241852,\n",
       "   0.41161176562309265,\n",
       "   0.410116583108902,\n",
       "   0.4099052846431732,\n",
       "   0.4087028205394745,\n",
       "   0.41092705726623535,\n",
       "   0.41318613290786743,\n",
       "   0.4138234555721283,\n",
       "   0.4139435589313507,\n",
       "   0.41246747970581055,\n",
       "   0.41043487191200256,\n",
       "   0.40998053550720215,\n",
       "   0.4119488298892975,\n",
       "   0.41115185618400574,\n",
       "   0.40942201018333435,\n",
       "   0.40880730748176575,\n",
       "   0.4084968566894531,\n",
       "   0.4089052379131317,\n",
       "   0.4078470766544342,\n",
       "   0.4074373245239258,\n",
       "   0.4084622263908386,\n",
       "   0.40916162729263306,\n",
       "   0.4104131758213043,\n",
       "   0.4071371257305145,\n",
       "   0.40641024708747864,\n",
       "   0.405667245388031,\n",
       "   0.40686896443367004,\n",
       "   0.4069465696811676,\n",
       "   0.40691080689430237,\n",
       "   0.4058879017829895,\n",
       "   0.4052400588989258,\n",
       "   0.40448421239852905,\n",
       "   0.4059443473815918,\n",
       "   0.40662649273872375,\n",
       "   0.40794137120246887,\n",
       "   0.40770992636680603,\n",
       "   0.4063323438167572,\n",
       "   0.4072001278400421,\n",
       "   0.4077393412590027,\n",
       "   0.4077065885066986,\n",
       "   0.40849021077156067,\n",
       "   0.40591365098953247,\n",
       "   0.40574315190315247,\n",
       "   0.40601545572280884,\n",
       "   0.4056816101074219,\n",
       "   0.4067745804786682,\n",
       "   0.4075392782688141,\n",
       "   0.40824005007743835,\n",
       "   0.40905001759529114,\n",
       "   0.4102373719215393,\n",
       "   0.41034558415412903,\n",
       "   0.40886035561561584,\n",
       "   0.4100472033023834,\n",
       "   0.41023606061935425,\n",
       "   0.4100116789340973,\n",
       "   0.4097733497619629,\n",
       "   0.4060845971107483,\n",
       "   0.40475866198539734,\n",
       "   0.4049525558948517,\n",
       "   0.40489181876182556,\n",
       "   0.40591296553611755,\n",
       "   0.40469905734062195,\n",
       "   0.4047956168651581,\n",
       "   0.40488192439079285,\n",
       "   0.40619561076164246,\n",
       "   0.40721407532691956,\n",
       "   0.4090151786804199,\n",
       "   0.408581018447876,\n",
       "   0.4073730409145355,\n",
       "   0.406080961227417,\n",
       "   0.4060033857822418,\n",
       "   0.40550827980041504,\n",
       "   0.405472993850708,\n",
       "   0.406171590089798,\n",
       "   0.4075080454349518,\n",
       "   0.40657493472099304,\n",
       "   0.40621209144592285,\n",
       "   0.4069738984107971,\n",
       "   0.4059426784515381,\n",
       "   0.40460389852523804,\n",
       "   0.40477797389030457,\n",
       "   0.4043577313423157,\n",
       "   0.4052684009075165,\n",
       "   0.4059409499168396,\n",
       "   0.405717134475708,\n",
       "   0.4055355489253998,\n",
       "   0.4045129120349884,\n",
       "   0.4056982100009918,\n",
       "   0.4055604636669159,\n",
       "   0.40603843331336975,\n",
       "   0.4062347114086151,\n",
       "   0.40669378638267517,\n",
       "   0.40637773275375366,\n",
       "   0.4078383147716522,\n",
       "   0.4070070683956146,\n",
       "   0.4069177210330963,\n",
       "   0.4067564308643341,\n",
       "   0.40569835901260376,\n",
       "   0.4052563011646271,\n",
       "   0.40477681159973145,\n",
       "   0.40473848581314087,\n",
       "   0.4050239026546478,\n",
       "   0.40591487288475037,\n",
       "   0.40667906403541565,\n",
       "   0.40718764066696167,\n",
       "   0.4075556695461273,\n",
       "   0.4076686501502991,\n",
       "   0.4076855182647705,\n",
       "   0.40843650698661804,\n",
       "   0.407966285943985,\n",
       "   0.40707287192344666,\n",
       "   0.40720683336257935,\n",
       "   0.40664297342300415,\n",
       "   0.41017693281173706,\n",
       "   0.40905997157096863,\n",
       "   0.4081672728061676,\n",
       "   0.40786412358283997,\n",
       "   0.40767258405685425,\n",
       "   0.4077318012714386,\n",
       "   0.40647047758102417,\n",
       "   0.4062359929084778,\n",
       "   0.40761271119117737,\n",
       "   0.4063764214515686,\n",
       "   0.4069414436817169,\n",
       "   0.40780529379844666,\n",
       "   0.4072495102882385,\n",
       "   0.4068986475467682,\n",
       "   0.40595269203186035,\n",
       "   0.4066454768180847,\n",
       "   0.40670037269592285,\n",
       "   0.40778645873069763,\n",
       "   0.40664950013160706],\n",
       "  'jsc_loss': [0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.39491939544677734,\n",
       "   0.3928961157798767,\n",
       "   0.3915834426879883,\n",
       "   0.3789285123348236,\n",
       "   0.38282695412635803,\n",
       "   0.3832084834575653,\n",
       "   0.3819834887981415,\n",
       "   0.3805612027645111,\n",
       "   0.3800777792930603,\n",
       "   0.3827076554298401,\n",
       "   0.3845846951007843,\n",
       "   0.3895496726036072,\n",
       "   0.3926014304161072,\n",
       "   0.39115339517593384,\n",
       "   0.39564529061317444,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.39634186029434204,\n",
       "   0.3964488208293915,\n",
       "   0.3970305621623993,\n",
       "   0.39382204413414,\n",
       "   0.39131322503089905,\n",
       "   0.3894246518611908,\n",
       "   0.3901582360267639,\n",
       "   0.387844979763031,\n",
       "   0.3887856900691986,\n",
       "   0.38788917660713196,\n",
       "   0.3885910212993622,\n",
       "   0.38970115780830383,\n",
       "   0.3938928246498108,\n",
       "   0.3945388197898865,\n",
       "   0.39310938119888306,\n",
       "   0.3916435241699219,\n",
       "   0.3910292088985443,\n",
       "   0.39486122131347656,\n",
       "   0.39614805579185486,\n",
       "   0.39684370160102844,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3948923647403717,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.39693403244018555,\n",
       "   0.39606380462646484,\n",
       "   0.39439958333969116,\n",
       "   0.39460256695747375,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.39603009819984436,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3924958407878876,\n",
       "   0.3907358944416046,\n",
       "   0.3912143111228943,\n",
       "   0.39121395349502563,\n",
       "   0.3930816650390625,\n",
       "   0.39071008563041687,\n",
       "   0.38824573159217834,\n",
       "   0.38777780532836914,\n",
       "   0.3873927891254425,\n",
       "   0.3862573206424713,\n",
       "   0.38345035910606384,\n",
       "   0.3853796422481537,\n",
       "   0.3860919773578644,\n",
       "   0.38701656460762024,\n",
       "   0.39067092537879944,\n",
       "   0.3914784789085388,\n",
       "   0.38972213864326477,\n",
       "   0.3925951421260834,\n",
       "   0.3933561146259308,\n",
       "   0.3943406641483307,\n",
       "   0.3952874541282654,\n",
       "   0.39067044854164124,\n",
       "   0.3933432400226593,\n",
       "   0.39527925848960876,\n",
       "   0.3952477276325226,\n",
       "   0.3922903835773468,\n",
       "   0.3899543881416321,\n",
       "   0.38726916909217834,\n",
       "   0.388201504945755,\n",
       "   0.3866472840309143,\n",
       "   0.3887959122657776,\n",
       "   0.3912254273891449,\n",
       "   0.38655751943588257,\n",
       "   0.3836781084537506,\n",
       "   0.3814306855201721,\n",
       "   0.3808031380176544,\n",
       "   0.3824000954627991,\n",
       "   0.38478532433509827,\n",
       "   0.3862968981266022,\n",
       "   0.38353246450424194,\n",
       "   0.3885541558265686,\n",
       "   0.3870103061199188,\n",
       "   0.3864805996417999,\n",
       "   0.38918039202690125,\n",
       "   0.3888874650001526,\n",
       "   0.3859170377254486,\n",
       "   0.3860967755317688,\n",
       "   0.3851597011089325,\n",
       "   0.38883188366889954,\n",
       "   0.3922294080257416,\n",
       "   0.39124780893325806,\n",
       "   0.39094606041908264,\n",
       "   0.3879300653934479,\n",
       "   0.3852745592594147,\n",
       "   0.3859653174877167,\n",
       "   0.3868444263935089,\n",
       "   0.3826289474964142,\n",
       "   0.38094058632850647,\n",
       "   0.38241836428642273,\n",
       "   0.38220152258872986,\n",
       "   0.38120588660240173,\n",
       "   0.3816787898540497,\n",
       "   0.3806576728820801,\n",
       "   0.3801529109477997,\n",
       "   0.38446345925331116,\n",
       "   0.385640949010849,\n",
       "   0.38362938165664673,\n",
       "   0.3892596364021301,\n",
       "   0.3867669999599457,\n",
       "   0.3844914436340332,\n",
       "   0.38528749346733093,\n",
       "   0.38727766275405884,\n",
       "   0.38355472683906555,\n",
       "   0.3835122287273407,\n",
       "   0.38056543469429016,\n",
       "   0.3807775676250458,\n",
       "   0.37868553400039673,\n",
       "   0.37718215584754944,\n",
       "   0.3804289400577545,\n",
       "   0.38057899475097656,\n",
       "   0.38035133481025696,\n",
       "   0.3810572624206543,\n",
       "   0.381602019071579,\n",
       "   0.38311702013015747,\n",
       "   0.38322973251342773,\n",
       "   0.38669735193252563,\n",
       "   0.38933563232421875,\n",
       "   0.3896649181842804,\n",
       "   0.39101749658584595,\n",
       "   0.3913991153240204,\n",
       "   0.39273056387901306,\n",
       "   0.3958251178264618,\n",
       "   0.39045530557632446,\n",
       "   0.3911581337451935,\n",
       "   0.3936626613140106,\n",
       "   0.39548757672309875,\n",
       "   0.39307668805122375,\n",
       "   0.39409780502319336,\n",
       "   0.3930343985557556,\n",
       "   0.39121246337890625,\n",
       "   0.3932706415653229,\n",
       "   0.3937895596027374,\n",
       "   0.3912915587425232,\n",
       "   0.39369842410087585,\n",
       "   0.38768380880355835,\n",
       "   0.3906387686729431,\n",
       "   0.39073827862739563,\n",
       "   0.3915252089500427,\n",
       "   0.3902685344219208,\n",
       "   0.38897010684013367,\n",
       "   0.3888605535030365,\n",
       "   0.390884667634964,\n",
       "   0.39118942618370056,\n",
       "   0.39052814245224,\n",
       "   0.3908200263977051,\n",
       "   0.39461612701416016,\n",
       "   0.39593663811683655,\n",
       "   0.39216238260269165,\n",
       "   0.392557829618454,\n",
       "   0.39172765612602234,\n",
       "   0.3935249149799347,\n",
       "   0.39110490679740906,\n",
       "   0.3895416855812073,\n",
       "   0.38747483491897583,\n",
       "   0.3897591233253479,\n",
       "   0.38801997900009155,\n",
       "   0.39108261466026306,\n",
       "   0.39161035418510437,\n",
       "   0.39164072275161743,\n",
       "   0.39155641198158264,\n",
       "   0.39552101492881775,\n",
       "   0.39597395062446594,\n",
       "   0.39427465200424194,\n",
       "   0.3936908543109894,\n",
       "   0.3888949751853943,\n",
       "   0.38954517245292664,\n",
       "   0.39264950156211853,\n",
       "   0.3920598030090332,\n",
       "   0.39383384585380554,\n",
       "   0.3933444023132324,\n",
       "   0.3917679190635681,\n",
       "   0.39170530438423157,\n",
       "   0.3923133313655853,\n",
       "   0.38814565539360046,\n",
       "   0.39353159070014954,\n",
       "   0.3960030972957611,\n",
       "   0.39439064264297485,\n",
       "   0.39019155502319336,\n",
       "   0.3880888521671295,\n",
       "   0.388313353061676,\n",
       "   0.3873598277568817,\n",
       "   0.3880355656147003,\n",
       "   0.38870444893836975,\n",
       "   0.3843267858028412,\n",
       "   0.385887086391449,\n",
       "   0.3924785256385803,\n",
       "   0.3887732923030853,\n",
       "   0.3948935866355896,\n",
       "   0.3962339758872986,\n",
       "   0.3951737582683563,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.39694908261299133,\n",
       "   0.3953079581260681,\n",
       "   0.3942215144634247,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3964627683162689,\n",
       "   0.3949616849422455,\n",
       "   0.396468847990036,\n",
       "   0.3943193256855011,\n",
       "   0.39057302474975586,\n",
       "   0.3859880268573761,\n",
       "   0.38573387265205383,\n",
       "   0.38463422656059265,\n",
       "   0.3838389217853546,\n",
       "   0.3857279419898987,\n",
       "   0.38815996050834656,\n",
       "   0.38739675283432007,\n",
       "   0.3893873393535614,\n",
       "   0.3884212374687195,\n",
       "   0.3896006643772125,\n",
       "   0.3931398391723633,\n",
       "   0.39284494519233704,\n",
       "   0.39509597420692444,\n",
       "   0.39641687273979187,\n",
       "   0.39689382910728455,\n",
       "   0.394483357667923,\n",
       "   0.3959682285785675,\n",
       "   0.3951961100101471,\n",
       "   0.393232136964798,\n",
       "   0.38958364725112915,\n",
       "   0.3926525115966797,\n",
       "   0.3928999900817871,\n",
       "   0.39228296279907227,\n",
       "   0.38976627588272095,\n",
       "   0.39215606451034546,\n",
       "   0.3940076231956482,\n",
       "   0.39067745208740234,\n",
       "   0.389480859041214,\n",
       "   0.38939952850341797,\n",
       "   0.3873179852962494,\n",
       "   0.3863812983036041,\n",
       "   0.3867591321468353,\n",
       "   0.3857530951499939,\n",
       "   0.38867542147636414,\n",
       "   0.3899862468242645,\n",
       "   0.3898431360721588,\n",
       "   0.3908262252807617,\n",
       "   0.38766732811927795,\n",
       "   0.3838787078857422,\n",
       "   0.38422033190727234,\n",
       "   0.38645729422569275,\n",
       "   0.3888072371482849,\n",
       "   0.38498297333717346,\n",
       "   0.3848440945148468,\n",
       "   0.3892991542816162,\n",
       "   0.3899895250797272,\n",
       "   0.3880440890789032,\n",
       "   0.38882240653038025,\n",
       "   0.3910375237464905,\n",
       "   0.3888099491596222,\n",
       "   0.38814204931259155,\n",
       "   0.3885129392147064,\n",
       "   0.38560888171195984,\n",
       "   0.3795216977596283,\n",
       "   0.3779125213623047,\n",
       "   0.3803017735481262,\n",
       "   0.3786751329898834,\n",
       "   0.3807658553123474,\n",
       "   0.38393136858940125,\n",
       "   0.38134637475013733,\n",
       "   0.3812558948993683,\n",
       "   0.3846645951271057,\n",
       "   0.3867669105529785,\n",
       "   0.39038318395614624,\n",
       "   0.3922341763973236,\n",
       "   0.39126840233802795,\n",
       "   0.3931386172771454,\n",
       "   0.3942875266075134,\n",
       "   0.3970305621623993,\n",
       "   0.3969997763633728,\n",
       "   0.3966338336467743,\n",
       "   0.39603498578071594,\n",
       "   0.3957200050354004,\n",
       "   0.39702898263931274,\n",
       "   0.3943362832069397,\n",
       "   0.39576831459999084,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.39292988181114197,\n",
       "   0.39257922768592834,\n",
       "   0.3918342590332031,\n",
       "   0.3970305621623993,\n",
       "   0.3969733417034149,\n",
       "   0.39588287472724915,\n",
       "   0.39556604623794556,\n",
       "   0.3956446647644043,\n",
       "   0.39449208974838257,\n",
       "   0.39042750000953674,\n",
       "   0.3891189992427826,\n",
       "   0.38988643884658813,\n",
       "   0.3863132894039154,\n",
       "   0.38805273175239563,\n",
       "   0.3856239318847656,\n",
       "   0.3866627514362335,\n",
       "   0.38648074865341187,\n",
       "   0.3872039020061493,\n",
       "   0.3862321078777313,\n",
       "   0.38555532693862915,\n",
       "   0.3843183219432831,\n",
       "   0.3838202655315399,\n",
       "   0.3830339312553406,\n",
       "   0.38168027997016907,\n",
       "   0.382255494594574,\n",
       "   0.38387393951416016,\n",
       "   0.3850422501564026,\n",
       "   0.3834209442138672,\n",
       "   0.3830817639827728,\n",
       "   0.3815867602825165,\n",
       "   0.38462549448013306,\n",
       "   0.38863399624824524,\n",
       "   0.38606294989585876,\n",
       "   0.38635364174842834,\n",
       "   0.38747644424438477,\n",
       "   0.3866204619407654,\n",
       "   0.3864130675792694,\n",
       "   0.38553932309150696,\n",
       "   0.38493743538856506,\n",
       "   0.3830385208129883,\n",
       "   0.3822496831417084,\n",
       "   0.38133785128593445,\n",
       "   0.38393959403038025,\n",
       "   0.38414302468299866,\n",
       "   0.3814661502838135,\n",
       "   0.37811312079429626,\n",
       "   0.37984371185302734,\n",
       "   0.38038650155067444,\n",
       "   0.38266631960868835,\n",
       "   0.38292407989501953,\n",
       "   0.38187694549560547,\n",
       "   0.3831957280635834,\n",
       "   0.3865869641304016,\n",
       "   0.38806071877479553,\n",
       "   0.39213308691978455,\n",
       "   0.3967452049255371,\n",
       "   0.39386171102523804,\n",
       "   0.39286407828330994,\n",
       "   0.3910890221595764,\n",
       "   0.3918673098087311,\n",
       "   0.3913009464740753,\n",
       "   0.3881951868534088,\n",
       "   0.3840150833129883,\n",
       "   0.38217779994010925,\n",
       "   0.37992411851882935,\n",
       "   0.3791956305503845,\n",
       "   0.37939977645874023,\n",
       "   0.3816535174846649,\n",
       "   0.383981853723526,\n",
       "   0.3872590661048889,\n",
       "   0.38898447155952454,\n",
       "   0.3872765898704529,\n",
       "   0.3892596364021301,\n",
       "   0.3882947564125061,\n",
       "   0.3850749433040619,\n",
       "   0.38758817315101624,\n",
       "   0.39038902521133423,\n",
       "   0.39046505093574524,\n",
       "   0.3871071934700012,\n",
       "   0.3926558494567871,\n",
       "   0.39189180731773376,\n",
       "   0.39186081290245056,\n",
       "   0.3889869153499603,\n",
       "   0.38734421133995056,\n",
       "   0.3883403241634369,\n",
       "   0.3840898871421814,\n",
       "   0.38488468527793884,\n",
       "   0.3868611752986908,\n",
       "   0.39094823598861694,\n",
       "   0.38710451126098633,\n",
       "   0.3890064060688019,\n",
       "   0.3903038799762726,\n",
       "   0.38873720169067383,\n",
       "   0.3942309021949768,\n",
       "   0.39194488525390625,\n",
       "   0.3918117880821228,\n",
       "   0.3934827446937561,\n",
       "   0.3932647705078125,\n",
       "   0.3950767517089844,\n",
       "   0.3921487629413605,\n",
       "   0.3954518139362335,\n",
       "   0.396474689245224,\n",
       "   0.3902010917663574,\n",
       "   0.38991889357566833,\n",
       "   0.3849629759788513,\n",
       "   0.38641881942749023,\n",
       "   0.3884925842285156,\n",
       "   0.38963577151298523,\n",
       "   0.3939303755760193,\n",
       "   0.39527103304862976,\n",
       "   0.3948517441749573,\n",
       "   0.3912075459957123,\n",
       "   0.3910679817199707,\n",
       "   0.3924301564693451,\n",
       "   0.3908465802669525,\n",
       "   0.3933803141117096,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3970305621623993,\n",
       "   0.3951689898967743,\n",
       "   0.3935597836971283,\n",
       "   0.39135757088661194,\n",
       "   0.39023569226264954,\n",
       "   0.389446884393692,\n",
       "   0.39050722122192383,\n",
       "   0.39147648215293884,\n",
       "   0.395085871219635,\n",
       "   0.39426156878471375,\n",
       "   0.39193984866142273,\n",
       "   0.39306551218032837,\n",
       "   0.3940480053424835,\n",
       "   0.3892706334590912,\n",
       "   0.3899077773094177,\n",
       "   0.390945166349411,\n",
       "   0.3874604403972626,\n",
       "   0.38471296429634094,\n",
       "   0.38527020812034607,\n",
       "   0.38735124468803406,\n",
       "   0.38359594345092773,\n",
       "   0.38317209482192993,\n",
       "   0.38103756308555603,\n",
       "   0.3831748962402344,\n",
       "   0.38641342520713806,\n",
       "   0.38154178857803345,\n",
       "   0.3810937702655792,\n",
       "   0.38489627838134766,\n",
       "   0.38581162691116333,\n",
       "   0.3866086006164551,\n",
       "   0.3860930800437927,\n",
       "   0.3873937427997589,\n",
       "   0.3862994611263275,\n",
       "   0.38562631607055664,\n",
       "   0.3875778019428253,\n",
       "   0.38802194595336914,\n",
       "   0.3860679268836975,\n",
       "   0.38307374715805054,\n",
       "   0.38296571373939514,\n",
       "   0.3827255666255951,\n",
       "   0.3819357454776764,\n",
       "   0.38280758261680603,\n",
       "   0.38281843066215515,\n",
       "   0.3822031021118164,\n",
       "   0.38123592734336853,\n",
       "   0.3816497325897217,\n",
       "   0.3793655037879944,\n",
       "   0.38126733899116516,\n",
       "   0.37937089800834656,\n",
       "   0.37806543707847595,\n",
       "   0.3791290819644928,\n",
       "   0.3764004409313202],\n",
       "  'ff_loss': [4.088380813598633,\n",
       "   2.9480140209198,\n",
       "   1.9751241207122803,\n",
       "   1.472751259803772,\n",
       "   1.3218860626220703,\n",
       "   1.3344329595565796,\n",
       "   1.325624704360962,\n",
       "   1.4340420961380005,\n",
       "   1.4957969188690186,\n",
       "   1.4968693256378174,\n",
       "   1.5489823818206787,\n",
       "   1.6052953004837036,\n",
       "   1.602487325668335,\n",
       "   1.7916059494018555,\n",
       "   1.8737967014312744,\n",
       "   2.006484270095825,\n",
       "   2.1012346744537354,\n",
       "   2.0459606647491455,\n",
       "   2.149240493774414,\n",
       "   2.126187562942505,\n",
       "   2.3642640113830566,\n",
       "   2.4085006713867188,\n",
       "   2.4265151023864746,\n",
       "   2.451667070388794,\n",
       "   2.438859701156616,\n",
       "   2.524676561355591,\n",
       "   2.5600948333740234,\n",
       "   2.6908986568450928,\n",
       "   2.614745616912842,\n",
       "   2.5845396518707275,\n",
       "   2.5288147926330566,\n",
       "   2.405172109603882,\n",
       "   2.426797866821289,\n",
       "   2.370389938354492,\n",
       "   2.45357346534729,\n",
       "   2.3835670948028564,\n",
       "   2.360793352127075,\n",
       "   2.3702855110168457,\n",
       "   2.5067520141601562,\n",
       "   2.52131724357605,\n",
       "   2.4216105937957764,\n",
       "   2.469074010848999,\n",
       "   2.533330202102661,\n",
       "   2.468580961227417,\n",
       "   2.4936068058013916,\n",
       "   2.5769975185394287,\n",
       "   2.6553757190704346,\n",
       "   2.5967350006103516,\n",
       "   2.6241345405578613,\n",
       "   2.6772384643554688,\n",
       "   2.7357680797576904,\n",
       "   2.7815589904785156,\n",
       "   2.7768876552581787,\n",
       "   2.5915791988372803,\n",
       "   2.5511300563812256,\n",
       "   2.4894309043884277,\n",
       "   2.4723706245422363,\n",
       "   2.548149824142456,\n",
       "   2.5891189575195312,\n",
       "   2.5325894355773926,\n",
       "   2.440080404281616,\n",
       "   2.4260315895080566,\n",
       "   2.308826208114624,\n",
       "   2.239499568939209,\n",
       "   2.259165048599243,\n",
       "   2.3702075481414795,\n",
       "   2.39471697807312,\n",
       "   2.477144718170166,\n",
       "   2.6007354259490967,\n",
       "   2.5713794231414795,\n",
       "   2.6656746864318848,\n",
       "   2.6950461864471436,\n",
       "   2.8355565071105957,\n",
       "   2.7860023975372314,\n",
       "   2.748666286468506,\n",
       "   2.7782912254333496,\n",
       "   2.7482590675354004,\n",
       "   2.6938414573669434,\n",
       "   2.6804964542388916,\n",
       "   2.69500994682312,\n",
       "   2.8037400245666504,\n",
       "   2.7777767181396484,\n",
       "   2.792659044265747,\n",
       "   2.7933998107910156,\n",
       "   2.738567590713501,\n",
       "   2.659954309463501,\n",
       "   2.6907808780670166,\n",
       "   2.692394733428955,\n",
       "   2.58579683303833,\n",
       "   2.539137125015259,\n",
       "   2.60546875,\n",
       "   2.654144525527954,\n",
       "   2.7362136840820312,\n",
       "   2.799865245819092,\n",
       "   2.765507221221924,\n",
       "   2.7851200103759766,\n",
       "   2.7938711643218994,\n",
       "   2.746800184249878,\n",
       "   2.805128574371338,\n",
       "   2.806486129760742,\n",
       "   2.7133214473724365,\n",
       "   2.7586562633514404,\n",
       "   2.835981607437134,\n",
       "   2.8105170726776123,\n",
       "   2.773176908493042,\n",
       "   2.77665376663208,\n",
       "   2.8117945194244385,\n",
       "   2.788465738296509,\n",
       "   2.780226469039917,\n",
       "   2.629448175430298,\n",
       "   2.5376484394073486,\n",
       "   2.5144753456115723,\n",
       "   2.5223796367645264,\n",
       "   2.5503315925598145,\n",
       "   2.4829211235046387,\n",
       "   2.4462921619415283,\n",
       "   2.4290449619293213,\n",
       "   2.3798205852508545,\n",
       "   2.3942277431488037,\n",
       "   2.362210988998413,\n",
       "   2.4197869300842285,\n",
       "   2.601320743560791,\n",
       "   2.6492927074432373,\n",
       "   2.6071784496307373,\n",
       "   2.544645309448242,\n",
       "   2.593630075454712,\n",
       "   2.7012972831726074,\n",
       "   2.803617238998413,\n",
       "   2.746441602706909,\n",
       "   2.8528428077697754,\n",
       "   2.798288106918335,\n",
       "   2.686978816986084,\n",
       "   2.8296701908111572,\n",
       "   2.7692415714263916,\n",
       "   2.694643259048462,\n",
       "   2.663264036178589,\n",
       "   2.7180659770965576,\n",
       "   2.6792728900909424,\n",
       "   2.6467835903167725,\n",
       "   2.8441429138183594,\n",
       "   2.9186041355133057,\n",
       "   2.9075939655303955,\n",
       "   2.81925892829895,\n",
       "   2.7070865631103516,\n",
       "   2.846696615219116,\n",
       "   2.8640494346618652,\n",
       "   2.821415662765503,\n",
       "   2.8007583618164062,\n",
       "   2.837535858154297,\n",
       "   2.8169281482696533,\n",
       "   2.8065106868743896,\n",
       "   2.8495917320251465,\n",
       "   2.8886704444885254,\n",
       "   2.991018533706665,\n",
       "   2.9911301136016846,\n",
       "   3.011068820953369,\n",
       "   3.0820908546447754,\n",
       "   3.2175841331481934,\n",
       "   3.219493865966797,\n",
       "   3.050287961959839,\n",
       "   2.9530770778656006,\n",
       "   3.0122218132019043,\n",
       "   2.9708175659179688,\n",
       "   2.9828693866729736,\n",
       "   3.0187833309173584,\n",
       "   3.1387031078338623,\n",
       "   3.187783718109131,\n",
       "   3.0645999908447266,\n",
       "   3.068570852279663,\n",
       "   3.1325809955596924,\n",
       "   3.0413310527801514,\n",
       "   3.041224241256714,\n",
       "   3.10335111618042,\n",
       "   2.9643547534942627,\n",
       "   2.995013952255249,\n",
       "   3.073629140853882,\n",
       "   3.109755754470825,\n",
       "   3.085222005844116,\n",
       "   2.998405933380127,\n",
       "   2.9809162616729736,\n",
       "   2.854362964630127,\n",
       "   2.8333489894866943,\n",
       "   2.8746376037597656,\n",
       "   2.8037006855010986,\n",
       "   2.804028034210205,\n",
       "   2.783796548843384,\n",
       "   2.9327704906463623,\n",
       "   2.8226797580718994,\n",
       "   2.7459070682525635,\n",
       "   2.690748929977417,\n",
       "   2.544398784637451,\n",
       "   2.6366119384765625,\n",
       "   2.6164205074310303,\n",
       "   2.6682965755462646,\n",
       "   2.7291605472564697,\n",
       "   2.753039836883545,\n",
       "   2.6698219776153564,\n",
       "   2.5734283924102783,\n",
       "   2.523426055908203,\n",
       "   2.535604476928711,\n",
       "   2.5586369037628174,\n",
       "   2.5807559490203857,\n",
       "   2.636159896850586,\n",
       "   2.550654888153076,\n",
       "   2.6281332969665527,\n",
       "   2.609140396118164,\n",
       "   2.6720659732818604,\n",
       "   2.855903148651123,\n",
       "   2.865109920501709,\n",
       "   2.8611953258514404,\n",
       "   2.5478057861328125,\n",
       "   2.544840097427368,\n",
       "   2.4745776653289795,\n",
       "   2.5815179347991943,\n",
       "   2.677790403366089,\n",
       "   2.6701111793518066,\n",
       "   2.6686437129974365,\n",
       "   2.598829984664917,\n",
       "   2.6124401092529297,\n",
       "   2.6435258388519287,\n",
       "   2.656207799911499,\n",
       "   2.633632183074951,\n",
       "   2.6715610027313232,\n",
       "   2.4957354068756104,\n",
       "   2.4463958740234375,\n",
       "   2.4663660526275635,\n",
       "   2.420891284942627,\n",
       "   2.400498867034912,\n",
       "   2.5122599601745605,\n",
       "   2.580353260040283,\n",
       "   2.5163934230804443,\n",
       "   2.4107418060302734,\n",
       "   2.42669677734375,\n",
       "   2.445568799972534,\n",
       "   2.4638514518737793,\n",
       "   2.4985203742980957,\n",
       "   2.508838653564453,\n",
       "   2.4566142559051514,\n",
       "   2.472909450531006,\n",
       "   2.4792935848236084,\n",
       "   2.6596434116363525,\n",
       "   2.564755916595459,\n",
       "   2.571075677871704,\n",
       "   2.551805257797241,\n",
       "   2.5628106594085693,\n",
       "   2.6555819511413574,\n",
       "   2.5936801433563232,\n",
       "   2.6172945499420166,\n",
       "   2.737917423248291,\n",
       "   2.645860433578491,\n",
       "   2.580089569091797,\n",
       "   2.517470121383667,\n",
       "   2.4022834300994873,\n",
       "   2.39607572555542,\n",
       "   2.3632376194000244,\n",
       "   2.3768582344055176,\n",
       "   2.436018228530884,\n",
       "   2.332592487335205,\n",
       "   2.211547613143921,\n",
       "   2.2567477226257324,\n",
       "   2.387395143508911,\n",
       "   2.2915799617767334,\n",
       "   2.3731484413146973,\n",
       "   2.392092227935791,\n",
       "   2.5364151000976562,\n",
       "   2.611905336380005,\n",
       "   2.602451801300049,\n",
       "   2.6906185150146484,\n",
       "   2.7103781700134277,\n",
       "   2.773392915725708,\n",
       "   2.6424999237060547,\n",
       "   2.6296310424804688,\n",
       "   2.572171449661255,\n",
       "   2.5467941761016846,\n",
       "   2.56611967086792,\n",
       "   2.385591506958008,\n",
       "   2.3350260257720947,\n",
       "   2.3423264026641846,\n",
       "   2.4710118770599365,\n",
       "   2.467102289199829,\n",
       "   2.4392268657684326,\n",
       "   2.5133559703826904,\n",
       "   2.584670305252075,\n",
       "   2.485177755355835,\n",
       "   2.4939606189727783,\n",
       "   2.667872190475464,\n",
       "   2.6487252712249756,\n",
       "   2.771739959716797,\n",
       "   2.6884772777557373,\n",
       "   2.7515501976013184,\n",
       "   2.7285096645355225,\n",
       "   2.69590425491333,\n",
       "   2.708876132965088,\n",
       "   2.7544682025909424,\n",
       "   2.7636430263519287,\n",
       "   2.8360791206359863,\n",
       "   2.8637759685516357,\n",
       "   2.8681235313415527,\n",
       "   2.932262420654297,\n",
       "   3.004901885986328,\n",
       "   2.9729390144348145,\n",
       "   3.008474111557007,\n",
       "   3.0200159549713135,\n",
       "   2.9446873664855957,\n",
       "   2.9896795749664307,\n",
       "   3.013169050216675,\n",
       "   2.831259250640869,\n",
       "   2.8973960876464844,\n",
       "   2.8743045330047607,\n",
       "   2.67571759223938,\n",
       "   2.6596322059631348,\n",
       "   2.6511142253875732,\n",
       "   2.721729040145874,\n",
       "   2.741685390472412,\n",
       "   2.735039472579956,\n",
       "   2.7309298515319824,\n",
       "   2.9064879417419434,\n",
       "   2.8984172344207764,\n",
       "   2.924913167953491,\n",
       "   2.8773417472839355,\n",
       "   2.8771841526031494,\n",
       "   2.7716639041900635,\n",
       "   2.6628975868225098,\n",
       "   2.6540844440460205,\n",
       "   2.6867568492889404,\n",
       "   2.70993971824646,\n",
       "   2.6383309364318848,\n",
       "   2.5650627613067627,\n",
       "   2.5271379947662354,\n",
       "   2.4373555183410645,\n",
       "   2.4854485988616943,\n",
       "   2.3487372398376465,\n",
       "   2.418382167816162,\n",
       "   2.4934589862823486,\n",
       "   2.5533764362335205,\n",
       "   2.579702138900757,\n",
       "   2.602033853530884,\n",
       "   2.4774515628814697,\n",
       "   2.536512851715088,\n",
       "   2.3417022228240967,\n",
       "   2.4176838397979736,\n",
       "   2.4379513263702393,\n",
       "   2.513953924179077,\n",
       "   2.5543930530548096,\n",
       "   2.605337142944336,\n",
       "   2.504251480102539,\n",
       "   2.579902410507202,\n",
       "   2.629608154296875,\n",
       "   2.6936378479003906,\n",
       "   2.6429121494293213,\n",
       "   2.7849583625793457,\n",
       "   2.7534053325653076,\n",
       "   2.8946948051452637,\n",
       "   2.8795299530029297,\n",
       "   2.8509960174560547,\n",
       "   2.8366596698760986,\n",
       "   2.8373074531555176,\n",
       "   2.7999320030212402,\n",
       "   2.7744803428649902,\n",
       "   2.681654214859009,\n",
       "   2.7543773651123047,\n",
       "   2.714341640472412,\n",
       "   2.6979691982269287,\n",
       "   2.7153830528259277,\n",
       "   2.760460138320923,\n",
       "   2.7671031951904297,\n",
       "   2.788165807723999,\n",
       "   2.779909610748291,\n",
       "   2.780003309249878,\n",
       "   2.8393070697784424,\n",
       "   2.824221134185791,\n",
       "   2.968777894973755,\n",
       "   2.7624666690826416,\n",
       "   2.6751179695129395,\n",
       "   2.5999207496643066,\n",
       "   2.607365131378174,\n",
       "   2.5833194255828857,\n",
       "   2.508948802947998,\n",
       "   2.5830862522125244,\n",
       "   2.6709787845611572,\n",
       "   2.6570210456848145,\n",
       "   2.600306987762451,\n",
       "   2.5314173698425293,\n",
       "   2.534217596054077,\n",
       "   2.5844101905822754,\n",
       "   2.5691335201263428,\n",
       "   2.5955708026885986,\n",
       "   2.579225540161133,\n",
       "   2.6206135749816895,\n",
       "   2.627257823944092,\n",
       "   2.7055981159210205,\n",
       "   2.75508713722229,\n",
       "   2.662370204925537,\n",
       "   2.6740100383758545,\n",
       "   2.6793086528778076,\n",
       "   2.8291454315185547,\n",
       "   2.8064334392547607,\n",
       "   2.8900928497314453,\n",
       "   2.8504068851470947,\n",
       "   2.9270405769348145,\n",
       "   2.995227098464966,\n",
       "   2.8902499675750732,\n",
       "   2.881338119506836,\n",
       "   2.8972480297088623,\n",
       "   2.9227046966552734,\n",
       "   3.055643081665039,\n",
       "   2.916454315185547,\n",
       "   2.88511061668396,\n",
       "   2.878816604614258,\n",
       "   2.762333393096924,\n",
       "   2.760021448135376,\n",
       "   2.595646619796753,\n",
       "   2.5976791381835938,\n",
       "   2.587744951248169,\n",
       "   2.6042609214782715,\n",
       "   2.5950560569763184,\n",
       "   2.5739762783050537,\n",
       "   2.6452982425689697,\n",
       "   2.601734161376953,\n",
       "   2.699566125869751,\n",
       "   2.751447916030884,\n",
       "   2.783787965774536,\n",
       "   2.757847547531128,\n",
       "   2.6829988956451416,\n",
       "   2.641340494155884,\n",
       "   2.751934766769409,\n",
       "   2.8303065299987793,\n",
       "   2.759705066680908,\n",
       "   2.7775352001190186,\n",
       "   2.8401987552642822,\n",
       "   2.9022393226623535,\n",
       "   2.9157767295837402,\n",
       "   2.9229743480682373,\n",
       "   2.857433795928955,\n",
       "   2.801365613937378,\n",
       "   2.7848875522613525,\n",
       "   2.6204044818878174,\n",
       "   2.5340535640716553,\n",
       "   2.420396089553833,\n",
       "   2.3685901165008545,\n",
       "   2.3950297832489014,\n",
       "   2.391648054122925,\n",
       "   2.5181658267974854,\n",
       "   2.454301118850708,\n",
       "   2.5200085639953613,\n",
       "   2.596334457397461,\n",
       "   2.5703601837158203,\n",
       "   2.721261739730835,\n",
       "   2.680940866470337,\n",
       "   2.6020748615264893,\n",
       "   2.6521496772766113,\n",
       "   2.6480586528778076,\n",
       "   2.686413288116455,\n",
       "   2.7794277667999268,\n",
       "   2.852935314178467,\n",
       "   2.8802947998046875,\n",
       "   2.81776762008667,\n",
       "   2.8441579341888428,\n",
       "   2.806993246078491,\n",
       "   2.680711030960083,\n",
       "   2.714637517929077,\n",
       "   2.8205904960632324,\n",
       "   2.7630035877227783,\n",
       "   2.9337964057922363,\n",
       "   2.8684659004211426,\n",
       "   2.817723035812378,\n",
       "   2.8406832218170166,\n",
       "   2.97731614112854,\n",
       "   3.0071468353271484,\n",
       "   3.023796796798706,\n",
       "   2.7863142490386963,\n",
       "   2.7983059883117676,\n",
       "   2.970822811126709,\n",
       "   2.9113852977752686,\n",
       "   2.970883846282959,\n",
       "   2.754868745803833,\n",
       "   2.7740864753723145,\n",
       "   2.8090174198150635,\n",
       "   2.8711864948272705,\n",
       "   2.8513638973236084,\n",
       "   2.907449245452881,\n",
       "   2.862825870513916,\n",
       "   2.92423677444458,\n",
       "   2.8685710430145264,\n",
       "   2.9156248569488525,\n",
       "   3.0106027126312256,\n",
       "   2.770561695098877,\n",
       "   2.7628891468048096,\n",
       "   2.7910120487213135,\n",
       "   2.7678873538970947,\n",
       "   2.711735963821411,\n",
       "   2.7357327938079834,\n",
       "   2.720217227935791,\n",
       "   2.712387800216675,\n",
       "   2.6824424266815186,\n",
       "   2.770578622817993,\n",
       "   2.766817092895508,\n",
       "   2.800511121749878,\n",
       "   2.7310893535614014,\n",
       "   2.697526693344116],\n",
       "  'test_losses': [36.85534593462944,\n",
       "   23.280305832624435,\n",
       "   15.129427403211594,\n",
       "   10.92533192038536,\n",
       "   8.437832683324814,\n",
       "   6.764573782682419,\n",
       "   5.639418929815292,\n",
       "   4.8520627319812775,\n",
       "   4.344166278839111,\n",
       "   3.8450968265533447,\n",
       "   3.5692649483680725,\n",
       "   3.352361172437668,\n",
       "   3.1149553656578064,\n",
       "   3.168280839920044,\n",
       "   3.16649666428566,\n",
       "   3.2227546870708466,\n",
       "   3.2549893260002136,\n",
       "   3.152697265148163,\n",
       "   3.2150834649801254,\n",
       "   3.1712978184223175,\n",
       "   3.3932262659072876,\n",
       "   3.4156572818756104,\n",
       "   3.4215433299541473,\n",
       "   3.43184532225132,\n",
       "   3.4037202298641205,\n",
       "   3.4845271557569504,\n",
       "   3.515304610133171,\n",
       "   3.652275711297989,\n",
       "   3.5668886303901672,\n",
       "   3.525045484304428,\n",
       "   3.4586905390024185,\n",
       "   3.33021392673254,\n",
       "   3.346616216003895,\n",
       "   3.2923424392938614,\n",
       "   3.3710778281092644,\n",
       "   3.302109971642494,\n",
       "   3.2845971807837486,\n",
       "   3.3053905367851257,\n",
       "   3.4366445168852806,\n",
       "   3.4577610194683075,\n",
       "   3.353923812508583,\n",
       "   3.3963020890951157,\n",
       "   3.4637432023882866,\n",
       "   3.4035335183143616,\n",
       "   3.4192122742533684,\n",
       "   3.497517019510269,\n",
       "   3.5742473751306534,\n",
       "   3.5109878256917,\n",
       "   3.5469584241509438,\n",
       "   3.60223226249218,\n",
       "   3.6626877039670944,\n",
       "   3.706684075295925,\n",
       "   3.7026249021291733,\n",
       "   3.511061556637287,\n",
       "   3.470236785709858,\n",
       "   3.4042982310056686,\n",
       "   3.3957840353250504,\n",
       "   3.4627500623464584,\n",
       "   3.505428060889244,\n",
       "   3.4506506621837616,\n",
       "   3.351872704923153,\n",
       "   3.33479193598032,\n",
       "   3.2180538326501846,\n",
       "   3.1520912125706673,\n",
       "   3.1727807596325874,\n",
       "   3.2895543724298477,\n",
       "   3.3111739978194237,\n",
       "   3.3942601829767227,\n",
       "   3.508934833109379,\n",
       "   3.4829741939902306,\n",
       "   3.5802536085247993,\n",
       "   3.614502862095833,\n",
       "   3.7545702010393143,\n",
       "   3.702670283615589,\n",
       "   3.664539322257042,\n",
       "   3.683591440320015,\n",
       "   3.65081375092268,\n",
       "   3.599040165543556,\n",
       "   3.585283510386944,\n",
       "   3.592849262058735,\n",
       "   3.6947183310985565,\n",
       "   3.6811249777674675,\n",
       "   3.6948471292853355,\n",
       "   3.6988720446825027,\n",
       "   3.638523556292057,\n",
       "   3.557514473795891,\n",
       "   3.5846979171037674,\n",
       "   3.592428997159004,\n",
       "   3.4877210408449173,\n",
       "   3.4449886828660965,\n",
       "   3.511109009385109,\n",
       "   3.5621158704161644,\n",
       "   3.6478699520230293,\n",
       "   3.716598428785801,\n",
       "   3.687913991510868,\n",
       "   3.7111044451594353,\n",
       "   3.7179044857621193,\n",
       "   3.667417459189892,\n",
       "   3.732780359685421,\n",
       "   3.7331752255558968,\n",
       "   3.634111575782299,\n",
       "   3.6729890406131744,\n",
       "   3.7474832236766815,\n",
       "   3.713154874742031,\n",
       "   3.6806782111525536,\n",
       "   3.6855456233024597,\n",
       "   3.719625271856785,\n",
       "   3.693201646208763,\n",
       "   3.6820718199014664,\n",
       "   3.5198687836527824,\n",
       "   3.4190368056297302,\n",
       "   3.3931806832551956,\n",
       "   3.403319336473942,\n",
       "   3.436082698404789,\n",
       "   3.372733823955059,\n",
       "   3.3324462100863457,\n",
       "   3.315098315477371,\n",
       "   3.2658265829086304,\n",
       "   3.277750924229622,\n",
       "   3.2482107132673264,\n",
       "   3.3110964819788933,\n",
       "   3.487781308591366,\n",
       "   3.531536467373371,\n",
       "   3.485792003571987,\n",
       "   3.4316088929772377,\n",
       "   3.4879391118884087,\n",
       "   3.5898653343319893,\n",
       "   3.697235755622387,\n",
       "   3.6376562491059303,\n",
       "   3.7445733845233917,\n",
       "   3.6925695464015007,\n",
       "   3.578003279864788,\n",
       "   3.724180206656456,\n",
       "   3.658975899219513,\n",
       "   3.5904186069965363,\n",
       "   3.5584672912955284,\n",
       "   3.603320173919201,\n",
       "   3.5661068856716156,\n",
       "   3.528517909348011,\n",
       "   3.7200462743639946,\n",
       "   3.8051540479063988,\n",
       "   3.789039194583893,\n",
       "   3.692840486764908,\n",
       "   3.578842915594578,\n",
       "   3.712458923459053,\n",
       "   3.733944535255432,\n",
       "   3.6871435940265656,\n",
       "   3.6644244268536568,\n",
       "   3.700040750205517,\n",
       "   3.6955491080880165,\n",
       "   3.6792973205447197,\n",
       "   3.726942017674446,\n",
       "   3.7669756561517715,\n",
       "   3.8672334775328636,\n",
       "   3.8655006140470505,\n",
       "   3.8836063593626022,\n",
       "   3.954885296523571,\n",
       "   4.089278891682625,\n",
       "   4.090552344918251,\n",
       "   3.9290378019213676,\n",
       "   3.8277944028377533,\n",
       "   3.8968419209122658,\n",
       "   3.858811154961586,\n",
       "   3.8780421316623688,\n",
       "   3.9123402163386345,\n",
       "   4.030682630836964,\n",
       "   4.074896119534969,\n",
       "   3.9527385011315346,\n",
       "   3.9589494690299034,\n",
       "   4.017780639231205,\n",
       "   3.9346651509404182,\n",
       "   3.938546746969223,\n",
       "   3.990513265132904,\n",
       "   3.83772374689579,\n",
       "   3.8631079643964767,\n",
       "   3.93078338727355,\n",
       "   3.966806720942259,\n",
       "   3.9430635273456573,\n",
       "   3.8565087020397186,\n",
       "   3.843178413808346,\n",
       "   3.717125914990902,\n",
       "   3.6951099559664726,\n",
       "   3.7341060303151608,\n",
       "   3.6591681763529778,\n",
       "   3.657801814377308,\n",
       "   3.641837190836668,\n",
       "   3.7901691794395447,\n",
       "   3.6795643977820873,\n",
       "   3.603547502309084,\n",
       "   3.552525397390127,\n",
       "   3.399783104658127,\n",
       "   3.5037809908390045,\n",
       "   3.4811690412461758,\n",
       "   3.5285726860165596,\n",
       "   3.592313390225172,\n",
       "   3.621757820248604,\n",
       "   3.5437133759260178,\n",
       "   3.4461815133690834,\n",
       "   3.397231914103031,\n",
       "   3.4152572229504585,\n",
       "   3.4326051250100136,\n",
       "   3.4561522379517555,\n",
       "   3.5144737362861633,\n",
       "   3.4217841401696205,\n",
       "   3.4982636496424675,\n",
       "   3.4838804230093956,\n",
       "   3.5481022521853447,\n",
       "   3.7307285517454147,\n",
       "   3.7395908534526825,\n",
       "   3.739299036562443,\n",
       "   3.4209979847073555,\n",
       "   3.42681897431612,\n",
       "   3.3538709059357643,\n",
       "   3.4644493982195854,\n",
       "   3.5611566677689552,\n",
       "   3.5594642609357834,\n",
       "   3.54260490834713,\n",
       "   3.4760843366384506,\n",
       "   3.491069756448269,\n",
       "   3.518097512423992,\n",
       "   3.534486584365368,\n",
       "   3.5132161304354668,\n",
       "   3.5473858639597893,\n",
       "   3.366798646748066,\n",
       "   3.313855990767479,\n",
       "   3.3361499309539795,\n",
       "   3.2880878150463104,\n",
       "   3.273039720952511,\n",
       "   3.3792788833379745,\n",
       "   3.4433175921440125,\n",
       "   3.381346859037876,\n",
       "   3.284816987812519,\n",
       "   3.299915090203285,\n",
       "   3.327088639140129,\n",
       "   3.350487880408764,\n",
       "   3.3890539780259132,\n",
       "   3.4017718136310577,\n",
       "   3.3429188653826714,\n",
       "   3.356081649661064,\n",
       "   3.3570141345262527,\n",
       "   3.5431280583143234,\n",
       "   3.4604188352823257,\n",
       "   3.4605047330260277,\n",
       "   3.4430594220757484,\n",
       "   3.44912251830101,\n",
       "   3.5371726751327515,\n",
       "   3.4778496772050858,\n",
       "   3.503114439547062,\n",
       "   3.625365048646927,\n",
       "   3.5252946242690086,\n",
       "   3.4612780958414078,\n",
       "   3.3960477113723755,\n",
       "   3.28909932076931,\n",
       "   3.2818323895335197,\n",
       "   3.251311130821705,\n",
       "   3.269474186003208,\n",
       "   3.330183684825897,\n",
       "   3.2268059328198433,\n",
       "   3.112050451338291,\n",
       "   3.1554246842861176,\n",
       "   3.275096945464611,\n",
       "   3.1914111524820328,\n",
       "   3.260551370680332,\n",
       "   3.2909341901540756,\n",
       "   3.4274335876107216,\n",
       "   3.511210985481739,\n",
       "   3.499025210738182,\n",
       "   3.590713992714882,\n",
       "   3.6052200347185135,\n",
       "   3.6686885058879852,\n",
       "   3.5341873168945312,\n",
       "   3.525525212287903,\n",
       "   3.461717411875725,\n",
       "   3.4299140125513077,\n",
       "   3.4554788395762444,\n",
       "   3.2789231538772583,\n",
       "   3.2267833426594734,\n",
       "   3.2334600389003754,\n",
       "   3.3605047687888145,\n",
       "   3.368638224899769,\n",
       "   3.3438068106770515,\n",
       "   3.403951421380043,\n",
       "   3.4796677604317665,\n",
       "   3.3717682883143425,\n",
       "   3.385084643959999,\n",
       "   3.5616548284888268,\n",
       "   3.5446667075157166,\n",
       "   3.6574840247631073,\n",
       "   3.5677839517593384,\n",
       "   3.6299210488796234,\n",
       "   3.6107693761587143,\n",
       "   3.586446724832058,\n",
       "   3.6007416248321533,\n",
       "   3.6443611159920692,\n",
       "   3.65635933727026,\n",
       "   3.7324197366833687,\n",
       "   3.7608176916837692,\n",
       "   3.7620872780680656,\n",
       "   3.825973965227604,\n",
       "   3.9005252942442894,\n",
       "   3.859478212893009,\n",
       "   3.9023688286542892,\n",
       "   3.923772767186165,\n",
       "   3.8397022634744644,\n",
       "   3.8806339353322983,\n",
       "   3.9133337885141373,\n",
       "   3.721904270350933,\n",
       "   3.7801441326737404,\n",
       "   3.758726105093956,\n",
       "   3.5631069019436836,\n",
       "   3.559969127178192,\n",
       "   3.5691448152065277,\n",
       "   3.6377391666173935,\n",
       "   3.656157001852989,\n",
       "   3.6503788456320763,\n",
       "   3.651043601334095,\n",
       "   3.8308450877666473,\n",
       "   3.8194824531674385,\n",
       "   3.8413034602999687,\n",
       "   3.788715474307537,\n",
       "   3.7975822538137436,\n",
       "   3.672021932899952,\n",
       "   3.585682049393654,\n",
       "   3.57935793697834,\n",
       "   3.6038161888718605,\n",
       "   3.6137733086943626,\n",
       "   3.5367488265037537,\n",
       "   3.4660248085856438,\n",
       "   3.4184616953134537,\n",
       "   3.340613417327404,\n",
       "   3.394587755203247,\n",
       "   3.2611570730805397,\n",
       "   3.3301078155636787,\n",
       "   3.419784590601921,\n",
       "   3.4792729392647743,\n",
       "   3.5236923843622208,\n",
       "   3.548732563853264,\n",
       "   3.4402603209018707,\n",
       "   3.500489130616188,\n",
       "   3.272217333316803,\n",
       "   3.33561535179615,\n",
       "   3.3492169082164764,\n",
       "   3.423865221440792,\n",
       "   3.453158713877201,\n",
       "   3.49045517295599,\n",
       "   3.3963535204529762,\n",
       "   3.4760602340102196,\n",
       "   3.529854044318199,\n",
       "   3.5873945206403732,\n",
       "   3.542190946638584,\n",
       "   3.686270534992218,\n",
       "   3.671040415763855,\n",
       "   3.8272735327482224,\n",
       "   3.8447443693876266,\n",
       "   3.8019770830869675,\n",
       "   3.7720999270677567,\n",
       "   3.7796214818954468,\n",
       "   3.7327673882246017,\n",
       "   3.7019257694482803,\n",
       "   3.6220254600048065,\n",
       "   3.700019806623459,\n",
       "   3.65350940823555,\n",
       "   3.625527650117874,\n",
       "   3.6328130438923836,\n",
       "   3.6879874616861343,\n",
       "   3.698220521211624,\n",
       "   3.7259667962789536,\n",
       "   3.7399544566869736,\n",
       "   3.750626489520073,\n",
       "   3.8020452111959457,\n",
       "   3.787424221634865,\n",
       "   3.93763667345047,\n",
       "   3.735151931643486,\n",
       "   3.655845418572426,\n",
       "   3.583243280649185,\n",
       "   3.5769989043474197,\n",
       "   3.562591850757599,\n",
       "   3.4806960374116898,\n",
       "   3.556691810488701,\n",
       "   3.6448552161455154,\n",
       "   3.662257134914398,\n",
       "   3.604496255517006,\n",
       "   3.5400028973817825,\n",
       "   3.531682327389717,\n",
       "   3.595233380794525,\n",
       "   3.575194373726845,\n",
       "   3.5953197926282883,\n",
       "   3.5607317835092545,\n",
       "   3.622227892279625,\n",
       "   3.612702503800392,\n",
       "   3.7039207369089127,\n",
       "   3.748822972178459,\n",
       "   3.6405196636915207,\n",
       "   3.678780674934387,\n",
       "   3.6841233372688293,\n",
       "   3.795865163207054,\n",
       "   3.7745960354804993,\n",
       "   3.8546668738126755,\n",
       "   3.808990642428398,\n",
       "   3.8827561140060425,\n",
       "   3.9584490209817886,\n",
       "   3.8755614310503006,\n",
       "   3.898099973797798,\n",
       "   3.9140981435775757,\n",
       "   3.932192251086235,\n",
       "   4.074642464518547,\n",
       "   3.9489289224147797,\n",
       "   3.9159033745527267,\n",
       "   3.8935896307229996,\n",
       "   3.786740154027939,\n",
       "   3.7850077003240585,\n",
       "   3.6102090179920197,\n",
       "   3.640456721186638,\n",
       "   3.6488130390644073,\n",
       "   3.630817398428917,\n",
       "   3.636183887720108,\n",
       "   3.649562895298004,\n",
       "   3.767880290746689,\n",
       "   3.681831270456314,\n",
       "   3.788165181875229,\n",
       "   3.864736944437027,\n",
       "   3.9488571286201477,\n",
       "   3.910104125738144,\n",
       "   3.839017868041992,\n",
       "   3.775335818529129,\n",
       "   3.8459948003292084,\n",
       "   3.8929914236068726,\n",
       "   3.824388802051544,\n",
       "   3.853287696838379,\n",
       "   3.899974912405014,\n",
       "   3.9715662002563477,\n",
       "   3.9619547724723816,\n",
       "   3.985118955373764,\n",
       "   3.929506540298462,\n",
       "   3.907936751842499,\n",
       "   3.873794734477997,\n",
       "   3.703831136226654,\n",
       "   3.639514744281769,\n",
       "   3.4985729455947876,\n",
       "   3.500033885240555,\n",
       "   3.5295730233192444,\n",
       "   3.521156758069992,\n",
       "   3.6425212919712067,\n",
       "   3.5111694037914276,\n",
       "   3.575716644525528,\n",
       "   3.6846980154514313,\n",
       "   3.6873359084129333,\n",
       "   3.8388121724128723,\n",
       "   3.8483662009239197,\n",
       "   3.757003515958786,\n",
       "   3.7972304821014404,\n",
       "   3.7585361301898956,\n",
       "   3.7955602407455444,\n",
       "   3.9128895103931427,\n",
       "   3.9782057106494904,\n",
       "   3.9922737181186676,\n",
       "   3.8974961638450623,\n",
       "   3.9466648399829865,\n",
       "   3.935278594493866,\n",
       "   3.809069961309433,\n",
       "   3.857017010450363,\n",
       "   3.972587823867798,\n",
       "   3.91773983836174,\n",
       "   4.122637152671814,\n",
       "   4.032790184020996,\n",
       "   3.955366373062134,\n",
       "   3.9805224239826202,\n",
       "   4.095523834228516,\n",
       "   4.166600853204727,\n",
       "   4.145203113555908,\n",
       "   3.90198814868927,\n",
       "   3.879471927881241,\n",
       "   4.047244429588318,\n",
       "   4.019497811794281,\n",
       "   4.05837020277977,\n",
       "   3.8361031115055084,\n",
       "   3.8351898789405823,\n",
       "   3.906912177801132,\n",
       "   3.9541534185409546,\n",
       "   3.9614102840423584,\n",
       "   3.9981419444084167,\n",
       "   3.952814668416977,\n",
       "   4.014101415872574,\n",
       "   3.9830501973629,\n",
       "   4.05548894405365,\n",
       "   4.2021439373493195,\n",
       "   3.9704598784446716,\n",
       "   3.9345113039016724,\n",
       "   3.9841625690460205,\n",
       "   3.969848185777664,\n",
       "   3.970488518476486,\n",
       "   3.962405413389206,\n",
       "   3.9301035404205322,\n",
       "   3.9418490827083588,\n",
       "   3.9440749883651733,\n",
       "   4.009428530931473,\n",
       "   3.987228035926819,\n",
       "   3.99816170334816,\n",
       "   3.939709395170212,\n",
       "   3.9095501005649567],\n",
       "  'pce_acc': [3232.432861328125,\n",
       "   2154.1962890625,\n",
       "   1375.995361328125,\n",
       "   757.6536865234375,\n",
       "   339.88494873046875,\n",
       "   236.44065856933594,\n",
       "   172.9612274169922,\n",
       "   164.34127807617188,\n",
       "   151.23931884765625,\n",
       "   138.15847778320312,\n",
       "   128.21139526367188,\n",
       "   118.59400177001953,\n",
       "   109.06597137451172,\n",
       "   101.99144744873047,\n",
       "   96.40255737304688,\n",
       "   90.25684356689453,\n",
       "   85.13442993164062,\n",
       "   83.7981185913086,\n",
       "   84.39527893066406,\n",
       "   84.96637725830078,\n",
       "   84.10348510742188,\n",
       "   83.48675537109375,\n",
       "   83.58740997314453,\n",
       "   82.64057159423828,\n",
       "   82.37342834472656,\n",
       "   81.79739379882812,\n",
       "   82.0654067993164,\n",
       "   82.79422760009766,\n",
       "   82.6668701171875,\n",
       "   81.56004333496094,\n",
       "   81.14069366455078,\n",
       "   80.32218933105469,\n",
       "   80.14004516601562,\n",
       "   80.50431823730469,\n",
       "   80.02922058105469,\n",
       "   80.03751373291016,\n",
       "   80.57040405273438,\n",
       "   81.35682678222656,\n",
       "   80.67985534667969,\n",
       "   81.45295715332031,\n",
       "   81.32083129882812,\n",
       "   80.75584411621094,\n",
       "   80.575927734375,\n",
       "   81.36184692382812,\n",
       "   80.37110900878906,\n",
       "   79.70536041259766,\n",
       "   79.42487335205078,\n",
       "   79.05992126464844,\n",
       "   79.84551239013672,\n",
       "   80.172119140625,\n",
       "   80.4739990234375,\n",
       "   80.2466812133789,\n",
       "   80.47447204589844,\n",
       "   79.5452651977539,\n",
       "   79.59441375732422,\n",
       "   79.15510559082031,\n",
       "   80.3371810913086,\n",
       "   79.11404418945312,\n",
       "   79.01602172851562,\n",
       "   79.21367645263672,\n",
       "   78.45844268798828,\n",
       "   77.87290954589844,\n",
       "   76.29026794433594,\n",
       "   78.51465606689453,\n",
       "   78.32146453857422,\n",
       "   79.10698699951172,\n",
       "   77.5474853515625,\n",
       "   76.86663818359375,\n",
       "   75.42082214355469,\n",
       "   66.55517578125,\n",
       "   65.72604370117188,\n",
       "   79.52108001708984,\n",
       "   64.04183959960938,\n",
       "   80.3004150390625,\n",
       "   82.19700622558594,\n",
       "   115.66365814208984,\n",
       "   105.74829864501953,\n",
       "   109.13497161865234,\n",
       "   89.53260803222656,\n",
       "   90.42204284667969,\n",
       "   97.93834686279297,\n",
       "   90.762939453125,\n",
       "   77.52877807617188,\n",
       "   77.54686737060547,\n",
       "   77.75301361083984,\n",
       "   76.95471954345703,\n",
       "   70.58464050292969,\n",
       "   76.23506927490234,\n",
       "   76.55894470214844,\n",
       "   78.98163604736328,\n",
       "   78.54568481445312,\n",
       "   79.02922058105469,\n",
       "   79.67816925048828,\n",
       "   79.86463165283203,\n",
       "   80.44065856933594,\n",
       "   80.71290588378906,\n",
       "   79.99547576904297,\n",
       "   78.7196273803711,\n",
       "   80.54859924316406,\n",
       "   80.01641082763672,\n",
       "   77.67547607421875,\n",
       "   79.16337585449219,\n",
       "   79.10388946533203,\n",
       "   78.42572021484375,\n",
       "   78.6471176147461,\n",
       "   79.15438079833984,\n",
       "   76.735595703125,\n",
       "   61.26746368408203,\n",
       "   69.53022003173828,\n",
       "   61.29131317138672,\n",
       "   70.81890106201172,\n",
       "   84.75634765625,\n",
       "   65.74188232421875,\n",
       "   57.984310150146484,\n",
       "   64.96148681640625,\n",
       "   84.23356628417969,\n",
       "   103.52651977539062,\n",
       "   106.56712341308594,\n",
       "   105.52407836914062,\n",
       "   107.9290542602539,\n",
       "   84.82510375976562,\n",
       "   92.53304290771484,\n",
       "   113.88480377197266,\n",
       "   111.76922607421875,\n",
       "   87.22241973876953,\n",
       "   86.78328704833984,\n",
       "   97.16307830810547,\n",
       "   84.17259979248047,\n",
       "   83.21292877197266,\n",
       "   64.57771301269531,\n",
       "   65.29275512695312,\n",
       "   63.06156539916992,\n",
       "   57.83256149291992,\n",
       "   60.73862838745117,\n",
       "   76.75897216796875,\n",
       "   75.38402557373047,\n",
       "   55.577796936035156,\n",
       "   77.24352264404297,\n",
       "   73.0577621459961,\n",
       "   82.33094024658203,\n",
       "   69.76264953613281,\n",
       "   91.5093765258789,\n",
       "   111.56478881835938,\n",
       "   139.6837921142578,\n",
       "   141.84579467773438,\n",
       "   118.52677154541016,\n",
       "   151.1597442626953,\n",
       "   147.58364868164062,\n",
       "   133.0078887939453,\n",
       "   80.74784088134766,\n",
       "   125.38017272949219,\n",
       "   93.77518463134766,\n",
       "   81.82340240478516,\n",
       "   81.78561401367188,\n",
       "   111.40161895751953,\n",
       "   98.98348999023438,\n",
       "   107.61799621582031,\n",
       "   137.05955505371094,\n",
       "   133.35472106933594,\n",
       "   110.2484130859375,\n",
       "   125.36622619628906,\n",
       "   124.54527282714844,\n",
       "   109.20606994628906,\n",
       "   80.70913696289062,\n",
       "   68.70209503173828,\n",
       "   93.04217529296875,\n",
       "   110.086181640625,\n",
       "   131.72560119628906,\n",
       "   116.90718078613281,\n",
       "   118.43862915039062,\n",
       "   111.12905883789062,\n",
       "   76.853515625,\n",
       "   83.0994873046875,\n",
       "   121.28113555908203,\n",
       "   137.5724639892578,\n",
       "   188.58314514160156,\n",
       "   211.04718017578125,\n",
       "   219.366455078125,\n",
       "   253.96099853515625,\n",
       "   245.8813934326172,\n",
       "   242.8348846435547,\n",
       "   219.03662109375,\n",
       "   233.52816772460938,\n",
       "   232.550048828125,\n",
       "   235.9615020751953,\n",
       "   232.24703979492188,\n",
       "   218.4610137939453,\n",
       "   217.40354919433594,\n",
       "   187.24142456054688,\n",
       "   155.08177185058594,\n",
       "   174.82296752929688,\n",
       "   153.06666564941406,\n",
       "   162.51502990722656,\n",
       "   147.9690704345703,\n",
       "   132.9071044921875,\n",
       "   121.96501159667969,\n",
       "   126.47706604003906,\n",
       "   118.9337387084961,\n",
       "   121.69149017333984,\n",
       "   115.50157928466797,\n",
       "   140.7406005859375,\n",
       "   116.62928771972656,\n",
       "   127.36952209472656,\n",
       "   152.8019561767578,\n",
       "   182.49676513671875,\n",
       "   211.6687469482422,\n",
       "   208.5186309814453,\n",
       "   230.55763244628906,\n",
       "   257.4754333496094,\n",
       "   255.36557006835938,\n",
       "   266.0709533691406,\n",
       "   290.62646484375,\n",
       "   255.0345001220703,\n",
       "   302.58447265625,\n",
       "   316.8327331542969,\n",
       "   342.2789306640625,\n",
       "   294.4337463378906,\n",
       "   310.94781494140625,\n",
       "   277.6219482421875,\n",
       "   268.2527160644531,\n",
       "   257.5511474609375,\n",
       "   250.07876586914062,\n",
       "   261.24700927734375,\n",
       "   269.12518310546875,\n",
       "   238.05032348632812,\n",
       "   222.507568359375,\n",
       "   204.7356414794922,\n",
       "   231.13885498046875,\n",
       "   225.90374755859375,\n",
       "   243.5194091796875,\n",
       "   207.35401916503906,\n",
       "   227.26673889160156,\n",
       "   228.62257385253906,\n",
       "   221.7247772216797,\n",
       "   203.56228637695312,\n",
       "   222.8555908203125,\n",
       "   246.20578002929688,\n",
       "   252.11642456054688,\n",
       "   256.0302734375,\n",
       "   247.32601928710938,\n",
       "   289.54571533203125,\n",
       "   313.83209228515625,\n",
       "   295.56622314453125,\n",
       "   268.1449890136719,\n",
       "   259.4622497558594,\n",
       "   250.98971557617188,\n",
       "   264.7167663574219,\n",
       "   275.746337890625,\n",
       "   292.2740783691406,\n",
       "   292.1646728515625,\n",
       "   312.4761962890625,\n",
       "   327.5570068359375,\n",
       "   358.88140869140625,\n",
       "   334.1043395996094,\n",
       "   348.0553894042969,\n",
       "   354.9443054199219,\n",
       "   367.5960693359375,\n",
       "   390.8776550292969,\n",
       "   410.79010009765625,\n",
       "   420.2106018066406,\n",
       "   380.8050537109375,\n",
       "   410.6798095703125,\n",
       "   362.0332336425781,\n",
       "   408.1907653808594,\n",
       "   393.502197265625,\n",
       "   409.3009338378906,\n",
       "   399.79669189453125,\n",
       "   390.008544921875,\n",
       "   370.96527099609375,\n",
       "   359.1871643066406,\n",
       "   341.4537353515625,\n",
       "   337.6731872558594,\n",
       "   307.9892578125,\n",
       "   298.503662109375,\n",
       "   327.6953125,\n",
       "   326.6280517578125,\n",
       "   315.96673583984375,\n",
       "   345.60137939453125,\n",
       "   361.9776916503906,\n",
       "   409.10687255859375,\n",
       "   425.94476318359375,\n",
       "   368.1798400878906,\n",
       "   396.68768310546875,\n",
       "   368.656005859375,\n",
       "   375.2036437988281,\n",
       "   383.3086853027344,\n",
       "   397.6506042480469,\n",
       "   370.8052062988281,\n",
       "   357.6695861816406,\n",
       "   375.9576416015625,\n",
       "   379.830810546875,\n",
       "   404.0482177734375,\n",
       "   397.4962158203125,\n",
       "   404.7578430175781,\n",
       "   398.4504089355469,\n",
       "   388.20953369140625,\n",
       "   391.1348876953125,\n",
       "   394.09881591796875,\n",
       "   395.59161376953125,\n",
       "   396.4761657714844,\n",
       "   367.3353576660156,\n",
       "   411.6140441894531,\n",
       "   446.5828857421875,\n",
       "   432.27117919921875,\n",
       "   435.12994384765625,\n",
       "   458.1940612792969,\n",
       "   432.7782287597656,\n",
       "   420.1354064941406,\n",
       "   414.235595703125,\n",
       "   419.94305419921875,\n",
       "   455.0126953125,\n",
       "   508.5194091796875,\n",
       "   498.6876525878906,\n",
       "   492.51202392578125,\n",
       "   488.7266540527344,\n",
       "   491.847900390625,\n",
       "   500.5068664550781,\n",
       "   485.6621398925781,\n",
       "   472.3577880859375,\n",
       "   446.1849670410156,\n",
       "   468.427978515625,\n",
       "   414.6878662109375,\n",
       "   485.3329162597656,\n",
       "   488.5810241699219,\n",
       "   466.576904296875,\n",
       "   433.276123046875,\n",
       "   414.650634765625,\n",
       "   425.40582275390625,\n",
       "   389.25634765625,\n",
       "   425.7978820800781,\n",
       "   446.03057861328125,\n",
       "   457.82708740234375,\n",
       "   447.1382751464844,\n",
       "   471.2760925292969,\n",
       "   473.7803955078125,\n",
       "   528.6248168945312,\n",
       "   534.7711181640625,\n",
       "   572.7597045898438,\n",
       "   573.52099609375,\n",
       "   499.4393310546875,\n",
       "   476.1614685058594,\n",
       "   452.5187072753906,\n",
       "   461.2998962402344,\n",
       "   415.1900634765625,\n",
       "   375.72222900390625,\n",
       "   393.5611267089844,\n",
       "   401.2377624511719,\n",
       "   406.3238220214844,\n",
       "   391.27294921875,\n",
       "   423.31146240234375,\n",
       "   432.8322448730469,\n",
       "   479.52398681640625,\n",
       "   514.845703125,\n",
       "   585.8709716796875,\n",
       "   565.2323608398438,\n",
       "   531.0242919921875,\n",
       "   541.2030029296875,\n",
       "   523.6798095703125,\n",
       "   511.3753662109375,\n",
       "   542.8528442382812,\n",
       "   546.0319213867188,\n",
       "   520.4244384765625,\n",
       "   498.5605773925781,\n",
       "   463.65545654296875,\n",
       "   479.10308837890625,\n",
       "   490.96112060546875,\n",
       "   495.7623596191406,\n",
       "   554.2041015625,\n",
       "   574.3710327148438,\n",
       "   569.0629272460938,\n",
       "   574.44873046875,\n",
       "   580.1521606445312,\n",
       "   593.1170043945312,\n",
       "   609.8927612304688,\n",
       "   619.9053344726562,\n",
       "   605.0472412109375,\n",
       "   623.3848876953125,\n",
       "   614.4859619140625,\n",
       "   619.119384765625,\n",
       "   617.4386596679688,\n",
       "   667.007080078125,\n",
       "   666.9547729492188,\n",
       "   662.5302124023438,\n",
       "   640.649169921875,\n",
       "   658.364990234375,\n",
       "   644.3992309570312,\n",
       "   637.1891479492188,\n",
       "   607.2870483398438,\n",
       "   640.6349487304688,\n",
       "   605.3038330078125,\n",
       "   637.1973876953125,\n",
       "   630.500244140625,\n",
       "   607.1002197265625,\n",
       "   660.8255615234375,\n",
       "   664.5060424804688,\n",
       "   593.3850708007812,\n",
       "   600.4609375,\n",
       "   599.1038818359375,\n",
       "   585.9461669921875,\n",
       "   573.2621459960938,\n",
       "   585.06787109375,\n",
       "   632.76904296875,\n",
       "   675.8580932617188,\n",
       "   680.5594482421875,\n",
       "   674.0130615234375,\n",
       "   686.7233276367188,\n",
       "   698.0353393554688,\n",
       "   695.1258544921875,\n",
       "   668.03173828125,\n",
       "   682.0476684570312,\n",
       "   678.521484375,\n",
       "   663.33740234375,\n",
       "   715.77880859375,\n",
       "   744.0245971679688,\n",
       "   693.1650390625,\n",
       "   722.1805419921875,\n",
       "   765.5429077148438,\n",
       "   824.883544921875,\n",
       "   762.1836547851562,\n",
       "   781.163818359375,\n",
       "   809.129150390625,\n",
       "   868.6825561523438,\n",
       "   855.4512939453125,\n",
       "   855.983154296875,\n",
       "   836.8097534179688,\n",
       "   786.9865112304688,\n",
       "   742.358642578125,\n",
       "   742.2268676757812,\n",
       "   755.3287963867188,\n",
       "   738.7015380859375,\n",
       "   750.5574951171875,\n",
       "   713.71923828125,\n",
       "   747.0054321289062,\n",
       "   757.5294189453125,\n",
       "   807.5731201171875,\n",
       "   785.2261352539062,\n",
       "   780.6812744140625,\n",
       "   802.1070556640625,\n",
       "   762.0402221679688,\n",
       "   831.1253662109375,\n",
       "   832.5941162109375,\n",
       "   825.2103881835938,\n",
       "   825.5474853515625,\n",
       "   740.860595703125,\n",
       "   738.9649047851562,\n",
       "   778.74072265625,\n",
       "   808.5015258789062,\n",
       "   805.46435546875,\n",
       "   865.6663818359375,\n",
       "   855.2844848632812,\n",
       "   844.1392822265625,\n",
       "   806.0050048828125,\n",
       "   804.6068115234375,\n",
       "   836.8052368164062,\n",
       "   826.0665893554688,\n",
       "   801.6654052734375,\n",
       "   752.6683349609375,\n",
       "   785.0650634765625,\n",
       "   822.9860229492188,\n",
       "   821.2490844726562,\n",
       "   837.1013793945312,\n",
       "   855.0206909179688,\n",
       "   859.4589233398438,\n",
       "   898.8762817382812,\n",
       "   876.069091796875,\n",
       "   852.900146484375,\n",
       "   858.8997802734375,\n",
       "   826.62353515625,\n",
       "   877.4296875,\n",
       "   833.6134643554688,\n",
       "   833.4556884765625,\n",
       "   781.4606323242188,\n",
       "   771.1537475585938,\n",
       "   823.4339599609375,\n",
       "   792.0521240234375,\n",
       "   786.2748413085938,\n",
       "   755.151123046875,\n",
       "   806.2977905273438,\n",
       "   786.4867553710938,\n",
       "   822.6563720703125,\n",
       "   798.0365600585938,\n",
       "   792.6630249023438,\n",
       "   787.46826171875,\n",
       "   822.9539184570312,\n",
       "   856.9674682617188,\n",
       "   918.3390502929688,\n",
       "   929.7922973632812,\n",
       "   900.6907348632812,\n",
       "   925.0936279296875,\n",
       "   929.5435180664062,\n",
       "   990.5015869140625,\n",
       "   956.1315307617188,\n",
       "   939.6837158203125,\n",
       "   958.666015625,\n",
       "   991.587158203125,\n",
       "   970.4679565429688,\n",
       "   950.1515502929688,\n",
       "   921.6098022460938,\n",
       "   931.446044921875,\n",
       "   939.6243286132812],\n",
       "  'voc_acc': [234.73696899414062,\n",
       "   207.9479217529297,\n",
       "   169.70928955078125,\n",
       "   148.51437377929688,\n",
       "   135.70989990234375,\n",
       "   122.0772705078125,\n",
       "   114.25650024414062,\n",
       "   108.4172592163086,\n",
       "   101.6748275756836,\n",
       "   94.70640563964844,\n",
       "   88.67451477050781,\n",
       "   84.44618225097656,\n",
       "   78.52774810791016,\n",
       "   75.21484375,\n",
       "   73.76925659179688,\n",
       "   73.35786437988281,\n",
       "   72.23748779296875,\n",
       "   71.88130950927734,\n",
       "   70.2572250366211,\n",
       "   68.80725860595703,\n",
       "   68.6641616821289,\n",
       "   68.64999389648438,\n",
       "   67.6690673828125,\n",
       "   67.37458038330078,\n",
       "   66.67899322509766,\n",
       "   66.65811157226562,\n",
       "   66.53421783447266,\n",
       "   66.39863586425781,\n",
       "   66.34001159667969,\n",
       "   66.394775390625,\n",
       "   65.87125396728516,\n",
       "   66.07825469970703,\n",
       "   65.90504455566406,\n",
       "   65.51912689208984,\n",
       "   65.30239868164062,\n",
       "   65.34269714355469,\n",
       "   65.55099487304688,\n",
       "   65.7536849975586,\n",
       "   65.77210998535156,\n",
       "   66.21402740478516,\n",
       "   66.48133850097656,\n",
       "   66.4736328125,\n",
       "   66.15291595458984,\n",
       "   65.97096252441406,\n",
       "   65.37450408935547,\n",
       "   65.35982513427734,\n",
       "   65.38787078857422,\n",
       "   65.57006072998047,\n",
       "   65.69975280761719,\n",
       "   65.5394287109375,\n",
       "   65.46307373046875,\n",
       "   66.08079528808594,\n",
       "   65.5338134765625,\n",
       "   65.94270324707031,\n",
       "   65.6514663696289,\n",
       "   65.7113037109375,\n",
       "   65.90594482421875,\n",
       "   65.87257385253906,\n",
       "   65.64461517333984,\n",
       "   65.81698608398438,\n",
       "   65.0472183227539,\n",
       "   65.37515258789062,\n",
       "   64.99276733398438,\n",
       "   65.05648803710938,\n",
       "   65.68622589111328,\n",
       "   65.55642700195312,\n",
       "   65.11308288574219,\n",
       "   66.24227905273438,\n",
       "   66.42383575439453,\n",
       "   66.61007690429688,\n",
       "   66.73409271240234,\n",
       "   66.97452545166016,\n",
       "   67.31136322021484,\n",
       "   67.1888656616211,\n",
       "   66.33332824707031,\n",
       "   65.49209594726562,\n",
       "   65.53335571289062,\n",
       "   65.01946258544922,\n",
       "   65.22795867919922,\n",
       "   65.07634735107422,\n",
       "   64.6768569946289,\n",
       "   65.55172729492188,\n",
       "   65.84491729736328,\n",
       "   65.77764129638672,\n",
       "   65.46348571777344,\n",
       "   65.25692749023438,\n",
       "   65.96183776855469,\n",
       "   65.78567504882812,\n",
       "   65.82357025146484,\n",
       "   64.69892883300781,\n",
       "   64.71599578857422,\n",
       "   64.94461059570312,\n",
       "   64.7769546508789,\n",
       "   64.8298568725586,\n",
       "   64.90451049804688,\n",
       "   64.81356048583984,\n",
       "   65.42991638183594,\n",
       "   65.69026184082031,\n",
       "   65.5108642578125,\n",
       "   65.71305847167969,\n",
       "   65.72264099121094,\n",
       "   65.31022644042969,\n",
       "   65.624755859375,\n",
       "   65.10746765136719,\n",
       "   65.80438995361328,\n",
       "   65.67155456542969,\n",
       "   65.66389465332031,\n",
       "   65.66398620605469,\n",
       "   65.72376251220703,\n",
       "   65.67476654052734,\n",
       "   65.8757553100586,\n",
       "   65.93112182617188,\n",
       "   65.23795318603516,\n",
       "   65.46411895751953,\n",
       "   66.11665344238281,\n",
       "   66.28295135498047,\n",
       "   66.26138305664062,\n",
       "   67.17031860351562,\n",
       "   66.02010345458984,\n",
       "   66.09256744384766,\n",
       "   66.20306396484375,\n",
       "   66.02264404296875,\n",
       "   65.70667266845703,\n",
       "   66.11036682128906,\n",
       "   66.2961654663086,\n",
       "   66.50572967529297,\n",
       "   66.61485290527344,\n",
       "   65.9186019897461,\n",
       "   65.66976165771484,\n",
       "   66.28596496582031,\n",
       "   66.6019058227539,\n",
       "   66.33103942871094,\n",
       "   66.64938354492188,\n",
       "   66.88025665283203,\n",
       "   66.70199584960938,\n",
       "   66.41655731201172,\n",
       "   66.59735107421875,\n",
       "   65.66699981689453,\n",
       "   65.46350860595703,\n",
       "   65.57437133789062,\n",
       "   65.63207244873047,\n",
       "   65.01702117919922,\n",
       "   65.02486419677734,\n",
       "   65.36897277832031,\n",
       "   65.19659423828125,\n",
       "   65.07044219970703,\n",
       "   65.60226440429688,\n",
       "   65.37300872802734,\n",
       "   64.40591430664062,\n",
       "   64.35093688964844,\n",
       "   64.66111755371094,\n",
       "   64.34373474121094,\n",
       "   64.18528747558594,\n",
       "   64.50265502929688,\n",
       "   64.62450408935547,\n",
       "   64.85098266601562,\n",
       "   64.80738830566406,\n",
       "   64.68140411376953,\n",
       "   64.57349395751953,\n",
       "   64.93811798095703,\n",
       "   64.59493255615234,\n",
       "   64.77379608154297,\n",
       "   65.02653503417969,\n",
       "   64.4893798828125,\n",
       "   64.76591491699219,\n",
       "   64.44857788085938,\n",
       "   65.2453842163086,\n",
       "   65.3846435546875,\n",
       "   65.72551727294922,\n",
       "   65.09384155273438,\n",
       "   65.0530776977539,\n",
       "   65.04318237304688,\n",
       "   64.74691009521484,\n",
       "   64.52931213378906,\n",
       "   64.64189147949219,\n",
       "   65.34910583496094,\n",
       "   65.05084228515625,\n",
       "   65.05641174316406,\n",
       "   64.82452392578125,\n",
       "   65.05717468261719,\n",
       "   64.332763671875,\n",
       "   64.23107147216797,\n",
       "   64.33930969238281,\n",
       "   64.23351287841797,\n",
       "   64.47318267822266,\n",
       "   64.82610321044922,\n",
       "   64.92444610595703,\n",
       "   65.29193878173828,\n",
       "   64.92340850830078,\n",
       "   64.6353530883789,\n",
       "   65.01493072509766,\n",
       "   65.46697998046875,\n",
       "   65.21562194824219,\n",
       "   65.27847290039062,\n",
       "   65.647216796875,\n",
       "   65.66645812988281,\n",
       "   65.6583251953125,\n",
       "   65.8036117553711,\n",
       "   65.96321868896484,\n",
       "   66.1298828125,\n",
       "   66.67477416992188,\n",
       "   65.92247009277344,\n",
       "   65.99854278564453,\n",
       "   65.96126556396484,\n",
       "   65.46443176269531,\n",
       "   65.67517852783203,\n",
       "   65.66139221191406,\n",
       "   65.35420227050781,\n",
       "   65.41571044921875,\n",
       "   65.52803802490234,\n",
       "   65.03902435302734,\n",
       "   65.18629455566406,\n",
       "   64.5434341430664,\n",
       "   65.07384490966797,\n",
       "   65.06180572509766,\n",
       "   64.67534637451172,\n",
       "   64.10597229003906,\n",
       "   64.07369995117188,\n",
       "   64.28240203857422,\n",
       "   64.6654281616211,\n",
       "   64.68324279785156,\n",
       "   64.73755645751953,\n",
       "   64.54798889160156,\n",
       "   64.56610107421875,\n",
       "   64.38465881347656,\n",
       "   65.38046264648438,\n",
       "   65.60301971435547,\n",
       "   65.66078186035156,\n",
       "   65.51283264160156,\n",
       "   65.42516326904297,\n",
       "   64.94020080566406,\n",
       "   64.68632507324219,\n",
       "   64.6657943725586,\n",
       "   64.87718200683594,\n",
       "   64.07721710205078,\n",
       "   64.7982177734375,\n",
       "   65.88416290283203,\n",
       "   66.0375747680664,\n",
       "   66.26740264892578,\n",
       "   65.20256042480469,\n",
       "   65.24720001220703,\n",
       "   64.75547790527344,\n",
       "   64.43402099609375,\n",
       "   64.56060028076172,\n",
       "   64.40563201904297,\n",
       "   64.59612274169922,\n",
       "   64.10945129394531,\n",
       "   64.32575225830078,\n",
       "   64.21459197998047,\n",
       "   63.931358337402344,\n",
       "   64.01738739013672,\n",
       "   64.60981750488281,\n",
       "   64.1158676147461,\n",
       "   64.25270080566406,\n",
       "   63.55080032348633,\n",
       "   63.587677001953125,\n",
       "   63.74615478515625,\n",
       "   63.85108947753906,\n",
       "   63.950984954833984,\n",
       "   64.01123046875,\n",
       "   63.74946975708008,\n",
       "   63.738525390625,\n",
       "   63.2730598449707,\n",
       "   63.11906433105469,\n",
       "   62.83270263671875,\n",
       "   63.50079345703125,\n",
       "   63.803558349609375,\n",
       "   64.14146423339844,\n",
       "   64.36962890625,\n",
       "   64.1755599975586,\n",
       "   64.44561004638672,\n",
       "   64.30592346191406,\n",
       "   64.2398681640625,\n",
       "   64.08280944824219,\n",
       "   64.30303192138672,\n",
       "   64.58749389648438,\n",
       "   65.89114379882812,\n",
       "   65.38078308105469,\n",
       "   65.48492431640625,\n",
       "   65.36101531982422,\n",
       "   65.53248596191406,\n",
       "   65.17724609375,\n",
       "   65.55274963378906,\n",
       "   65.34869384765625,\n",
       "   64.91493225097656,\n",
       "   64.76792907714844,\n",
       "   65.1455078125,\n",
       "   65.37259674072266,\n",
       "   64.92471313476562,\n",
       "   64.95948791503906,\n",
       "   64.92889404296875,\n",
       "   65.65469360351562,\n",
       "   65.70587921142578,\n",
       "   65.7537612915039,\n",
       "   65.62178802490234,\n",
       "   66.21452331542969,\n",
       "   65.92768859863281,\n",
       "   65.63760375976562,\n",
       "   65.66233825683594,\n",
       "   65.23653411865234,\n",
       "   64.91818237304688,\n",
       "   64.78614044189453,\n",
       "   64.51826477050781,\n",
       "   64.5391616821289,\n",
       "   64.8421630859375,\n",
       "   65.12522888183594,\n",
       "   64.55770111083984,\n",
       "   64.14488220214844,\n",
       "   63.91075897216797,\n",
       "   63.974571228027344,\n",
       "   64.19281005859375,\n",
       "   64.071533203125,\n",
       "   63.706260681152344,\n",
       "   63.4501953125,\n",
       "   64.33878326416016,\n",
       "   64.60850524902344,\n",
       "   64.64783477783203,\n",
       "   65.13585662841797,\n",
       "   65.28631591796875,\n",
       "   65.01142120361328,\n",
       "   64.71304321289062,\n",
       "   64.86339569091797,\n",
       "   65.1983413696289,\n",
       "   65.66621398925781,\n",
       "   65.76085662841797,\n",
       "   66.27435302734375,\n",
       "   66.01441192626953,\n",
       "   65.97740173339844,\n",
       "   66.07882690429688,\n",
       "   66.56675720214844,\n",
       "   66.1465072631836,\n",
       "   65.97017669677734,\n",
       "   65.0187759399414,\n",
       "   64.81267547607422,\n",
       "   64.587890625,\n",
       "   64.8077163696289,\n",
       "   64.87251281738281,\n",
       "   64.93024444580078,\n",
       "   65.37449645996094,\n",
       "   65.52014923095703,\n",
       "   64.77149963378906,\n",
       "   64.62681579589844,\n",
       "   64.7210922241211,\n",
       "   64.53369140625,\n",
       "   64.56717681884766,\n",
       "   65.16605377197266,\n",
       "   65.21987915039062,\n",
       "   65.04202270507812,\n",
       "   64.91545867919922,\n",
       "   64.72675323486328,\n",
       "   65.261474609375,\n",
       "   65.00225067138672,\n",
       "   64.86446380615234,\n",
       "   64.37445068359375,\n",
       "   64.42903137207031,\n",
       "   64.3260269165039,\n",
       "   64.19835662841797,\n",
       "   65.1966781616211,\n",
       "   65.21961975097656,\n",
       "   65.59095764160156,\n",
       "   64.75305938720703,\n",
       "   64.815673828125,\n",
       "   64.92162322998047,\n",
       "   65.0093002319336,\n",
       "   65.46327209472656,\n",
       "   64.46879577636719,\n",
       "   64.49971771240234,\n",
       "   64.94754791259766,\n",
       "   64.90997314453125,\n",
       "   64.603759765625,\n",
       "   64.26836395263672,\n",
       "   64.02008819580078,\n",
       "   64.62487030029297,\n",
       "   64.60527801513672,\n",
       "   64.82638549804688,\n",
       "   64.72225189208984,\n",
       "   64.7593002319336,\n",
       "   65.06275939941406,\n",
       "   65.21412658691406,\n",
       "   65.225830078125,\n",
       "   64.51748657226562,\n",
       "   64.94263458251953,\n",
       "   64.64020538330078,\n",
       "   64.93981170654297,\n",
       "   65.58171844482422,\n",
       "   65.71952819824219,\n",
       "   65.21446990966797,\n",
       "   65.00540161132812,\n",
       "   64.08639526367188,\n",
       "   64.14176940917969,\n",
       "   64.5067138671875,\n",
       "   64.06207275390625,\n",
       "   63.9775390625,\n",
       "   63.68299865722656,\n",
       "   64.1373062133789,\n",
       "   64.37631225585938,\n",
       "   64.23551177978516,\n",
       "   63.616310119628906,\n",
       "   63.52473068237305,\n",
       "   63.41106414794922,\n",
       "   64.25341033935547,\n",
       "   65.2262191772461,\n",
       "   65.13460540771484,\n",
       "   65.60223388671875,\n",
       "   64.98137664794922,\n",
       "   65.14116668701172,\n",
       "   65.40122985839844,\n",
       "   65.77128601074219,\n",
       "   66.07605743408203,\n",
       "   65.13563537597656,\n",
       "   65.08793640136719,\n",
       "   64.8109359741211,\n",
       "   64.9307861328125,\n",
       "   65.08614349365234,\n",
       "   64.96695709228516,\n",
       "   65.70001220703125,\n",
       "   66.12325286865234,\n",
       "   66.6690444946289,\n",
       "   66.56216430664062,\n",
       "   65.67240905761719,\n",
       "   65.8524398803711,\n",
       "   65.89058685302734,\n",
       "   65.35681915283203,\n",
       "   65.35562896728516,\n",
       "   64.87728118896484,\n",
       "   64.51707458496094,\n",
       "   64.04705047607422,\n",
       "   63.79475402832031,\n",
       "   63.44352340698242,\n",
       "   63.67396545410156,\n",
       "   64.07503509521484,\n",
       "   64.82006072998047,\n",
       "   64.95667266845703,\n",
       "   65.40013122558594,\n",
       "   65.22865295410156,\n",
       "   65.41978454589844,\n",
       "   65.09476470947266,\n",
       "   64.41925811767578,\n",
       "   64.40930938720703,\n",
       "   64.76412963867188,\n",
       "   64.98201751708984,\n",
       "   65.42100524902344,\n",
       "   65.98001861572266,\n",
       "   65.49791717529297,\n",
       "   65.24169921875,\n",
       "   65.42779541015625,\n",
       "   64.77066040039062,\n",
       "   64.22027587890625,\n",
       "   64.13927459716797,\n",
       "   63.92012405395508,\n",
       "   64.5343246459961,\n",
       "   64.80680847167969,\n",
       "   64.99681091308594,\n",
       "   64.74378967285156,\n",
       "   64.501953125,\n",
       "   64.76494598388672,\n",
       "   65.03277587890625,\n",
       "   65.34043884277344,\n",
       "   65.48673248291016,\n",
       "   65.67994689941406,\n",
       "   65.35810089111328,\n",
       "   65.94600677490234,\n",
       "   65.53189086914062,\n",
       "   65.23590087890625,\n",
       "   65.22603607177734,\n",
       "   64.67841339111328,\n",
       "   64.46227264404297,\n",
       "   64.0252685546875,\n",
       "   64.46944427490234,\n",
       "   64.712158203125,\n",
       "   65.11090087890625,\n",
       "   65.1103515625,\n",
       "   65.83511352539062,\n",
       "   66.01514434814453,\n",
       "   66.0444107055664,\n",
       "   66.05010986328125,\n",
       "   66.26597595214844,\n",
       "   66.07231140136719,\n",
       "   65.70160675048828,\n",
       "   65.7179946899414,\n",
       "   65.51128387451172,\n",
       "   66.7782974243164,\n",
       "   66.42056274414062,\n",
       "   66.1731185913086,\n",
       "   65.99317932128906,\n",
       "   65.9308090209961,\n",
       "   66.00523376464844,\n",
       "   65.31238555908203,\n",
       "   64.90654754638672,\n",
       "   64.98988342285156,\n",
       "   65.1219711303711,\n",
       "   65.12361145019531,\n",
       "   65.4343032836914,\n",
       "   65.15457153320312,\n",
       "   64.9892807006836,\n",
       "   64.71041107177734,\n",
       "   64.44551086425781,\n",
       "   65.10189056396484,\n",
       "   65.36537170410156,\n",
       "   65.1037368774414],\n",
       "  'jsc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.87557220458984,\n",
       "   99.75528717041016,\n",
       "   99.67669677734375,\n",
       "   98.89501953125,\n",
       "   99.14067077636719,\n",
       "   99.16447448730469,\n",
       "   99.08790588378906,\n",
       "   98.99845886230469,\n",
       "   98.9679183959961,\n",
       "   99.13321685791016,\n",
       "   99.2499771118164,\n",
       "   99.5540542602539,\n",
       "   99.73768615722656,\n",
       "   99.65086364746094,\n",
       "   99.91847229003906,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.93608856201172,\n",
       "   99.94602966308594,\n",
       "   100.0,\n",
       "   99.69973754882812,\n",
       "   99.46038055419922,\n",
       "   99.27742004394531,\n",
       "   99.3487777709961,\n",
       "   99.12246704101562,\n",
       "   99.21495056152344,\n",
       "   99.12682342529297,\n",
       "   99.19586181640625,\n",
       "   99.30435180664062,\n",
       "   99.7064208984375,\n",
       "   99.76737213134766,\n",
       "   99.63216400146484,\n",
       "   99.49212646484375,\n",
       "   99.43302154541016,\n",
       "   99.79768371582031,\n",
       "   99.91805267333984,\n",
       "   99.98268127441406,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.80060577392578,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.9910659790039,\n",
       "   99.91019439697266,\n",
       "   99.7542495727539,\n",
       "   99.77336883544922,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.90705871582031,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.57372283935547,\n",
       "   99.40470886230469,\n",
       "   99.45085906982422,\n",
       "   99.4508285522461,\n",
       "   99.62953186035156,\n",
       "   99.4022216796875,\n",
       "   99.16194152832031,\n",
       "   99.11582946777344,\n",
       "   99.07776641845703,\n",
       "   98.96489715576172,\n",
       "   98.681640625,\n",
       "   98.87698364257812,\n",
       "   98.94837951660156,\n",
       "   99.04048156738281,\n",
       "   99.39842987060547,\n",
       "   99.47628021240234,\n",
       "   99.306396484375,\n",
       "   99.58320617675781,\n",
       "   99.65559387207031,\n",
       "   99.74870300292969,\n",
       "   99.8376693725586,\n",
       "   99.39838409423828,\n",
       "   99.65437316894531,\n",
       "   99.83690643310547,\n",
       "   99.83394622802734,\n",
       "   99.5541000366211,\n",
       "   99.32898712158203,\n",
       "   99.0655288696289,\n",
       "   99.15758514404297,\n",
       "   99.00377655029297,\n",
       "   99.21595764160156,\n",
       "   99.4519271850586,\n",
       "   98.99482727050781,\n",
       "   98.70484924316406,\n",
       "   98.47394561767578,\n",
       "   98.4087142944336,\n",
       "   98.57405090332031,\n",
       "   98.8171157836914,\n",
       "   98.9688491821289,\n",
       "   98.69000244140625,\n",
       "   99.19224548339844,\n",
       "   99.03985595703125,\n",
       "   98.9871597290039,\n",
       "   99.25357055664062,\n",
       "   99.22492218017578,\n",
       "   98.9308853149414,\n",
       "   98.94886016845703,\n",
       "   98.85486602783203,\n",
       "   99.21947479248047,\n",
       "   99.54827117919922,\n",
       "   99.45408630371094,\n",
       "   99.42500305175781,\n",
       "   99.130859375,\n",
       "   98.86641693115234,\n",
       "   98.93571472167969,\n",
       "   99.02337646484375,\n",
       "   98.59756469726562,\n",
       "   98.42301940917969,\n",
       "   98.57593536376953,\n",
       "   98.5536117553711,\n",
       "   98.45061492919922,\n",
       "   98.49964141845703,\n",
       "   98.39353942871094,\n",
       "   98.34075927734375,\n",
       "   98.78457641601562,\n",
       "   98.9032211303711,\n",
       "   98.69989013671875,\n",
       "   99.26131439208984,\n",
       "   99.01568603515625,\n",
       "   98.78740692138672,\n",
       "   98.86771392822266,\n",
       "   99.06636810302734,\n",
       "   98.6922836303711,\n",
       "   98.68795013427734,\n",
       "   98.38391876220703,\n",
       "   98.40604400634766,\n",
       "   98.18609619140625,\n",
       "   98.02559661865234,\n",
       "   98.36965942382812,\n",
       "   98.38533020019531,\n",
       "   98.36154174804688,\n",
       "   98.43516540527344,\n",
       "   98.49168395996094,\n",
       "   98.64759063720703,\n",
       "   98.65911865234375,\n",
       "   99.00875854492188,\n",
       "   99.26873016357422,\n",
       "   99.30082702636719,\n",
       "   99.43190002441406,\n",
       "   99.4686508178711,\n",
       "   99.59611511230469,\n",
       "   99.887939453125,\n",
       "   99.37757110595703,\n",
       "   99.44544982910156,\n",
       "   99.68464660644531,\n",
       "   99.85639190673828,\n",
       "   99.62905883789062,\n",
       "   99.72579193115234,\n",
       "   99.62503051757812,\n",
       "   99.45067596435547,\n",
       "   99.6474838256836,\n",
       "   99.69666290283203,\n",
       "   99.45829772949219,\n",
       "   99.68804168701172,\n",
       "   99.1065444946289,\n",
       "   99.39531707763672,\n",
       "   99.40494537353516,\n",
       "   99.48076629638672,\n",
       "   99.3594741821289,\n",
       "   99.2330093383789,\n",
       "   99.22228240966797,\n",
       "   99.41907501220703,\n",
       "   99.4484634399414,\n",
       "   99.3846206665039,\n",
       "   99.41283416748047,\n",
       "   99.77464294433594,\n",
       "   99.8983383178711,\n",
       "   99.54185485839844,\n",
       "   99.57963562011719,\n",
       "   99.50020599365234,\n",
       "   99.67160034179688,\n",
       "   99.44032287597656,\n",
       "   99.28883361816406,\n",
       "   99.08589172363281,\n",
       "   99.31000518798828,\n",
       "   99.13970947265625,\n",
       "   99.43817138671875,\n",
       "   99.48894500732422,\n",
       "   99.49186706542969,\n",
       "   99.48377227783203,\n",
       "   99.85952758789062,\n",
       "   99.90181732177734,\n",
       "   99.74247741699219,\n",
       "   99.68732452392578,\n",
       "   99.22565460205078,\n",
       "   99.28916931152344,\n",
       "   99.58838653564453,\n",
       "   99.53204345703125,\n",
       "   99.70085144042969,\n",
       "   99.65447998046875,\n",
       "   99.50407409667969,\n",
       "   99.4980697631836,\n",
       "   99.55628967285156,\n",
       "   99.15208435058594,\n",
       "   99.67223358154297,\n",
       "   99.904541015625,\n",
       "   99.75341796875,\n",
       "   99.35201263427734,\n",
       "   99.14649963378906,\n",
       "   99.16859436035156,\n",
       "   99.07451629638672,\n",
       "   99.14125061035156,\n",
       "   99.20698547363281,\n",
       "   98.77074432373047,\n",
       "   98.92788696289062,\n",
       "   99.57207489013672,\n",
       "   99.21373748779297,\n",
       "   99.80072784423828,\n",
       "   99.92604064941406,\n",
       "   99.82701110839844,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.99246215820312,\n",
       "   99.83958435058594,\n",
       "   99.73747253417969,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.94732666015625,\n",
       "   99.80712127685547,\n",
       "   99.94789123535156,\n",
       "   99.74668884277344,\n",
       "   99.38896179199219,\n",
       "   98.93798065185547,\n",
       "   98.91252899169922,\n",
       "   98.80184936523438,\n",
       "   98.72122192382812,\n",
       "   98.91194152832031,\n",
       "   99.15350341796875,\n",
       "   99.07816314697266,\n",
       "   99.27377319335938,\n",
       "   99.17919158935547,\n",
       "   99.29457092285156,\n",
       "   99.63505554199219,\n",
       "   99.60700225830078,\n",
       "   99.81971740722656,\n",
       "   99.94306182861328,\n",
       "   99.9873275756836,\n",
       "   99.76215362548828,\n",
       "   99.90129089355469,\n",
       "   99.82910919189453,\n",
       "   99.6438217163086,\n",
       "   99.29291534423828,\n",
       "   99.58866882324219,\n",
       "   99.61225128173828,\n",
       "   99.55339050292969,\n",
       "   99.31069946289062,\n",
       "   99.54125213623047,\n",
       "   99.71727752685547,\n",
       "   99.39906311035156,\n",
       "   99.28289794921875,\n",
       "   99.27497100830078,\n",
       "   99.07037353515625,\n",
       "   98.97726440429688,\n",
       "   99.01490020751953,\n",
       "   98.91445922851562,\n",
       "   99.20414733886719,\n",
       "   99.33208465576172,\n",
       "   99.31816864013672,\n",
       "   99.41343688964844,\n",
       "   99.10491943359375,\n",
       "   98.72526550292969,\n",
       "   98.75994873046875,\n",
       "   98.9848403930664,\n",
       "   99.2170639038086,\n",
       "   98.83705139160156,\n",
       "   98.82304382324219,\n",
       "   99.26517486572266,\n",
       "   99.33238983154297,\n",
       "   99.14208984375,\n",
       "   99.21854400634766,\n",
       "   99.43383026123047,\n",
       "   99.21733093261719,\n",
       "   99.1517333984375,\n",
       "   99.18819427490234,\n",
       "   98.89999389648438,\n",
       "   98.27446746826172,\n",
       "   98.10382080078125,\n",
       "   98.35635375976562,\n",
       "   98.18498229980469,\n",
       "   98.40482330322266,\n",
       "   98.7306137084961,\n",
       "   98.46519470214844,\n",
       "   98.45580291748047,\n",
       "   98.80492401123047,\n",
       "   99.01567077636719,\n",
       "   99.37059020996094,\n",
       "   99.5487289428711,\n",
       "   99.45606231689453,\n",
       "   99.63494110107422,\n",
       "   99.74369049072266,\n",
       "   100.0,\n",
       "   99.99714660644531,\n",
       "   99.96321868896484,\n",
       "   99.90750885009766,\n",
       "   99.87812042236328,\n",
       "   99.99984741210938,\n",
       "   99.74829864501953,\n",
       "   99.88262939453125,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.6150894165039,\n",
       "   99.58168029785156,\n",
       "   99.51042938232422,\n",
       "   100.0,\n",
       "   99.99469757080078,\n",
       "   99.89332580566406,\n",
       "   99.86373901367188,\n",
       "   99.87107849121094,\n",
       "   99.76296997070312,\n",
       "   99.3748779296875,\n",
       "   99.24756622314453,\n",
       "   99.32238006591797,\n",
       "   98.9704818725586,\n",
       "   99.14293670654297,\n",
       "   98.9015121459961,\n",
       "   99.00531005859375,\n",
       "   98.9871826171875,\n",
       "   99.05906677246094,\n",
       "   98.96238708496094,\n",
       "   98.89462280273438,\n",
       "   98.76988220214844,\n",
       "   98.71931457519531,\n",
       "   98.63909149169922,\n",
       "   98.49978637695312,\n",
       "   98.55915832519531,\n",
       "   98.72477722167969,\n",
       "   98.84303283691406,\n",
       "   98.67863464355469,\n",
       "   98.64398193359375,\n",
       "   98.49010467529297,\n",
       "   98.80097198486328,\n",
       "   99.20008850097656,\n",
       "   98.94547271728516,\n",
       "   98.9745101928711,\n",
       "   99.08605194091797,\n",
       "   99.00110626220703,\n",
       "   98.98043060302734,\n",
       "   98.89302062988281,\n",
       "   98.83246612548828,\n",
       "   98.63955688476562,\n",
       "   98.55856323242188,\n",
       "   98.46430969238281,\n",
       "   98.73145294189453,\n",
       "   98.75210571289062,\n",
       "   98.47761535644531,\n",
       "   98.1252212524414,\n",
       "   98.30833435058594,\n",
       "   98.3652114868164,\n",
       "   98.60140991210938,\n",
       "   98.62783813476562,\n",
       "   98.52012634277344,\n",
       "   98.65563201904297,\n",
       "   98.99776458740234,\n",
       "   99.14373016357422,\n",
       "   99.53905487060547,\n",
       "   99.97355651855469,\n",
       "   99.7034912109375,\n",
       "   99.60883331298828,\n",
       "   99.43878936767578,\n",
       "   99.51359558105469,\n",
       "   99.4592056274414,\n",
       "   99.15696716308594,\n",
       "   98.73912048339844,\n",
       "   98.55116271972656,\n",
       "   98.31678009033203,\n",
       "   98.24007415771484,\n",
       "   98.26161193847656,\n",
       "   98.49701690673828,\n",
       "   98.7357406616211,\n",
       "   99.06452941894531,\n",
       "   99.23442077636719,\n",
       "   99.0662612915039,\n",
       "   99.26131439208984,\n",
       "   99.16676330566406,\n",
       "   98.84632873535156,\n",
       "   99.09709930419922,\n",
       "   99.37115478515625,\n",
       "   99.3785171508789,\n",
       "   99.04946899414062,\n",
       "   99.58898162841797,\n",
       "   99.51595306396484,\n",
       "   99.51297760009766,\n",
       "   99.23465728759766,\n",
       "   99.07296752929688,\n",
       "   99.1712417602539,\n",
       "   98.74671936035156,\n",
       "   98.82714080810547,\n",
       "   99.0250473022461,\n",
       "   99.42521667480469,\n",
       "   99.04920196533203,\n",
       "   99.23656463623047,\n",
       "   99.36289978027344,\n",
       "   99.210205078125,\n",
       "   99.73834991455078,\n",
       "   99.52102661132812,\n",
       "   99.50827026367188,\n",
       "   99.6676025390625,\n",
       "   99.64691925048828,\n",
       "   99.81791687011719,\n",
       "   99.54055786132812,\n",
       "   99.8530502319336,\n",
       "   99.94843292236328,\n",
       "   99.35293579101562,\n",
       "   99.32553100585938,\n",
       "   98.83503723144531,\n",
       "   98.98100280761719,\n",
       "   99.18620300292969,\n",
       "   99.29798889160156,\n",
       "   99.70997619628906,\n",
       "   99.83612060546875,\n",
       "   99.79679870605469,\n",
       "   99.45021057128906,\n",
       "   99.43675994873047,\n",
       "   99.56745147705078,\n",
       "   99.41539764404297,\n",
       "   99.65789031982422,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.82656860351562,\n",
       "   99.67491149902344,\n",
       "   99.46464538574219,\n",
       "   99.3563003540039,\n",
       "   99.27957916259766,\n",
       "   99.38259887695312,\n",
       "   99.47608947753906,\n",
       "   99.81877136230469,\n",
       "   99.74124908447266,\n",
       "   99.52055358886719,\n",
       "   99.62799835205078,\n",
       "   99.72109985351562,\n",
       "   99.26239013671875,\n",
       "   99.32445526123047,\n",
       "   99.42491912841797,\n",
       "   99.08447265625,\n",
       "   98.80980682373047,\n",
       "   98.86598205566406,\n",
       "   99.07366180419922,\n",
       "   98.69647979736328,\n",
       "   98.6532211303711,\n",
       "   98.43311309814453,\n",
       "   98.65350341796875,\n",
       "   98.98046875,\n",
       "   98.48545837402344,\n",
       "   98.43896484375,\n",
       "   98.82831573486328,\n",
       "   98.92032623291016,\n",
       "   98.99992370605469,\n",
       "   98.94849395751953,\n",
       "   99.07786560058594,\n",
       "   98.96910858154297,\n",
       "   98.90174865722656,\n",
       "   99.0960693359375,\n",
       "   99.13990783691406,\n",
       "   98.94596862792969,\n",
       "   98.64315795898438,\n",
       "   98.63209533691406,\n",
       "   98.60748291015625,\n",
       "   98.52619171142578,\n",
       "   98.61589813232422,\n",
       "   98.61701202392578,\n",
       "   98.55376434326172,\n",
       "   98.45372772216797,\n",
       "   98.49662780761719,\n",
       "   98.25800323486328,\n",
       "   98.45699310302734,\n",
       "   98.25857543945312,\n",
       "   98.12014770507812,\n",
       "   98.23303985595703,\n",
       "   97.9413070678711],\n",
       "  'ff_acc': [251.49459838867188,\n",
       "   232.0157470703125,\n",
       "   201.1729736328125,\n",
       "   190.1772918701172,\n",
       "   193.98681640625,\n",
       "   203.25030517578125,\n",
       "   208.8321990966797,\n",
       "   218.8547821044922,\n",
       "   223.1187744140625,\n",
       "   221.5323944091797,\n",
       "   223.30606079101562,\n",
       "   227.45925903320312,\n",
       "   227.49827575683594,\n",
       "   240.23182678222656,\n",
       "   244.90597534179688,\n",
       "   252.60443115234375,\n",
       "   257.5056457519531,\n",
       "   253.56021118164062,\n",
       "   258.559814453125,\n",
       "   256.54510498046875,\n",
       "   271.2593078613281,\n",
       "   274.2674255371094,\n",
       "   275.54180908203125,\n",
       "   277.1762390136719,\n",
       "   276.94915771484375,\n",
       "   282.3923645019531,\n",
       "   284.02313232421875,\n",
       "   291.5863037109375,\n",
       "   287.5194091796875,\n",
       "   285.7720947265625,\n",
       "   282.650634765625,\n",
       "   276.3199462890625,\n",
       "   277.470458984375,\n",
       "   274.3658752441406,\n",
       "   278.9483337402344,\n",
       "   274.7765197753906,\n",
       "   273.67022705078125,\n",
       "   274.4765625,\n",
       "   282.5345764160156,\n",
       "   283.34832763671875,\n",
       "   277.9310607910156,\n",
       "   280.7583312988281,\n",
       "   284.5880126953125,\n",
       "   280.68109130859375,\n",
       "   282.196044921875,\n",
       "   286.9611511230469,\n",
       "   291.1919250488281,\n",
       "   288.49908447265625,\n",
       "   290.33905029296875,\n",
       "   293.2360534667969,\n",
       "   295.90399169921875,\n",
       "   297.861328125,\n",
       "   297.1744689941406,\n",
       "   287.5801696777344,\n",
       "   285.48931884765625,\n",
       "   282.47479248046875,\n",
       "   281.094482421875,\n",
       "   285.4710693359375,\n",
       "   287.7795104980469,\n",
       "   284.9691162109375,\n",
       "   280.16278076171875,\n",
       "   279.83001708984375,\n",
       "   273.1396484375,\n",
       "   269.0729064941406,\n",
       "   269.9835510253906,\n",
       "   276.13836669921875,\n",
       "   277.5689392089844,\n",
       "   281.96221923828125,\n",
       "   288.6434326171875,\n",
       "   286.5953369140625,\n",
       "   291.6591796875,\n",
       "   293.2520751953125,\n",
       "   300.36761474609375,\n",
       "   297.65185546875,\n",
       "   295.8834228515625,\n",
       "   297.5635681152344,\n",
       "   296.1060791015625,\n",
       "   293.28765869140625,\n",
       "   292.21112060546875,\n",
       "   293.376220703125,\n",
       "   298.7142333984375,\n",
       "   297.09027099609375,\n",
       "   297.860107421875,\n",
       "   298.253173828125,\n",
       "   295.0811767578125,\n",
       "   291.169189453125,\n",
       "   292.85540771484375,\n",
       "   292.8362121582031,\n",
       "   287.04229736328125,\n",
       "   284.6778259277344,\n",
       "   288.79425048828125,\n",
       "   291.7059020996094,\n",
       "   295.8254699707031,\n",
       "   299.0686340332031,\n",
       "   297.55938720703125,\n",
       "   298.9850158691406,\n",
       "   299.43743896484375,\n",
       "   296.7361755371094,\n",
       "   299.5578918457031,\n",
       "   299.85675048828125,\n",
       "   294.7686767578125,\n",
       "   297.20819091796875,\n",
       "   301.4552001953125,\n",
       "   299.9554748535156,\n",
       "   298.0191345214844,\n",
       "   298.24139404296875,\n",
       "   300.2753601074219,\n",
       "   298.9607238769531,\n",
       "   298.14263916015625,\n",
       "   290.1474609375,\n",
       "   285.5425109863281,\n",
       "   284.20831298828125,\n",
       "   284.64208984375,\n",
       "   285.6950378417969,\n",
       "   281.8807067871094,\n",
       "   280.2974853515625,\n",
       "   279.3736572265625,\n",
       "   276.3670349121094,\n",
       "   276.7349548339844,\n",
       "   274.9931945800781,\n",
       "   278.4264221191406,\n",
       "   288.6300964355469,\n",
       "   290.9477844238281,\n",
       "   288.74200439453125,\n",
       "   285.2259521484375,\n",
       "   288.4261779785156,\n",
       "   294.3046875,\n",
       "   299.8898010253906,\n",
       "   296.9646911621094,\n",
       "   302.2086486816406,\n",
       "   299.4240417480469,\n",
       "   293.3000183105469,\n",
       "   301.1813659667969,\n",
       "   298.26300048828125,\n",
       "   294.2242431640625,\n",
       "   292.489013671875,\n",
       "   295.02923583984375,\n",
       "   292.83050537109375,\n",
       "   290.852294921875,\n",
       "   301.43701171875,\n",
       "   305.08709716796875,\n",
       "   304.5672912597656,\n",
       "   300.3441467285156,\n",
       "   294.20587158203125,\n",
       "   301.6960754394531,\n",
       "   302.32623291015625,\n",
       "   300.0127868652344,\n",
       "   298.6236572265625,\n",
       "   300.5706787109375,\n",
       "   299.4827880859375,\n",
       "   299.1197204589844,\n",
       "   301.4095458984375,\n",
       "   303.75164794921875,\n",
       "   309.1090087890625,\n",
       "   308.8397216796875,\n",
       "   310.08087158203125,\n",
       "   313.9657897949219,\n",
       "   320.5272521972656,\n",
       "   320.3955078125,\n",
       "   312.2245788574219,\n",
       "   307.52392578125,\n",
       "   311.0287170410156,\n",
       "   308.2129821777344,\n",
       "   308.55218505859375,\n",
       "   309.9212951660156,\n",
       "   315.70416259765625,\n",
       "   318.2577819824219,\n",
       "   312.5880126953125,\n",
       "   312.9327697753906,\n",
       "   316.13128662109375,\n",
       "   311.191162109375,\n",
       "   311.7486572265625,\n",
       "   314.8795471191406,\n",
       "   307.8327331542969,\n",
       "   308.5168151855469,\n",
       "   312.470947265625,\n",
       "   314.15618896484375,\n",
       "   313.0702819824219,\n",
       "   308.5429992675781,\n",
       "   307.5534362792969,\n",
       "   301.8126220703125,\n",
       "   300.85723876953125,\n",
       "   302.5465087890625,\n",
       "   298.6573791503906,\n",
       "   299.2334899902344,\n",
       "   297.62567138671875,\n",
       "   305.72674560546875,\n",
       "   299.7760009765625,\n",
       "   295.9313049316406,\n",
       "   293.1482238769531,\n",
       "   285.5099182128906,\n",
       "   290.65087890625,\n",
       "   289.32470703125,\n",
       "   292.1605529785156,\n",
       "   295.0686950683594,\n",
       "   296.2931213378906,\n",
       "   291.8934020996094,\n",
       "   286.1478576660156,\n",
       "   283.7315673828125,\n",
       "   284.5278625488281,\n",
       "   285.4413146972656,\n",
       "   286.8994140625,\n",
       "   289.8536376953125,\n",
       "   285.6070861816406,\n",
       "   290.14190673828125,\n",
       "   288.8709411621094,\n",
       "   292.4136657714844,\n",
       "   302.0754699707031,\n",
       "   301.6283264160156,\n",
       "   301.6330871582031,\n",
       "   285.336669921875,\n",
       "   284.9067077636719,\n",
       "   280.6305236816406,\n",
       "   285.8869934082031,\n",
       "   291.5058288574219,\n",
       "   291.0408630371094,\n",
       "   291.1495056152344,\n",
       "   287.6489562988281,\n",
       "   288.5360412597656,\n",
       "   290.03271484375,\n",
       "   291.05230712890625,\n",
       "   289.58251953125,\n",
       "   291.47259521484375,\n",
       "   281.6378479003906,\n",
       "   278.99163818359375,\n",
       "   280.0776062011719,\n",
       "   277.6942138671875,\n",
       "   276.4752502441406,\n",
       "   282.9842224121094,\n",
       "   286.93902587890625,\n",
       "   283.31884765625,\n",
       "   278.0025329589844,\n",
       "   278.6934509277344,\n",
       "   279.68603515625,\n",
       "   280.5288391113281,\n",
       "   282.3625793457031,\n",
       "   282.57562255859375,\n",
       "   279.54217529296875,\n",
       "   280.6231994628906,\n",
       "   280.75921630859375,\n",
       "   290.7327575683594,\n",
       "   285.84906005859375,\n",
       "   286.38653564453125,\n",
       "   285.2289733886719,\n",
       "   285.32891845703125,\n",
       "   290.6570739746094,\n",
       "   287.4400939941406,\n",
       "   289.3237609863281,\n",
       "   295.77777099609375,\n",
       "   290.7305603027344,\n",
       "   287.0756530761719,\n",
       "   283.6814270019531,\n",
       "   277.035400390625,\n",
       "   276.4367370605469,\n",
       "   274.2470397949219,\n",
       "   275.0657958984375,\n",
       "   278.65576171875,\n",
       "   272.5286560058594,\n",
       "   265.671875,\n",
       "   267.9096984863281,\n",
       "   275.5450744628906,\n",
       "   270.1284484863281,\n",
       "   274.65399169921875,\n",
       "   275.9071350097656,\n",
       "   283.7027893066406,\n",
       "   287.7606201171875,\n",
       "   287.0159606933594,\n",
       "   291.70172119140625,\n",
       "   292.9994201660156,\n",
       "   296.111328125,\n",
       "   289.58123779296875,\n",
       "   289.23175048828125,\n",
       "   286.2550964355469,\n",
       "   284.87017822265625,\n",
       "   285.83221435546875,\n",
       "   275.981201171875,\n",
       "   273.3964538574219,\n",
       "   273.9488220214844,\n",
       "   280.9804382324219,\n",
       "   280.6133728027344,\n",
       "   278.9375915527344,\n",
       "   282.9765930175781,\n",
       "   286.8026123046875,\n",
       "   281.29010009765625,\n",
       "   281.4614562988281,\n",
       "   291.02691650390625,\n",
       "   289.65972900390625,\n",
       "   296.2528076171875,\n",
       "   292.2399597167969,\n",
       "   295.82659912109375,\n",
       "   294.95220947265625,\n",
       "   293.1455993652344,\n",
       "   293.62457275390625,\n",
       "   295.9740905761719,\n",
       "   296.7017517089844,\n",
       "   300.49041748046875,\n",
       "   301.831787109375,\n",
       "   301.8786315917969,\n",
       "   305.670654296875,\n",
       "   309.3027648925781,\n",
       "   307.6548767089844,\n",
       "   309.7605285644531,\n",
       "   310.51422119140625,\n",
       "   305.9675598144531,\n",
       "   307.7845764160156,\n",
       "   309.2173767089844,\n",
       "   300.2928466796875,\n",
       "   303.3870849609375,\n",
       "   302.5388488769531,\n",
       "   291.7010192871094,\n",
       "   291.3262634277344,\n",
       "   290.78857421875,\n",
       "   294.583740234375,\n",
       "   295.44500732421875,\n",
       "   295.1319580078125,\n",
       "   294.6856689453125,\n",
       "   303.8189392089844,\n",
       "   304.05126953125,\n",
       "   305.2854919433594,\n",
       "   303.0440673828125,\n",
       "   302.8524475097656,\n",
       "   297.3305358886719,\n",
       "   291.8982238769531,\n",
       "   291.3321228027344,\n",
       "   292.789306640625,\n",
       "   294.0936584472656,\n",
       "   289.989013671875,\n",
       "   286.1702575683594,\n",
       "   284.0699768066406,\n",
       "   279.023681640625,\n",
       "   281.4819030761719,\n",
       "   273.75823974609375,\n",
       "   277.61505126953125,\n",
       "   281.59881591796875,\n",
       "   284.8900451660156,\n",
       "   286.30908203125,\n",
       "   287.7748107910156,\n",
       "   280.61376953125,\n",
       "   283.7281799316406,\n",
       "   272.7224426269531,\n",
       "   277.0899353027344,\n",
       "   278.2669677734375,\n",
       "   282.9033203125,\n",
       "   284.9158020019531,\n",
       "   287.5115661621094,\n",
       "   281.6642150878906,\n",
       "   285.4627990722656,\n",
       "   288.3586120605469,\n",
       "   291.2567443847656,\n",
       "   289.04510498046875,\n",
       "   296.558349609375,\n",
       "   295.2837219238281,\n",
       "   302.25860595703125,\n",
       "   301.5690002441406,\n",
       "   300.0716552734375,\n",
       "   299.23175048828125,\n",
       "   299.35504150390625,\n",
       "   297.261962890625,\n",
       "   295.93328857421875,\n",
       "   291.4168701171875,\n",
       "   295.78094482421875,\n",
       "   293.75860595703125,\n",
       "   293.2004699707031,\n",
       "   294.3209228515625,\n",
       "   296.6360168457031,\n",
       "   296.9500427246094,\n",
       "   298.2742614746094,\n",
       "   297.39520263671875,\n",
       "   297.26397705078125,\n",
       "   300.0314636230469,\n",
       "   299.0606994628906,\n",
       "   306.7209777832031,\n",
       "   296.04736328125,\n",
       "   291.33282470703125,\n",
       "   287.4322509765625,\n",
       "   287.575927734375,\n",
       "   286.5771484375,\n",
       "   282.1312561035156,\n",
       "   286.2152099609375,\n",
       "   290.2215270996094,\n",
       "   289.2030334472656,\n",
       "   287.20458984375,\n",
       "   283.4001159667969,\n",
       "   282.88360595703125,\n",
       "   285.8542785644531,\n",
       "   285.2205810546875,\n",
       "   287.0790100097656,\n",
       "   285.8126220703125,\n",
       "   288.3706359863281,\n",
       "   288.9531555175781,\n",
       "   292.92803955078125,\n",
       "   295.8533020019531,\n",
       "   291.0756530761719,\n",
       "   291.7027282714844,\n",
       "   292.00244140625,\n",
       "   299.5312194824219,\n",
       "   298.2191467285156,\n",
       "   302.3050537109375,\n",
       "   300.6225891113281,\n",
       "   304.36712646484375,\n",
       "   308.4107971191406,\n",
       "   303.0233459472656,\n",
       "   302.741943359375,\n",
       "   303.7801208496094,\n",
       "   305.00079345703125,\n",
       "   311.2699890136719,\n",
       "   304.43939208984375,\n",
       "   302.9975891113281,\n",
       "   302.3613586425781,\n",
       "   296.3470764160156,\n",
       "   296.0241394042969,\n",
       "   286.8876037597656,\n",
       "   287.0072021484375,\n",
       "   286.76348876953125,\n",
       "   287.582763671875,\n",
       "   286.8333435058594,\n",
       "   286.05242919921875,\n",
       "   289.6089172363281,\n",
       "   287.34033203125,\n",
       "   292.4360046386719,\n",
       "   294.7991638183594,\n",
       "   296.8422546386719,\n",
       "   295.5019226074219,\n",
       "   291.64093017578125,\n",
       "   289.6330261230469,\n",
       "   295.6292419433594,\n",
       "   300.0047302246094,\n",
       "   295.967041015625,\n",
       "   297.0678405761719,\n",
       "   300.1061096191406,\n",
       "   302.96942138671875,\n",
       "   303.45135498046875,\n",
       "   303.3403625488281,\n",
       "   300.04156494140625,\n",
       "   297.25653076171875,\n",
       "   296.6822204589844,\n",
       "   288.19940185546875,\n",
       "   282.8484802246094,\n",
       "   276.587646484375,\n",
       "   273.48876953125,\n",
       "   275.2015686035156,\n",
       "   274.9550476074219,\n",
       "   282.16851806640625,\n",
       "   279.0084228515625,\n",
       "   282.9230651855469,\n",
       "   287.1078796386719,\n",
       "   285.7160339355469,\n",
       "   293.9796447753906,\n",
       "   291.278076171875,\n",
       "   286.6512756347656,\n",
       "   289.33758544921875,\n",
       "   288.99139404296875,\n",
       "   290.5735168457031,\n",
       "   295.3992004394531,\n",
       "   299.217529296875,\n",
       "   301.192626953125,\n",
       "   297.8299255371094,\n",
       "   299.2304382324219,\n",
       "   297.2998352050781,\n",
       "   290.8937072753906,\n",
       "   292.7696838378906,\n",
       "   298.8254699707031,\n",
       "   295.6981506347656,\n",
       "   304.9891052246094,\n",
       "   301.89300537109375,\n",
       "   299.31915283203125,\n",
       "   300.260498046875,\n",
       "   307.21038818359375,\n",
       "   309.13397216796875,\n",
       "   309.62579345703125,\n",
       "   298.0729675292969,\n",
       "   298.20953369140625,\n",
       "   307.610595703125,\n",
       "   304.8053894042969,\n",
       "   308.03717041015625,\n",
       "   296.6731872558594,\n",
       "   297.726318359375,\n",
       "   299.3704528808594,\n",
       "   303.0416259765625,\n",
       "   301.3257751464844,\n",
       "   303.7744445800781,\n",
       "   301.55914306640625,\n",
       "   304.6217041015625,\n",
       "   301.5802001953125,\n",
       "   303.88629150390625,\n",
       "   308.8233947753906,\n",
       "   296.9618225097656,\n",
       "   296.8065185546875,\n",
       "   298.2707214355469,\n",
       "   296.9877624511719,\n",
       "   293.8297119140625,\n",
       "   295.2380676269531,\n",
       "   294.3684997558594,\n",
       "   293.83294677734375,\n",
       "   292.639404296875,\n",
       "   297.0977783203125,\n",
       "   296.8154602050781,\n",
       "   298.8307800292969,\n",
       "   295.0730895996094,\n",
       "   292.9951477050781],\n",
       "  'test_accs': [3818.6644287109375,\n",
       "   2694.159957885742,\n",
       "   1846.8776245117188,\n",
       "   1196.3453521728516,\n",
       "   769.5816650390625,\n",
       "   661.7682342529297,\n",
       "   596.0499267578125,\n",
       "   591.6133193969727,\n",
       "   575.9084930419922,\n",
       "   554.1525650024414,\n",
       "   539.8686676025391,\n",
       "   529.3944625854492,\n",
       "   514.232666015625,\n",
       "   516.6025924682617,\n",
       "   514.1656951904297,\n",
       "   515.2175979614258,\n",
       "   513.8454818725586,\n",
       "   508.3728561401367,\n",
       "   512.4622955322266,\n",
       "   509.87279510498047,\n",
       "   523.7646408081055,\n",
       "   526.0550384521484,\n",
       "   526.7167587280273,\n",
       "   527.1913909912109,\n",
       "   526.001579284668,\n",
       "   530.7839584350586,\n",
       "   532.5687866210938,\n",
       "   540.779167175293,\n",
       "   536.2260284423828,\n",
       "   533.1872940063477,\n",
       "   528.9400024414062,\n",
       "   522.0691680908203,\n",
       "   522.6380157470703,\n",
       "   519.6042709350586,\n",
       "   523.4067764282227,\n",
       "   519.3525924682617,\n",
       "   519.0959777832031,\n",
       "   521.2934951782227,\n",
       "   528.7539138793945,\n",
       "   530.6474761962891,\n",
       "   525.2253570556641,\n",
       "   527.4208297729492,\n",
       "   531.1145401000977,\n",
       "   527.9319534301758,\n",
       "   527.9243392944336,\n",
       "   532.0263366699219,\n",
       "   536.0046691894531,\n",
       "   532.9296722412109,\n",
       "   535.8843154907227,\n",
       "   538.9476013183594,\n",
       "   541.841064453125,\n",
       "   544.1888046264648,\n",
       "   543.1827545166016,\n",
       "   533.0681381225586,\n",
       "   530.7262649536133,\n",
       "   527.2513961791992,\n",
       "   527.0918579101562,\n",
       "   530.2310562133789,\n",
       "   532.4401473999023,\n",
       "   529.9997787475586,\n",
       "   523.6684417724609,\n",
       "   523.0780792236328,\n",
       "   514.4226837158203,\n",
       "   512.5511093139648,\n",
       "   513.9912414550781,\n",
       "   520.8017807006836,\n",
       "   520.2295074462891,\n",
       "   525.0711364746094,\n",
       "   530.4880905151367,\n",
       "   519.7605895996094,\n",
       "   524.1193161010742,\n",
       "   539.7476806640625,\n",
       "   531.720817565918,\n",
       "   545.1411361694336,\n",
       "   544.4137573242188,\n",
       "   578.7193222045898,\n",
       "   566.9614562988281,\n",
       "   566.8468017578125,\n",
       "   546.4225463867188,\n",
       "   548.325439453125,\n",
       "   560.9589691162109,\n",
       "   552.8071594238281,\n",
       "   540.3957443237305,\n",
       "   540.6935119628906,\n",
       "   537.3754425048828,\n",
       "   532.3457336425781,\n",
       "   528.0835266113281,\n",
       "   533.7339401245117,\n",
       "   528.3731918334961,\n",
       "   527.3988723754883,\n",
       "   531.4543609619141,\n",
       "   535.1560134887695,\n",
       "   539.5869903564453,\n",
       "   543.3463287353516,\n",
       "   542.5601501464844,\n",
       "   544.2601852416992,\n",
       "   544.7005004882812,\n",
       "   540.5444488525391,\n",
       "   545.271728515625,\n",
       "   545.4231262207031,\n",
       "   538.0007400512695,\n",
       "   541.2358932495117,\n",
       "   545.5128326416016,\n",
       "   542.5541915893555,\n",
       "   541.6282272338867,\n",
       "   542.0711059570312,\n",
       "   541.8908081054688,\n",
       "   525.3441009521484,\n",
       "   532.3914489746094,\n",
       "   515.8183898925781,\n",
       "   520.7111129760742,\n",
       "   533.3044967651367,\n",
       "   514.1959762573242,\n",
       "   507.9605827331543,\n",
       "   511.92769622802734,\n",
       "   529.5040054321289,\n",
       "   548.3538055419922,\n",
       "   549.1443328857422,\n",
       "   547.2662963867188,\n",
       "   548.2683868408203,\n",
       "   528.6795120239258,\n",
       "   546.1166687011719,\n",
       "   569.4881210327148,\n",
       "   565.4764633178711,\n",
       "   537.9640121459961,\n",
       "   541.2634658813477,\n",
       "   557.5367050170898,\n",
       "   549.406005859375,\n",
       "   544.9782409667969,\n",
       "   531.9387435913086,\n",
       "   530.2544174194336,\n",
       "   521.7159996032715,\n",
       "   524.2608757019043,\n",
       "   524.3049049377441,\n",
       "   536.2611465454102,\n",
       "   532.8432083129883,\n",
       "   515.6549987792969,\n",
       "   534.2406692504883,\n",
       "   527.7671051025391,\n",
       "   547.6830825805664,\n",
       "   539.2663955688477,\n",
       "   559.9969100952148,\n",
       "   575.6336898803711,\n",
       "   598.5199508666992,\n",
       "   607.754150390625,\n",
       "   584.7108535766602,\n",
       "   615.6425094604492,\n",
       "   610.6466827392578,\n",
       "   596.6767654418945,\n",
       "   543.2695159912109,\n",
       "   587.5449295043945,\n",
       "   557.9345092773438,\n",
       "   547.9464340209961,\n",
       "   553.4228744506836,\n",
       "   583.2355041503906,\n",
       "   572.3006744384766,\n",
       "   584.7527160644531,\n",
       "   620.7033767700195,\n",
       "   616.8154067993164,\n",
       "   586.0587005615234,\n",
       "   596.1442031860352,\n",
       "   599.3565444946289,\n",
       "   581.7143173217773,\n",
       "   553.0515289306641,\n",
       "   542.8212051391602,\n",
       "   572.6635665893555,\n",
       "   593.1854629516602,\n",
       "   609.5861968994141,\n",
       "   594.9430389404297,\n",
       "   599.1092071533203,\n",
       "   587.0579452514648,\n",
       "   553.5017471313477,\n",
       "   562.3550033569336,\n",
       "   593.3689727783203,\n",
       "   610.356201171875,\n",
       "   665.853874206543,\n",
       "   689.9016952514648,\n",
       "   697.189811706543,\n",
       "   726.7868194580078,\n",
       "   718.180046081543,\n",
       "   708.0868148803711,\n",
       "   683.5202484130859,\n",
       "   699.8189315795898,\n",
       "   694.9217071533203,\n",
       "   699.0276489257812,\n",
       "   693.9318237304688,\n",
       "   688.3344879150391,\n",
       "   681.8905639648438,\n",
       "   647.5446014404297,\n",
       "   612.2499694824219,\n",
       "   624.7606506347656,\n",
       "   608.9591674804688,\n",
       "   616.9536972045898,\n",
       "   604.949951171875,\n",
       "   593.2026519775391,\n",
       "   583.4247970581055,\n",
       "   583.7003936767578,\n",
       "   570.3255310058594,\n",
       "   570.6751098632812,\n",
       "   565.2452163696289,\n",
       "   592.1666946411133,\n",
       "   568.5908813476562,\n",
       "   582.6598739624023,\n",
       "   603.8592529296875,\n",
       "   637.594970703125,\n",
       "   665.6986389160156,\n",
       "   666.4532165527344,\n",
       "   697.8891220092773,\n",
       "   724.2619476318359,\n",
       "   722.2140197753906,\n",
       "   715.6723022460938,\n",
       "   740.0086364746094,\n",
       "   699.7968444824219,\n",
       "   753.0773544311523,\n",
       "   773.1012191772461,\n",
       "   797.6496200561523,\n",
       "   749.1932983398438,\n",
       "   762.1685409545898,\n",
       "   729.9966812133789,\n",
       "   722.1029434204102,\n",
       "   712.9589309692383,\n",
       "   704.3033828735352,\n",
       "   717.0210113525391,\n",
       "   714.6811447143555,\n",
       "   680.5731201171875,\n",
       "   667.1342315673828,\n",
       "   647.1073913574219,\n",
       "   672.4161376953125,\n",
       "   673.6077880859375,\n",
       "   694.6543426513672,\n",
       "   654.5409545898438,\n",
       "   669.5276718139648,\n",
       "   671.195556640625,\n",
       "   666.0887222290039,\n",
       "   648.0943832397461,\n",
       "   669.8433990478516,\n",
       "   694.6655654907227,\n",
       "   697.696174621582,\n",
       "   702.9133377075195,\n",
       "   693.1273803710938,\n",
       "   745.2631454467773,\n",
       "   764.4366302490234,\n",
       "   746.3867797851562,\n",
       "   717.9345626831055,\n",
       "   709.1968002319336,\n",
       "   706.2429122924805,\n",
       "   716.2136383056641,\n",
       "   729.2029724121094,\n",
       "   752.2143325805664,\n",
       "   746.5732803344727,\n",
       "   762.9581985473633,\n",
       "   774.7862319946289,\n",
       "   798.9452056884766,\n",
       "   773.5956268310547,\n",
       "   784.5744514465332,\n",
       "   792.5097198486328,\n",
       "   809.1514892578125,\n",
       "   826.335563659668,\n",
       "   839.6867332458496,\n",
       "   851.3107223510742,\n",
       "   819.3941688537598,\n",
       "   844.1818389892578,\n",
       "   799.5672874450684,\n",
       "   847.0366821289062,\n",
       "   839.9807510375977,\n",
       "   860.549674987793,\n",
       "   850.3783645629883,\n",
       "   845.7530212402344,\n",
       "   828.1634292602539,\n",
       "   819.1178741455078,\n",
       "   794.7734985351562,\n",
       "   790.7995300292969,\n",
       "   758.0964736938477,\n",
       "   747.0100402832031,\n",
       "   777.1412582397461,\n",
       "   766.7379989624023,\n",
       "   754.9716110229492,\n",
       "   784.3300476074219,\n",
       "   807.7259521484375,\n",
       "   854.3562316894531,\n",
       "   869.4852142333984,\n",
       "   815.3109436035156,\n",
       "   848.0579452514648,\n",
       "   814.2092590332031,\n",
       "   820.7841796875,\n",
       "   838.4356155395508,\n",
       "   851.7740097045898,\n",
       "   831.8440475463867,\n",
       "   813.9391784667969,\n",
       "   835.468994140625,\n",
       "   838.4718627929688,\n",
       "   861.8333511352539,\n",
       "   856.0437316894531,\n",
       "   865.3227462768555,\n",
       "   859.5969924926758,\n",
       "   854.1796493530273,\n",
       "   858.2267532348633,\n",
       "   860.7571411132812,\n",
       "   866.1431503295898,\n",
       "   870.4492950439453,\n",
       "   839.1257476806641,\n",
       "   885.3124465942383,\n",
       "   920.8035659790039,\n",
       "   901.6778945922852,\n",
       "   906.0311508178711,\n",
       "   930.6404876708984,\n",
       "   895.9851303100586,\n",
       "   885.8523559570312,\n",
       "   879.0900268554688,\n",
       "   874.3492584228516,\n",
       "   908.9969635009766,\n",
       "   961.835319519043,\n",
       "   955.7825775146484,\n",
       "   950.4228973388672,\n",
       "   947.567985534668,\n",
       "   950.690803527832,\n",
       "   968.4297027587891,\n",
       "   954.4842071533203,\n",
       "   942.6732864379883,\n",
       "   914.2404556274414,\n",
       "   935.9906158447266,\n",
       "   876.8450164794922,\n",
       "   942.3369903564453,\n",
       "   945.4574813842773,\n",
       "   925.1269149780273,\n",
       "   893.3924331665039,\n",
       "   870.5366897583008,\n",
       "   877.5534820556641,\n",
       "   839.4051513671875,\n",
       "   871.3883209228516,\n",
       "   893.2740783691406,\n",
       "   897.1371841430664,\n",
       "   889.2825317382812,\n",
       "   917.6875839233398,\n",
       "   923.2530288696289,\n",
       "   979.6349411010742,\n",
       "   987.2821807861328,\n",
       "   1018.1747970581055,\n",
       "   1022.3866424560547,\n",
       "   937.0568008422852,\n",
       "   917.2704696655273,\n",
       "   894.7348709106445,\n",
       "   907.8947906494141,\n",
       "   863.7824935913086,\n",
       "   826.7024841308594,\n",
       "   839.3967056274414,\n",
       "   850.9076232910156,\n",
       "   858.7835235595703,\n",
       "   846.4075393676758,\n",
       "   875.9779434204102,\n",
       "   893.4219512939453,\n",
       "   938.5292739868164,\n",
       "   980.6078643798828,\n",
       "   1050.314208984375,\n",
       "   1028.2922058105469,\n",
       "   993.3068466186523,\n",
       "   1003.5994338989258,\n",
       "   984.8170852661133,\n",
       "   971.1722564697266,\n",
       "   998.3507766723633,\n",
       "   1005.3668975830078,\n",
       "   978.1988067626953,\n",
       "   955.6281433105469,\n",
       "   921.9601898193359,\n",
       "   940.2884292602539,\n",
       "   951.3810653686523,\n",
       "   957.5167694091797,\n",
       "   1015.4398727416992,\n",
       "   1035.3774490356445,\n",
       "   1032.3377075195312,\n",
       "   1036.3363571166992,\n",
       "   1049.357536315918,\n",
       "   1052.5206909179688,\n",
       "   1064.5829696655273,\n",
       "   1070.641586303711,\n",
       "   1055.4706420898438,\n",
       "   1073.029670715332,\n",
       "   1060.0451889038086,\n",
       "   1069.150131225586,\n",
       "   1071.5138549804688,\n",
       "   1119.2477264404297,\n",
       "   1117.7576293945312,\n",
       "   1109.5682983398438,\n",
       "   1087.6163177490234,\n",
       "   1109.3400421142578,\n",
       "   1095.3128967285156,\n",
       "   1089.1861190795898,\n",
       "   1057.7139053344727,\n",
       "   1092.5307693481445,\n",
       "   1057.912353515625,\n",
       "   1094.0913467407227,\n",
       "   1089.5725860595703,\n",
       "   1060.8925323486328,\n",
       "   1114.762451171875,\n",
       "   1118.9625701904297,\n",
       "   1055.5326766967773,\n",
       "   1061.1772079467773,\n",
       "   1063.5222625732422,\n",
       "   1048.8292274475098,\n",
       "   1040.104866027832,\n",
       "   1056.9664993286133,\n",
       "   1100.0848693847656,\n",
       "   1142.9959564208984,\n",
       "   1149.1085662841797,\n",
       "   1142.8415603637695,\n",
       "   1162.2315826416016,\n",
       "   1167.2471160888672,\n",
       "   1163.2732467651367,\n",
       "   1135.5186233520508,\n",
       "   1143.1193618774414,\n",
       "   1139.149513244629,\n",
       "   1114.5489196777344,\n",
       "   1166.9514541625977,\n",
       "   1194.9471969604492,\n",
       "   1144.886001586914,\n",
       "   1173.4606170654297,\n",
       "   1216.5457305908203,\n",
       "   1280.1865539550781,\n",
       "   1215.5113677978516,\n",
       "   1238.321434020996,\n",
       "   1269.017318725586,\n",
       "   1330.7782974243164,\n",
       "   1315.5202407836914,\n",
       "   1312.7180633544922,\n",
       "   1290.8410873413086,\n",
       "   1246.641098022461,\n",
       "   1206.078025817871,\n",
       "   1201.6355819702148,\n",
       "   1215.6580772399902,\n",
       "   1202.0221710205078,\n",
       "   1217.4550018310547,\n",
       "   1181.9390869140625,\n",
       "   1214.655403137207,\n",
       "   1222.296646118164,\n",
       "   1268.8933410644531,\n",
       "   1246.3091430664062,\n",
       "   1233.1616439819336,\n",
       "   1248.6727828979492,\n",
       "   1202.7471542358398,\n",
       "   1269.2143859863281,\n",
       "   1272.5745010375977,\n",
       "   1265.0366516113281,\n",
       "   1273.1327819824219,\n",
       "   1184.9343872070312,\n",
       "   1186.545066833496,\n",
       "   1230.9342880249023,\n",
       "   1258.9882202148438,\n",
       "   1263.6642761230469,\n",
       "   1321.0837326049805,\n",
       "   1305.6824531555176,\n",
       "   1297.6861038208008,\n",
       "   1259.2678527832031,\n",
       "   1259.5334396362305,\n",
       "   1296.2278060913086,\n",
       "   1289.1686706542969,\n",
       "   1267.0990676879883,\n",
       "   1215.3498077392578,\n",
       "   1249.3771896362305,\n",
       "   1285.2931442260742,\n",
       "   1277.4507369995117,\n",
       "   1294.9502639770508,\n",
       "   1319.054557800293,\n",
       "   1320.0134201049805,\n",
       "   1368.5262069702148,\n",
       "   1342.272605895996,\n",
       "   1315.70751953125,\n",
       "   1322.4885330200195,\n",
       "   1296.9328536987305,\n",
       "   1349.7295837402344,\n",
       "   1306.604637145996,\n",
       "   1295.0726699829102,\n",
       "   1243.4340209960938,\n",
       "   1243.5799255371094,\n",
       "   1292.7399520874023,\n",
       "   1264.5726699829102,\n",
       "   1247.8264541625977,\n",
       "   1218.0637435913086,\n",
       "   1270.740478515625,\n",
       "   1254.178482055664,\n",
       "   1288.7780075073242,\n",
       "   1266.2913970947266,\n",
       "   1259.902214050293,\n",
       "   1257.6065979003906,\n",
       "   1289.8471450805664,\n",
       "   1325.7929077148438,\n",
       "   1391.7364120483398,\n",
       "   1391.3914489746094,\n",
       "   1361.417121887207,\n",
       "   1386.7970886230469,\n",
       "   1390.137062072754,\n",
       "   1448.0702819824219,\n",
       "   1415.046974182129,\n",
       "   1397.9402465820312,\n",
       "   1416.150161743164,\n",
       "   1447.4738464355469,\n",
       "   1430.733139038086,\n",
       "   1409.6710968017578,\n",
       "   1383.6626205444336,\n",
       "   1390.117546081543,\n",
       "   1395.6645202636719],\n",
       "  'pce_r2': [-3.097609074902455,\n",
       "   -1.8731804779940244,\n",
       "   -1.0491236064237133,\n",
       "   -0.5162125615436661,\n",
       "   -0.1893095365997206,\n",
       "   -0.08621740090565488,\n",
       "   0.002671409963586391,\n",
       "   0.08678130974446852,\n",
       "   0.1426698316184677,\n",
       "   0.20537798326865564,\n",
       "   0.2559510567849762,\n",
       "   0.30713687365919484,\n",
       "   0.3623672296603877,\n",
       "   0.3999497553529908,\n",
       "   0.42520084847675665,\n",
       "   0.45174335854592496,\n",
       "   0.47182809576226825,\n",
       "   0.4905299539902923,\n",
       "   0.49733430378903776,\n",
       "   0.49043650170386,\n",
       "   0.501722065539947,\n",
       "   0.5076426072492906,\n",
       "   0.5021287686470477,\n",
       "   0.5135208476948199,\n",
       "   0.5100786592629274,\n",
       "   0.5160973102055837,\n",
       "   0.49363853209946773,\n",
       "   0.47867217578004484,\n",
       "   0.46093235056196447,\n",
       "   0.4722549980233659,\n",
       "   0.4624525708545051,\n",
       "   0.4860164077952934,\n",
       "   0.4877547423698416,\n",
       "   0.48412760011590894,\n",
       "   0.49410402338930526,\n",
       "   0.4925600419696803,\n",
       "   0.47322267322665545,\n",
       "   0.4556946082850004,\n",
       "   0.4834002370526992,\n",
       "   0.4769197258474196,\n",
       "   0.4516878441433795,\n",
       "   0.4543424757607787,\n",
       "   0.4695675214446763,\n",
       "   0.43581864220561817,\n",
       "   0.4605075465902756,\n",
       "   0.45938029203356767,\n",
       "   0.4653990910913237,\n",
       "   0.5044635789266353,\n",
       "   0.4886829000684616,\n",
       "   0.4598258800284827,\n",
       "   0.4472843642983857,\n",
       "   0.4064372569131772,\n",
       "   0.4240847065271386,\n",
       "   0.44657711758996554,\n",
       "   0.45669176910463194,\n",
       "   0.4495900800488355,\n",
       "   0.4137956535690358,\n",
       "   0.4566852276071741,\n",
       "   0.42803088745044104,\n",
       "   0.4156333956380258,\n",
       "   0.4672815051344156,\n",
       "   0.5070094778932013,\n",
       "   0.5233535958439322,\n",
       "   0.510404823311523,\n",
       "   0.5146123103812976,\n",
       "   0.48526402848308825,\n",
       "   0.5106023217354334,\n",
       "   0.5182788222621422,\n",
       "   0.5248300368633577,\n",
       "   0.5210807530397339,\n",
       "   0.49843240758367313,\n",
       "   0.4921266868646271,\n",
       "   0.4871008351447772,\n",
       "   0.47694589084953787,\n",
       "   0.5019783183252063,\n",
       "   0.5022507052277693,\n",
       "   0.4982144150564921,\n",
       "   0.47345155541393724,\n",
       "   0.49379438557840305,\n",
       "   0.5229896754939825,\n",
       "   0.5138719402690708,\n",
       "   0.46792342757101546,\n",
       "   0.5169691837843,\n",
       "   0.5264736818735245,\n",
       "   0.5103313204241945,\n",
       "   0.5026232506995414,\n",
       "   0.534484078932141,\n",
       "   0.5000472768890993,\n",
       "   0.5332342616841323,\n",
       "   0.499877863696661,\n",
       "   0.5225047882996556,\n",
       "   0.49409813112126477,\n",
       "   0.4648912470231856,\n",
       "   0.4575172767999356,\n",
       "   0.44738693778599126,\n",
       "   0.453363419427093,\n",
       "   0.47807641445180105,\n",
       "   0.47385117696946555,\n",
       "   0.44942964081671266,\n",
       "   0.48279345398268725,\n",
       "   0.48345634414295835,\n",
       "   0.47052094102997766,\n",
       "   0.4710324567133958,\n",
       "   0.50025485589703,\n",
       "   0.5022367269690535,\n",
       "   0.4762081189516467,\n",
       "   0.490622250493268,\n",
       "   0.4929379171646573,\n",
       "   0.5141355581600355,\n",
       "   0.5330312083582482,\n",
       "   0.5602791679570782,\n",
       "   0.5733800119264965,\n",
       "   0.5464105195739305,\n",
       "   0.5575422941630923,\n",
       "   0.5340635664741391,\n",
       "   0.5296636659740384,\n",
       "   0.5055837207912101,\n",
       "   0.4774175465157504,\n",
       "   0.502048507314965,\n",
       "   0.5032676226154471,\n",
       "   0.4844301203246132,\n",
       "   0.4710260457299953,\n",
       "   0.48836749105938904,\n",
       "   0.5029672736974715,\n",
       "   0.5046833079372559,\n",
       "   0.48618037433296946,\n",
       "   0.5075853313096979,\n",
       "   0.48453543177772107,\n",
       "   0.48111691337346973,\n",
       "   0.4664127215162327,\n",
       "   0.45120239199685097,\n",
       "   0.43499804557337485,\n",
       "   0.39818692982136294,\n",
       "   0.46620358245770555,\n",
       "   0.45717880408188205,\n",
       "   0.4866364616839286,\n",
       "   0.5127533266397419,\n",
       "   0.49311175773652693,\n",
       "   0.49670087637087745,\n",
       "   0.5111605556091061,\n",
       "   0.5163303802774526,\n",
       "   0.5322363518645474,\n",
       "   0.505161593903532,\n",
       "   0.5204971863432969,\n",
       "   0.5330198890011214,\n",
       "   0.5230395760810612,\n",
       "   0.5019918322336931,\n",
       "   0.48638327833006567,\n",
       "   0.4839128530082596,\n",
       "   0.46054957277328756,\n",
       "   0.43071784224067977,\n",
       "   0.43786735171587843,\n",
       "   0.43105062073534994,\n",
       "   0.4465628225596132,\n",
       "   0.48033046466120044,\n",
       "   0.4567027896926894,\n",
       "   0.4450197961144402,\n",
       "   0.3802770687862439,\n",
       "   0.42835339202208456,\n",
       "   0.40517484118544245,\n",
       "   0.40890855555922445,\n",
       "   0.41699225077376056,\n",
       "   0.45399046733915516,\n",
       "   0.4356286434621157,\n",
       "   0.44796506256133795,\n",
       "   0.42209079217916157,\n",
       "   0.46367980688916477,\n",
       "   0.415946802300159,\n",
       "   0.41389986664393796,\n",
       "   0.4355632584042999,\n",
       "   0.42669518743393,\n",
       "   0.4402487834975003,\n",
       "   0.4887421376654911,\n",
       "   0.5143678722392292,\n",
       "   0.5254027256632957,\n",
       "   0.5201498347758029,\n",
       "   0.504713501940742,\n",
       "   0.48685362114843556,\n",
       "   0.4016843232744326,\n",
       "   0.3881863866701779,\n",
       "   0.3775310811775552,\n",
       "   0.4813484086441199,\n",
       "   0.4714089231308305,\n",
       "   0.4996244532930504,\n",
       "   0.48323631825571567,\n",
       "   0.4955672202043635,\n",
       "   0.49117454352174506,\n",
       "   0.5099601746606542,\n",
       "   0.5161688776939478,\n",
       "   0.5331352861305565,\n",
       "   0.5005878741048293,\n",
       "   0.5247789143709027,\n",
       "   0.4989507262744187,\n",
       "   0.5113282129402804,\n",
       "   0.5311701336464874,\n",
       "   0.4977519761140682,\n",
       "   0.4780613750581144,\n",
       "   0.4876763860822013,\n",
       "   0.4938118600134209,\n",
       "   0.4637769327935507,\n",
       "   0.46266674911407624,\n",
       "   0.5114673553334983,\n",
       "   0.45760346249730843,\n",
       "   0.4983785434985656,\n",
       "   0.47227872008661986,\n",
       "   0.43107093886243475,\n",
       "   0.46161182438589454,\n",
       "   0.4343900812360806,\n",
       "   0.3450995203392856,\n",
       "   0.29883655392906083,\n",
       "   0.27543154853778573,\n",
       "   0.21099308461472677,\n",
       "   0.3396387402091394,\n",
       "   0.22531899839333902,\n",
       "   0.18190360977996967,\n",
       "   0.03701013016876731,\n",
       "   0.22573572837155054,\n",
       "   0.1495504766583925,\n",
       "   0.25978332713502683,\n",
       "   0.26168953765676783,\n",
       "   0.25154431468859506,\n",
       "   0.26726225289220595,\n",
       "   0.20215501446620376,\n",
       "   0.20513443342135207,\n",
       "   0.24958316301348848,\n",
       "   0.27786381355029965,\n",
       "   0.2561396537578141,\n",
       "   0.18850663911269927,\n",
       "   0.3191818419096003,\n",
       "   0.2593003173770385,\n",
       "   0.31639365390789886,\n",
       "   0.23353095877290841,\n",
       "   0.2638514128365499,\n",
       "   0.27583565379922426,\n",
       "   0.33825697593185666,\n",
       "   0.31696043819769326,\n",
       "   0.27073238098872476,\n",
       "   0.25946343132028127,\n",
       "   0.2726455493012837,\n",
       "   0.27067136275693826,\n",
       "   0.13644457283392342,\n",
       "   0.047450654252384705,\n",
       "   0.12338093885223611,\n",
       "   0.2186469381882592,\n",
       "   0.21688022112593497,\n",
       "   0.24295163025594047,\n",
       "   0.23666741101838962,\n",
       "   0.17048799715793272,\n",
       "   0.1706093092566855,\n",
       "   0.1635626201762137,\n",
       "   0.023124128973141866,\n",
       "   -0.031799979279449664,\n",
       "   -0.18507941735721056,\n",
       "   -0.09856495355772066,\n",
       "   -0.23188597596867933,\n",
       "   -0.32199435988981295,\n",
       "   -0.4824635937217001,\n",
       "   -0.6430687254407612,\n",
       "   -0.7353713922671421,\n",
       "   -0.8907128736303476,\n",
       "   -0.5062727129424602,\n",
       "   -0.7559274749206328,\n",
       "   -0.28454891051377795,\n",
       "   -0.5486752186571877,\n",
       "   -0.3591903758177404,\n",
       "   -0.4788824198916417,\n",
       "   -0.42363846635901736,\n",
       "   -0.30672386227030857,\n",
       "   -0.16472609780130232,\n",
       "   -0.0846650367143369,\n",
       "   0.03446036902649474,\n",
       "   0.015049415765350127,\n",
       "   0.08627308329087102,\n",
       "   0.07211711312936653,\n",
       "   -0.05005275911632445,\n",
       "   -0.04136784932589843,\n",
       "   -0.09906784037796834,\n",
       "   -0.34415293082881937,\n",
       "   -0.5071290786421923,\n",
       "   -0.9117682795647923,\n",
       "   -1.2449936266923287,\n",
       "   -0.58240085747019,\n",
       "   -0.9045139031357525,\n",
       "   -0.6169981448270851,\n",
       "   -0.633218424766969,\n",
       "   -0.8617148379536628,\n",
       "   -1.322790630413964,\n",
       "   -0.8409097772036498,\n",
       "   -0.5546275120879522,\n",
       "   -0.7747304835546958,\n",
       "   -1.0124107515856182,\n",
       "   -1.2201688393571706,\n",
       "   -1.1409472315832287,\n",
       "   -1.3944650352837251,\n",
       "   -1.4004992222857235,\n",
       "   -1.0307752005734914,\n",
       "   -1.0355544824065785,\n",
       "   -1.1641699364513713,\n",
       "   -0.9637642155268771,\n",
       "   -0.7676026848458668,\n",
       "   -0.4733416398246313,\n",
       "   -0.9936738808422858,\n",
       "   -1.6892638788812042,\n",
       "   -1.5042609980989599,\n",
       "   -1.3438668474155246,\n",
       "   -1.57194592920523,\n",
       "   -1.1888394927933184,\n",
       "   -1.092874416409622,\n",
       "   -1.0229914172693624,\n",
       "   -0.862839384711312,\n",
       "   -1.2600856341764208,\n",
       "   -2.3042324962638765,\n",
       "   -2.1156244260953145,\n",
       "   -1.9388562508996205,\n",
       "   -2.17183145197389,\n",
       "   -1.9335096797538402,\n",
       "   -2.3631062265190885,\n",
       "   -2.4762046557638557,\n",
       "   -2.0904368876949833,\n",
       "   -1.6276716507279803,\n",
       "   -1.997148310245365,\n",
       "   -1.2088856747036951,\n",
       "   -2.737369686279106,\n",
       "   -2.8466229416844326,\n",
       "   -2.3195088881064394,\n",
       "   -1.8175109270438687,\n",
       "   -1.2343137436039169,\n",
       "   -1.529962131391819,\n",
       "   -0.9007906050243448,\n",
       "   -1.2804505467824598,\n",
       "   -1.820597476704605,\n",
       "   -2.2620269016593393,\n",
       "   -2.0826474238245103,\n",
       "   -3.062262672911899,\n",
       "   -3.117756717464192,\n",
       "   -4.6728723740959115,\n",
       "   -5.079523538659322,\n",
       "   -6.825961791976108,\n",
       "   -6.089157789396151,\n",
       "   -3.165450513726322,\n",
       "   -3.074928293118777,\n",
       "   -2.415224404356852,\n",
       "   -2.385169807966972,\n",
       "   -1.5867810473084765,\n",
       "   -1.0927083671851583,\n",
       "   -1.087565929182257,\n",
       "   -1.2732139743732342,\n",
       "   -1.1954818458917318,\n",
       "   -0.7941363950885802,\n",
       "   -1.2666215790780697,\n",
       "   -1.4267712521970903,\n",
       "   -2.091333851246702,\n",
       "   -2.8373334172283537,\n",
       "   -5.139862447807087,\n",
       "   -4.430446093536996,\n",
       "   -3.8351849450810525,\n",
       "   -3.9643604977768288,\n",
       "   -3.1476422203204146,\n",
       "   -2.603815314496365,\n",
       "   -3.393437083846642,\n",
       "   -3.0677530441887146,\n",
       "   -2.5714852780547175,\n",
       "   -2.4977646583095923,\n",
       "   -1.704050392498957,\n",
       "   -2.0202116666355483,\n",
       "   -2.105655295842521,\n",
       "   -2.0293562069702027,\n",
       "   -3.6337544751444995,\n",
       "   -3.8942921556677987,\n",
       "   -3.9009179546276513,\n",
       "   -4.642194449504964,\n",
       "   -4.2649009412339955,\n",
       "   -4.859263484912517,\n",
       "   -6.125922638391426,\n",
       "   -6.963716778768738,\n",
       "   -6.836586596183575,\n",
       "   -9.424432231802157,\n",
       "   -8.983016069506812,\n",
       "   -10.709571777724157,\n",
       "   -9.130178239287599,\n",
       "   -14.207161970623659,\n",
       "   -15.034143644263722,\n",
       "   -10.595719199741842,\n",
       "   -8.47564262895304,\n",
       "   -9.41571793766754,\n",
       "   -8.30603610671835,\n",
       "   -6.9212622188910515,\n",
       "   -5.8568921315847815,\n",
       "   -7.278330814197199,\n",
       "   -4.885871051559895,\n",
       "   -6.409798348920633,\n",
       "   -5.943413609937035,\n",
       "   -4.802760290258653,\n",
       "   -6.575064092084986,\n",
       "   -6.5945176655074045,\n",
       "   -4.799768836751877,\n",
       "   -4.852941481035901,\n",
       "   -6.614479544300844,\n",
       "   -6.074555489362499,\n",
       "   -6.382287169520202,\n",
       "   -7.4768480106735105,\n",
       "   -11.181339216886402,\n",
       "   -13.892478095291441,\n",
       "   -13.752806906235477,\n",
       "   -11.62279719585126,\n",
       "   -11.422484157719268,\n",
       "   -12.587982134037766,\n",
       "   -10.556473669979988,\n",
       "   -8.728718238348797,\n",
       "   -9.707676835666097,\n",
       "   -9.019820832126419,\n",
       "   -8.235442440410555,\n",
       "   -13.584326191246111,\n",
       "   -18.228558773986812,\n",
       "   -13.801667236434954,\n",
       "   -18.098276723765817,\n",
       "   -24.050705589136292,\n",
       "   -45.62982881738891,\n",
       "   -24.60413992207998,\n",
       "   -27.693860901364495,\n",
       "   -32.27666781895898,\n",
       "   -46.27773764794257,\n",
       "   -37.985784852872165,\n",
       "   -42.602322182414355,\n",
       "   -40.278387775718784,\n",
       "   -27.691541812072593,\n",
       "   -21.469339563911927,\n",
       "   -18.285997325982546,\n",
       "   -22.091332092729537,\n",
       "   -17.83361765827478,\n",
       "   -15.770761203016718,\n",
       "   -16.38007279743674,\n",
       "   -15.632833588740308,\n",
       "   -17.277553640175586,\n",
       "   -21.8683249204312,\n",
       "   -22.025659940870973,\n",
       "   -29.434890469560322,\n",
       "   -26.803789321540098,\n",
       "   -20.8755983846252,\n",
       "   -27.037985784431275,\n",
       "   -30.129347020488144,\n",
       "   -25.734552461468898,\n",
       "   -29.418507608880642,\n",
       "   -21.549827166227285,\n",
       "   -22.74242590826458,\n",
       "   -26.828676233933532,\n",
       "   -33.07940449421927,\n",
       "   -25.725601945037628,\n",
       "   -30.049982098338766,\n",
       "   -30.680675388886296,\n",
       "   -29.423783283064086,\n",
       "   -32.11136865681144,\n",
       "   -29.999183717443156,\n",
       "   -32.78363346770686,\n",
       "   -31.044670518152444,\n",
       "   -22.86972103303952,\n",
       "   -17.43617011835517,\n",
       "   -20.337089927521465,\n",
       "   -25.165234608391962,\n",
       "   -25.184513769991177,\n",
       "   -26.01808384836238,\n",
       "   -30.715421871563954,\n",
       "   -30.662104013833492,\n",
       "   -36.71760061826705,\n",
       "   -38.24774957130744,\n",
       "   -43.75929909434323,\n",
       "   -41.433506130605004,\n",
       "   -33.46999918994727,\n",
       "   -39.33980446050244,\n",
       "   -39.125241443167965,\n",
       "   -44.169784404782575,\n",
       "   -32.89427484380717,\n",
       "   -35.38073687080036,\n",
       "   -42.03025080561525,\n",
       "   -32.38160659122338,\n",
       "   -32.34505768518209,\n",
       "   -25.55393988808041,\n",
       "   -36.11806359944599,\n",
       "   -28.47873333686598,\n",
       "   -33.1806928602417,\n",
       "   -31.375487515413425,\n",
       "   -29.21428618702185,\n",
       "   -23.787327678965898,\n",
       "   -29.6392638288212,\n",
       "   -34.00846165651455,\n",
       "   -30.744530862023915,\n",
       "   -30.356027949114107,\n",
       "   -34.60886997321466,\n",
       "   -37.249524040920065,\n",
       "   -28.855355569101384,\n",
       "   -27.06801847638489,\n",
       "   -29.52153897114634,\n",
       "   -29.789628469175423,\n",
       "   -27.262391805491436,\n",
       "   -26.448093138465683,\n",
       "   -31.42224754972809,\n",
       "   -30.408152724523873,\n",
       "   -29.424164059831092,\n",
       "   -32.015572744410996,\n",
       "   -29.304533437774644],\n",
       "  'voc_r2': [-0.24169926710197487,\n",
       "   -0.16935359479761902,\n",
       "   -0.09067265517117296,\n",
       "   -0.044950403892276114,\n",
       "   -0.019680124160358137,\n",
       "   -9.76730434087969e-05,\n",
       "   0.004614855729564327,\n",
       "   0.002563647999567986,\n",
       "   -0.008540966625208757,\n",
       "   -0.03494103993028563,\n",
       "   -0.0746371061153488,\n",
       "   -0.11801294669147411,\n",
       "   -0.2051737754899079,\n",
       "   -0.2732397472861241,\n",
       "   -0.3075385034698028,\n",
       "   -0.316415976212852,\n",
       "   -0.3489374266425118,\n",
       "   -0.3576861026185725,\n",
       "   -0.40606928639628936,\n",
       "   -0.4983994437786974,\n",
       "   -0.5260790027660627,\n",
       "   -0.5548364809601385,\n",
       "   -0.6511909519443908,\n",
       "   -0.7113303027217714,\n",
       "   -0.8588032222479678,\n",
       "   -0.8727048190980362,\n",
       "   -0.868357599289348,\n",
       "   -0.8941122489425892,\n",
       "   -0.9036056702957478,\n",
       "   -0.864004423076749,\n",
       "   -0.9854298967072115,\n",
       "   -0.9253567293041669,\n",
       "   -0.969014632736173,\n",
       "   -1.0573259340213093,\n",
       "   -1.1156214260323822,\n",
       "   -1.1080791439323283,\n",
       "   -1.2281348107141223,\n",
       "   -1.1562119101791954,\n",
       "   -1.1730039726877948,\n",
       "   -1.186392436126504,\n",
       "   -1.1572220905274833,\n",
       "   -1.1192480718115676,\n",
       "   -1.1003062901507175,\n",
       "   -1.2445173600537607,\n",
       "   -1.2930851070316218,\n",
       "   -1.2213165988349073,\n",
       "   -1.1408031532930774,\n",
       "   -1.2440956136790495,\n",
       "   -1.1839696814429508,\n",
       "   -1.0245083875734058,\n",
       "   -1.1513584736975049,\n",
       "   -1.2757394488876805,\n",
       "   -1.281899586152146,\n",
       "   -1.3774729985869514,\n",
       "   -1.4789180707477367,\n",
       "   -1.4681744673074513,\n",
       "   -1.3842801326292995,\n",
       "   -1.2608961211193424,\n",
       "   -1.27285527503701,\n",
       "   -1.3056219698398142,\n",
       "   -1.2795626264068019,\n",
       "   -1.243833880207792,\n",
       "   -1.1550526616194685,\n",
       "   -1.1275505099695513,\n",
       "   -0.931106018165303,\n",
       "   -0.9939564935784002,\n",
       "   -1.1835826877179008,\n",
       "   -1.3346752898917158,\n",
       "   -1.322238717080917,\n",
       "   -1.302418874176706,\n",
       "   -1.2978209270787109,\n",
       "   -1.4104436727776446,\n",
       "   -1.47810265796342,\n",
       "   -1.3862022635486717,\n",
       "   -1.161131772451729,\n",
       "   -1.0850833312903938,\n",
       "   -1.1234294886104235,\n",
       "   -1.0590231352017643,\n",
       "   -0.9983266034881486,\n",
       "   -1.0100534821195266,\n",
       "   -1.0872227671716317,\n",
       "   -0.9012147709461871,\n",
       "   -0.8474811043932762,\n",
       "   -0.8610552513907093,\n",
       "   -0.9103469207175714,\n",
       "   -0.9540445849398653,\n",
       "   -1.0854277062150817,\n",
       "   -1.1344906756478013,\n",
       "   -1.1495100791140063,\n",
       "   -1.0799449960440866,\n",
       "   -1.1086885653640572,\n",
       "   -1.1619157817675143,\n",
       "   -1.0711090468314386,\n",
       "   -1.0539653822314499,\n",
       "   -1.085154148024544,\n",
       "   -1.070013692531417,\n",
       "   -0.9427884670966025,\n",
       "   -0.9177102054836248,\n",
       "   -0.932214372175092,\n",
       "   -0.8948987285348471,\n",
       "   -0.9165380649081116,\n",
       "   -0.9731516535085711,\n",
       "   -1.0818446429519555,\n",
       "   -1.063705243737826,\n",
       "   -1.042780429496153,\n",
       "   -1.0651351685378239,\n",
       "   -1.0941070968861948,\n",
       "   -1.0831071871167581,\n",
       "   -0.9341580338458095,\n",
       "   -0.9258582050362765,\n",
       "   -1.0541170588893047,\n",
       "   -1.0031969227139905,\n",
       "   -1.0256548141078858,\n",
       "   -1.0970349753895654,\n",
       "   -1.1075815330004524,\n",
       "   -1.1514025702696182,\n",
       "   -1.1482498033951374,\n",
       "   -1.182404481129875,\n",
       "   -1.092506191934346,\n",
       "   -1.1046554132325954,\n",
       "   -1.2685641407049029,\n",
       "   -1.1970775946755703,\n",
       "   -1.0789134011185748,\n",
       "   -1.201653658510185,\n",
       "   -1.2711954166562789,\n",
       "   -1.4231101150042669,\n",
       "   -1.4257855430410111,\n",
       "   -1.3793613714565467,\n",
       "   -1.3265238691288066,\n",
       "   -1.4210707850008326,\n",
       "   -1.5578607566348364,\n",
       "   -1.4313466102875942,\n",
       "   -1.4514118860957979,\n",
       "   -1.3409393163963426,\n",
       "   -1.3114782000498968,\n",
       "   -1.352752660530609,\n",
       "   -1.2809591172391013,\n",
       "   -1.102214720210677,\n",
       "   -1.0836799245881825,\n",
       "   -1.1950950654235428,\n",
       "   -1.1852064418342692,\n",
       "   -1.1551117971302158,\n",
       "   -1.26538938433188,\n",
       "   -1.370822169680797,\n",
       "   -1.3420680180612243,\n",
       "   -1.4493305432997183,\n",
       "   -1.5086988798387257,\n",
       "   -1.3393212015513183,\n",
       "   -1.10846189328028,\n",
       "   -1.177307521966633,\n",
       "   -1.296402456064683,\n",
       "   -1.1874781708511901,\n",
       "   -1.1291403114130594,\n",
       "   -1.1649260661749747,\n",
       "   -1.0295766928103554,\n",
       "   -0.9625162662264768,\n",
       "   -0.9866346867656184,\n",
       "   -1.0004205672751274,\n",
       "   -1.0177014043979407,\n",
       "   -0.9502483002053663,\n",
       "   -0.9933794111842458,\n",
       "   -0.9115187188060281,\n",
       "   -0.8557638970908565,\n",
       "   -0.9611581253787875,\n",
       "   -1.0398810911893417,\n",
       "   -0.9916565154493848,\n",
       "   -1.0844358976266117,\n",
       "   -1.078564047862249,\n",
       "   -1.059767855924516,\n",
       "   -0.9846356431142942,\n",
       "   -0.8821370890568923,\n",
       "   -0.9044405580781092,\n",
       "   -0.940713089393276,\n",
       "   -1.0437915156924351,\n",
       "   -1.0633312336223004,\n",
       "   -1.1443725646438634,\n",
       "   -1.1287120059179192,\n",
       "   -1.1702713597861858,\n",
       "   -1.093165053876957,\n",
       "   -1.1131070228692352,\n",
       "   -1.0208079716050942,\n",
       "   -1.0478101624188363,\n",
       "   -1.0677440433260448,\n",
       "   -1.1142127364050642,\n",
       "   -1.0646946685561551,\n",
       "   -1.0526968485641928,\n",
       "   -0.9865442796824948,\n",
       "   -0.9839269529095223,\n",
       "   -0.9578243987641897,\n",
       "   -0.9248398565321929,\n",
       "   -1.0121768159238012,\n",
       "   -1.0599770956627919,\n",
       "   -0.964137814807996,\n",
       "   -1.0652995995046677,\n",
       "   -1.223784931611032,\n",
       "   -1.1644636360654137,\n",
       "   -1.2602565802848242,\n",
       "   -1.298445450782253,\n",
       "   -1.3321989754557135,\n",
       "   -1.503625355969798,\n",
       "   -1.559267522362655,\n",
       "   -1.530746986422928,\n",
       "   -1.5065140561597756,\n",
       "   -1.4054497582067373,\n",
       "   -1.277360007297391,\n",
       "   -1.2217707878843442,\n",
       "   -1.1122912198465147,\n",
       "   -1.1087627318720772,\n",
       "   -1.1622362909908746,\n",
       "   -1.1977651471116944,\n",
       "   -1.2001272821729256,\n",
       "   -1.2188918640218058,\n",
       "   -1.1473388115854708,\n",
       "   -1.1020912248543802,\n",
       "   -1.1470960024498145,\n",
       "   -1.0642138127211687,\n",
       "   -1.1237467817234816,\n",
       "   -1.1672663598377775,\n",
       "   -1.2337637996381075,\n",
       "   -1.2353993810246338,\n",
       "   -1.2279367830621455,\n",
       "   -1.2047144874511786,\n",
       "   -1.2450011387029516,\n",
       "   -1.2145398872726902,\n",
       "   -1.1843701240568807,\n",
       "   -1.29132081282023,\n",
       "   -1.5015181215468516,\n",
       "   -1.4514938517501124,\n",
       "   -1.599652197618517,\n",
       "   -1.5826551743792128,\n",
       "   -1.6240291654587846,\n",
       "   -1.6310583546599835,\n",
       "   -1.621621575672247,\n",
       "   -1.75210970755351,\n",
       "   -1.4480108262440723,\n",
       "   -1.4620230414113347,\n",
       "   -1.670523778065521,\n",
       "   -1.5888016717991302,\n",
       "   -1.5774856320076354,\n",
       "   -1.381525686423735,\n",
       "   -1.2842356250448135,\n",
       "   -1.1898393562412308,\n",
       "   -1.2497994020765533,\n",
       "   -1.2349341799219462,\n",
       "   -1.212586594813065,\n",
       "   -1.173923226511631,\n",
       "   -1.0554251513989055,\n",
       "   -0.9994106627648665,\n",
       "   -0.9631580853997097,\n",
       "   -1.1000704513858577,\n",
       "   -1.00395339263514,\n",
       "   -1.0848428555952987,\n",
       "   -1.008306333674462,\n",
       "   -1.0885815134891899,\n",
       "   -1.0716925144246154,\n",
       "   -1.0570828203347347,\n",
       "   -1.0611207233639681,\n",
       "   -0.9930616695170924,\n",
       "   -0.9648970426451906,\n",
       "   -0.9657621332663102,\n",
       "   -1.0435590953824407,\n",
       "   -1.079530220771995,\n",
       "   -1.1375877988855803,\n",
       "   -1.1638373340663457,\n",
       "   -1.173005742585696,\n",
       "   -1.2089932741102096,\n",
       "   -1.11444495548063,\n",
       "   -1.118444499243056,\n",
       "   -1.008592887867783,\n",
       "   -0.9792194949350781,\n",
       "   -0.8827328992546577,\n",
       "   -0.8735555765871419,\n",
       "   -0.8985436641242965,\n",
       "   -0.9112341496218848,\n",
       "   -0.8723756879880533,\n",
       "   -0.9359935497236977,\n",
       "   -1.0892358964618447,\n",
       "   -1.1021318597856897,\n",
       "   -1.1264179110265888,\n",
       "   -1.0215138681516738,\n",
       "   -0.984181051749812,\n",
       "   -0.9062546152766529,\n",
       "   -0.9370466851371901,\n",
       "   -0.9179735328009666,\n",
       "   -0.8299544086062545,\n",
       "   -0.9074628763504886,\n",
       "   -0.9304098157566889,\n",
       "   -0.9380093835394943,\n",
       "   -0.8278469021906059,\n",
       "   -0.836748034537969,\n",
       "   -0.8718894186012143,\n",
       "   -0.9842758335308524,\n",
       "   -0.9327840486838985,\n",
       "   -0.9109654108564553,\n",
       "   -0.8762716586294994,\n",
       "   -0.9092863323324787,\n",
       "   -0.8654564591000007,\n",
       "   -0.8291055790924757,\n",
       "   -0.8421352165052025,\n",
       "   -0.8403453045260045,\n",
       "   -0.8331523353180865,\n",
       "   -0.8666985477947451,\n",
       "   -0.8726766543572673,\n",
       "   -0.8911646553022405,\n",
       "   -0.8247287677093866,\n",
       "   -0.7957285943680228,\n",
       "   -0.8745597018985607,\n",
       "   -0.9336215003605857,\n",
       "   -0.9784393069186157,\n",
       "   -0.9888788563460842,\n",
       "   -0.9704855554604881,\n",
       "   -0.9781423935130515,\n",
       "   -1.0358901038358894,\n",
       "   -1.0798841787057927,\n",
       "   -1.1571525695480682,\n",
       "   -1.2511927029434404,\n",
       "   -1.2383785421049165,\n",
       "   -1.2365965925233415,\n",
       "   -1.296244712125231,\n",
       "   -1.0971376426987103,\n",
       "   -1.1514352015607021,\n",
       "   -1.1821376420600327,\n",
       "   -1.26394497348753,\n",
       "   -1.1943206700002418,\n",
       "   -1.334988946589712,\n",
       "   -1.349291076776427,\n",
       "   -1.2034641883989137,\n",
       "   -1.2479891612825584,\n",
       "   -1.2515280381206915,\n",
       "   -1.3096574616917804,\n",
       "   -1.310504890199991,\n",
       "   -1.2902694506388874,\n",
       "   -1.197036147233847,\n",
       "   -1.1241236166962518,\n",
       "   -1.095375570744626,\n",
       "   -1.0433070571494967,\n",
       "   -0.9416207450393119,\n",
       "   -0.9973469072079888,\n",
       "   -1.0300745602905272,\n",
       "   -1.042129291454656,\n",
       "   -1.0130839667569713,\n",
       "   -0.9692362864408093,\n",
       "   -1.0556697200198264,\n",
       "   -1.083611579487397,\n",
       "   -1.230081950281321,\n",
       "   -1.307080346021936,\n",
       "   -1.2935434103292178,\n",
       "   -1.1172592638830654,\n",
       "   -1.0584656647598076,\n",
       "   -1.0696091970535235,\n",
       "   -1.0918543145778026,\n",
       "   -1.0925303903664974,\n",
       "   -1.1110825210650228,\n",
       "   -1.1478387709490754,\n",
       "   -1.1277725313089362,\n",
       "   -1.1237060501949347,\n",
       "   -1.0794994927845525,\n",
       "   -1.2316263179991052,\n",
       "   -1.1750597948648789,\n",
       "   -1.18692495209794,\n",
       "   -1.1351851552447974,\n",
       "   -1.0973398972869335,\n",
       "   -1.0503263001038636,\n",
       "   -1.0526872828649774,\n",
       "   -1.0180715625750447,\n",
       "   -0.9327498983271532,\n",
       "   -0.856999638114883,\n",
       "   -0.7568576384636889,\n",
       "   -0.7488175196454938,\n",
       "   -0.8046526461055981,\n",
       "   -0.8544714287683828,\n",
       "   -0.8874419086125511,\n",
       "   -0.9941852172525407,\n",
       "   -0.9018011094809661,\n",
       "   -0.8508836557899355,\n",
       "   -0.8152082575678312,\n",
       "   -0.7986769301761707,\n",
       "   -0.8909351666214431,\n",
       "   -0.9835944801059899,\n",
       "   -1.001244169399826,\n",
       "   -0.8655027103360722,\n",
       "   -0.925544598748067,\n",
       "   -0.9640336919991592,\n",
       "   -1.0235678374848591,\n",
       "   -1.1520127439871706,\n",
       "   -1.1524493611752793,\n",
       "   -1.1272017754840555,\n",
       "   -1.1159600728887322,\n",
       "   -0.9508526618056685,\n",
       "   -0.8970292132930866,\n",
       "   -0.8566826607220259,\n",
       "   -1.020704512436204,\n",
       "   -1.0508946792407374,\n",
       "   -1.0447201376725497,\n",
       "   -1.0447751006470782,\n",
       "   -1.0652020221276643,\n",
       "   -1.0478250968887273,\n",
       "   -1.0244532925548735,\n",
       "   -1.0282391276305103,\n",
       "   -1.0973858305353574,\n",
       "   -1.1068295141853155,\n",
       "   -1.2571781894517224,\n",
       "   -1.1130011406750322,\n",
       "   -1.2468614492929544,\n",
       "   -1.20143588714466,\n",
       "   -1.1592899085255817,\n",
       "   -1.1690462190270083,\n",
       "   -1.2654315528253837,\n",
       "   -1.2853810568725428,\n",
       "   -1.2977316023741259,\n",
       "   -1.311270198391095,\n",
       "   -1.1867197986051061,\n",
       "   -1.2541483459173532,\n",
       "   -1.182551714257524,\n",
       "   -1.0977564802722966,\n",
       "   -1.207183651466976,\n",
       "   -1.2486053045864032,\n",
       "   -1.2852162009188217,\n",
       "   -1.2404703078628039,\n",
       "   -1.1398670512352966,\n",
       "   -1.091082982336911,\n",
       "   -1.0850574450527857,\n",
       "   -1.01511361384006,\n",
       "   -1.0257088098485498,\n",
       "   -1.198464443598739,\n",
       "   -1.2427694887152332,\n",
       "   -1.122685199559427,\n",
       "   -1.0798077537837463,\n",
       "   -0.9797948617822123,\n",
       "   -1.072555890797159,\n",
       "   -1.1287152895941723,\n",
       "   -1.337985785793582,\n",
       "   -1.2057244754912442,\n",
       "   -1.2075980798671666,\n",
       "   -1.0404180943161023,\n",
       "   -1.0937658760422728,\n",
       "   -1.1149821228930898,\n",
       "   -1.0898002163391447,\n",
       "   -1.095792504176147,\n",
       "   -1.2189908968058605,\n",
       "   -1.2925824373187806,\n",
       "   -1.3450915960331686,\n",
       "   -1.3986323878323184,\n",
       "   -1.3262148845410784,\n",
       "   -1.293228302318869,\n",
       "   -1.2587809550520173,\n",
       "   -1.2089635738404723,\n",
       "   -1.2147778618113523,\n",
       "   -1.1794498873839894,\n",
       "   -1.1637140595459785,\n",
       "   -1.2019910612014733,\n",
       "   -1.2043959482496263,\n",
       "   -1.295570294271473,\n",
       "   -1.2388823377454838,\n",
       "   -1.3198507877850116,\n",
       "   -1.2188719784228108,\n",
       "   -1.3304321982103944,\n",
       "   -1.4165678468806022,\n",
       "   -1.4610182009473842,\n",
       "   -1.4373118172887005,\n",
       "   -1.3127105247199937,\n",
       "   -1.3717314522867867,\n",
       "   -1.329123985167786,\n",
       "   -1.2366601588136636,\n",
       "   -1.2425117783408388,\n",
       "   -1.2082760673899058,\n",
       "   -1.2127639854326917,\n",
       "   -1.149222801629803,\n",
       "   -1.2551151135599947,\n",
       "   -1.2903235144535112,\n",
       "   -1.2734829707295243,\n",
       "   -1.1898187307629553,\n",
       "   -1.4008929142584026,\n",
       "   -1.4740088551877122,\n",
       "   -1.416367997604921,\n",
       "   -1.4865428791095052,\n",
       "   -1.6314370011919492,\n",
       "   -1.579066785717787,\n",
       "   -1.5518814625634718,\n",
       "   -1.4494127727040733,\n",
       "   -1.5798204247278664,\n",
       "   -1.7278947255908284,\n",
       "   -1.6774242913652966,\n",
       "   -1.4913578988707341,\n",
       "   -1.513538826035616,\n",
       "   -1.6196321757172982,\n",
       "   -1.4879940549242856,\n",
       "   -1.3350596706259856,\n",
       "   -1.206439014263351,\n",
       "   -1.1026998177148686,\n",
       "   -1.23851918369584,\n",
       "   -1.1828380200191715,\n",
       "   -1.1726322943026473,\n",
       "   -1.151883851561506,\n",
       "   -1.1504891710666327,\n",
       "   -1.1725866795895286,\n",
       "   -1.0801117215023317,\n",
       "   -1.2020587965587062,\n",
       "   -1.1694596090486864,\n",
       "   -1.2143187378720377],\n",
       "  'jsc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -49894.54585055119,\n",
       "   -12832.584395217655,\n",
       "   -7327.0852700171645,\n",
       "   -606.0508464307504,\n",
       "   -1013.054075256411,\n",
       "   -1072.7129958045282,\n",
       "   -897.1322423340288,\n",
       "   -741.0970239310474,\n",
       "   -696.9333555307976,\n",
       "   -995.3871946414267,\n",
       "   -1336.2630781897124,\n",
       "   -3830.5614010398554,\n",
       "   -11159.425708396882,\n",
       "   -6275.508025139254,\n",
       "   -116444.27787283422,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -299459.4610608926,\n",
       "   -420053.2408981806,\n",
       "   0.0,\n",
       "   -13481.896392646935,\n",
       "   -4147.05913303781,\n",
       "   -2301.2018694441954,\n",
       "   -2838.6910115742658,\n",
       "   -1553.594863672199,\n",
       "   -1946.1950826578752,\n",
       "   -1569.3379282922483,\n",
       "   -1853.917704762415,\n",
       "   -2484.719755804767,\n",
       "   -14106.093651532115,\n",
       "   -22502.285570894786,\n",
       "   -8966.893883934184,\n",
       "   -4685.7923798048405,\n",
       "   -3753.6202731886337,\n",
       "   -29776.433418864,\n",
       "   -182073.86656890225,\n",
       "   -4087119.6734031406,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -30657.560296763702,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -15332589.492410092,\n",
       "   -151581.526162111,\n",
       "   -20157.439635621682,\n",
       "   -23713.591526627,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -141504.5109752129,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -6666.142553446414,\n",
       "   -3402.3987665482464,\n",
       "   -4003.4225632062353,\n",
       "   -4002.9180190329907,\n",
       "   -8839.304969390054,\n",
       "   -3373.8846259276124,\n",
       "   -1705.259756792038,\n",
       "   -1530.0919411274674,\n",
       "   -1404.938553591669,\n",
       "   -1111.748005385291,\n",
       "   -679.9775198264174,\n",
       "   -942.206616321439,\n",
       "   -1076.6137167227807,\n",
       "   -1296.5192190189287,\n",
       "   -3331.2007527425503,\n",
       "   -4404.477492499471,\n",
       "   -2499.509283368856,\n",
       "   -6974.561144137722,\n",
       "   -10235.26555405643,\n",
       "   -19273.436915004477,\n",
       "   -46298.8394362708,\n",
       "   -3330.67150253775,\n",
       "   -10162.637904975694,\n",
       "   -45863.63803110068,\n",
       "   -44242.657845918955,\n",
       "   -6089.163755478448,\n",
       "   -2672.2669522846454,\n",
       "   -1367.8983470877497,\n",
       "   -1687.4937333737735,\n",
       "   -1201.500885073542,\n",
       "   -1951.2218839344473,\n",
       "   -4019.211413279126,\n",
       "   -1179.9383521039188,\n",
       "   -705.0185941811953,\n",
       "   -504.5494978250233,\n",
       "   -463.1865923367311,\n",
       "   -579.4953609512538,\n",
       "   -847.8400194295755,\n",
       "   -1120.4118793324162,\n",
       "   -688.8501513141549,\n",
       "   -1837.1703902723777,\n",
       "   -1294.8055383318379,\n",
       "   -1161.8989022927237,\n",
       "   -2155.077622302225,\n",
       "   -1997.1501784829993,\n",
       "   -1041.1588204346856,\n",
       "   -1077.6113209806188,\n",
       "   -905.5992712094854,\n",
       "   -1969.0832195419769,\n",
       "   -5932.067948979306,\n",
       "   -4051.272085748881,\n",
       "   -3648.8151532477527,\n",
       "   -1584.103507536609,\n",
       "   -924.454376786157,\n",
       "   -1050.7685845526935,\n",
       "   -1250.9162641619384,\n",
       "   -599.4896461355061,\n",
       "   -471.8236364180189,\n",
       "   -581.0594200971061,\n",
       "   -562.9115279021357,\n",
       "   -489.1530603300811,\n",
       "   -522.3539827886468,\n",
       "   -454.28675928507187,\n",
       "   -425.2200651372149,\n",
       "   -802.3265559666504,\n",
       "   -988.5381093284813,\n",
       "   -699.555761124562,\n",
       "   -2200.960068478443,\n",
       "   -1231.1672793837517,\n",
       "   -806.1401970074065,\n",
       "   -926.6121009193864,\n",
       "   -1370.3976187646504,\n",
       "   -691.2886965876313,\n",
       "   -686.6501923173672,\n",
       "   -448.773271943747,\n",
       "   -461.6058849359009,\n",
       "   -354.2532618582837,\n",
       "   -297.6539981419678,\n",
       "   -440.78003857813206,\n",
       "   -449.57610874844903,\n",
       "   -436.32421000155085,\n",
       "   -479.33291043971917,\n",
       "   -516.7511317521089,\n",
       "   -645.5532891042086,\n",
       "   -656.9075797036865,\n",
       "   -1213.7854887070798,\n",
       "   -2246.3001286925223,\n",
       "   -2459.451348673885,\n",
       "   -3738.6429884743225,\n",
       "   -4278.029476551252,\n",
       "   -7430.347995319988,\n",
       "   -97292.65008236087,\n",
       "   -3109.8749151121106,\n",
       "   -3925.0905428552715,\n",
       "   -12217.751899661278,\n",
       "   -59197.059432858085,\n",
       "   -8816.643865302449,\n",
       "   -16177.438163855035,\n",
       "   -8627.535210816366,\n",
       "   -4000.7596079297327,\n",
       "   -9767.087295337937,\n",
       "   -13208.759580320233,\n",
       "   -4114.951053486261,\n",
       "   -12485.771960723403,\n",
       "   -1498.0764002808014,\n",
       "   -3296.7019119787187,\n",
       "   -3405.0948151441153,\n",
       "   -4481.578156223906,\n",
       "   -2935.1661027782566,\n",
       "   -2039.9150050588821,\n",
       "   -1983.4810763399971,\n",
       "   -3574.202277135113,\n",
       "   -3968.4609730160364,\n",
       "   -3182.1333316236496,\n",
       "   -3498.052323988274,\n",
       "   -23983.476782472688,\n",
       "   -118257.24327057674,\n",
       "   -5766.08385453661,\n",
       "   -6856.184497616559,\n",
       "   -4839.613479712493,\n",
       "   -11262.219212048338,\n",
       "   -3852.9604203578406,\n",
       "   -2376.376119988197,\n",
       "   -1430.3334662843793,\n",
       "   -2525.9047193533634,\n",
       "   -1617.2898575854122,\n",
       "   -3823.2688850651834,\n",
       "   -4627.220943316744,\n",
       "   -4680.916518352921,\n",
       "   -4534.174982282974,\n",
       "   -61868.93061256272,\n",
       "   -126794.09173189489,\n",
       "   -18350.529375842583,\n",
       "   -12428.564545571282,\n",
       "   -2000.9632370864272,\n",
       "   -2378.656274473563,\n",
       "   -7152.491990212757,\n",
       "   -5525.306327399357,\n",
       "   -13583.067654752556,\n",
       "   -10168.948392095534,\n",
       "   -4915.733420803136,\n",
       "   -4798.048955829352,\n",
       "   -6149.77245633895,\n",
       "   -1665.4239459444395,\n",
       "   -11305.98641354629,\n",
       "   -134127.53117339636,\n",
       "   -20020.168890832207,\n",
       "   -2867.3494809346985,\n",
       "   -1643.4160778706553,\n",
       "   -1732.9841038802501,\n",
       "   -1394.9274616243615,\n",
       "   -1623.1542747864307,\n",
       "   -1906.8770030865708,\n",
       "   -784.0653102629918,\n",
       "   -1035.2562525701499,\n",
       "   -6614.532938094205,\n",
       "   -1940.1230013621505,\n",
       "   -30694.975729609705,\n",
       "   -223611.20718591908,\n",
       "   -40760.60347556507,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -21529012.16117798,\n",
       "   -47416.92074410038,\n",
       "   -17654.481679084856,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -441112.1697356113,\n",
       "   -32768.46425656146,\n",
       "   -450657.8235061207,\n",
       "   -18967.94831945831,\n",
       "   -3227.9048645582525,\n",
       "   -1055.3336091085669,\n",
       "   -1005.80464273069,\n",
       "   -826.0202434500918,\n",
       "   -723.5111166522714,\n",
       "   -1004.6978207555312,\n",
       "   -1671.0466677481177,\n",
       "   -1406.1469957026654,\n",
       "   -2277.937988354996,\n",
       "   -1778.5606923996088,\n",
       "   -2415.6051123166903,\n",
       "   -9110.454417554804,\n",
       "   -7850.220264834068,\n",
       "   -37522.509712711086,\n",
       "   -377360.4795485766,\n",
       "   -7635353.6562221,\n",
       "   -21522.697307832448,\n",
       "   -125433.63137336026,\n",
       "   -41768.33404455815,\n",
       "   -9566.854930562895,\n",
       "   -2404.2094948004133,\n",
       "   -7162.317968449876,\n",
       "   -8065.032095906823,\n",
       "   -6069.675396914597,\n",
       "   -2531.060845238249,\n",
       "   -5750.873564650113,\n",
       "   -15213.857306493916,\n",
       "   -3338.2490208227605,\n",
       "   -2336.8555218644574,\n",
       "   -2285.504183325471,\n",
       "   -1382.3715625961427,\n",
       "   -1139.2024074775109,\n",
       "   -1229.18654594689,\n",
       "   -1009.4376946329046,\n",
       "   -1893.1284067200218,\n",
       "   -2697.3132427820724,\n",
       "   -2587.403458485651,\n",
       "   -3505.283371767679,\n",
       "   -1492.5746355966107,\n",
       "   -728.1933978232413,\n",
       "   -770.247861858901,\n",
       "   -1156.5179275722244,\n",
       "   -1956.8128888898007,\n",
       "   -877.6457026625868,\n",
       "   -856.5451263580717,\n",
       "   -2224.364639314896,\n",
       "   -2699.878436091289,\n",
       "   -1626.3713687574768,\n",
       "   -1964.3447533625642,\n",
       "   -3764.4074875755978,\n",
       "   -1958.1749288299843,\n",
       "   -1664.027450796338,\n",
       "   -1818.6997228365644,\n",
       "   -982.6732069675962,\n",
       "   -392.4420898870363,\n",
       "   -323.43354607805884,\n",
       "   -433.51445056264356,\n",
       "   -353.8090369268086,\n",
       "   -460.8830808828382,\n",
       "   -734.4532179003618,\n",
       "   -498.6905977645025,\n",
       "   -492.5192055085366,\n",
       "   -830.3422192924195,\n",
       "   -1231.146236141981,\n",
       "   -3040.648023210589,\n",
       "   -5944.066662228111,\n",
       "   -4081.01956979979,\n",
       "   -9104.592117373111,\n",
       "   -18525.219109463953,\n",
       "   0.0,\n",
       "   -150739867.50433916,\n",
       "   -904909.8657898101,\n",
       "   -142892.92448088643,\n",
       "   -82231.10971080362,\n",
       "   -56252057244.54101,\n",
       "   -19210.98668092626,\n",
       "   -88682.51320819685,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -8185.2876924101965,\n",
       "   -6923.726477112974,\n",
       "   -5045.274676047082,\n",
       "   0.0,\n",
       "   -43623934.72358616,\n",
       "   -107386.05004233618,\n",
       "   -65759.43834646879,\n",
       "   -73483.18190808668,\n",
       "   -21672.378951239745,\n",
       "   -3082.901375925764,\n",
       "   -2120.500408740503,\n",
       "   -2619.941955351833,\n",
       "   -1124.0340806767338,\n",
       "   -1629.653743773701,\n",
       "   -985.4214335300335,\n",
       "   -1205.277235891618,\n",
       "   -1161.9307681324108,\n",
       "   -1348.920358892893,\n",
       "   -1106.295285396951,\n",
       "   -972.9975514006512,\n",
       "   -782.9502023414706,\n",
       "   -721.3322051837023,\n",
       "   -637.3603334698402,\n",
       "   -522.4627460900557,\n",
       "   -567.349466629311,\n",
       "   -727.6300225962244,\n",
       "   -886.8775265913532,\n",
       "   -676.8294893248857,\n",
       "   -642.0612141614039,\n",
       "   -515.6440400867874,\n",
       "   -824.7893733179156,\n",
       "   -1873.740656155938,\n",
       "   -1070.607159518415,\n",
       "   -1132.9997240145262,\n",
       "   -1430.8383506136154,\n",
       "   -1195.0037896095448,\n",
       "   -1146.3921718033414,\n",
       "   -970.1399531135376,\n",
       "   -870.6481945051681,\n",
       "   -637.8087837555748,\n",
       "   -566.8690076076128,\n",
       "   -498.10650016607286,\n",
       "   -735.4440312861661,\n",
       "   -760.4283768748516,\n",
       "   -507.0391583374129,\n",
       "   -331.05691260339256,\n",
       "   -408.70027287605683,\n",
       "   -438.3338665981121,\n",
       "   -602.8503357339366,\n",
       "   -626.7589869136901,\n",
       "   -537.2227476240773,\n",
       "   -653.4506701638289,\n",
       "   -1186.9512926916166,\n",
       "   -1632.6843064666468,\n",
       "   -5695.726492157127,\n",
       "   -1750780.0156689375,\n",
       "   -13826.62682146611,\n",
       "   -7923.987997719544,\n",
       "   -3831.769406987982,\n",
       "   -5111.58657499658,\n",
       "   -4128.828785114888,\n",
       "   -1684.9744439000717,\n",
       "   -744.5756205644303,\n",
       "   -560.9741073879667,\n",
       "   -412.9082290442582,\n",
       "   -376.89104151170665,\n",
       "   -386.52053267803285,\n",
       "   -520.4970931402722,\n",
       "   -740.5345684190789,\n",
       "   -1364.936191386319,\n",
       "   -2047.513518088234,\n",
       "   -1370.0791060575427,\n",
       "   -2200.960068478443,\n",
       "   -1725.3053031707504,\n",
       "   -892.0400655876462,\n",
       "   -1466.5089941661508,\n",
       "   -3046.19906950661,\n",
       "   -3119.410247512431,\n",
       "   -1321.492135984531,\n",
       "   -7173.511998125255,\n",
       "   -5161.774652922274,\n",
       "   -5098.4761419965225,\n",
       "   -2048.7982581996234,\n",
       "   -1390.222105478989,\n",
       "   -1744.216765030505,\n",
       "   -753.788384751352,\n",
       "   -862.642316613365,\n",
       "   -1255.2563741826987,\n",
       "   -3651.560522504844,\n",
       "   -1320.746077285952,\n",
       "   -2059.148154042665,\n",
       "   -2967.09891970524,\n",
       "   -1922.5804292530897,\n",
       "   -17774.27239487574,\n",
       "   -5272.570202756617,\n",
       "   -5000.7664302562835,\n",
       "   -10991.834599755732,\n",
       "   -9736.358752694934,\n",
       "   -36780.47964399769,\n",
       "   -5733.335477425032,\n",
       "   -56525.08602852018,\n",
       "   -460257.71627784014,\n",
       "   -2875.6080258316642,\n",
       "   -2644.762310959264,\n",
       "   -874.5637848463517,\n",
       "   -1147.697044173838,\n",
       "   -1809.6746541388427,\n",
       "   -2439.4061798480398,\n",
       "   -14454.775190863773,\n",
       "   -45431.63371315528,\n",
       "   -29516.147541698847,\n",
       "   -3993.84711553354,\n",
       "   -3804.053310191266,\n",
       "   -6473.206571861103,\n",
       "   -3529.0498121202745,\n",
       "   -10373.567185107157,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -40549.66339253534,\n",
       "   -11493.680651016304,\n",
       "   -4213.8843727022095,\n",
       "   -2905.976720403294,\n",
       "   -2315.1810464011755,\n",
       "   -3161.1751330827187,\n",
       "   -4401.233169103311,\n",
       "   -37129.40354860562,\n",
       "   -18175.168214970094,\n",
       "   -5261.9578775986265,\n",
       "   -8766.22324103089,\n",
       "   -15635.103309279446,\n",
       "   -2207.4451137647316,\n",
       "   -2636.2309782913503,\n",
       "   -3647.747758058423,\n",
       "   -1425.8312463189295,\n",
       "   -837.2864780026118,\n",
       "   -923.7296835197591,\n",
       "   -1392.3390319116263,\n",
       "   -695.8328746354595,\n",
       "   -651.0669651328186,\n",
       "   -478.054841033794,\n",
       "   -651.3455223152371,\n",
       "   -1146.4788680639404,\n",
       "   -512.4137680513069,\n",
       "   -481.7207456833073,\n",
       "   -864.3885076029766,\n",
       "   -1020.5987509142137,\n",
       "   -1192.1441585678047,\n",
       "   -1076.844572517743,\n",
       "   -1405.2269634260256,\n",
       "   -1120.9861352453265,\n",
       "   -985.8616536614903,\n",
       "   -1463.147999103193,\n",
       "   -1618.0455631614675,\n",
       "   -1071.638563174395,\n",
       "   -641.2657688436857,\n",
       "   -630.7428719288235,\n",
       "   -608.228736535693,\n",
       "   -541.7495427602087,\n",
       "   -615.7861603153891,\n",
       "   -616.7991126479249,\n",
       "   -563.0403752997788,\n",
       "   -491.17134832131615,\n",
       "   -520.2215906367477,\n",
       "   -384.8798785621579,\n",
       "   -493.29204388338337,\n",
       "   -385.1385465059348,\n",
       "   -329.22342071218736,\n",
       "   -373.82240118102436,\n",
       "   -273.13108602420624],\n",
       "  'ff_r2': [-1.7692103765037817,\n",
       "   -2.3290009460764503,\n",
       "   -2.718227545959881,\n",
       "   -4.339997401446136,\n",
       "   -9.075435414149165,\n",
       "   -19.469535283970636,\n",
       "   -41.15205873997212,\n",
       "   -27.021626944359657,\n",
       "   -15.773889461534406,\n",
       "   -10.097472828135402,\n",
       "   -7.388859284195743,\n",
       "   -6.1360646729553485,\n",
       "   -4.473214534018084,\n",
       "   -3.8380672385809005,\n",
       "   -3.1784556085770648,\n",
       "   -2.8863694436502545,\n",
       "   -2.6288532873732158,\n",
       "   -2.472294333783873,\n",
       "   -2.2803551211020965,\n",
       "   -2.2007681966331365,\n",
       "   -2.117797555663581,\n",
       "   -1.9885152318513062,\n",
       "   -1.9438453456237328,\n",
       "   -1.93115580990722,\n",
       "   -1.829251698267902,\n",
       "   -1.8044005592829553,\n",
       "   -1.7977980708492778,\n",
       "   -1.7698503621175252,\n",
       "   -1.761102888712676,\n",
       "   -1.7865569672914425,\n",
       "   -1.7605706381833364,\n",
       "   -1.7769509583729932,\n",
       "   -1.7819618730924178,\n",
       "   -1.7562168622928898,\n",
       "   -1.7831481219828715,\n",
       "   -1.7896492243693558,\n",
       "   -1.821003940173092,\n",
       "   -1.8416384157874361,\n",
       "   -1.8324190440436348,\n",
       "   -1.801751937185835,\n",
       "   -1.8448322410520919,\n",
       "   -1.795369755172184,\n",
       "   -1.7354816018901658,\n",
       "   -1.773958048183188,\n",
       "   -1.7764251373490447,\n",
       "   -1.8058964363013414,\n",
       "   -1.7919740757478166,\n",
       "   -1.7382714548798828,\n",
       "   -1.754974594278687,\n",
       "   -1.782582913684129,\n",
       "   -1.7588954660830325,\n",
       "   -1.8232804256948825,\n",
       "   -1.855805609419015,\n",
       "   -1.7785353007046192,\n",
       "   -1.7780543864089768,\n",
       "   -1.7489564349253608,\n",
       "   -1.7602770579051388,\n",
       "   -1.7528107882626958,\n",
       "   -1.7621589565603721,\n",
       "   -1.7595875856651806,\n",
       "   -1.742203220060785,\n",
       "   -1.7174446098959435,\n",
       "   -1.7416584153099937,\n",
       "   -1.7266149448477957,\n",
       "   -1.725405597186786,\n",
       "   -1.738208043520094,\n",
       "   -1.7579188107796968,\n",
       "   -1.7985357995430604,\n",
       "   -1.7832152301345263,\n",
       "   -1.7973538096808768,\n",
       "   -1.785863544993096,\n",
       "   -1.8311497789882463,\n",
       "   -1.8940710475442022,\n",
       "   -1.8726282369197476,\n",
       "   -1.8797136261847727,\n",
       "   -1.9013632970617058,\n",
       "   -1.8730476731433856,\n",
       "   -1.8651233432670655,\n",
       "   -1.907007268042666,\n",
       "   -1.8798507631832106,\n",
       "   -1.9035882004666824,\n",
       "   -1.8787956258763638,\n",
       "   -1.8960400692652168,\n",
       "   -1.8672543908043076,\n",
       "   -1.8871271578634405,\n",
       "   -1.8577033243813057,\n",
       "   -1.848803893044248,\n",
       "   -1.8819319182820324,\n",
       "   -1.9013120844028242,\n",
       "   -1.9313569317149457,\n",
       "   -1.898684519003928,\n",
       "   -1.884870921688489,\n",
       "   -1.8963606973666716,\n",
       "   -1.8878133509283788,\n",
       "   -1.8356299868658676,\n",
       "   -1.813122289285956,\n",
       "   -1.83474349176372,\n",
       "   -1.8529957911794415,\n",
       "   -1.8665340907941443,\n",
       "   -1.8205677735315744,\n",
       "   -1.812734690938544,\n",
       "   -1.8365757839926986,\n",
       "   -1.8331831158537306,\n",
       "   -1.8571745520529328,\n",
       "   -1.8547868300378614,\n",
       "   -1.8463968185215598,\n",
       "   -1.8277066401972162,\n",
       "   -1.8337543659730322,\n",
       "   -1.8615800224778365,\n",
       "   -1.869112249940347,\n",
       "   -1.826275319705839,\n",
       "   -1.8356780466041496,\n",
       "   -1.86617312852692,\n",
       "   -1.8661301920063194,\n",
       "   -1.8615791733118523,\n",
       "   -1.786686986962855,\n",
       "   -1.7649518973322547,\n",
       "   -1.8242650056638419,\n",
       "   -1.8444216324340839,\n",
       "   -1.8275942761010455,\n",
       "   -1.8368137879823303,\n",
       "   -1.8107428964803174,\n",
       "   -1.8471537258947044,\n",
       "   -1.8518906583080557,\n",
       "   -1.844197203247019,\n",
       "   -1.7956367436890757,\n",
       "   -1.8005818622196146,\n",
       "   -1.7903320204610234,\n",
       "   -1.7814232858298586,\n",
       "   -1.801762538763319,\n",
       "   -1.7936682877272387,\n",
       "   -1.8286903911250452,\n",
       "   -1.7836099749400245,\n",
       "   -1.7605669266177193,\n",
       "   -1.7608508118249406,\n",
       "   -1.7486206866748795,\n",
       "   -1.8226738104921223,\n",
       "   -1.8233198564253712,\n",
       "   -1.7836475821474953,\n",
       "   -1.8096807548470082,\n",
       "   -1.811729546261735,\n",
       "   -1.8278418555224722,\n",
       "   -1.8036643527955074,\n",
       "   -1.802222799493145,\n",
       "   -1.7974575983636525,\n",
       "   -1.8276363666192106,\n",
       "   -1.8617774638622757,\n",
       "   -1.9122863748905283,\n",
       "   -1.9146918026738908,\n",
       "   -1.9204862825548314,\n",
       "   -1.8865550270075326,\n",
       "   -1.8452630581146456,\n",
       "   -1.8275015963545682,\n",
       "   -1.8082071633399739,\n",
       "   -1.8514147784286719,\n",
       "   -1.8219735204424428,\n",
       "   -1.8215989785605724,\n",
       "   -1.8185843034955562,\n",
       "   -1.8312328577418602,\n",
       "   -1.8057791466509285,\n",
       "   -1.7518774507221804,\n",
       "   -1.7341172333678196,\n",
       "   -1.7900086321347133,\n",
       "   -1.8275494773873793,\n",
       "   -1.8569115279789856,\n",
       "   -1.8593965504114287,\n",
       "   -1.8414581011219848,\n",
       "   -1.8356981529470335,\n",
       "   -1.8135575273109903,\n",
       "   -1.820843833840772,\n",
       "   -1.8456926958346251,\n",
       "   -1.8292110250280484,\n",
       "   -1.8263298343840098,\n",
       "   -1.8313221156255697,\n",
       "   -1.8939668475630498,\n",
       "   -1.871696099696969,\n",
       "   -1.8861986800254225,\n",
       "   -1.8791974280371009,\n",
       "   -1.8723315809360255,\n",
       "   -1.9177145499729646,\n",
       "   -1.8383619601782186,\n",
       "   -1.8306572268307901,\n",
       "   -1.8412734147481675,\n",
       "   -1.8360322808689218,\n",
       "   -1.8172522173329644,\n",
       "   -1.8205851796763812,\n",
       "   -1.8008049217716962,\n",
       "   -1.8234627127596772,\n",
       "   -1.8467875837531889,\n",
       "   -1.8191504853882807,\n",
       "   -1.7568618689704274,\n",
       "   -1.77379626652385,\n",
       "   -1.7808069589660236,\n",
       "   -1.7935032310704941,\n",
       "   -1.8197401808701748,\n",
       "   -1.810034320663008,\n",
       "   -1.8006592557859435,\n",
       "   -1.83367836839325,\n",
       "   -1.752220369159451,\n",
       "   -1.7815893975062562,\n",
       "   -1.8038463607518667,\n",
       "   -1.789000048168019,\n",
       "   -1.795383710768177,\n",
       "   -1.7692610804779023,\n",
       "   -1.7545258205077543,\n",
       "   -1.7749643948613443,\n",
       "   -1.779921460783843,\n",
       "   -1.752157236111648,\n",
       "   -1.811157248031344,\n",
       "   -1.8009443370017433,\n",
       "   -1.769066948909206,\n",
       "   -1.7711727304378941,\n",
       "   -1.7933344567343248,\n",
       "   -1.8253496551167845,\n",
       "   -1.8212450499477657,\n",
       "   -1.8447557559584928,\n",
       "   -1.85226058998061,\n",
       "   -1.8384309852025895,\n",
       "   -1.8113039704030007,\n",
       "   -1.8161482915656655,\n",
       "   -1.7727538990119873,\n",
       "   -1.761702246904731,\n",
       "   -1.7556803524090077,\n",
       "   -1.7470608958476475,\n",
       "   -1.7001067046293974,\n",
       "   -1.6930882504418343,\n",
       "   -1.724706251715665,\n",
       "   -1.7385517785347506,\n",
       "   -1.758979659085964,\n",
       "   -1.8074004819488376,\n",
       "   -1.8488844898514056,\n",
       "   -1.8398573509560205,\n",
       "   -1.9183216401904115,\n",
       "   -1.868687934229325,\n",
       "   -1.8530485808830384,\n",
       "   -1.9167707319314595,\n",
       "   -1.9186556218831612,\n",
       "   -1.8747678661494485,\n",
       "   -1.8728332643572219,\n",
       "   -1.8634650627168163,\n",
       "   -1.850358142241134,\n",
       "   -1.8611718001801778,\n",
       "   -1.7801705176174205,\n",
       "   -1.7755124976542551,\n",
       "   -1.7908783602167206,\n",
       "   -1.7344024893573589,\n",
       "   -1.7698934629869156,\n",
       "   -1.7339919679843296,\n",
       "   -1.7330059422579924,\n",
       "   -1.7577902784612025,\n",
       "   -1.770768454205402,\n",
       "   -1.7551756307589654,\n",
       "   -1.7282985369620913,\n",
       "   -1.7443147656209108,\n",
       "   -1.7752102556396419,\n",
       "   -1.8164046796686684,\n",
       "   -1.8100693216454107,\n",
       "   -1.8053498823467473,\n",
       "   -1.8464542572012879,\n",
       "   -1.8245995766899457,\n",
       "   -1.7965428297242774,\n",
       "   -1.7984075853529142,\n",
       "   -1.8511069322693423,\n",
       "   -1.8655895091331969,\n",
       "   -1.8918618104713087,\n",
       "   -1.8503569977178262,\n",
       "   -1.8819683370557847,\n",
       "   -1.939361011959603,\n",
       "   -1.931822756262616,\n",
       "   -1.9508862554121147,\n",
       "   -1.8889253863445759,\n",
       "   -1.8781274253385178,\n",
       "   -1.8031380375006298,\n",
       "   -1.8178934339719954,\n",
       "   -1.811802140298978,\n",
       "   -1.7895725031123613,\n",
       "   -1.8142185008911107,\n",
       "   -1.758766957060391,\n",
       "   -1.7920642584476947,\n",
       "   -1.7024964270508645,\n",
       "   -1.7307346596749378,\n",
       "   -1.7561449157523885,\n",
       "   -1.7487518971785452,\n",
       "   -1.7298441525505983,\n",
       "   -1.7454619891526817,\n",
       "   -1.7640073996478018,\n",
       "   -1.7344666255091452,\n",
       "   -1.7454252931353835,\n",
       "   -1.6999950745910315,\n",
       "   -1.7186533763200282,\n",
       "   -1.691160753921257,\n",
       "   -1.7212514600285922,\n",
       "   -1.7393384353441732,\n",
       "   -1.7477029856355757,\n",
       "   -1.7551702898476447,\n",
       "   -1.7309940547383134,\n",
       "   -1.7035417480298873,\n",
       "   -1.68882545955696,\n",
       "   -1.6641146124553892,\n",
       "   -1.7144327974519329,\n",
       "   -1.7325954803458106,\n",
       "   -1.7134890709530786,\n",
       "   -1.7364572153319275,\n",
       "   -1.783813416231057,\n",
       "   -1.780436416599792,\n",
       "   -1.799812051316437,\n",
       "   -1.7363417678103232,\n",
       "   -1.7726733118279414,\n",
       "   -1.7697696701545715,\n",
       "   -1.7995615148739792,\n",
       "   -1.819395758146379,\n",
       "   -1.8169467367897827,\n",
       "   -1.765957654396105,\n",
       "   -1.8015146528361652,\n",
       "   -1.7771443334838968,\n",
       "   -1.8059134444024858,\n",
       "   -1.8270291240540382,\n",
       "   -1.8090076393309964,\n",
       "   -1.773992087495476,\n",
       "   -1.7709701138391263,\n",
       "   -1.8029721295541665,\n",
       "   -1.8286485048476857,\n",
       "   -1.8018957291503854,\n",
       "   -1.8126149493656336,\n",
       "   -1.8266955925944015,\n",
       "   -1.8122990071659042,\n",
       "   -1.8102231648679155,\n",
       "   -1.8166452663136616,\n",
       "   -1.7603987551211624,\n",
       "   -1.73741725034788,\n",
       "   -1.7506150239239573,\n",
       "   -1.7499676687735133,\n",
       "   -1.7375043168082898,\n",
       "   -1.7466814548694116,\n",
       "   -1.750658690918439,\n",
       "   -1.7078703685289764,\n",
       "   -1.717167497766423,\n",
       "   -1.725124837591899,\n",
       "   -1.732589431588151,\n",
       "   -1.6719008863695004,\n",
       "   -1.657431010263807,\n",
       "   -1.6646098170034702,\n",
       "   -1.6886989223677662,\n",
       "   -1.7222385399829725,\n",
       "   -1.7401322381337558,\n",
       "   -1.7311050156768921,\n",
       "   -1.7443784425730975,\n",
       "   -1.7483355859870708,\n",
       "   -1.761033523885986,\n",
       "   -1.7893484243057833,\n",
       "   -1.8161755753920312,\n",
       "   -1.7775929799362093,\n",
       "   -1.8493732973603714,\n",
       "   -1.8621207462104117,\n",
       "   -1.8632815519798145,\n",
       "   -1.8220350482321677,\n",
       "   -1.8103073508779293,\n",
       "   -1.8547472162734957,\n",
       "   -1.8651267865385175,\n",
       "   -1.8328359791049955,\n",
       "   -1.7921883258713467,\n",
       "   -1.8118281280101556,\n",
       "   -1.7836909741715186,\n",
       "   -1.7550192532397304,\n",
       "   -1.7447375936253513,\n",
       "   -1.7670557046977495,\n",
       "   -1.7276369592140082,\n",
       "   -1.7894235476038407,\n",
       "   -1.8427576520329687,\n",
       "   -1.85746329374104,\n",
       "   -1.8736955120422283,\n",
       "   -1.810582467862098,\n",
       "   -1.7856382483271451,\n",
       "   -1.799116075145998,\n",
       "   -1.784238117697432,\n",
       "   -1.8084981691926045,\n",
       "   -1.8108787112458966,\n",
       "   -1.8218115234567192,\n",
       "   -1.8160702743095634,\n",
       "   -1.84376714114231,\n",
       "   -1.8443938742828903,\n",
       "   -1.884524788098477,\n",
       "   -1.8901301425366426,\n",
       "   -1.9298836243559667,\n",
       "   -1.92775646539208,\n",
       "   -1.8968700199154775,\n",
       "   -1.8597150159583427,\n",
       "   -1.918225908558425,\n",
       "   -1.9192466442208125,\n",
       "   -1.8869531543567688,\n",
       "   -1.8851714662404664,\n",
       "   -1.852224535316621,\n",
       "   -1.8462834437072346,\n",
       "   -1.8919817907155059,\n",
       "   -1.8793355441194182,\n",
       "   -1.9208581746456375,\n",
       "   -1.9773043212810917,\n",
       "   -1.9658904999949756,\n",
       "   -1.9237363187336167,\n",
       "   -1.9139902973103826,\n",
       "   -1.848332746585664,\n",
       "   -1.8505678858802517,\n",
       "   -1.8470821630153083,\n",
       "   -1.780253136089975,\n",
       "   -1.7850948500975248,\n",
       "   -1.8327579705969996,\n",
       "   -1.8374840110813122,\n",
       "   -1.8220518143816333,\n",
       "   -1.8211116435219203,\n",
       "   -1.8090494554794154,\n",
       "   -1.7942157483843708,\n",
       "   -1.7963362437924184,\n",
       "   -1.8238233759489342,\n",
       "   -1.8211250111087138,\n",
       "   -1.807634312331126,\n",
       "   -1.8541276558918232,\n",
       "   -1.8192476035620913,\n",
       "   -1.8601502725307237,\n",
       "   -1.8190005682397379,\n",
       "   -1.8157957609522093,\n",
       "   -1.8458430530424197,\n",
       "   -1.884096407439022,\n",
       "   -1.8318501362071786,\n",
       "   -1.8184882135334282,\n",
       "   -1.8258486095108881,\n",
       "   -1.8292828861696013,\n",
       "   -1.8088970601054015,\n",
       "   -1.7971401910973022,\n",
       "   -1.7707534422363693,\n",
       "   -1.7958253714724255,\n",
       "   -1.8494323420485528,\n",
       "   -1.8756244545960432,\n",
       "   -1.881098934440169,\n",
       "   -1.8670288766436496,\n",
       "   -1.8310810205970798,\n",
       "   -1.790213010794739,\n",
       "   -1.7108260179711987,\n",
       "   -1.7156026730991836,\n",
       "   -1.697196115912702,\n",
       "   -1.733729948393083,\n",
       "   -1.7474059734541099,\n",
       "   -1.7829716936531277,\n",
       "   -1.7673767634097155,\n",
       "   -1.698278349670356,\n",
       "   -1.7014698421073948,\n",
       "   -1.692730719574393,\n",
       "   -1.7113918956886711,\n",
       "   -1.751238679500756,\n",
       "   -1.782466379595054,\n",
       "   -1.7869912594387602,\n",
       "   -1.7800614056303292,\n",
       "   -1.7904992565689088,\n",
       "   -1.8393891240671212,\n",
       "   -1.8223952018805902,\n",
       "   -1.8058800026728075,\n",
       "   -1.7982849911422978,\n",
       "   -1.82257337024462,\n",
       "   -1.8510058496818451,\n",
       "   -1.8140320070580187,\n",
       "   -1.816147006365647,\n",
       "   -1.8200960967953876,\n",
       "   -1.8070361718589725,\n",
       "   -1.8385491550029855,\n",
       "   -1.823085963168296,\n",
       "   -1.765018520625433,\n",
       "   -1.76382756686895,\n",
       "   -1.787508611185748,\n",
       "   -1.7738266197617518,\n",
       "   -1.7794794225979085,\n",
       "   -1.8155084216426975,\n",
       "   -1.7972212475400449,\n",
       "   -1.826952266717091,\n",
       "   -1.81457501808072,\n",
       "   -1.7851053921077296,\n",
       "   -1.7226692647228523,\n",
       "   -1.7424787260877026,\n",
       "   -1.7211995153194866,\n",
       "   -1.7228457605698448,\n",
       "   -1.7139553134185768,\n",
       "   -1.7370140592634526,\n",
       "   -1.7681211370122227,\n",
       "   -1.7582326335249627,\n",
       "   -1.7344783648993682,\n",
       "   -1.7699205752691562,\n",
       "   -1.756585800024045,\n",
       "   -1.7408471509378387,\n",
       "   -1.7167081924226513,\n",
       "   -1.7910837744655592,\n",
       "   -1.7949180387682717,\n",
       "   -1.7920413374620163,\n",
       "   -1.7668843851677174,\n",
       "   -1.7273873615216337,\n",
       "   -1.755290309008926,\n",
       "   -1.8191072586440002,\n",
       "   -1.7966668826866972,\n",
       "   -1.829949280675053,\n",
       "   -1.8080164014024458,\n",
       "   -1.77684408952782,\n",
       "   -1.7371544182046317,\n",
       "   -1.736792313915136],\n",
       "  'test_r2s': [-5.108518718508211,\n",
       "   -4.371535018868094,\n",
       "   -3.8580238075547673,\n",
       "   -4.9011603668820785,\n",
       "   -9.284425074909244,\n",
       "   -19.5558503579197,\n",
       "   -41.144772474278966,\n",
       "   -26.93228198661562,\n",
       "   -49910.18561114773,\n",
       "   -12842.511431102452,\n",
       "   -7334.292815350691,\n",
       "   -611.9977871767381,\n",
       "   -1017.3700963362586,\n",
       "   -1076.4243530350423,\n",
       "   -900.193035597599,\n",
       "   -743.8480659923645,\n",
       "   -699.439318149051,\n",
       "   -997.7266451238389,\n",
       "   -1338.4521682934217,\n",
       "   -3832.7701321785635,\n",
       "   -11161.567862889773,\n",
       "   -6277.543734244816,\n",
       "   -116446.37078036314,\n",
       "   -2.1289652649341715,\n",
       "   -2.177976261252942,\n",
       "   -299461.6220689608,\n",
       "   -420055.4134153186,\n",
       "   -2.1852904352800695,\n",
       "   -13484.100168855382,\n",
       "   -4149.237439430155,\n",
       "   -2303.485417408231,\n",
       "   -2840.907302854148,\n",
       "   -1555.8580854356576,\n",
       "   -1948.5244978540734,\n",
       "   -1571.7425938168742,\n",
       "   -1856.3228730887472,\n",
       "   -2487.295671882427,\n",
       "   -14108.635807249797,\n",
       "   -22504.807593674464,\n",
       "   -8969.40510858165,\n",
       "   -4688.342746292276,\n",
       "   -3756.0805485398564,\n",
       "   -29778.799639234592,\n",
       "   -182076.44922566827,\n",
       "   -4087122.2824058384,\n",
       "   -2.567832743102681,\n",
       "   -2.46737813794957,\n",
       "   -30660.038200253333,\n",
       "   -2.4502613756531764,\n",
       "   -2.347265421229052,\n",
       "   -2.462969575482152,\n",
       "   -2.6925826176693857,\n",
       "   -2.7136204890440223,\n",
       "   -2.7094311817016052,\n",
       "   -15332592.29269078,\n",
       "   -151584.29370293318,\n",
       "   -20160.17039715865,\n",
       "   -23716.148548308774,\n",
       "   -2.6069833441469408,\n",
       "   -2.649576159866969,\n",
       "   -2.5544843413331715,\n",
       "   -2.454269012210534,\n",
       "   -2.37335748108553,\n",
       "   -141506.85473584442,\n",
       "   -2.1418993049707913,\n",
       "   -2.246900508615406,\n",
       "   -2.430899176762164,\n",
       "   -2.614932267172634,\n",
       "   -2.5806239103520854,\n",
       "   -2.5786919308178486,\n",
       "   -2.5852520644881336,\n",
       "   -2.7494667649012636,\n",
       "   -2.885072870362845,\n",
       "   -2.7818846096188814,\n",
       "   -2.538867080311295,\n",
       "   -2.4841959231243305,\n",
       "   -6668.640816193111,\n",
       "   -3404.8494614713013,\n",
       "   -4005.834102692188,\n",
       "   -4005.2849336027994,\n",
       "   -8841.781908417423,\n",
       "   -3376.1967128968636,\n",
       "   -1707.4863087819122,\n",
       "   -1532.293777087789,\n",
       "   -1407.2256963498257,\n",
       "   -1114.0571300439128,\n",
       "   -682.3772673467445,\n",
       "   -944.7229916384797,\n",
       "   -1079.1313046246134,\n",
       "   -1299.030643082991,\n",
       "   -3333.6856210386186,\n",
       "   -4407.030181071806,\n",
       "   -2502.011861866031,\n",
       "   -6977.045405594083,\n",
       "   -10237.738951253536,\n",
       "   -19275.866687566864,\n",
       "   -46301.13889181521,\n",
       "   -3332.9683573574434,\n",
       "   -10164.987223797847,\n",
       "   -45865.870704148765,\n",
       "   -44244.90366233066,\n",
       "   -6091.50296197492,\n",
       "   -2674.710947586738,\n",
       "   -1370.3189720276434,\n",
       "   -1689.8890639063386,\n",
       "   -1203.9362089416495,\n",
       "   -1953.6530754210376,\n",
       "   -4021.6353369150515,\n",
       "   -1182.2199546020825,\n",
       "   -707.2805334278138,\n",
       "   -506.86961103566136,\n",
       "   -465.45208729412275,\n",
       "   -581.8407783743147,\n",
       "   -850.2456423028082,\n",
       "   -1122.8469764722545,\n",
       "   -691.2585772054134,\n",
       "   -1839.578008252314,\n",
       "   -1297.334790272116,\n",
       "   -1164.3337816097771,\n",
       "   -2157.506604368943,\n",
       "   -1999.7711262913617,\n",
       "   -1043.6956148801116,\n",
       "   -1080.0490206165728,\n",
       "   -908.1498482526063,\n",
       "   -1971.693928853943,\n",
       "   -5934.800515463666,\n",
       "   -4053.990867822832,\n",
       "   -3651.5003112078925,\n",
       "   -1586.730337778194,\n",
       "   -927.2107973884049,\n",
       "   -1053.6689112050587,\n",
       "   -1253.7413031177778,\n",
       "   -602.3264810667206,\n",
       "   -474.45893907857527,\n",
       "   -583.6745703048991,\n",
       "   -565.5262647876573,\n",
       "   -491.7439399311726,\n",
       "   -524.7864056075464,\n",
       "   -456.6573859154367,\n",
       "   -427.71368040187633,\n",
       "   -804.807161574469,\n",
       "   -990.9888266292694,\n",
       "   -702.1196532677858,\n",
       "   -2203.6126162612736,\n",
       "   -1233.7737851111756,\n",
       "   -808.8941243412444,\n",
       "   -929.4805854308537,\n",
       "   -1373.1628430627622,\n",
       "   -693.8279374305772,\n",
       "   -689.2874365491155,\n",
       "   -451.5255115845785,\n",
       "   -464.20075881315086,\n",
       "   -356.77885314531596,\n",
       "   -300.1805685489232,\n",
       "   -443.1806995847099,\n",
       "   -451.90389574542525,\n",
       "   -438.68742387076264,\n",
       "   -481.7716382417036,\n",
       "   -519.1717126222266,\n",
       "   -647.9041417098795,\n",
       "   -659.2439280100336,\n",
       "   -1216.01413240848,\n",
       "   -2248.4919107544088,\n",
       "   -2461.8044276331893,\n",
       "   -3741.0918160309293,\n",
       "   -4280.458438824933,\n",
       "   -7432.8102095118475,\n",
       "   -97295.14839775937,\n",
       "   -3112.334340628702,\n",
       "   -3927.460459073822,\n",
       "   -12220.053034258735,\n",
       "   -59199.35283565769,\n",
       "   -8818.922166088561,\n",
       "   -16179.798909614114,\n",
       "   -8629.96710617189,\n",
       "   -4003.255526759298,\n",
       "   -9769.59749252194,\n",
       "   -13211.322195486908,\n",
       "   -4117.5148657978,\n",
       "   -12488.414595909575,\n",
       "   -1500.5580391314072,\n",
       "   -3299.099030959324,\n",
       "   -3407.532423679059,\n",
       "   -4484.028776787887,\n",
       "   -2937.5648133458903,\n",
       "   -2042.2927198669183,\n",
       "   -1985.7772509979295,\n",
       "   -3576.499706626121,\n",
       "   -3970.7494161208597,\n",
       "   -3184.3441866794396,\n",
       "   -3500.3207747990637,\n",
       "   -23985.785776920504,\n",
       "   -118259.48926462424,\n",
       "   -5768.431329154245,\n",
       "   -6858.696852595394,\n",
       "   -4842.090225693108,\n",
       "   -11264.802066509352,\n",
       "   -3855.604867790934,\n",
       "   -2378.966727472799,\n",
       "   -1433.1549041050616,\n",
       "   -2528.8051664873637,\n",
       "   -1620.0981372646695,\n",
       "   -3826.113179369614,\n",
       "   -4629.89727561193,\n",
       "   -4683.47612546064,\n",
       "   -4536.740646526857,\n",
       "   -61871.36121341896,\n",
       "   -126796.51826178165,\n",
       "   -18353.157669861266,\n",
       "   -12431.264418501465,\n",
       "   -2003.6569997689717,\n",
       "   -2381.435345983408,\n",
       "   -7155.093024740868,\n",
       "   -5528.008449280935,\n",
       "   -13585.854092195175,\n",
       "   -10171.820351534045,\n",
       "   -4918.483692446469,\n",
       "   -4800.905102697734,\n",
       "   -6152.557740781856,\n",
       "   -1668.2138040793732,\n",
       "   -11308.735559913675,\n",
       "   -134130.23032787783,\n",
       "   -20022.967417308853,\n",
       "   -2870.105947284397,\n",
       "   -1646.050971536328,\n",
       "   -1735.690649129962,\n",
       "   -1397.8975463438662,\n",
       "   -1626.1558137776028,\n",
       "   -1909.9164531013657,\n",
       "   -787.1960656019429,\n",
       "   -1038.412772571552,\n",
       "   -6617.770322841047,\n",
       "   -1943.3990931651767,\n",
       "   -30698.320691597688,\n",
       "   -223614.16998835027,\n",
       "   -40763.665308900214,\n",
       "   -3.3184470189599575,\n",
       "   -3.2041061066282976,\n",
       "   -21529015.33885133,\n",
       "   -47419.895063486765,\n",
       "   -17657.47982827931,\n",
       "   -3.003560502169024,\n",
       "   -2.906588980841738,\n",
       "   -2.791799739387942,\n",
       "   -2.7865847339038505,\n",
       "   -2.6653740856130494,\n",
       "   -441114.7583868147,\n",
       "   -32771.027171195055,\n",
       "   -450660.3490608391,\n",
       "   -18970.642617567984,\n",
       "   -3230.6564622761202,\n",
       "   -1058.2054275742005,\n",
       "   -1008.7263270186838,\n",
       "   -828.9517046827596,\n",
       "   -726.5899053983044,\n",
       "   -1007.8933026154244,\n",
       "   -1674.4003213868486,\n",
       "   -1409.58847597997,\n",
       "   -2281.4847110471096,\n",
       "   -1782.2417669831955,\n",
       "   -2418.9514869547397,\n",
       "   -9114.088282835848,\n",
       "   -7853.493508475737,\n",
       "   -37526.087814772945,\n",
       "   -377363.9036065054,\n",
       "   -7635357.194454792,\n",
       "   -21526.117359591346,\n",
       "   -125436.99590273373,\n",
       "   -41771.43918630008,\n",
       "   -9569.869701349957,\n",
       "   -2406.946692716986,\n",
       "   -7165.054602036037,\n",
       "   -8067.647504525157,\n",
       "   -6072.332407385061,\n",
       "   -2533.795075825652,\n",
       "   -5753.640498552274,\n",
       "   -15216.859828731647,\n",
       "   -3341.454072570435,\n",
       "   -2340.281133112574,\n",
       "   -2289.139961900238,\n",
       "   -1386.3314719342598,\n",
       "   -1142.44720786601,\n",
       "   -1232.7768584323414,\n",
       "   -1012.7025104630833,\n",
       "   -1896.3370415425477,\n",
       "   -2700.846427896024,\n",
       "   -2591.3911255573303,\n",
       "   -3508.8077162215573,\n",
       "   -1495.6571050854802,\n",
       "   -731.523529717654,\n",
       "   -773.8233227830092,\n",
       "   -1160.443623705141,\n",
       "   -1960.625958605412,\n",
       "   -881.6988360943626,\n",
       "   -860.5770675288346,\n",
       "   -2228.0356949025404,\n",
       "   -2703.4829887808255,\n",
       "   -1630.0534697325777,\n",
       "   -1967.8147674070517,\n",
       "   -3767.7298683624217,\n",
       "   -1961.2140182854728,\n",
       "   -1667.601312295928,\n",
       "   -1822.9981205851348,\n",
       "   -986.8524460372284,\n",
       "   -396.391121918761,\n",
       "   -327.60103265294856,\n",
       "   -437.31419152514576,\n",
       "   -357.6082061554067,\n",
       "   -464.65428127718076,\n",
       "   -738.1044976562931,\n",
       "   -502.7405647122858,\n",
       "   -497.61852713510325,\n",
       "   -835.2596914767468,\n",
       "   -1235.9664912244227,\n",
       "   -3045.7541515655953,\n",
       "   -5949.057278055212,\n",
       "   -4086.4480836924686,\n",
       "   -9110.11392626073,\n",
       "   -18530.379783151267,\n",
       "   -4.495779407265816,\n",
       "   -150739872.4558948,\n",
       "   -904914.0854616317,\n",
       "   -142898.72769127536,\n",
       "   -82236.96326936467,\n",
       "   -56252057250.0222,\n",
       "   -19215.965781937244,\n",
       "   -88686.76120929372,\n",
       "   -4.594596558988039,\n",
       "   -3.9127173982661985,\n",
       "   -4.327525258822121,\n",
       "   -8190.169409801026,\n",
       "   -6929.028741134046,\n",
       "   -5050.291863934949,\n",
       "   -5.9330677444775635,\n",
       "   -43623940.68737713,\n",
       "   -107393.47409213595,\n",
       "   -65767.17665825026,\n",
       "   -73492.73034162346,\n",
       "   -21681.23077302102,\n",
       "   -3088.780856617315,\n",
       "   -2126.2458520106425,\n",
       "   -2624.991025859634,\n",
       "   -1129.1636191270884,\n",
       "   -1634.0463749404798,\n",
       "   -989.4843560856337,\n",
       "   -1209.402987182499,\n",
       "   -1166.2419039596864,\n",
       "   -1352.981435588655,\n",
       "   -1109.9089209806853,\n",
       "   -977.1231306010886,\n",
       "   -787.2850034836374,\n",
       "   -726.2936624052517,\n",
       "   -643.1581227054938,\n",
       "   -530.6125680550223,\n",
       "   -574.7709668061368,\n",
       "   -734.4109486397325,\n",
       "   -893.7316939327925,\n",
       "   -683.0635050794787,\n",
       "   -647.7052160573037,\n",
       "   -522.057238101837,\n",
       "   -830.7844998432206,\n",
       "   -1879.2213094592898,\n",
       "   -1075.9389414510001,\n",
       "   -1137.5114809431298,\n",
       "   -1435.6213714364512,\n",
       "   -1199.809250508412,\n",
       "   -1151.0061646076404,\n",
       "   -976.3199887747496,\n",
       "   -877.1340618325145,\n",
       "   -644.3718176500491,\n",
       "   -574.2393689979284,\n",
       "   -505.0694254837815,\n",
       "   -743.0831182366584,\n",
       "   -769.25521669787,\n",
       "   -516.637996889669,\n",
       "   -340.5172056263366,\n",
       "   -420.734260749281,\n",
       "   -450.0296293576971,\n",
       "   -616.3595722660764,\n",
       "   -638.7341764635198,\n",
       "   -554.13980617932,\n",
       "   -671.2948831949392,\n",
       "   -1200.4011757258943,\n",
       "   -1644.1134005574409,\n",
       "   -5708.221979304174,\n",
       "   -1750791.3710244251,\n",
       "   -13836.535000476444,\n",
       "   -7932.879075832576,\n",
       "   -3841.917837108206,\n",
       "   -5119.25642841579,\n",
       "   -4137.9804375907715,\n",
       "   -1693.7907865577615,\n",
       "   -752.2755589776369,\n",
       "   -570.4858734084397,\n",
       "   -422.4268573545321,\n",
       "   -384.6768705452318,\n",
       "   -394.3986035772386,\n",
       "   -530.1019164771229,\n",
       "   -749.5610993548056,\n",
       "   -1374.329854683685,\n",
       "   -2057.9455283596785,\n",
       "   -1384.3681913497612,\n",
       "   -2217.812629877425,\n",
       "   -1742.0852246623688,\n",
       "   -906.6493935207395,\n",
       "   -1480.9235262029927,\n",
       "   -3061.7935818707565,\n",
       "   -3133.054204549618,\n",
       "   -1333.327346923274,\n",
       "   -7186.326456018774,\n",
       "   -5173.899959701175,\n",
       "   -5109.694640479331,\n",
       "   -2065.460556112736,\n",
       "   -1411.454340978342,\n",
       "   -1760.9238230595436,\n",
       "   -774.9479727824767,\n",
       "   -889.7608751106497,\n",
       "   -1304.031569473537,\n",
       "   -3679.224133303026,\n",
       "   -1351.3956009995038,\n",
       "   -2094.361747897003,\n",
       "   -3016.345811205674,\n",
       "   -1963.413177856009,\n",
       "   -17819.718914081535,\n",
       "   -5315.872903585445,\n",
       "   -5031.530024443241,\n",
       "   -11016.235521579309,\n",
       "   -9757.521697965798,\n",
       "   -36805.32152439444,\n",
       "   -5754.037476345577,\n",
       "   -56543.83493735483,\n",
       "   -460277.30996087793,\n",
       "   -2894.327682830336,\n",
       "   -2665.1144915559507,\n",
       "   -899.303608881696,\n",
       "   -1172.6066830015461,\n",
       "   -1841.9353527492672,\n",
       "   -2469.015372059018,\n",
       "   -14478.443777868486,\n",
       "   -45461.62441978491,\n",
       "   -29549.31687713011,\n",
       "   -4022.7097312846954,\n",
       "   -3836.6378269513884,\n",
       "   -6497.780892261542,\n",
       "   -3554.7869361729654,\n",
       "   -10403.347373015717,\n",
       "   -35.99975996374841,\n",
       "   -28.691618486349736,\n",
       "   -33.01189836531781,\n",
       "   -40583.29477324321,\n",
       "   -11526.0864867662,\n",
       "   -4248.99063656384,\n",
       "   -2939.110863539076,\n",
       "   -2351.0259574085085,\n",
       "   -3195.3455343913292,\n",
       "   -4427.1200471059165,\n",
       "   -37149.992724292424,\n",
       "   -18198.77287859418,\n",
       "   -5290.398162415024,\n",
       "   -8794.661213624535,\n",
       "   -15664.254199749323,\n",
       "   -2241.3393032604413,\n",
       "   -2670.0607554453545,\n",
       "   -3687.525104798672,\n",
       "   -1467.0865261892034,\n",
       "   -884.017880731214,\n",
       "   -968.1634622469826,\n",
       "   -1428.7320805229651,\n",
       "   -738.2072736321198,\n",
       "   -693.2980385120827,\n",
       "   -525.2953296568461,\n",
       "   -687.2565681565243,\n",
       "   -1185.0750728670798,\n",
       "   -557.7031331042176,\n",
       "   -517.2413895368585,\n",
       "   -899.9625868933559,\n",
       "   -1049.5053273188057,\n",
       "   -1231.5641347135384,\n",
       "   -1108.5891426305911,\n",
       "   -1441.5940831182347,\n",
       "   -1155.70956432248,\n",
       "   -1018.5620672076279,\n",
       "   -1490.3472294384235,\n",
       "   -1650.9461054644287,\n",
       "   -1108.9171494569691,\n",
       "   -675.3707790323648,\n",
       "   -664.3036021252846,\n",
       "   -645.9637499539991,\n",
       "   -582.0004238541604,\n",
       "   -647.5362570396674,\n",
       "   -646.8725346931733,\n",
       "   -595.472139652466,\n",
       "   -523.8888993938032,\n",
       "   -550.4549735524446,\n",
       "   -414.2751277543769,\n",
       "   -527.716827393376,\n",
       "   -418.43482735336346,\n",
       "   -361.62648765810496,\n",
       "   -408.7445879526887,\n",
       "   -305.38673051376804],\n",
       "  'train_pce_loss': [2.4995570182800293,\n",
       "   2.588369607925415,\n",
       "   2.138327121734619,\n",
       "   2.16208553314209,\n",
       "   2.764173746109009,\n",
       "   2.4930591583251953,\n",
       "   2.600080728530884,\n",
       "   2.4138855934143066,\n",
       "   2.0634989738464355,\n",
       "   2.5743680000305176,\n",
       "   2.4398436546325684,\n",
       "   2.5144126415252686,\n",
       "   2.451598644256592,\n",
       "   2.758876085281372,\n",
       "   2.116152763366699,\n",
       "   2.483452796936035,\n",
       "   2.4049291610717773,\n",
       "   2.423478126525879,\n",
       "   2.4619250297546387,\n",
       "   2.4914052486419678,\n",
       "   1.9665180444717407,\n",
       "   2.7508294582366943,\n",
       "   2.3889172077178955,\n",
       "   2.2870748043060303,\n",
       "   2.343125581741333,\n",
       "   2.451608180999756,\n",
       "   2.0386219024658203,\n",
       "   2.352872371673584,\n",
       "   2.657017469406128,\n",
       "   2.2987754344940186,\n",
       "   2.2368767261505127,\n",
       "   2.421034812927246,\n",
       "   2.872042655944824,\n",
       "   1.9606324434280396,\n",
       "   2.287078857421875,\n",
       "   2.1370139122009277,\n",
       "   1.9707297086715698,\n",
       "   2.155576467514038,\n",
       "   2.331512928009033,\n",
       "   2.4748213291168213,\n",
       "   2.5077974796295166,\n",
       "   2.2569117546081543,\n",
       "   2.163884162902832,\n",
       "   2.0493009090423584,\n",
       "   2.190060615539551,\n",
       "   2.804912805557251,\n",
       "   2.091245651245117,\n",
       "   2.7856087684631348,\n",
       "   2.597991943359375,\n",
       "   2.772235631942749,\n",
       "   2.963172435760498,\n",
       "   2.387645721435547,\n",
       "   2.1342997550964355,\n",
       "   1.9162405729293823,\n",
       "   1.8760855197906494,\n",
       "   2.6912410259246826,\n",
       "   2.3050904273986816,\n",
       "   2.618919610977173,\n",
       "   2.2670469284057617,\n",
       "   2.0490365028381348,\n",
       "   2.3454694747924805,\n",
       "   2.724102735519409,\n",
       "   1.934418797492981,\n",
       "   2.2824482917785645,\n",
       "   2.4598472118377686,\n",
       "   2.548139810562134,\n",
       "   2.2049365043640137,\n",
       "   2.5222866535186768,\n",
       "   2.5098869800567627,\n",
       "   2.295546531677246,\n",
       "   2.3811309337615967,\n",
       "   2.4780197143554688,\n",
       "   2.1453287601470947,\n",
       "   2.2307522296905518,\n",
       "   2.797461986541748,\n",
       "   2.375659465789795,\n",
       "   2.597118616104126,\n",
       "   2.5064666271209717,\n",
       "   2.5018982887268066,\n",
       "   2.3680598735809326,\n",
       "   2.2423629760742188,\n",
       "   2.0538740158081055,\n",
       "   2.291440725326538,\n",
       "   2.5097711086273193,\n",
       "   2.2063324451446533,\n",
       "   2.490187883377075,\n",
       "   1.8655813932418823,\n",
       "   2.639556646347046,\n",
       "   2.371076822280884,\n",
       "   2.242110252380371,\n",
       "   2.202740430831909,\n",
       "   2.1539714336395264,\n",
       "   2.3881189823150635,\n",
       "   2.025986671447754,\n",
       "   2.4529712200164795,\n",
       "   2.309518814086914,\n",
       "   2.205906391143799,\n",
       "   2.4085397720336914,\n",
       "   2.3249549865722656,\n",
       "   1.8992881774902344,\n",
       "   2.2978854179382324,\n",
       "   2.9344565868377686,\n",
       "   2.1371071338653564,\n",
       "   2.1946444511413574,\n",
       "   2.51749324798584,\n",
       "   1.8487921953201294,\n",
       "   2.078368663787842,\n",
       "   2.0035901069641113,\n",
       "   2.2959635257720947,\n",
       "   2.8950512409210205,\n",
       "   2.427361011505127,\n",
       "   1.9855111837387085,\n",
       "   2.4150185585021973,\n",
       "   2.666228771209717,\n",
       "   2.466418504714966,\n",
       "   2.4840011596679688,\n",
       "   2.861077070236206,\n",
       "   2.2713284492492676,\n",
       "   2.9191465377807617,\n",
       "   1.8859127759933472,\n",
       "   2.2958569526672363,\n",
       "   2.651716470718384,\n",
       "   2.492074966430664,\n",
       "   2.6285712718963623,\n",
       "   1.9377707242965698,\n",
       "   2.4383533000946045,\n",
       "   2.5131239891052246,\n",
       "   2.4389963150024414,\n",
       "   2.2709085941314697,\n",
       "   2.044994592666626,\n",
       "   2.567913293838501,\n",
       "   2.5576400756835938,\n",
       "   2.5454745292663574,\n",
       "   2.308936357498169,\n",
       "   1.8553972244262695,\n",
       "   2.1567001342773438,\n",
       "   2.2321507930755615,\n",
       "   2.195019245147705,\n",
       "   2.769376516342163,\n",
       "   2.459871768951416,\n",
       "   2.6153006553649902,\n",
       "   2.435325860977173,\n",
       "   2.147233009338379,\n",
       "   2.383925676345825,\n",
       "   2.9799628257751465,\n",
       "   3.1338322162628174,\n",
       "   1.949144959449768,\n",
       "   2.4453604221343994,\n",
       "   2.4723384380340576,\n",
       "   2.3334851264953613,\n",
       "   2.7481298446655273,\n",
       "   2.358511447906494,\n",
       "   2.2515594959259033,\n",
       "   1.8641797304153442,\n",
       "   2.04679536819458,\n",
       "   2.505466938018799,\n",
       "   2.1742920875549316,\n",
       "   2.222684860229492,\n",
       "   2.730095148086548,\n",
       "   1.9733966588974,\n",
       "   2.2399981021881104,\n",
       "   2.6876184940338135,\n",
       "   2.1157448291778564,\n",
       "   2.116586685180664,\n",
       "   2.0474963188171387,\n",
       "   2.5883374214172363,\n",
       "   2.356982707977295,\n",
       "   2.003272771835327,\n",
       "   2.4547691345214844,\n",
       "   2.2923333644866943,\n",
       "   2.4209346771240234,\n",
       "   1.7485660314559937,\n",
       "   2.101961135864258,\n",
       "   2.517911672592163,\n",
       "   2.1344404220581055,\n",
       "   2.0272111892700195,\n",
       "   2.3895881175994873,\n",
       "   1.9905680418014526,\n",
       "   2.649599552154541,\n",
       "   2.264169216156006,\n",
       "   2.0643651485443115,\n",
       "   1.8707294464111328,\n",
       "   2.0829811096191406,\n",
       "   2.144043207168579,\n",
       "   2.071761131286621,\n",
       "   2.2716546058654785,\n",
       "   2.0330467224121094,\n",
       "   1.9697928428649902,\n",
       "   2.3158481121063232,\n",
       "   2.2493700981140137,\n",
       "   2.3709640502929688,\n",
       "   1.947357177734375,\n",
       "   2.7629289627075195,\n",
       "   2.229792833328247,\n",
       "   1.8821032047271729,\n",
       "   2.17901349067688,\n",
       "   2.079153060913086,\n",
       "   1.749219536781311,\n",
       "   2.2863404750823975,\n",
       "   2.1859288215637207,\n",
       "   2.60638165473938,\n",
       "   2.1752383708953857,\n",
       "   2.866459369659424,\n",
       "   1.8821487426757812,\n",
       "   2.2680728435516357,\n",
       "   2.3808796405792236,\n",
       "   2.2881922721862793,\n",
       "   2.4468631744384766,\n",
       "   2.397233247756958,\n",
       "   2.356992483139038,\n",
       "   2.525298833847046,\n",
       "   2.035062313079834,\n",
       "   2.5716850757598877,\n",
       "   2.490540027618408,\n",
       "   2.5329720973968506,\n",
       "   2.321852684020996,\n",
       "   2.6226015090942383,\n",
       "   2.3190255165100098,\n",
       "   2.233832359313965,\n",
       "   1.7756273746490479,\n",
       "   2.0902743339538574,\n",
       "   2.1401443481445312,\n",
       "   2.7246856689453125,\n",
       "   1.888939619064331,\n",
       "   2.6278021335601807,\n",
       "   3.062171459197998,\n",
       "   2.0877723693847656,\n",
       "   1.9470902681350708,\n",
       "   2.2754178047180176,\n",
       "   2.1953792572021484,\n",
       "   1.955274224281311,\n",
       "   1.8187000751495361,\n",
       "   2.310898780822754,\n",
       "   1.9231619834899902,\n",
       "   2.166703939437866,\n",
       "   1.9431867599487305,\n",
       "   2.3516128063201904,\n",
       "   2.469695806503296,\n",
       "   2.286661148071289,\n",
       "   2.461409568786621,\n",
       "   2.054171085357666,\n",
       "   2.1796231269836426,\n",
       "   2.308405637741089,\n",
       "   2.3823299407958984,\n",
       "   2.0717763900756836,\n",
       "   2.2208988666534424,\n",
       "   1.9768332242965698,\n",
       "   2.0899577140808105,\n",
       "   2.057551383972168,\n",
       "   2.0872936248779297,\n",
       "   2.512253761291504,\n",
       "   2.2173569202423096,\n",
       "   2.1284875869750977,\n",
       "   2.44608473777771,\n",
       "   2.158947229385376,\n",
       "   2.0055320262908936,\n",
       "   2.1771743297576904,\n",
       "   2.1268534660339355,\n",
       "   2.3246748447418213,\n",
       "   2.1148345470428467,\n",
       "   2.4141573905944824,\n",
       "   2.1050477027893066,\n",
       "   2.2575111389160156,\n",
       "   1.9223966598510742,\n",
       "   2.983574151992798,\n",
       "   1.8753974437713623,\n",
       "   2.1726784706115723,\n",
       "   1.8311736583709717,\n",
       "   2.5537214279174805,\n",
       "   2.2593119144439697,\n",
       "   1.8050012588500977,\n",
       "   2.0222911834716797,\n",
       "   2.415012836456299,\n",
       "   2.878904104232788,\n",
       "   1.9216256141662598,\n",
       "   1.905803918838501,\n",
       "   1.95766019821167,\n",
       "   2.407653331756592,\n",
       "   2.2643067836761475,\n",
       "   1.9859670400619507,\n",
       "   2.149663209915161,\n",
       "   2.2385668754577637,\n",
       "   1.8965468406677246,\n",
       "   1.8033466339111328,\n",
       "   2.0623250007629395,\n",
       "   1.9913924932479858,\n",
       "   2.829606056213379,\n",
       "   2.0686378479003906,\n",
       "   2.1382784843444824,\n",
       "   1.9865062236785889,\n",
       "   2.8845837116241455,\n",
       "   2.4140143394470215,\n",
       "   1.8940494060516357,\n",
       "   2.0530076026916504,\n",
       "   2.0270204544067383,\n",
       "   1.8963173627853394,\n",
       "   2.247135877609253,\n",
       "   2.320488691329956,\n",
       "   1.8457542657852173,\n",
       "   1.9616254568099976,\n",
       "   2.0183522701263428,\n",
       "   2.036959648132324,\n",
       "   2.142000675201416,\n",
       "   2.181579113006592,\n",
       "   1.9040021896362305,\n",
       "   2.2385916709899902,\n",
       "   2.356985330581665,\n",
       "   2.211085557937622,\n",
       "   2.165889024734497,\n",
       "   2.070033073425293,\n",
       "   2.1007513999938965,\n",
       "   2.1128644943237305,\n",
       "   2.007962942123413,\n",
       "   2.1412501335144043,\n",
       "   2.437138080596924,\n",
       "   2.047968864440918,\n",
       "   2.1022305488586426,\n",
       "   2.127289056777954,\n",
       "   2.5792407989501953,\n",
       "   2.0855042934417725,\n",
       "   2.401644229888916,\n",
       "   2.032214403152466,\n",
       "   2.2970478534698486,\n",
       "   2.2851858139038086,\n",
       "   2.0691494941711426,\n",
       "   2.0463662147521973,\n",
       "   2.413743257522583,\n",
       "   2.718442916870117,\n",
       "   2.3838932514190674,\n",
       "   2.155308246612549,\n",
       "   2.3346989154815674,\n",
       "   2.640383243560791,\n",
       "   2.2506837844848633,\n",
       "   2.062716245651245,\n",
       "   1.894891381263733,\n",
       "   2.6659343242645264,\n",
       "   1.7483794689178467,\n",
       "   2.2675325870513916,\n",
       "   2.2354307174682617,\n",
       "   2.2536537647247314,\n",
       "   2.4836156368255615,\n",
       "   2.025059223175049,\n",
       "   1.784230351448059,\n",
       "   2.236280918121338,\n",
       "   2.4029152393341064,\n",
       "   2.000595808029175,\n",
       "   2.0282325744628906,\n",
       "   2.0199036598205566,\n",
       "   2.1325132846832275,\n",
       "   2.2091686725616455,\n",
       "   1.9318556785583496,\n",
       "   2.4001715183258057,\n",
       "   2.444429636001587,\n",
       "   2.126251459121704,\n",
       "   2.7109546661376953,\n",
       "   2.2126729488372803,\n",
       "   2.11309814453125,\n",
       "   2.56545352935791,\n",
       "   2.1884827613830566,\n",
       "   2.6532161235809326,\n",
       "   1.8873162269592285,\n",
       "   1.8834364414215088,\n",
       "   2.8516108989715576,\n",
       "   2.084214687347412,\n",
       "   2.154175281524658,\n",
       "   1.9895899295806885,\n",
       "   2.2388999462127686,\n",
       "   1.524842381477356,\n",
       "   2.143552303314209,\n",
       "   1.905576229095459,\n",
       "   2.023179531097412,\n",
       "   2.2196342945098877,\n",
       "   1.8318579196929932,\n",
       "   2.451578378677368,\n",
       "   1.9249521493911743,\n",
       "   2.702285051345825,\n",
       "   2.349846363067627,\n",
       "   2.1250040531158447,\n",
       "   2.1820106506347656,\n",
       "   2.0054426193237305,\n",
       "   2.1562390327453613,\n",
       "   2.0695278644561768,\n",
       "   2.5904018878936768,\n",
       "   2.320150852203369,\n",
       "   2.088794231414795,\n",
       "   1.7306684255599976,\n",
       "   2.8586997985839844,\n",
       "   2.22780704498291,\n",
       "   2.076153039932251,\n",
       "   1.9034290313720703,\n",
       "   1.8878846168518066,\n",
       "   2.366462230682373,\n",
       "   2.5649969577789307,\n",
       "   2.247878313064575,\n",
       "   2.053771734237671,\n",
       "   2.163215160369873,\n",
       "   1.8555793762207031,\n",
       "   2.127394676208496,\n",
       "   1.924874186515808,\n",
       "   2.5190231800079346,\n",
       "   2.4193167686462402,\n",
       "   1.6814136505126953,\n",
       "   2.03336238861084,\n",
       "   2.1628592014312744,\n",
       "   2.1387667655944824,\n",
       "   2.6636979579925537,\n",
       "   1.9938576221466064,\n",
       "   1.929809331893921,\n",
       "   2.3432626724243164,\n",
       "   1.802586317062378,\n",
       "   2.1735870838165283,\n",
       "   2.1264286041259766,\n",
       "   2.1435301303863525,\n",
       "   2.185542345046997,\n",
       "   2.3600523471832275,\n",
       "   2.137216567993164,\n",
       "   1.871759057044983,\n",
       "   2.0368447303771973,\n",
       "   1.9914629459381104,\n",
       "   2.077028751373291,\n",
       "   1.9118939638137817,\n",
       "   2.326981544494629,\n",
       "   1.9095120429992676,\n",
       "   2.169745922088623,\n",
       "   2.270650863647461,\n",
       "   2.076936960220337,\n",
       "   2.105026960372925,\n",
       "   1.846990942955017,\n",
       "   2.4028406143188477,\n",
       "   1.9685603380203247,\n",
       "   2.038348436355591,\n",
       "   1.9219424724578857,\n",
       "   2.4762866497039795,\n",
       "   2.1736602783203125,\n",
       "   1.9535444974899292,\n",
       "   2.110154151916504,\n",
       "   2.09757924079895,\n",
       "   2.2002015113830566,\n",
       "   2.1214776039123535,\n",
       "   2.01326584815979,\n",
       "   2.292464256286621,\n",
       "   2.1932642459869385,\n",
       "   1.962665319442749,\n",
       "   2.1081299781799316,\n",
       "   2.110142230987549,\n",
       "   2.247865915298462,\n",
       "   1.938354730606079,\n",
       "   2.3407130241394043,\n",
       "   1.7795064449310303,\n",
       "   1.9473984241485596,\n",
       "   2.270195722579956,\n",
       "   1.9248466491699219,\n",
       "   2.313509464263916,\n",
       "   1.6906476020812988,\n",
       "   2.1814048290252686,\n",
       "   2.4982993602752686,\n",
       "   2.0184080600738525,\n",
       "   2.173565149307251,\n",
       "   1.9103679656982422,\n",
       "   2.018247365951538,\n",
       "   2.1399683952331543,\n",
       "   1.9974817037582397,\n",
       "   2.14166259765625,\n",
       "   2.1112420558929443,\n",
       "   1.8717669248580933,\n",
       "   2.1942052841186523,\n",
       "   2.5144309997558594,\n",
       "   2.5842947959899902,\n",
       "   2.448575496673584,\n",
       "   2.7092783451080322,\n",
       "   2.2331290245056152,\n",
       "   1.9077783823013306,\n",
       "   1.9737155437469482,\n",
       "   1.9372820854187012,\n",
       "   2.486973524093628,\n",
       "   1.9152441024780273,\n",
       "   2.0331263542175293,\n",
       "   2.3436384201049805,\n",
       "   2.1705873012542725,\n",
       "   1.8590002059936523,\n",
       "   1.9438925981521606,\n",
       "   2.1208858489990234,\n",
       "   2.5046417713165283,\n",
       "   1.9301559925079346,\n",
       "   1.9302825927734375,\n",
       "   2.0389034748077393,\n",
       "   2.05375599861145,\n",
       "   1.923701286315918,\n",
       "   1.968563199043274,\n",
       "   1.975080132484436,\n",
       "   2.0394785404205322,\n",
       "   1.9846552610397339,\n",
       "   2.7865679264068604,\n",
       "   2.0600814819335938,\n",
       "   1.9028953313827515,\n",
       "   2.021484613418579,\n",
       "   1.7855497598648071,\n",
       "   2.0942227840423584,\n",
       "   2.0204410552978516,\n",
       "   1.9331960678100586],\n",
       "  'train_voc_loss': [0.612265944480896,\n",
       "   0.6916984915733337,\n",
       "   0.7126117944717407,\n",
       "   0.6779881715774536,\n",
       "   0.639176070690155,\n",
       "   1.1381369829177856,\n",
       "   0.6516496539115906,\n",
       "   1.0078072547912598,\n",
       "   0.6327220797538757,\n",
       "   0.8764285445213318,\n",
       "   0.5755062699317932,\n",
       "   0.6069380640983582,\n",
       "   0.6615961790084839,\n",
       "   0.7290725708007812,\n",
       "   0.5764485001564026,\n",
       "   0.8237196803092957,\n",
       "   0.5879936218261719,\n",
       "   0.6356561779975891,\n",
       "   0.6690313816070557,\n",
       "   0.4726698696613312,\n",
       "   0.7369986176490784,\n",
       "   0.6168842911720276,\n",
       "   0.6082023978233337,\n",
       "   0.8141668438911438,\n",
       "   0.9044495224952698,\n",
       "   0.5701172947883606,\n",
       "   0.6865748167037964,\n",
       "   0.5678870677947998,\n",
       "   1.1144262552261353,\n",
       "   0.7355429530143738,\n",
       "   0.5023908615112305,\n",
       "   0.6846655011177063,\n",
       "   1.0037486553192139,\n",
       "   0.5550897121429443,\n",
       "   0.8317424654960632,\n",
       "   0.7894073128700256,\n",
       "   1.0155032873153687,\n",
       "   0.6873264908790588,\n",
       "   0.6557093262672424,\n",
       "   0.8537900447845459,\n",
       "   0.5603399276733398,\n",
       "   0.8735369443893433,\n",
       "   1.134669542312622,\n",
       "   0.644858717918396,\n",
       "   1.0080931186676025,\n",
       "   0.7737939953804016,\n",
       "   1.0008906126022339,\n",
       "   0.4958336651325226,\n",
       "   0.7110922336578369,\n",
       "   0.557761549949646,\n",
       "   0.6665082573890686,\n",
       "   0.564798891544342,\n",
       "   0.7363908886909485,\n",
       "   0.7682084441184998,\n",
       "   0.6152045726776123,\n",
       "   0.5852647423744202,\n",
       "   0.5705820918083191,\n",
       "   0.6856613159179688,\n",
       "   0.9423384070396423,\n",
       "   1.0175997018814087,\n",
       "   1.0703283548355103,\n",
       "   1.0978307723999023,\n",
       "   0.5497943758964539,\n",
       "   0.44611433148384094,\n",
       "   0.5662267804145813,\n",
       "   0.6452764272689819,\n",
       "   0.6646585464477539,\n",
       "   0.8818826079368591,\n",
       "   0.7413597106933594,\n",
       "   0.47836074233055115,\n",
       "   0.5852190852165222,\n",
       "   0.7471259236335754,\n",
       "   0.7824569940567017,\n",
       "   0.7312449216842651,\n",
       "   0.7839311957359314,\n",
       "   0.3747704029083252,\n",
       "   0.716445803642273,\n",
       "   0.6121047735214233,\n",
       "   0.6335771679878235,\n",
       "   0.7708225250244141,\n",
       "   0.48697414994239807,\n",
       "   0.7389763593673706,\n",
       "   0.5231273770332336,\n",
       "   0.7313713431358337,\n",
       "   0.5894690155982971,\n",
       "   0.5578393936157227,\n",
       "   0.6655920147895813,\n",
       "   0.5328273773193359,\n",
       "   0.7459686994552612,\n",
       "   0.5053601264953613,\n",
       "   1.0567976236343384,\n",
       "   0.5741900205612183,\n",
       "   0.5510158538818359,\n",
       "   0.7369178533554077,\n",
       "   0.9123464226722717,\n",
       "   1.0124564170837402,\n",
       "   0.5857251286506653,\n",
       "   0.7731676697731018,\n",
       "   0.6844967007637024,\n",
       "   0.6963217258453369,\n",
       "   0.704329252243042,\n",
       "   0.5867720246315002,\n",
       "   0.9025865793228149,\n",
       "   0.6605883836746216,\n",
       "   0.4370145797729492,\n",
       "   0.6420978307723999,\n",
       "   0.5186818838119507,\n",
       "   0.673389196395874,\n",
       "   0.5833266973495483,\n",
       "   0.6922628283500671,\n",
       "   1.0914937257766724,\n",
       "   0.5657516717910767,\n",
       "   0.6368962526321411,\n",
       "   0.8510849475860596,\n",
       "   0.9050165414810181,\n",
       "   0.6664305925369263,\n",
       "   0.4696348011493683,\n",
       "   0.8824365735054016,\n",
       "   1.1071056127548218,\n",
       "   0.8273993730545044,\n",
       "   0.7102139592170715,\n",
       "   0.7023770213127136,\n",
       "   0.45653510093688965,\n",
       "   0.7790008187294006,\n",
       "   0.7147153615951538,\n",
       "   1.0409667491912842,\n",
       "   0.7248476147651672,\n",
       "   0.9006677269935608,\n",
       "   0.4796716272830963,\n",
       "   0.4721875786781311,\n",
       "   0.7527390122413635,\n",
       "   0.4298185706138611,\n",
       "   0.48997393250465393,\n",
       "   0.7565190196037292,\n",
       "   0.7785013318061829,\n",
       "   0.6848374605178833,\n",
       "   0.6914016604423523,\n",
       "   0.5599636435508728,\n",
       "   0.7671471238136292,\n",
       "   0.64389967918396,\n",
       "   1.0650382041931152,\n",
       "   0.6935150623321533,\n",
       "   0.5930712819099426,\n",
       "   0.5575461983680725,\n",
       "   0.6676912903785706,\n",
       "   1.1188523769378662,\n",
       "   0.5680499076843262,\n",
       "   0.5341930985450745,\n",
       "   0.8266631364822388,\n",
       "   0.5648549795150757,\n",
       "   0.6428263187408447,\n",
       "   0.7520233988761902,\n",
       "   0.3831515908241272,\n",
       "   0.6133870482444763,\n",
       "   0.8284212350845337,\n",
       "   0.6232683062553406,\n",
       "   0.5870451927185059,\n",
       "   0.7235935926437378,\n",
       "   0.6615110635757446,\n",
       "   0.39524516463279724,\n",
       "   0.5995191931724548,\n",
       "   0.5112890601158142,\n",
       "   0.49920809268951416,\n",
       "   0.6930594444274902,\n",
       "   0.8234212398529053,\n",
       "   0.7893081903457642,\n",
       "   0.6155698299407959,\n",
       "   0.4248160421848297,\n",
       "   0.770810067653656,\n",
       "   0.40849122405052185,\n",
       "   0.5609604120254517,\n",
       "   0.5759595632553101,\n",
       "   0.47119101881980896,\n",
       "   0.7293688058853149,\n",
       "   0.8318585753440857,\n",
       "   0.5925838351249695,\n",
       "   0.5887633562088013,\n",
       "   0.581362247467041,\n",
       "   0.6970107555389404,\n",
       "   0.6668587327003479,\n",
       "   0.7115906476974487,\n",
       "   0.9730340838432312,\n",
       "   0.7991384267807007,\n",
       "   0.4658566415309906,\n",
       "   0.9107108116149902,\n",
       "   0.773705005645752,\n",
       "   0.4111867845058441,\n",
       "   0.7016451954841614,\n",
       "   0.7830597162246704,\n",
       "   0.589392900466919,\n",
       "   0.7172626852989197,\n",
       "   0.5364203453063965,\n",
       "   0.763028621673584,\n",
       "   0.5029187202453613,\n",
       "   0.9182959198951721,\n",
       "   0.8691858649253845,\n",
       "   0.7105572819709778,\n",
       "   0.6981672644615173,\n",
       "   0.3865678310394287,\n",
       "   0.6265808343887329,\n",
       "   0.8507943749427795,\n",
       "   0.6662778258323669,\n",
       "   0.6613093018531799,\n",
       "   0.6092701554298401,\n",
       "   0.8631642460823059,\n",
       "   0.4891771376132965,\n",
       "   0.6045569777488708,\n",
       "   0.48105907440185547,\n",
       "   0.391128271818161,\n",
       "   0.9161062836647034,\n",
       "   0.5633437037467957,\n",
       "   0.658484160900116,\n",
       "   0.7567956447601318,\n",
       "   0.776987612247467,\n",
       "   0.7173792719841003,\n",
       "   0.6398295164108276,\n",
       "   0.5686177015304565,\n",
       "   0.9019771814346313,\n",
       "   0.5444775819778442,\n",
       "   0.8224925398826599,\n",
       "   0.8165684342384338,\n",
       "   0.8009150624275208,\n",
       "   0.7943015098571777,\n",
       "   0.7242867946624756,\n",
       "   0.7014007568359375,\n",
       "   0.7491061091423035,\n",
       "   0.7564182281494141,\n",
       "   0.8418315052986145,\n",
       "   0.634342610836029,\n",
       "   0.7685472965240479,\n",
       "   0.8106729388237,\n",
       "   0.5185360312461853,\n",
       "   0.6259775161743164,\n",
       "   0.6651047468185425,\n",
       "   0.5800044536590576,\n",
       "   0.6883397698402405,\n",
       "   0.72515869140625,\n",
       "   0.8295384645462036,\n",
       "   0.9507575631141663,\n",
       "   0.881507158279419,\n",
       "   0.621320903301239,\n",
       "   0.5223579406738281,\n",
       "   0.5340873599052429,\n",
       "   1.220611810684204,\n",
       "   0.617647111415863,\n",
       "   0.4080125391483307,\n",
       "   0.7001721858978271,\n",
       "   0.6830840110778809,\n",
       "   0.8718724250793457,\n",
       "   0.5134304165840149,\n",
       "   0.5541670322418213,\n",
       "   0.8411955237388611,\n",
       "   0.5941713452339172,\n",
       "   0.8688293695449829,\n",
       "   0.4450576901435852,\n",
       "   0.6838218569755554,\n",
       "   0.8980003595352173,\n",
       "   0.7319682836532593,\n",
       "   0.6865424513816833,\n",
       "   0.467228502035141,\n",
       "   0.5900959968566895,\n",
       "   0.7703630924224854,\n",
       "   0.557571291923523,\n",
       "   0.7428746819496155,\n",
       "   0.4265104830265045,\n",
       "   0.6151213049888611,\n",
       "   0.5998523831367493,\n",
       "   0.5925549864768982,\n",
       "   0.8799174427986145,\n",
       "   0.7428328394889832,\n",
       "   0.589824914932251,\n",
       "   0.431833416223526,\n",
       "   1.101934790611267,\n",
       "   0.45672744512557983,\n",
       "   0.4985373616218567,\n",
       "   0.7471933960914612,\n",
       "   0.48628729581832886,\n",
       "   0.38798126578330994,\n",
       "   0.566087007522583,\n",
       "   0.6945772767066956,\n",
       "   0.45743435621261597,\n",
       "   0.6834350228309631,\n",
       "   0.7798935770988464,\n",
       "   0.618087112903595,\n",
       "   0.7893663048744202,\n",
       "   0.5330376625061035,\n",
       "   0.8800014853477478,\n",
       "   0.5845654606819153,\n",
       "   0.7322806715965271,\n",
       "   0.9245556592941284,\n",
       "   0.6384238600730896,\n",
       "   0.784777820110321,\n",
       "   0.7637757062911987,\n",
       "   1.007868766784668,\n",
       "   0.6857478618621826,\n",
       "   0.6647478938102722,\n",
       "   0.9865646958351135,\n",
       "   0.7544699907302856,\n",
       "   0.6419934034347534,\n",
       "   0.8194445371627808,\n",
       "   0.7076702117919922,\n",
       "   0.5521141290664673,\n",
       "   0.5877753496170044,\n",
       "   0.4667206108570099,\n",
       "   0.6211690902709961,\n",
       "   0.7680957317352295,\n",
       "   0.6913682222366333,\n",
       "   0.6045594215393066,\n",
       "   0.6952680349349976,\n",
       "   0.7586089372634888,\n",
       "   0.44235044717788696,\n",
       "   1.035921335220337,\n",
       "   0.6881227493286133,\n",
       "   0.5791934728622437,\n",
       "   0.8358995318412781,\n",
       "   0.5434166789054871,\n",
       "   0.8967762589454651,\n",
       "   0.54417884349823,\n",
       "   0.7271838188171387,\n",
       "   0.8184687495231628,\n",
       "   0.64708012342453,\n",
       "   0.5668421983718872,\n",
       "   0.7948012351989746,\n",
       "   0.7865603566169739,\n",
       "   1.304329514503479,\n",
       "   0.693503201007843,\n",
       "   0.47911059856414795,\n",
       "   0.7224317193031311,\n",
       "   0.47797515988349915,\n",
       "   0.7930498123168945,\n",
       "   0.8162539601325989,\n",
       "   0.6524962186813354,\n",
       "   0.7438153624534607,\n",
       "   0.4775052070617676,\n",
       "   0.5714365243911743,\n",
       "   0.6112431883811951,\n",
       "   0.711072564125061,\n",
       "   0.636976957321167,\n",
       "   0.9997885823249817,\n",
       "   0.7949556112289429,\n",
       "   0.7264547348022461,\n",
       "   0.55161052942276,\n",
       "   0.949769139289856,\n",
       "   0.7026804089546204,\n",
       "   0.8415067791938782,\n",
       "   0.6132556200027466,\n",
       "   0.7449843883514404,\n",
       "   0.7320957779884338,\n",
       "   0.6663135886192322,\n",
       "   0.6530364155769348,\n",
       "   0.6870411038398743,\n",
       "   0.6675607562065125,\n",
       "   0.45724526047706604,\n",
       "   0.7142173647880554,\n",
       "   0.398415207862854,\n",
       "   0.4289490878582001,\n",
       "   0.616660475730896,\n",
       "   0.5919507741928101,\n",
       "   0.6437274813652039,\n",
       "   0.4863753318786621,\n",
       "   0.8984129428863525,\n",
       "   0.5909426212310791,\n",
       "   0.5580122470855713,\n",
       "   0.542691707611084,\n",
       "   0.8897737264633179,\n",
       "   0.5319241285324097,\n",
       "   0.5233785510063171,\n",
       "   0.535212516784668,\n",
       "   0.8272683024406433,\n",
       "   0.9687122702598572,\n",
       "   0.4841620922088623,\n",
       "   1.2457373142242432,\n",
       "   0.6396026611328125,\n",
       "   0.7183387875556946,\n",
       "   0.53963303565979,\n",
       "   0.6464663147926331,\n",
       "   0.5013340711593628,\n",
       "   0.7013037204742432,\n",
       "   0.6022075414657593,\n",
       "   0.7026880979537964,\n",
       "   0.8196222186088562,\n",
       "   0.5182409882545471,\n",
       "   0.6768314242362976,\n",
       "   0.5118632316589355,\n",
       "   0.8037039041519165,\n",
       "   0.8955681324005127,\n",
       "   0.5224599838256836,\n",
       "   0.7027133107185364,\n",
       "   1.0682952404022217,\n",
       "   0.39624151587486267,\n",
       "   0.8129266500473022,\n",
       "   0.5514138340950012,\n",
       "   0.4382512867450714,\n",
       "   0.7555371522903442,\n",
       "   0.396438866853714,\n",
       "   0.5885629057884216,\n",
       "   0.5167123675346375,\n",
       "   0.6238803863525391,\n",
       "   0.9690126180648804,\n",
       "   0.6939852833747864,\n",
       "   0.5970953702926636,\n",
       "   0.7420462965965271,\n",
       "   0.5618780851364136,\n",
       "   0.8063328266143799,\n",
       "   0.4395599663257599,\n",
       "   0.8672394156455994,\n",
       "   0.5016791820526123,\n",
       "   0.6896510124206543,\n",
       "   0.6209716796875,\n",
       "   0.5036783814430237,\n",
       "   0.700080931186676,\n",
       "   0.5997995138168335,\n",
       "   0.6025959849357605,\n",
       "   0.565964937210083,\n",
       "   0.6425124406814575,\n",
       "   0.5688210725784302,\n",
       "   1.003385066986084,\n",
       "   0.5898388624191284,\n",
       "   0.5646591782569885,\n",
       "   0.49923017621040344,\n",
       "   0.6295321583747864,\n",
       "   1.0834710597991943,\n",
       "   0.6449498534202576,\n",
       "   0.6637877225875854,\n",
       "   0.5419688820838928,\n",
       "   0.7169702053070068,\n",
       "   0.5794553160667419,\n",
       "   0.5824769735336304,\n",
       "   0.4584561288356781,\n",
       "   0.8845361471176147,\n",
       "   0.5460129976272583,\n",
       "   1.0269994735717773,\n",
       "   0.4631321132183075,\n",
       "   0.6804389357566833,\n",
       "   0.9594772458076477,\n",
       "   0.6821079850196838,\n",
       "   0.7544811367988586,\n",
       "   1.1605700254440308,\n",
       "   0.5496039390563965,\n",
       "   0.9150235056877136,\n",
       "   1.228269338607788,\n",
       "   0.5438019633293152,\n",
       "   0.4699726402759552,\n",
       "   0.6131501793861389,\n",
       "   0.5480470657348633,\n",
       "   0.9081060886383057,\n",
       "   0.7320377826690674,\n",
       "   0.6304057836532593,\n",
       "   0.8912279605865479,\n",
       "   0.6418974995613098,\n",
       "   0.38234785199165344,\n",
       "   0.5449420809745789,\n",
       "   0.5947394967079163,\n",
       "   0.5637860298156738,\n",
       "   0.6130020022392273,\n",
       "   1.0137081146240234,\n",
       "   0.7557135224342346,\n",
       "   0.7151067852973938,\n",
       "   0.97760409116745,\n",
       "   0.8621242046356201,\n",
       "   0.5763577222824097,\n",
       "   0.6014614105224609,\n",
       "   0.5447372198104858,\n",
       "   0.62071293592453,\n",
       "   0.5641453862190247,\n",
       "   0.6567333936691284,\n",
       "   0.8942183256149292,\n",
       "   0.629817545413971,\n",
       "   0.6204842925071716,\n",
       "   0.7103954553604126,\n",
       "   0.36978310346603394,\n",
       "   0.3865949809551239,\n",
       "   0.908606767654419,\n",
       "   0.5445252656936646,\n",
       "   0.47748079895973206,\n",
       "   0.7963657379150391,\n",
       "   0.6399780511856079,\n",
       "   0.4825951159000397,\n",
       "   0.7115704417228699,\n",
       "   0.6647201180458069,\n",
       "   0.6422500610351562,\n",
       "   0.840692937374115,\n",
       "   0.7890605330467224,\n",
       "   0.41314026713371277,\n",
       "   0.8963175415992737,\n",
       "   0.5005645751953125,\n",
       "   0.5541990399360657,\n",
       "   0.45768895745277405,\n",
       "   0.5139182806015015,\n",
       "   0.49168241024017334,\n",
       "   0.6059663891792297,\n",
       "   0.824286699295044,\n",
       "   0.7255386710166931,\n",
       "   0.7247413992881775,\n",
       "   0.6878712773323059,\n",
       "   0.5402661561965942,\n",
       "   0.8589006066322327,\n",
       "   0.890739381313324,\n",
       "   0.6393623352050781,\n",
       "   0.7168845534324646],\n",
       "  'train_jsc_loss': [0.6788586974143982,\n",
       "   0.6629895567893982,\n",
       "   0.6521109342575073,\n",
       "   0.6901000738143921,\n",
       "   0.6254725456237793,\n",
       "   0.63437819480896,\n",
       "   0.5948508381843567,\n",
       "   0.6568949818611145,\n",
       "   0.5981869697570801,\n",
       "   0.6194762587547302,\n",
       "   0.6362905502319336,\n",
       "   0.6324748396873474,\n",
       "   0.6490924954414368,\n",
       "   0.6761327981948853,\n",
       "   0.6771417260169983,\n",
       "   0.6357572674751282,\n",
       "   0.5995877981185913,\n",
       "   0.6456695795059204,\n",
       "   0.6302614808082581,\n",
       "   0.6476219296455383,\n",
       "   0.6235832571983337,\n",
       "   0.6490321755409241,\n",
       "   0.6653952598571777,\n",
       "   0.6180593371391296,\n",
       "   0.6323176026344299,\n",
       "   0.6162331700325012,\n",
       "   0.6361701488494873,\n",
       "   0.6585657596588135,\n",
       "   0.6102995276451111,\n",
       "   0.614485502243042,\n",
       "   0.6316544413566589,\n",
       "   0.6041688919067383,\n",
       "   0.6599445343017578,\n",
       "   0.6636401414871216,\n",
       "   0.6041534543037415,\n",
       "   0.6282228231430054,\n",
       "   0.6026211977005005,\n",
       "   0.6498062610626221,\n",
       "   0.6262856125831604,\n",
       "   0.667573094367981,\n",
       "   0.6581628918647766,\n",
       "   0.6760236620903015,\n",
       "   0.6728690266609192,\n",
       "   0.6829023957252502,\n",
       "   0.663295328617096,\n",
       "   0.6556033492088318,\n",
       "   0.6245971322059631,\n",
       "   0.626849353313446,\n",
       "   0.6434741020202637,\n",
       "   0.6500640511512756,\n",
       "   0.6713671088218689,\n",
       "   0.6326853632926941,\n",
       "   0.6321455240249634,\n",
       "   0.6306512355804443,\n",
       "   0.6863512396812439,\n",
       "   0.6374907493591309,\n",
       "   0.6431384086608887,\n",
       "   0.6021245718002319,\n",
       "   0.6500541567802429,\n",
       "   0.6135798096656799,\n",
       "   0.6580262780189514,\n",
       "   0.6273094415664673,\n",
       "   0.6278546452522278,\n",
       "   0.6366223096847534,\n",
       "   0.6512759327888489,\n",
       "   0.6224148273468018,\n",
       "   0.6421291828155518,\n",
       "   0.6381645202636719,\n",
       "   0.6107837557792664,\n",
       "   0.6581777334213257,\n",
       "   0.6475704312324524,\n",
       "   0.6361367106437683,\n",
       "   0.6335058808326721,\n",
       "   0.6542662382125854,\n",
       "   0.6678677201271057,\n",
       "   0.590219259262085,\n",
       "   0.6324766278266907,\n",
       "   0.667994499206543,\n",
       "   0.6706640124320984,\n",
       "   0.6062756776809692,\n",
       "   0.6066535711288452,\n",
       "   0.6853715181350708,\n",
       "   0.621803879737854,\n",
       "   0.6536361575126648,\n",
       "   0.6280009746551514,\n",
       "   0.6243683695793152,\n",
       "   0.6385111808776855,\n",
       "   0.6092171669006348,\n",
       "   0.6311604976654053,\n",
       "   0.625781238079071,\n",
       "   0.6467206478118896,\n",
       "   0.6048039793968201,\n",
       "   0.6214045286178589,\n",
       "   0.6277943849563599,\n",
       "   0.6451762318611145,\n",
       "   0.6085045337677002,\n",
       "   0.6368793845176697,\n",
       "   0.685958206653595,\n",
       "   0.6043250560760498,\n",
       "   0.6602927446365356,\n",
       "   0.6079050898551941,\n",
       "   0.6830364465713501,\n",
       "   0.6540796160697937,\n",
       "   0.6064237356185913,\n",
       "   0.637170672416687,\n",
       "   0.6353897452354431,\n",
       "   0.6313385963439941,\n",
       "   0.6423720121383667,\n",
       "   0.6604326367378235,\n",
       "   0.680460512638092,\n",
       "   0.6218447685241699,\n",
       "   0.6575800180435181,\n",
       "   0.6620243787765503,\n",
       "   0.6617137789726257,\n",
       "   0.6912990808486938,\n",
       "   0.6349728107452393,\n",
       "   0.6358658671379089,\n",
       "   0.6453701257705688,\n",
       "   0.6325667500495911,\n",
       "   0.6036018133163452,\n",
       "   0.669685423374176,\n",
       "   0.6505737900733948,\n",
       "   0.6733289361000061,\n",
       "   0.6251733899116516,\n",
       "   0.6509125828742981,\n",
       "   0.6427693367004395,\n",
       "   0.7139973044395447,\n",
       "   0.6421568393707275,\n",
       "   0.6277620196342468,\n",
       "   0.6856879591941833,\n",
       "   0.6561022400856018,\n",
       "   0.6401710510253906,\n",
       "   0.595592200756073,\n",
       "   0.6546503901481628,\n",
       "   0.6946767568588257,\n",
       "   0.6951464414596558,\n",
       "   0.6254630088806152,\n",
       "   0.6431950926780701,\n",
       "   0.6236229538917542,\n",
       "   0.6930497884750366,\n",
       "   0.6246628165245056,\n",
       "   0.6439769268035889,\n",
       "   0.6176233291625977,\n",
       "   0.6783536672592163,\n",
       "   0.6383073925971985,\n",
       "   0.6423699259757996,\n",
       "   0.7040834426879883,\n",
       "   0.6473251581192017,\n",
       "   0.651938259601593,\n",
       "   0.6151788830757141,\n",
       "   0.6795427203178406,\n",
       "   0.6462419033050537,\n",
       "   0.648388147354126,\n",
       "   0.6242095828056335,\n",
       "   0.6378644108772278,\n",
       "   0.6367989182472229,\n",
       "   0.6291711926460266,\n",
       "   0.6530979871749878,\n",
       "   0.6084470152854919,\n",
       "   0.6719500422477722,\n",
       "   0.6821301579475403,\n",
       "   0.6403638124465942,\n",
       "   0.6272000074386597,\n",
       "   0.6632921695709229,\n",
       "   0.6213117837905884,\n",
       "   0.6516469120979309,\n",
       "   0.6328567862510681,\n",
       "   0.6533145308494568,\n",
       "   0.6848664283752441,\n",
       "   0.6817940473556519,\n",
       "   0.7223084568977356,\n",
       "   0.6609026789665222,\n",
       "   0.6468117833137512,\n",
       "   0.6358906626701355,\n",
       "   0.6452356576919556,\n",
       "   0.5963125824928284,\n",
       "   0.6138045787811279,\n",
       "   0.6783047318458557,\n",
       "   0.6018311381340027,\n",
       "   0.6304275393486023,\n",
       "   0.6117252707481384,\n",
       "   0.6357297897338867,\n",
       "   0.6485536694526672,\n",
       "   0.6378389596939087,\n",
       "   0.631601095199585,\n",
       "   0.6258257627487183,\n",
       "   0.620597779750824,\n",
       "   0.6433069109916687,\n",
       "   0.6437385082244873,\n",
       "   0.6142058968544006,\n",
       "   0.6568943858146667,\n",
       "   0.633337676525116,\n",
       "   0.6480169892311096,\n",
       "   0.5776408314704895,\n",
       "   0.6174262762069702,\n",
       "   0.7346209287643433,\n",
       "   0.5976122617721558,\n",
       "   0.6576758623123169,\n",
       "   0.6141992211341858,\n",
       "   0.6265741586685181,\n",
       "   0.6284835934638977,\n",
       "   0.6234238147735596,\n",
       "   0.6450268030166626,\n",
       "   0.6430081129074097,\n",
       "   0.6526480317115784,\n",
       "   0.6312668323516846,\n",
       "   0.6535857319831848,\n",
       "   0.6499127149581909,\n",
       "   0.646163284778595,\n",
       "   0.6431434750556946,\n",
       "   0.5977694392204285,\n",
       "   0.6388581991195679,\n",
       "   0.6856426000595093,\n",
       "   0.6136300563812256,\n",
       "   0.5871989130973816,\n",
       "   0.6659572124481201,\n",
       "   0.6376057267189026,\n",
       "   0.6742419600486755,\n",
       "   0.63910973072052,\n",
       "   0.6607458591461182,\n",
       "   0.6951581835746765,\n",
       "   0.6824307441711426,\n",
       "   0.6773597002029419,\n",
       "   0.6192324757575989,\n",
       "   0.6627258658409119,\n",
       "   0.6385471224784851,\n",
       "   0.6716004610061646,\n",
       "   0.6754182577133179,\n",
       "   0.6729243397712708,\n",
       "   0.6317777037620544,\n",
       "   0.6124482154846191,\n",
       "   0.6173813343048096,\n",
       "   0.6709768176078796,\n",
       "   0.6304953694343567,\n",
       "   0.6566981673240662,\n",
       "   0.638224720954895,\n",
       "   0.6537876725196838,\n",
       "   0.6579235792160034,\n",
       "   0.6398541331291199,\n",
       "   0.599945604801178,\n",
       "   0.6230854988098145,\n",
       "   0.6797264218330383,\n",
       "   0.65800541639328,\n",
       "   0.6428049206733704,\n",
       "   0.6471863389015198,\n",
       "   0.6435097455978394,\n",
       "   0.6084266901016235,\n",
       "   0.6454205513000488,\n",
       "   0.600966215133667,\n",
       "   0.6293030977249146,\n",
       "   0.6579729318618774,\n",
       "   0.689137876033783,\n",
       "   0.6495727300643921,\n",
       "   0.6351991891860962,\n",
       "   0.6338543891906738,\n",
       "   0.6589062213897705,\n",
       "   0.6329496502876282,\n",
       "   0.6159970760345459,\n",
       "   0.6497519016265869,\n",
       "   0.6346601247787476,\n",
       "   0.6189302802085876,\n",
       "   0.6744080781936646,\n",
       "   0.6186279058456421,\n",
       "   0.6260225772857666,\n",
       "   0.6307844519615173,\n",
       "   0.6261984705924988,\n",
       "   0.6157083511352539,\n",
       "   0.648571789264679,\n",
       "   0.6174367070198059,\n",
       "   0.6560233235359192,\n",
       "   0.5711669325828552,\n",
       "   0.646217942237854,\n",
       "   0.6177341938018799,\n",
       "   0.6389033794403076,\n",
       "   0.6253579258918762,\n",
       "   0.6259018778800964,\n",
       "   0.6486426591873169,\n",
       "   0.6538026332855225,\n",
       "   0.7243977189064026,\n",
       "   0.6705577969551086,\n",
       "   0.7083367109298706,\n",
       "   0.6734265089035034,\n",
       "   0.6996346712112427,\n",
       "   0.6839749813079834,\n",
       "   0.616409420967102,\n",
       "   0.6192646026611328,\n",
       "   0.6547422409057617,\n",
       "   0.6509492993354797,\n",
       "   0.6702064871788025,\n",
       "   0.6282382607460022,\n",
       "   0.643754243850708,\n",
       "   0.654452919960022,\n",
       "   0.6106300354003906,\n",
       "   0.6349419951438904,\n",
       "   0.6558771133422852,\n",
       "   0.7070261836051941,\n",
       "   0.6349117755889893,\n",
       "   0.663581907749176,\n",
       "   0.6296671032905579,\n",
       "   0.6461971998214722,\n",
       "   0.6596322655677795,\n",
       "   0.695580244064331,\n",
       "   0.6701076626777649,\n",
       "   0.6551244854927063,\n",
       "   0.6163337826728821,\n",
       "   0.5956230163574219,\n",
       "   0.6513035893440247,\n",
       "   0.6449688076972961,\n",
       "   0.6416553854942322,\n",
       "   0.6021307110786438,\n",
       "   0.6083760261535645,\n",
       "   0.691389799118042,\n",
       "   0.5861636996269226,\n",
       "   0.6905576586723328,\n",
       "   0.614611029624939,\n",
       "   0.6296009421348572,\n",
       "   0.6987367868423462,\n",
       "   0.6481344103813171,\n",
       "   0.6372703313827515,\n",
       "   0.6041980981826782,\n",
       "   0.6616925001144409,\n",
       "   0.6820386052131653,\n",
       "   0.6102333664894104,\n",
       "   0.6416583061218262,\n",
       "   0.6604439616203308,\n",
       "   0.6285102367401123,\n",
       "   0.6020820140838623,\n",
       "   0.6226125359535217,\n",
       "   0.6496909260749817,\n",
       "   0.6581531167030334,\n",
       "   0.6132100820541382,\n",
       "   0.6753706932067871,\n",
       "   0.67259281873703,\n",
       "   0.6168907284736633,\n",
       "   0.6278176307678223,\n",
       "   0.6468601822853088,\n",
       "   0.6599118113517761,\n",
       "   0.6032437682151794,\n",
       "   0.6301736235618591,\n",
       "   0.6287851929664612,\n",
       "   0.6373383402824402,\n",
       "   0.6322758197784424,\n",
       "   0.6475523114204407,\n",
       "   0.6626901030540466,\n",
       "   0.6391407251358032,\n",
       "   0.656076967716217,\n",
       "   0.6381878852844238,\n",
       "   0.6718639731407166,\n",
       "   0.6828196048736572,\n",
       "   0.6874966025352478,\n",
       "   0.6051876544952393,\n",
       "   0.6467281579971313,\n",
       "   0.6834489703178406,\n",
       "   0.6209755539894104,\n",
       "   0.6064889430999756,\n",
       "   0.6639835238456726,\n",
       "   0.6429393291473389,\n",
       "   0.6359694004058838,\n",
       "   0.6631048917770386,\n",
       "   0.6358823776245117,\n",
       "   0.6454097628593445,\n",
       "   0.6607206463813782,\n",
       "   0.6492174863815308,\n",
       "   0.6211957931518555,\n",
       "   0.6342582702636719,\n",
       "   0.6739456057548523,\n",
       "   0.7250082492828369,\n",
       "   0.6765586137771606,\n",
       "   0.6576799154281616,\n",
       "   0.6111879348754883,\n",
       "   0.6225147843360901,\n",
       "   0.6381452679634094,\n",
       "   0.6741430759429932,\n",
       "   0.6558161377906799,\n",
       "   0.6170216202735901,\n",
       "   0.6267427206039429,\n",
       "   0.6741687655448914,\n",
       "   0.6512524485588074,\n",
       "   0.6140287518501282,\n",
       "   0.6186424493789673,\n",
       "   0.6358606815338135,\n",
       "   0.590950608253479,\n",
       "   0.6950436234474182,\n",
       "   0.6322544813156128,\n",
       "   0.6331426501274109,\n",
       "   0.6428640484809875,\n",
       "   0.6529343128204346,\n",
       "   0.6514833569526672,\n",
       "   0.631799042224884,\n",
       "   0.633576512336731,\n",
       "   0.6198894381523132,\n",
       "   0.641814649105072,\n",
       "   0.6263707876205444,\n",
       "   0.6643280982971191,\n",
       "   0.637418806552887,\n",
       "   0.6480205059051514,\n",
       "   0.6235661506652832,\n",
       "   0.6288072466850281,\n",
       "   0.6834412813186646,\n",
       "   0.6701155304908752,\n",
       "   0.6650928854942322,\n",
       "   0.618342936038971,\n",
       "   0.6274371147155762,\n",
       "   0.623910129070282,\n",
       "   0.6370679140090942,\n",
       "   0.6359642744064331,\n",
       "   0.677582859992981,\n",
       "   0.648269772529602,\n",
       "   0.6634125113487244,\n",
       "   0.6035091280937195,\n",
       "   0.6490148901939392,\n",
       "   0.6213308572769165,\n",
       "   0.6226180791854858,\n",
       "   0.6674945950508118,\n",
       "   0.6483347415924072,\n",
       "   0.670589804649353,\n",
       "   0.6333423852920532,\n",
       "   0.620046079158783,\n",
       "   0.6303905248641968,\n",
       "   0.6412674784660339,\n",
       "   0.6308492422103882,\n",
       "   0.5793935060501099,\n",
       "   0.6729595065116882,\n",
       "   0.6633270382881165,\n",
       "   0.6269821524620056,\n",
       "   0.5985371470451355,\n",
       "   0.6086122393608093,\n",
       "   0.63637775182724,\n",
       "   0.6568461060523987,\n",
       "   0.6201939582824707,\n",
       "   0.6405236124992371,\n",
       "   0.6376022696495056,\n",
       "   0.6372057795524597,\n",
       "   0.6761481761932373,\n",
       "   0.6673176288604736,\n",
       "   0.6323351263999939,\n",
       "   0.6447174549102783,\n",
       "   0.6337981820106506,\n",
       "   0.6192340850830078,\n",
       "   0.6566619873046875,\n",
       "   0.6651537418365479,\n",
       "   0.6295323371887207,\n",
       "   0.6251673102378845,\n",
       "   0.5976747870445251,\n",
       "   0.6215440034866333,\n",
       "   0.6216043829917908,\n",
       "   0.6573687195777893,\n",
       "   0.6572315692901611,\n",
       "   0.6430363059043884,\n",
       "   0.6385363340377808,\n",
       "   0.6420261263847351,\n",
       "   0.6606460809707642,\n",
       "   0.669966459274292,\n",
       "   0.5967285633087158,\n",
       "   0.6476303339004517,\n",
       "   0.6087351441383362,\n",
       "   0.6668785214424133,\n",
       "   0.6436085104942322,\n",
       "   0.6621277928352356,\n",
       "   0.605387270450592,\n",
       "   0.6429314613342285,\n",
       "   0.6313806176185608,\n",
       "   0.6523950695991516,\n",
       "   0.6720026135444641,\n",
       "   0.6361073851585388,\n",
       "   0.7064159512519836,\n",
       "   0.6094326972961426,\n",
       "   0.5951759219169617,\n",
       "   0.6849652528762817,\n",
       "   0.6605352163314819,\n",
       "   0.6630944013595581,\n",
       "   0.6473144888877869,\n",
       "   0.6483683586120605,\n",
       "   0.6843396425247192,\n",
       "   0.6422540545463562,\n",
       "   0.6614580750465393,\n",
       "   0.6556119322776794,\n",
       "   0.6456146240234375,\n",
       "   0.6674323081970215,\n",
       "   0.6729869842529297,\n",
       "   0.660447359085083,\n",
       "   0.6210620403289795,\n",
       "   0.6369736194610596,\n",
       "   0.5769957304000854,\n",
       "   0.5981320142745972,\n",
       "   0.6165962815284729,\n",
       "   0.7036462426185608,\n",
       "   0.6530997157096863,\n",
       "   0.6440092921257019,\n",
       "   0.599597692489624,\n",
       "   0.6006321907043457,\n",
       "   0.6671669483184814,\n",
       "   0.6209251880645752,\n",
       "   0.6338090896606445,\n",
       "   0.6578685641288757,\n",
       "   0.5942680239677429,\n",
       "   0.6304888725280762,\n",
       "   0.6351566314697266,\n",
       "   0.634708821773529,\n",
       "   0.6616591215133667],\n",
       "  'train_ff_loss': [0.7775169014930725,\n",
       "   0.7834249138832092,\n",
       "   0.7616246938705444,\n",
       "   0.74397873878479,\n",
       "   0.8949218988418579,\n",
       "   0.7216542363166809,\n",
       "   1.0560126304626465,\n",
       "   0.7623093724250793,\n",
       "   0.5229580998420715,\n",
       "   0.6731996536254883,\n",
       "   0.5779528021812439,\n",
       "   0.705985426902771,\n",
       "   0.8670206069946289,\n",
       "   0.6246263384819031,\n",
       "   1.0395210981369019,\n",
       "   0.9150945544242859,\n",
       "   0.6540754437446594,\n",
       "   1.1278027296066284,\n",
       "   0.8628413081169128,\n",
       "   0.7740364074707031,\n",
       "   0.7959316968917847,\n",
       "   1.0414941310882568,\n",
       "   0.7332587242126465,\n",
       "   0.9836989045143127,\n",
       "   0.91221022605896,\n",
       "   0.6182057857513428,\n",
       "   0.7910780906677246,\n",
       "   0.5448845624923706,\n",
       "   0.6275125741958618,\n",
       "   0.5284709930419922,\n",
       "   0.7952367663383484,\n",
       "   0.5620506405830383,\n",
       "   0.670333206653595,\n",
       "   0.8848776817321777,\n",
       "   0.6967939734458923,\n",
       "   0.7934872508049011,\n",
       "   0.7698321342468262,\n",
       "   0.724989652633667,\n",
       "   0.6985013484954834,\n",
       "   0.8907452821731567,\n",
       "   0.6362752914428711,\n",
       "   0.8781450390815735,\n",
       "   1.4112069606781006,\n",
       "   0.7472798824310303,\n",
       "   1.1087327003479004,\n",
       "   0.8270966410636902,\n",
       "   1.1449165344238281,\n",
       "   0.7315335869789124,\n",
       "   0.8063755035400391,\n",
       "   0.8707155585289001,\n",
       "   0.7440345883369446,\n",
       "   0.8166923522949219,\n",
       "   0.7272722125053406,\n",
       "   0.8301157355308533,\n",
       "   0.8928049802780151,\n",
       "   1.0252926349639893,\n",
       "   0.6896380186080933,\n",
       "   0.8645921349525452,\n",
       "   0.8810054659843445,\n",
       "   1.1448613405227661,\n",
       "   0.7529432773590088,\n",
       "   0.783950924873352,\n",
       "   0.6741573810577393,\n",
       "   0.8339524269104004,\n",
       "   0.7086454033851624,\n",
       "   0.7383078932762146,\n",
       "   0.8082324266433716,\n",
       "   0.6414874196052551,\n",
       "   0.7638751268386841,\n",
       "   0.6530531048774719,\n",
       "   0.8711919188499451,\n",
       "   0.6701211929321289,\n",
       "   0.8795578479766846,\n",
       "   0.9634920358657837,\n",
       "   1.0316450595855713,\n",
       "   0.6726663708686829,\n",
       "   0.7823963761329651,\n",
       "   0.715563952922821,\n",
       "   0.9499958157539368,\n",
       "   0.5427142977714539,\n",
       "   0.6459485292434692,\n",
       "   0.8723154664039612,\n",
       "   0.6939734816551208,\n",
       "   0.664473295211792,\n",
       "   0.5648043155670166,\n",
       "   0.5511913895606995,\n",
       "   1.1257123947143555,\n",
       "   0.8474302291870117,\n",
       "   0.9302783012390137,\n",
       "   0.8900213837623596,\n",
       "   0.931416392326355,\n",
       "   0.7293800115585327,\n",
       "   0.561714768409729,\n",
       "   0.7029811143875122,\n",
       "   0.637930691242218,\n",
       "   0.8521578907966614,\n",
       "   0.7211130261421204,\n",
       "   0.6609037518501282,\n",
       "   0.7356392741203308,\n",
       "   0.7433282136917114,\n",
       "   0.6505630612373352,\n",
       "   0.6286195516586304,\n",
       "   0.9045720100402832,\n",
       "   0.7891008853912354,\n",
       "   1.0991120338439941,\n",
       "   0.7396283149719238,\n",
       "   0.7972771525382996,\n",
       "   0.9503031373023987,\n",
       "   0.5938774943351746,\n",
       "   0.6713822484016418,\n",
       "   0.9339705109596252,\n",
       "   1.0157058238983154,\n",
       "   1.2364248037338257,\n",
       "   0.4859441816806793,\n",
       "   0.6332605481147766,\n",
       "   0.8064764738082886,\n",
       "   0.7471758127212524,\n",
       "   1.1123827695846558,\n",
       "   0.5775749087333679,\n",
       "   0.8448468446731567,\n",
       "   0.6505334973335266,\n",
       "   0.7151915431022644,\n",
       "   0.917000949382782,\n",
       "   0.8514617681503296,\n",
       "   0.7203490138053894,\n",
       "   1.2260202169418335,\n",
       "   0.7427486181259155,\n",
       "   0.8121154308319092,\n",
       "   0.7724151611328125,\n",
       "   0.949597179889679,\n",
       "   0.7657645344734192,\n",
       "   0.8704732656478882,\n",
       "   0.8328455090522766,\n",
       "   0.8456708192825317,\n",
       "   0.7620121836662292,\n",
       "   0.7709205150604248,\n",
       "   0.912595272064209,\n",
       "   0.5518189668655396,\n",
       "   1.2600953578948975,\n",
       "   0.6392985582351685,\n",
       "   0.7179591059684753,\n",
       "   0.8438554406166077,\n",
       "   1.0173368453979492,\n",
       "   0.8275817632675171,\n",
       "   0.7518064975738525,\n",
       "   1.0371465682983398,\n",
       "   0.890730619430542,\n",
       "   0.7413249611854553,\n",
       "   0.5500815510749817,\n",
       "   0.5239058136940002,\n",
       "   0.6574612855911255,\n",
       "   0.7477373480796814,\n",
       "   0.6278300285339355,\n",
       "   0.47788962721824646,\n",
       "   0.8059810400009155,\n",
       "   0.8623855113983154,\n",
       "   0.8539767861366272,\n",
       "   0.6624709367752075,\n",
       "   0.917321503162384,\n",
       "   0.8269174098968506,\n",
       "   0.8248468041419983,\n",
       "   0.9219226837158203,\n",
       "   0.5375056862831116,\n",
       "   0.6210921406745911,\n",
       "   0.580406665802002,\n",
       "   0.7904230356216431,\n",
       "   0.9633325338363647,\n",
       "   0.7304414510726929,\n",
       "   0.7310346961021423,\n",
       "   1.107692837715149,\n",
       "   0.7881438732147217,\n",
       "   0.7071306109428406,\n",
       "   0.5860698223114014,\n",
       "   0.7611437439918518,\n",
       "   0.7864924073219299,\n",
       "   0.47989994287490845,\n",
       "   0.8157776594161987,\n",
       "   0.5953671336174011,\n",
       "   0.9612075686454773,\n",
       "   0.7798408269882202,\n",
       "   0.8952136039733887,\n",
       "   0.6094240546226501,\n",
       "   0.9534331560134888,\n",
       "   0.7286706566810608,\n",
       "   0.8812068700790405,\n",
       "   0.7221454977989197,\n",
       "   0.5737935900688171,\n",
       "   0.594172477722168,\n",
       "   0.6151961088180542,\n",
       "   0.6049609184265137,\n",
       "   0.5208683609962463,\n",
       "   1.0372943878173828,\n",
       "   0.6137408018112183,\n",
       "   1.0052984952926636,\n",
       "   0.88405841588974,\n",
       "   0.768680214881897,\n",
       "   0.8779427409172058,\n",
       "   0.7786774635314941,\n",
       "   0.8718306422233582,\n",
       "   0.4877612590789795,\n",
       "   0.6568670868873596,\n",
       "   0.7326297760009766,\n",
       "   0.9332616329193115,\n",
       "   1.0819413661956787,\n",
       "   0.8723999261856079,\n",
       "   0.9730812907218933,\n",
       "   0.703271210193634,\n",
       "   1.0359858274459839,\n",
       "   0.6010643243789673,\n",
       "   0.6830193400382996,\n",
       "   0.8714286088943481,\n",
       "   0.8164933919906616,\n",
       "   0.7797321081161499,\n",
       "   0.5825054049491882,\n",
       "   0.6586960554122925,\n",
       "   0.8088492155075073,\n",
       "   0.7306246161460876,\n",
       "   0.6029136180877686,\n",
       "   0.7163252830505371,\n",
       "   0.8710149526596069,\n",
       "   0.6920750141143799,\n",
       "   0.9531014561653137,\n",
       "   0.6779200434684753,\n",
       "   0.6162115335464478,\n",
       "   0.8981639742851257,\n",
       "   0.4606686234474182,\n",
       "   0.8495147228240967,\n",
       "   0.726845920085907,\n",
       "   0.48394280672073364,\n",
       "   0.8268495798110962,\n",
       "   0.7735859751701355,\n",
       "   0.620988130569458,\n",
       "   1.0695205926895142,\n",
       "   0.6771330237388611,\n",
       "   0.6774085164070129,\n",
       "   0.7832529544830322,\n",
       "   0.6866962909698486,\n",
       "   0.8506482839584351,\n",
       "   0.5701583027839661,\n",
       "   0.5504828095436096,\n",
       "   0.9561547040939331,\n",
       "   0.737095832824707,\n",
       "   0.6391764283180237,\n",
       "   0.7796408534049988,\n",
       "   0.5056365132331848,\n",
       "   0.9820147156715393,\n",
       "   0.7989645004272461,\n",
       "   0.8854330778121948,\n",
       "   0.5599519610404968,\n",
       "   0.7134594917297363,\n",
       "   0.6004763245582581,\n",
       "   0.6642696261405945,\n",
       "   0.6345115303993225,\n",
       "   0.6355873942375183,\n",
       "   0.8451273441314697,\n",
       "   1.042920708656311,\n",
       "   0.8032361268997192,\n",
       "   0.724514901638031,\n",
       "   0.9001008868217468,\n",
       "   0.6766897439956665,\n",
       "   0.7021514773368835,\n",
       "   0.8455764651298523,\n",
       "   0.5550718903541565,\n",
       "   0.8975332379341125,\n",
       "   0.757206380367279,\n",
       "   0.7797542214393616,\n",
       "   0.6723058819770813,\n",
       "   0.8634082674980164,\n",
       "   0.8608165383338928,\n",
       "   0.6903570890426636,\n",
       "   0.6991791129112244,\n",
       "   0.8127739429473877,\n",
       "   0.5758406519889832,\n",
       "   0.6921095252037048,\n",
       "   0.8672038316726685,\n",
       "   0.5526869297027588,\n",
       "   0.929644763469696,\n",
       "   0.802927553653717,\n",
       "   0.9817777276039124,\n",
       "   0.821833610534668,\n",
       "   0.889266848564148,\n",
       "   0.7588299512863159,\n",
       "   0.8750650882720947,\n",
       "   0.853011965751648,\n",
       "   0.697026789188385,\n",
       "   0.6260135173797607,\n",
       "   0.989409863948822,\n",
       "   0.774156928062439,\n",
       "   0.6356730461120605,\n",
       "   0.8452994227409363,\n",
       "   0.8864631652832031,\n",
       "   0.5890558362007141,\n",
       "   0.6215640306472778,\n",
       "   0.9316607117652893,\n",
       "   0.7544945478439331,\n",
       "   0.5351886749267578,\n",
       "   0.5952267646789551,\n",
       "   0.5796956419944763,\n",
       "   0.6929258704185486,\n",
       "   0.866646409034729,\n",
       "   0.8725559711456299,\n",
       "   0.6901666522026062,\n",
       "   0.8955187201499939,\n",
       "   0.8533849120140076,\n",
       "   0.49282369017601013,\n",
       "   0.8466800451278687,\n",
       "   0.5446131229400635,\n",
       "   0.7073366641998291,\n",
       "   0.7136674523353577,\n",
       "   0.8903184533119202,\n",
       "   0.692118227481842,\n",
       "   0.5339503288269043,\n",
       "   0.6224759221076965,\n",
       "   0.7886783480644226,\n",
       "   1.4113497734069824,\n",
       "   0.7840887308120728,\n",
       "   0.6186676025390625,\n",
       "   0.8411048650741577,\n",
       "   1.1294703483581543,\n",
       "   0.735388457775116,\n",
       "   0.6635815501213074,\n",
       "   0.7862475514411926,\n",
       "   0.8425077199935913,\n",
       "   0.6904886364936829,\n",
       "   0.5607696175575256,\n",
       "   0.9328919053077698,\n",
       "   0.8151352405548096,\n",
       "   0.9599750638008118,\n",
       "   0.6096841096878052,\n",
       "   0.6979193091392517,\n",
       "   0.6030192375183105,\n",
       "   1.0103604793548584,\n",
       "   0.7468758821487427,\n",
       "   0.7850149273872375,\n",
       "   1.0017491579055786,\n",
       "   0.5521719455718994,\n",
       "   0.8206580281257629,\n",
       "   0.7493386268615723,\n",
       "   1.0065855979919434,\n",
       "   0.7818126082420349,\n",
       "   0.7998157143592834,\n",
       "   0.6122837066650391,\n",
       "   0.7778783440589905,\n",
       "   0.5929235816001892,\n",
       "   0.9245755672454834,\n",
       "   0.7711715698242188,\n",
       "   0.7755716443061829,\n",
       "   0.797366738319397,\n",
       "   0.6389859914779663,\n",
       "   0.5713972449302673,\n",
       "   1.1623533964157104,\n",
       "   0.5858425498008728,\n",
       "   0.8252339363098145,\n",
       "   0.670015811920166,\n",
       "   0.5654907822608948,\n",
       "   0.5670878887176514,\n",
       "   0.6660802364349365,\n",
       "   1.1261940002441406,\n",
       "   0.5006276369094849,\n",
       "   0.39234215021133423,\n",
       "   0.7108293175697327,\n",
       "   0.6427454948425293,\n",
       "   0.5371379256248474,\n",
       "   0.5765376687049866,\n",
       "   0.8582493662834167,\n",
       "   0.7235410809516907,\n",
       "   0.5396043062210083,\n",
       "   1.0840321779251099,\n",
       "   0.9430712461471558,\n",
       "   0.6005394458770752,\n",
       "   0.6661322712898254,\n",
       "   0.5124566555023193,\n",
       "   0.5169733166694641,\n",
       "   0.573239803314209,\n",
       "   0.8057935237884521,\n",
       "   0.734029233455658,\n",
       "   0.7263309955596924,\n",
       "   0.8143463134765625,\n",
       "   0.6257257461547852,\n",
       "   0.6468772888183594,\n",
       "   0.6689742207527161,\n",
       "   0.9111211895942688,\n",
       "   0.6925893425941467,\n",
       "   0.9737993478775024,\n",
       "   0.5665667057037354,\n",
       "   0.7973387241363525,\n",
       "   0.8443432450294495,\n",
       "   0.649349570274353,\n",
       "   0.6781419515609741,\n",
       "   1.2217888832092285,\n",
       "   0.9790719747543335,\n",
       "   0.7309060096740723,\n",
       "   0.637155294418335,\n",
       "   0.3811377286911011,\n",
       "   0.6370908617973328,\n",
       "   0.7904614210128784,\n",
       "   0.6591871380805969,\n",
       "   0.5880307555198669,\n",
       "   0.9341274499893188,\n",
       "   0.5722523331642151,\n",
       "   0.6455954909324646,\n",
       "   0.7707409858703613,\n",
       "   0.6653615236282349,\n",
       "   0.8496583104133606,\n",
       "   0.8284568190574646,\n",
       "   0.6695863604545593,\n",
       "   0.7456804513931274,\n",
       "   0.6596618890762329,\n",
       "   0.6143916249275208,\n",
       "   0.9733737707138062,\n",
       "   0.8950300812721252,\n",
       "   1.2370706796646118,\n",
       "   0.8579495549201965,\n",
       "   0.9950742125511169,\n",
       "   0.5010321736335754,\n",
       "   0.6717373728752136,\n",
       "   0.8750125765800476,\n",
       "   0.5702571868896484,\n",
       "   0.6752326488494873,\n",
       "   0.6541124582290649,\n",
       "   0.6564381718635559,\n",
       "   0.7739524841308594,\n",
       "   0.6703412532806396,\n",
       "   0.5467429757118225,\n",
       "   0.559757649898529,\n",
       "   0.6956852674484253,\n",
       "   0.7653526067733765,\n",
       "   0.8171923160552979,\n",
       "   0.636626124382019,\n",
       "   0.5762383937835693,\n",
       "   0.9995554685592651,\n",
       "   0.9931944608688354,\n",
       "   0.7547016739845276,\n",
       "   0.7283961772918701,\n",
       "   0.6191048622131348,\n",
       "   0.6067509055137634,\n",
       "   1.0559197664260864,\n",
       "   0.9353501796722412,\n",
       "   0.713818371295929,\n",
       "   0.6071921586990356,\n",
       "   0.9459239840507507,\n",
       "   0.9348911643028259,\n",
       "   0.8352758884429932,\n",
       "   0.8448200821876526,\n",
       "   0.7137828469276428,\n",
       "   0.7186368107795715,\n",
       "   1.010077714920044,\n",
       "   0.772264838218689,\n",
       "   0.9217915534973145,\n",
       "   0.9326886534690857,\n",
       "   0.5438494682312012,\n",
       "   1.0339173078536987,\n",
       "   0.7190545201301575,\n",
       "   0.8038374185562134,\n",
       "   0.9351856708526611,\n",
       "   0.6687113046646118,\n",
       "   0.7252213954925537,\n",
       "   0.7972438335418701,\n",
       "   0.7162028551101685,\n",
       "   0.6554965376853943,\n",
       "   0.8805081844329834,\n",
       "   0.7463299632072449,\n",
       "   0.6744773983955383,\n",
       "   0.624440610408783,\n",
       "   0.7819705009460449,\n",
       "   0.6169579029083252,\n",
       "   0.5893914699554443,\n",
       "   0.6889991164207458,\n",
       "   0.43613287806510925,\n",
       "   0.5862521529197693,\n",
       "   0.9617583155632019,\n",
       "   0.8253766298294067,\n",
       "   0.8425519466400146,\n",
       "   0.7662121057510376,\n",
       "   0.8328686952590942,\n",
       "   0.7230852246284485,\n",
       "   0.34185677766799927,\n",
       "   0.8259744644165039,\n",
       "   0.7544673681259155,\n",
       "   0.638680636882782,\n",
       "   0.7705071568489075,\n",
       "   0.8659578561782837,\n",
       "   0.45073845982551575,\n",
       "   0.6295145750045776,\n",
       "   0.924004852771759,\n",
       "   0.890008270740509,\n",
       "   1.1744270324707031,\n",
       "   0.6667896509170532,\n",
       "   0.699270486831665,\n",
       "   0.8163568377494812,\n",
       "   0.5242124199867249,\n",
       "   0.8061403036117554,\n",
       "   1.08134126663208,\n",
       "   0.8082574605941772,\n",
       "   0.6538234949111938,\n",
       "   0.7684054970741272,\n",
       "   0.7313293218612671,\n",
       "   0.8640035390853882,\n",
       "   0.779653787612915,\n",
       "   0.6953903436660767]},\n",
       " 4: {'lr': 1e-06,\n",
       "  'best_loss_epoch': 139,\n",
       "  'best_acc_epoch': 139,\n",
       "  'best_r2_epoch': 1,\n",
       "  'pce_loss': [2.367520809173584,\n",
       "   1.6457172632217407,\n",
       "   1.1792020797729492,\n",
       "   0.8370475172996521,\n",
       "   0.6705002784729004,\n",
       "   0.5303111672401428,\n",
       "   0.462965726852417,\n",
       "   0.4094513952732086,\n",
       "   0.3904411196708679,\n",
       "   0.3718462586402893,\n",
       "   0.35770875215530396,\n",
       "   0.33737078309059143,\n",
       "   0.33078187704086304,\n",
       "   0.3169863522052765,\n",
       "   0.32022279500961304,\n",
       "   0.32123348116874695,\n",
       "   0.33117491006851196,\n",
       "   0.327771931886673,\n",
       "   0.32877376675605774,\n",
       "   0.33890989422798157,\n",
       "   0.3263671398162842,\n",
       "   0.33663567900657654,\n",
       "   0.35245582461357117,\n",
       "   0.3447769582271576,\n",
       "   0.35164496302604675,\n",
       "   0.3509809374809265,\n",
       "   0.34440937638282776,\n",
       "   0.3617657721042633,\n",
       "   0.3658738434314728,\n",
       "   0.3589518666267395,\n",
       "   0.3940463662147522,\n",
       "   0.39005541801452637,\n",
       "   0.3653492331504822,\n",
       "   0.36658748984336853,\n",
       "   0.3996705412864685,\n",
       "   0.39661455154418945,\n",
       "   0.41374629735946655,\n",
       "   0.42423465847969055,\n",
       "   0.4023590683937073,\n",
       "   0.40464335680007935,\n",
       "   0.4074668884277344,\n",
       "   0.408372700214386,\n",
       "   0.40940043330192566,\n",
       "   0.4066736400127411,\n",
       "   0.41397738456726074,\n",
       "   0.41681426763534546,\n",
       "   0.42101725935935974,\n",
       "   0.4198720455169678,\n",
       "   0.40760543942451477,\n",
       "   0.3907530903816223,\n",
       "   0.41376566886901855,\n",
       "   0.4196341931819916,\n",
       "   0.41873759031295776,\n",
       "   0.42622849345207214,\n",
       "   0.4414379298686981,\n",
       "   0.44482678174972534,\n",
       "   0.45621395111083984,\n",
       "   0.44909635186195374,\n",
       "   0.45942750573158264,\n",
       "   0.45642587542533875,\n",
       "   0.46663495898246765,\n",
       "   0.4574708044528961,\n",
       "   0.4687786102294922,\n",
       "   0.4386097490787506,\n",
       "   0.44159188866615295,\n",
       "   0.45525774359703064,\n",
       "   0.4499008357524872,\n",
       "   0.4549899399280548,\n",
       "   0.43562254309654236,\n",
       "   0.45442479848861694,\n",
       "   0.4605131447315216,\n",
       "   0.4654304087162018,\n",
       "   0.46949753165245056,\n",
       "   0.46535348892211914,\n",
       "   0.4567573070526123,\n",
       "   0.45778194069862366,\n",
       "   0.4544110894203186,\n",
       "   0.4242429733276367,\n",
       "   0.4266270697116852,\n",
       "   0.4526839256286621,\n",
       "   0.4469475746154785,\n",
       "   0.4502939283847809,\n",
       "   0.4363599419593811,\n",
       "   0.44047826528549194,\n",
       "   0.4127620756626129,\n",
       "   0.4240402281284332,\n",
       "   0.42858919501304626,\n",
       "   0.4252636134624481,\n",
       "   0.39786794781684875,\n",
       "   0.39221954345703125,\n",
       "   0.36614227294921875,\n",
       "   0.379362016916275,\n",
       "   0.3891795575618744,\n",
       "   0.37901848554611206,\n",
       "   0.3988969922065735,\n",
       "   0.3923984467983246,\n",
       "   0.4066517651081085,\n",
       "   0.41258957982063293,\n",
       "   0.4226456582546234,\n",
       "   0.4092476963996887,\n",
       "   0.4145544171333313,\n",
       "   0.403963565826416,\n",
       "   0.4039526581764221,\n",
       "   0.4098300635814667,\n",
       "   0.41652435064315796,\n",
       "   0.4010620415210724,\n",
       "   0.40865352749824524,\n",
       "   0.4052550792694092,\n",
       "   0.4141632616519928,\n",
       "   0.41730165481567383,\n",
       "   0.43012526631355286,\n",
       "   0.4319557249546051,\n",
       "   0.43192583322525024,\n",
       "   0.4447665810585022,\n",
       "   0.45287418365478516,\n",
       "   0.46248355507850647,\n",
       "   0.4627384543418884,\n",
       "   0.46387773752212524,\n",
       "   0.4574602246284485,\n",
       "   0.46542230248451233,\n",
       "   0.45412492752075195,\n",
       "   0.4546429216861725,\n",
       "   0.4420998990535736,\n",
       "   0.45773109793663025,\n",
       "   0.44457799196243286,\n",
       "   0.4485814869403839,\n",
       "   0.4739740788936615,\n",
       "   0.482599675655365,\n",
       "   0.48204952478408813,\n",
       "   0.48506423830986023,\n",
       "   0.4819129407405853,\n",
       "   0.4736548960208893,\n",
       "   0.47006866335868835,\n",
       "   0.4616708755493164,\n",
       "   0.46758705377578735,\n",
       "   0.4670332968235016,\n",
       "   0.4470877945423126,\n",
       "   0.462958961725235,\n",
       "   0.45058131217956543,\n",
       "   0.4206094741821289,\n",
       "   0.442754328250885,\n",
       "   0.4354921877384186,\n",
       "   0.41407105326652527,\n",
       "   0.430980920791626,\n",
       "   0.43873006105422974,\n",
       "   0.44088050723075867,\n",
       "   0.45149752497673035,\n",
       "   0.4480402171611786,\n",
       "   0.4569911062717438,\n",
       "   0.4466187357902527,\n",
       "   0.4188709259033203,\n",
       "   0.4513353407382965,\n",
       "   0.43840059638023376,\n",
       "   0.44247204065322876,\n",
       "   0.41739049553871155,\n",
       "   0.4263838827610016,\n",
       "   0.407366007566452,\n",
       "   0.437704473733902,\n",
       "   0.4299263060092926,\n",
       "   0.4301890432834625,\n",
       "   0.4126782715320587,\n",
       "   0.41176095604896545,\n",
       "   0.39634376764297485,\n",
       "   0.41607901453971863,\n",
       "   0.42635902762413025,\n",
       "   0.42561134696006775,\n",
       "   0.4364814758300781,\n",
       "   0.4180305004119873,\n",
       "   0.4053712487220764,\n",
       "   0.4088113009929657,\n",
       "   0.4369052052497864,\n",
       "   0.4315849840641022,\n",
       "   0.447162002325058,\n",
       "   0.4597192406654358,\n",
       "   0.4325026869773865,\n",
       "   0.43921342492103577,\n",
       "   0.432754784822464,\n",
       "   0.44944244623184204,\n",
       "   0.43091756105422974,\n",
       "   0.4491686224937439,\n",
       "   0.4344990849494934,\n",
       "   0.45583441853523254,\n",
       "   0.4417808949947357,\n",
       "   0.40317994356155396,\n",
       "   0.4154253900051117,\n",
       "   0.40358060598373413,\n",
       "   0.3929598033428192,\n",
       "   0.4012708067893982,\n",
       "   0.39485201239585876,\n",
       "   0.4094574451446533,\n",
       "   0.40427908301353455,\n",
       "   0.4107275605201721,\n",
       "   0.41214364767074585,\n",
       "   0.40889301896095276,\n",
       "   0.4344196021556854,\n",
       "   0.42964690923690796,\n",
       "   0.42356640100479126,\n",
       "   0.42755454778671265,\n",
       "   0.45797744393348694,\n",
       "   0.4561087191104889,\n",
       "   0.43044835329055786,\n",
       "   0.4506552517414093,\n",
       "   0.4375135898590088,\n",
       "   0.4193229377269745,\n",
       "   0.42977872490882874,\n",
       "   0.4328911602497101,\n",
       "   0.43000802397727966,\n",
       "   0.44984200596809387,\n",
       "   0.43527457118034363,\n",
       "   0.4611893594264984,\n",
       "   0.47162124514579773,\n",
       "   0.46708235144615173,\n",
       "   0.4615556299686432,\n",
       "   0.4583302140235901,\n",
       "   0.43992504477500916,\n",
       "   0.42855650186538696,\n",
       "   0.4147140085697174,\n",
       "   0.41054490208625793,\n",
       "   0.40690332651138306,\n",
       "   0.3920252323150635,\n",
       "   0.4002702832221985,\n",
       "   0.4033811688423157,\n",
       "   0.41080430150032043,\n",
       "   0.4093037545681,\n",
       "   0.41034385561943054,\n",
       "   0.41193723678588867,\n",
       "   0.40784648060798645,\n",
       "   0.4244903028011322,\n",
       "   0.41579729318618774,\n",
       "   0.38846102356910706,\n",
       "   0.39141014218330383,\n",
       "   0.40960702300071716,\n",
       "   0.4289025664329529,\n",
       "   0.41527289152145386,\n",
       "   0.4498603940010071,\n",
       "   0.4622096121311188,\n",
       "   0.4549865126609802,\n",
       "   0.46082809567451477,\n",
       "   0.45841240882873535,\n",
       "   0.4735608696937561,\n",
       "   0.46753624081611633,\n",
       "   0.46589431166648865,\n",
       "   0.4686378836631775,\n",
       "   0.44247904419898987,\n",
       "   0.4473406672477722,\n",
       "   0.4226602017879486,\n",
       "   0.44322988390922546,\n",
       "   0.4489748179912567,\n",
       "   0.4435512125492096,\n",
       "   0.4570487439632416,\n",
       "   0.4678569436073303,\n",
       "   0.4760400950908661,\n",
       "   0.4842586815357208,\n",
       "   0.49690723419189453,\n",
       "   0.4923620820045471,\n",
       "   0.4959897994995117,\n",
       "   0.48384952545166016,\n",
       "   0.4852275848388672,\n",
       "   0.47686001658439636,\n",
       "   0.4837549030780792,\n",
       "   0.4872414767742157,\n",
       "   0.4858497977256775,\n",
       "   0.477800577878952,\n",
       "   0.46346667408943176,\n",
       "   0.44826269149780273,\n",
       "   0.44416528940200806,\n",
       "   0.42968088388442993,\n",
       "   0.4266915023326874,\n",
       "   0.44303128123283386,\n",
       "   0.44125646352767944,\n",
       "   0.44479885697364807,\n",
       "   0.4624338150024414,\n",
       "   0.4529077708721161,\n",
       "   0.4558938145637512,\n",
       "   0.47266677021980286,\n",
       "   0.46678754687309265,\n",
       "   0.4735552966594696,\n",
       "   0.47372937202453613,\n",
       "   0.47557953000068665,\n",
       "   0.4725552201271057,\n",
       "   0.46972420811653137,\n",
       "   0.46447888016700745,\n",
       "   0.47102466225624084,\n",
       "   0.4573522210121155,\n",
       "   0.46494510769844055,\n",
       "   0.4595847725868225,\n",
       "   0.43286392092704773,\n",
       "   0.4311029613018036,\n",
       "   0.4506653845310211,\n",
       "   0.47110268473625183,\n",
       "   0.4497893452644348,\n",
       "   0.4617789089679718,\n",
       "   0.46245118975639343,\n",
       "   0.4320876896381378,\n",
       "   0.44335150718688965,\n",
       "   0.46230974793434143,\n",
       "   0.4470655024051666,\n",
       "   0.4545445740222931,\n",
       "   0.48050180077552795,\n",
       "   0.4812231957912445,\n",
       "   0.4702449440956116,\n",
       "   0.47623929381370544,\n",
       "   0.4529114365577698,\n",
       "   0.4690198302268982,\n",
       "   0.44743794202804565,\n",
       "   0.45525479316711426,\n",
       "   0.45255663990974426,\n",
       "   0.4570905864238739,\n",
       "   0.4759370982646942,\n",
       "   0.48573964834213257,\n",
       "   0.4854305684566498,\n",
       "   0.4824378192424774,\n",
       "   0.47966518998146057,\n",
       "   0.482538640499115,\n",
       "   0.48355478048324585,\n",
       "   0.4813518524169922,\n",
       "   0.48138490319252014,\n",
       "   0.46495652198791504,\n",
       "   0.4499632716178894,\n",
       "   0.44277459383010864,\n",
       "   0.44932815432548523,\n",
       "   0.45421722531318665,\n",
       "   0.4302952289581299,\n",
       "   0.4190675914287567,\n",
       "   0.411411851644516,\n",
       "   0.44082146883010864,\n",
       "   0.4225032925605774,\n",
       "   0.4505494236946106,\n",
       "   0.4540915787220001,\n",
       "   0.47251325845718384,\n",
       "   0.4766499698162079,\n",
       "   0.4637596607208252,\n",
       "   0.46002253890037537,\n",
       "   0.44707122445106506,\n",
       "   0.4361923038959503,\n",
       "   0.43211111426353455,\n",
       "   0.43645843863487244,\n",
       "   0.43262672424316406,\n",
       "   0.4432527422904968,\n",
       "   0.4185979664325714,\n",
       "   0.42306557297706604,\n",
       "   0.43301287293434143,\n",
       "   0.45203739404678345,\n",
       "   0.4662643074989319,\n",
       "   0.4357689321041107,\n",
       "   0.44758400321006775,\n",
       "   0.46280211210250854,\n",
       "   0.43636178970336914,\n",
       "   0.4336070716381073,\n",
       "   0.42512568831443787,\n",
       "   0.40906959772109985,\n",
       "   0.4401998519897461,\n",
       "   0.4406992793083191,\n",
       "   0.4159645736217499,\n",
       "   0.42490094900131226,\n",
       "   0.4456041753292084,\n",
       "   0.4742465317249298,\n",
       "   0.4776823818683624,\n",
       "   0.48616647720336914,\n",
       "   0.474866658449173,\n",
       "   0.4761238694190979,\n",
       "   0.4726569950580597,\n",
       "   0.4642527401447296,\n",
       "   0.47477349638938904,\n",
       "   0.45917823910713196,\n",
       "   0.46513718366622925,\n",
       "   0.46783772110939026,\n",
       "   0.46046358346939087,\n",
       "   0.4706287980079651,\n",
       "   0.4804621636867523,\n",
       "   0.46299582719802856,\n",
       "   0.4778149724006653,\n",
       "   0.4622252583503723,\n",
       "   0.4674816131591797,\n",
       "   0.47483354806900024,\n",
       "   0.43681126832962036,\n",
       "   0.44159629940986633,\n",
       "   0.440378338098526,\n",
       "   0.42047542333602905,\n",
       "   0.44475722312927246,\n",
       "   0.432618111371994,\n",
       "   0.43101951479911804,\n",
       "   0.4338701367378235,\n",
       "   0.43444761633872986,\n",
       "   0.4281446635723114,\n",
       "   0.40346938371658325,\n",
       "   0.4047814905643463,\n",
       "   0.41873231530189514,\n",
       "   0.4124617576599121,\n",
       "   0.40966519713401794,\n",
       "   0.422666996717453,\n",
       "   0.404017835855484,\n",
       "   0.4284599721431732,\n",
       "   0.4330310523509979,\n",
       "   0.3979579508304596,\n",
       "   0.437940388917923,\n",
       "   0.41059479117393494,\n",
       "   0.39568859338760376,\n",
       "   0.4053628146648407,\n",
       "   0.39989492297172546,\n",
       "   0.3963221609592438,\n",
       "   0.41021856665611267,\n",
       "   0.41376280784606934,\n",
       "   0.41388753056526184,\n",
       "   0.41244572401046753,\n",
       "   0.4347805976867676,\n",
       "   0.43888363242149353,\n",
       "   0.4460524022579193,\n",
       "   0.43907445669174194,\n",
       "   0.4556633532047272,\n",
       "   0.4591643214225769,\n",
       "   0.4319940209388733,\n",
       "   0.43647700548171997,\n",
       "   0.419410765171051,\n",
       "   0.4288123548030853,\n",
       "   0.4337298274040222,\n",
       "   0.4392189085483551,\n",
       "   0.46385493874549866,\n",
       "   0.44344449043273926,\n",
       "   0.43430569767951965,\n",
       "   0.42132988572120667,\n",
       "   0.3779420554637909,\n",
       "   0.38844332098960876,\n",
       "   0.38175535202026367,\n",
       "   0.37557855248451233,\n",
       "   0.3810066878795624,\n",
       "   0.3842281401157379,\n",
       "   0.40267056226730347,\n",
       "   0.4174942970275879,\n",
       "   0.40545573830604553,\n",
       "   0.43443602323532104,\n",
       "   0.42084169387817383,\n",
       "   0.39568862318992615,\n",
       "   0.3883131146430969,\n",
       "   0.39071688055992126,\n",
       "   0.4076290726661682,\n",
       "   0.40229034423828125,\n",
       "   0.393768846988678,\n",
       "   0.39090585708618164,\n",
       "   0.4101596474647522,\n",
       "   0.4099632799625397,\n",
       "   0.4223984181880951,\n",
       "   0.3964473605155945,\n",
       "   0.38731497526168823,\n",
       "   0.407900333404541,\n",
       "   0.4205823540687561,\n",
       "   0.4467061460018158,\n",
       "   0.43730494379997253,\n",
       "   0.46854379773139954,\n",
       "   0.46289533376693726,\n",
       "   0.44825229048728943,\n",
       "   0.4522327482700348,\n",
       "   0.43125924468040466,\n",
       "   0.40181589126586914,\n",
       "   0.4157366454601288,\n",
       "   0.4252900779247284,\n",
       "   0.41472533345222473,\n",
       "   0.4342837929725647,\n",
       "   0.4558749794960022,\n",
       "   0.438947468996048,\n",
       "   0.4510119557380676,\n",
       "   0.4586136043071747,\n",
       "   0.4297848343849182,\n",
       "   0.4310775697231293,\n",
       "   0.4544602036476135,\n",
       "   0.49197784066200256,\n",
       "   0.5076509714126587,\n",
       "   0.5063027143478394,\n",
       "   0.4920632541179657,\n",
       "   0.500331699848175,\n",
       "   0.5094470381736755,\n",
       "   0.5177686810493469,\n",
       "   0.5115404725074768,\n",
       "   0.5066866874694824,\n",
       "   0.5013874769210815,\n",
       "   0.5000871419906616,\n",
       "   0.4966158866882324,\n",
       "   0.4876663386821747,\n",
       "   0.49281182885169983,\n",
       "   0.4902421534061432,\n",
       "   0.48335298895835876,\n",
       "   0.49441856145858765,\n",
       "   0.48527422547340393,\n",
       "   0.4746895432472229,\n",
       "   0.46454912424087524,\n",
       "   0.45853954553604126,\n",
       "   0.43071553111076355,\n",
       "   0.44066309928894043,\n",
       "   0.41124555468559265,\n",
       "   0.38891634345054626,\n",
       "   0.41076579689979553,\n",
       "   0.4170871376991272,\n",
       "   0.4041305184364319,\n",
       "   0.39457717537879944,\n",
       "   0.4289347231388092,\n",
       "   0.4506003260612488,\n",
       "   0.4217686653137207,\n",
       "   0.45281320810317993,\n",
       "   0.46783027052879333,\n",
       "   0.4595247209072113],\n",
       "  'voc_loss': [0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6844539046287537,\n",
       "   0.6806020736694336,\n",
       "   0.6427236795425415,\n",
       "   0.620154857635498,\n",
       "   0.6054182052612305,\n",
       "   0.5966076254844666,\n",
       "   0.5920200347900391,\n",
       "   0.5675661563873291,\n",
       "   0.5352552533149719,\n",
       "   0.5064458250999451,\n",
       "   0.4917425513267517,\n",
       "   0.4741017520427704,\n",
       "   0.46679067611694336,\n",
       "   0.4615648686885834,\n",
       "   0.451386034488678,\n",
       "   0.4470982551574707,\n",
       "   0.4406464099884033,\n",
       "   0.4439285397529602,\n",
       "   0.43933868408203125,\n",
       "   0.4410798251628876,\n",
       "   0.435384064912796,\n",
       "   0.432938814163208,\n",
       "   0.4342634081840515,\n",
       "   0.434827595949173,\n",
       "   0.43309301137924194,\n",
       "   0.43648865818977356,\n",
       "   0.43442246317863464,\n",
       "   0.44153523445129395,\n",
       "   0.4391079545021057,\n",
       "   0.44489750266075134,\n",
       "   0.4427454471588135,\n",
       "   0.4447195529937744,\n",
       "   0.45456522703170776,\n",
       "   0.447885125875473,\n",
       "   0.44816941022872925,\n",
       "   0.453278124332428,\n",
       "   0.457174688577652,\n",
       "   0.45434117317199707,\n",
       "   0.44602471590042114,\n",
       "   0.4458274245262146,\n",
       "   0.4495100975036621,\n",
       "   0.4473491609096527,\n",
       "   0.43770402669906616,\n",
       "   0.43385979533195496,\n",
       "   0.4265270531177521,\n",
       "   0.41916805505752563,\n",
       "   0.4252018332481384,\n",
       "   0.4254574775695801,\n",
       "   0.4202008843421936,\n",
       "   0.4157431721687317,\n",
       "   0.4154840111732483,\n",
       "   0.41378873586654663,\n",
       "   0.407925546169281,\n",
       "   0.39164844155311584,\n",
       "   0.39423736929893494,\n",
       "   0.38158655166625977,\n",
       "   0.3734605014324188,\n",
       "   0.3730437457561493,\n",
       "   0.37021586298942566,\n",
       "   0.3762320876121521,\n",
       "   0.3600585162639618,\n",
       "   0.35662636160850525,\n",
       "   0.34786680340766907,\n",
       "   0.34973472356796265,\n",
       "   0.34448808431625366,\n",
       "   0.34755921363830566,\n",
       "   0.34374260902404785,\n",
       "   0.32330432534217834,\n",
       "   0.32945504784584045,\n",
       "   0.32662633061408997,\n",
       "   0.33831748366355896,\n",
       "   0.31615373492240906,\n",
       "   0.3187377452850342,\n",
       "   0.307160347700119,\n",
       "   0.2936652600765228,\n",
       "   0.2737550437450409,\n",
       "   0.26567551493644714,\n",
       "   0.263801246881485,\n",
       "   0.2579893171787262,\n",
       "   0.2583096921443939,\n",
       "   0.25565749406814575,\n",
       "   0.25058501958847046,\n",
       "   0.24634502828121185,\n",
       "   0.245464488863945,\n",
       "   0.25339847803115845,\n",
       "   0.24573373794555664,\n",
       "   0.23794780671596527,\n",
       "   0.23146626353263855,\n",
       "   0.2271052896976471,\n",
       "   0.2231830358505249,\n",
       "   0.2192610204219818,\n",
       "   0.21916420757770538,\n",
       "   0.22173693776130676,\n",
       "   0.22137172520160675,\n",
       "   0.22480840981006622,\n",
       "   0.22515760362148285,\n",
       "   0.22635102272033691,\n",
       "   0.22917704284191132,\n",
       "   0.23652920126914978,\n",
       "   0.24656662344932556,\n",
       "   0.24810054898262024,\n",
       "   0.24757841229438782,\n",
       "   0.2613057494163513,\n",
       "   0.24102842807769775,\n",
       "   0.235381618142128,\n",
       "   0.23773039877414703,\n",
       "   0.24494300782680511,\n",
       "   0.23839318752288818,\n",
       "   0.23024597764015198,\n",
       "   0.22007529437541962,\n",
       "   0.23543016612529755,\n",
       "   0.22008691728115082,\n",
       "   0.2125215232372284,\n",
       "   0.20610566437244415,\n",
       "   0.21445666253566742,\n",
       "   0.21079930663108826,\n",
       "   0.207330122590065,\n",
       "   0.20141005516052246,\n",
       "   0.18944157660007477,\n",
       "   0.18733298778533936,\n",
       "   0.186964750289917,\n",
       "   0.19018344581127167,\n",
       "   0.19971251487731934,\n",
       "   0.19514451920986176,\n",
       "   0.22680070996284485,\n",
       "   0.21918225288391113,\n",
       "   0.22188402712345123,\n",
       "   0.2412281334400177,\n",
       "   0.20988023281097412,\n",
       "   0.1987171620130539,\n",
       "   0.20894476771354675,\n",
       "   0.19944298267364502,\n",
       "   0.19787046313285828,\n",
       "   0.20380742847919464,\n",
       "   0.21555134654045105,\n",
       "   0.229981929063797,\n",
       "   0.23259234428405762,\n",
       "   0.2618238031864166,\n",
       "   0.2596184313297272,\n",
       "   0.24683848023414612,\n",
       "   0.2727676033973694,\n",
       "   0.25248175859451294,\n",
       "   0.2555287182331085,\n",
       "   0.2580253481864929,\n",
       "   0.24474863708019257,\n",
       "   0.23403452336788177,\n",
       "   0.23298224806785583,\n",
       "   0.24460890889167786,\n",
       "   0.2789274752140045,\n",
       "   0.2500266134738922,\n",
       "   0.2344144731760025,\n",
       "   0.22928310930728912,\n",
       "   0.24615348875522614,\n",
       "   0.21570153534412384,\n",
       "   0.21360714733600616,\n",
       "   0.1964392513036728,\n",
       "   0.20299045741558075,\n",
       "   0.20757970213890076,\n",
       "   0.22674040496349335,\n",
       "   0.22826460003852844,\n",
       "   0.22361168265342712,\n",
       "   0.235958531498909,\n",
       "   0.2642587125301361,\n",
       "   0.281015545129776,\n",
       "   0.26955124735832214,\n",
       "   0.2687986195087433,\n",
       "   0.2812093198299408,\n",
       "   0.28100231289863586,\n",
       "   0.30152440071105957,\n",
       "   0.2941047251224518,\n",
       "   0.3035140335559845,\n",
       "   0.30033063888549805,\n",
       "   0.29988980293273926,\n",
       "   0.28359559178352356,\n",
       "   0.29022032022476196,\n",
       "   0.3017031252384186,\n",
       "   0.29751887917518616,\n",
       "   0.3175472617149353,\n",
       "   0.31507793068885803,\n",
       "   0.2968549132347107,\n",
       "   0.3483981490135193,\n",
       "   0.3443434238433838,\n",
       "   0.3682762086391449,\n",
       "   0.3770700991153717,\n",
       "   0.36626192927360535,\n",
       "   0.3585250973701477,\n",
       "   0.37841033935546875,\n",
       "   0.3522842228412628,\n",
       "   0.3452875018119812,\n",
       "   0.37081602215766907,\n",
       "   0.3383832573890686,\n",
       "   0.34035158157348633,\n",
       "   0.35941940546035767,\n",
       "   0.30652543902397156,\n",
       "   0.2929310202598572,\n",
       "   0.29579946398735046,\n",
       "   0.2946970760822296,\n",
       "   0.2862725853919983,\n",
       "   0.2922532558441162,\n",
       "   0.3095833361148834,\n",
       "   0.31562212109565735,\n",
       "   0.31643080711364746,\n",
       "   0.33425775170326233,\n",
       "   0.3095408082008362,\n",
       "   0.3194372355937958,\n",
       "   0.32960066199302673,\n",
       "   0.31746652722358704,\n",
       "   0.31399673223495483,\n",
       "   0.3009762763977051,\n",
       "   0.2998742461204529,\n",
       "   0.30248671770095825,\n",
       "   0.2807159423828125,\n",
       "   0.2815350890159607,\n",
       "   0.28341421484947205,\n",
       "   0.2697823941707611,\n",
       "   0.256658136844635,\n",
       "   0.263156533241272,\n",
       "   0.2518022358417511,\n",
       "   0.2513023614883423,\n",
       "   0.2529211938381195,\n",
       "   0.25888869166374207,\n",
       "   0.2611493170261383,\n",
       "   0.2734142243862152,\n",
       "   0.2729012966156006,\n",
       "   0.27153804898262024,\n",
       "   0.275119811296463,\n",
       "   0.27670255303382874,\n",
       "   0.2710147798061371,\n",
       "   0.2897246778011322,\n",
       "   0.27763858437538147,\n",
       "   0.27830877900123596,\n",
       "   0.2958502471446991,\n",
       "   0.29439643025398254,\n",
       "   0.2909175455570221,\n",
       "   0.28921082615852356,\n",
       "   0.29458266496658325,\n",
       "   0.31331807374954224,\n",
       "   0.31151095032691956,\n",
       "   0.3447280824184418,\n",
       "   0.35319751501083374,\n",
       "   0.34212103486061096,\n",
       "   0.3280032277107239,\n",
       "   0.33666130900382996,\n",
       "   0.3503379225730896,\n",
       "   0.36236244440078735,\n",
       "   0.36633196473121643,\n",
       "   0.3731304705142975,\n",
       "   0.3514247536659241,\n",
       "   0.3465547561645508,\n",
       "   0.3264874815940857,\n",
       "   0.3376629948616028,\n",
       "   0.34670373797416687,\n",
       "   0.3222236931324005,\n",
       "   0.30986151099205017,\n",
       "   0.3039402961730957,\n",
       "   0.3188827931880951,\n",
       "   0.3341250717639923,\n",
       "   0.31942370533943176,\n",
       "   0.3254764974117279,\n",
       "   0.33490675687789917,\n",
       "   0.344251424074173,\n",
       "   0.3472246527671814,\n",
       "   0.34047555923461914,\n",
       "   0.3580147325992584,\n",
       "   0.3510398268699646,\n",
       "   0.36064404249191284,\n",
       "   0.38343051075935364,\n",
       "   0.3900110125541687,\n",
       "   0.3802645206451416,\n",
       "   0.4135728180408478,\n",
       "   0.4126686751842499,\n",
       "   0.39419493079185486,\n",
       "   0.3700762093067169,\n",
       "   0.38635557889938354,\n",
       "   0.38787075877189636,\n",
       "   0.39116865396499634,\n",
       "   0.3848131597042084,\n",
       "   0.37461474537849426,\n",
       "   0.3520815372467041,\n",
       "   0.35307857394218445,\n",
       "   0.35311299562454224,\n",
       "   0.33489149808883667,\n",
       "   0.31881722807884216,\n",
       "   0.32001572847366333,\n",
       "   0.32739824056625366,\n",
       "   0.3289797306060791,\n",
       "   0.3202088177204132,\n",
       "   0.3269343674182892,\n",
       "   0.3343110680580139,\n",
       "   0.30149906873703003,\n",
       "   0.2937135696411133,\n",
       "   0.29397112131118774,\n",
       "   0.3184494972229004,\n",
       "   0.3088676333427429,\n",
       "   0.2960243225097656,\n",
       "   0.2959490120410919,\n",
       "   0.296596497297287,\n",
       "   0.2995913326740265,\n",
       "   0.296771377325058,\n",
       "   0.30688047409057617,\n",
       "   0.31150245666503906,\n",
       "   0.32146790623664856,\n",
       "   0.32022398710250854,\n",
       "   0.3212950825691223,\n",
       "   0.3171686828136444,\n",
       "   0.3169070780277252,\n",
       "   0.33219286799430847,\n",
       "   0.32378607988357544,\n",
       "   0.312246173620224,\n",
       "   0.30078330636024475,\n",
       "   0.30147239565849304,\n",
       "   0.3044576346874237,\n",
       "   0.3038862347602844,\n",
       "   0.31710919737815857,\n",
       "   0.314737468957901,\n",
       "   0.3249017298221588,\n",
       "   0.3175041079521179,\n",
       "   0.3275412619113922,\n",
       "   0.34406423568725586,\n",
       "   0.35500553250312805,\n",
       "   0.3551228940486908,\n",
       "   0.3637945055961609,\n",
       "   0.368181437253952,\n",
       "   0.3910203278064728,\n",
       "   0.397124320268631,\n",
       "   0.3802126348018646,\n",
       "   0.3583393692970276,\n",
       "   0.3857652246952057,\n",
       "   0.38706812262535095,\n",
       "   0.3755486011505127,\n",
       "   0.38701629638671875,\n",
       "   0.37104400992393494,\n",
       "   0.370572566986084,\n",
       "   0.360353946685791,\n",
       "   0.35679173469543457,\n",
       "   0.36646899580955505,\n",
       "   0.3640329837799072,\n",
       "   0.34957289695739746,\n",
       "   0.34398820996284485,\n",
       "   0.34945032000541687,\n",
       "   0.3501463234424591,\n",
       "   0.35513681173324585,\n",
       "   0.33908483386039734,\n",
       "   0.3477315306663513,\n",
       "   0.3463851809501648,\n",
       "   0.339162141084671,\n",
       "   0.3310152590274811,\n",
       "   0.32915493845939636,\n",
       "   0.34210294485092163,\n",
       "   0.3315621018409729,\n",
       "   0.34481531381607056,\n",
       "   0.3502768576145172,\n",
       "   0.34687599539756775,\n",
       "   0.35177356004714966,\n",
       "   0.34750863909721375,\n",
       "   0.3555998206138611,\n",
       "   0.3552019000053406,\n",
       "   0.3540515899658203,\n",
       "   0.333721399307251,\n",
       "   0.3219203054904938,\n",
       "   0.31029120087623596,\n",
       "   0.30858317017555237,\n",
       "   0.31256476044654846,\n",
       "   0.31027451157569885,\n",
       "   0.31230422854423523,\n",
       "   0.3222556412220001,\n",
       "   0.3137684762477875,\n",
       "   0.31133705377578735,\n",
       "   0.31228235363960266,\n",
       "   0.3060685992240906,\n",
       "   0.31638479232788086,\n",
       "   0.3189292550086975,\n",
       "   0.31020644307136536,\n",
       "   0.326535701751709,\n",
       "   0.3244447708129883,\n",
       "   0.339987576007843,\n",
       "   0.3259008824825287,\n",
       "   0.33971819281578064,\n",
       "   0.3288108706474304,\n",
       "   0.32992634177207947,\n",
       "   0.32540634274482727,\n",
       "   0.3317382335662842,\n",
       "   0.31395936012268066,\n",
       "   0.31049758195877075,\n",
       "   0.31446924805641174,\n",
       "   0.3094145357608795,\n",
       "   0.2991911768913269,\n",
       "   0.3069024682044983,\n",
       "   0.3044227063655853,\n",
       "   0.30336540937423706,\n",
       "   0.2908158600330353,\n",
       "   0.2943689823150635,\n",
       "   0.2933023273944855,\n",
       "   0.2953912317752838,\n",
       "   0.2974516749382019,\n",
       "   0.30022019147872925,\n",
       "   0.30458709597587585,\n",
       "   0.3011837601661682,\n",
       "   0.3010399043560028,\n",
       "   0.2959127128124237,\n",
       "   0.3144914507865906,\n",
       "   0.311614990234375,\n",
       "   0.3089929223060608,\n",
       "   0.320972740650177,\n",
       "   0.3099852204322815,\n",
       "   0.3074696660041809,\n",
       "   0.3090723156929016,\n",
       "   0.3069482445716858,\n",
       "   0.3098180890083313,\n",
       "   0.31550437211990356,\n",
       "   0.3149637281894684,\n",
       "   0.3163376450538635,\n",
       "   0.3107181191444397,\n",
       "   0.31864404678344727,\n",
       "   0.3111163079738617,\n",
       "   0.3045903742313385,\n",
       "   0.30201801657676697,\n",
       "   0.29049190878868103,\n",
       "   0.29213133454322815,\n",
       "   0.29270079731941223,\n",
       "   0.29177162051200867,\n",
       "   0.2904883325099945,\n",
       "   0.29117974638938904,\n",
       "   0.28804606199264526,\n",
       "   0.2837173640727997,\n",
       "   0.2889736294746399,\n",
       "   0.29648879170417786,\n",
       "   0.27923089265823364,\n",
       "   0.27773573994636536,\n",
       "   0.27903980016708374,\n",
       "   0.27952316403388977,\n",
       "   0.27929896116256714,\n",
       "   0.29242685437202454,\n",
       "   0.3070848882198334,\n",
       "   0.2973225712776184,\n",
       "   0.29427462816238403,\n",
       "   0.2939126491546631,\n",
       "   0.29389700293540955,\n",
       "   0.2977294921875,\n",
       "   0.2898678481578827,\n",
       "   0.2916305363178253,\n",
       "   0.28560373187065125,\n",
       "   0.2880966067314148,\n",
       "   0.3012924790382385,\n",
       "   0.30701568722724915,\n",
       "   0.31130367517471313,\n",
       "   0.3130455017089844,\n",
       "   0.31207969784736633,\n",
       "   0.30156800150871277,\n",
       "   0.2936561405658722,\n",
       "   0.2874908149242401,\n",
       "   0.28977590799331665,\n",
       "   0.290351927280426,\n",
       "   0.296418696641922,\n",
       "   0.2946506440639496,\n",
       "   0.2924215495586395,\n",
       "   0.29585808515548706,\n",
       "   0.3125670850276947,\n",
       "   0.30666279792785645,\n",
       "   0.3013172745704651,\n",
       "   0.30141592025756836,\n",
       "   0.29771965742111206,\n",
       "   0.30553174018859863,\n",
       "   0.29088956117630005,\n",
       "   0.2850803732872009,\n",
       "   0.28423112630844116,\n",
       "   0.2779121994972229,\n",
       "   0.2782256603240967,\n",
       "   0.2753817141056061,\n",
       "   0.2810741364955902,\n",
       "   0.2872907221317291,\n",
       "   0.27754878997802734,\n",
       "   0.28236791491508484,\n",
       "   0.2791665494441986,\n",
       "   0.2878098785877228,\n",
       "   0.2849894165992737,\n",
       "   0.29592928290367126,\n",
       "   0.3038347363471985,\n",
       "   0.306890070438385,\n",
       "   0.3089643120765686,\n",
       "   0.3184642791748047,\n",
       "   0.31570950150489807,\n",
       "   0.3129997253417969,\n",
       "   0.3007040023803711,\n",
       "   0.30755385756492615,\n",
       "   0.31810733675956726,\n",
       "   0.3214567303657532,\n",
       "   0.3383210599422455,\n",
       "   0.33697500824928284,\n",
       "   0.33751723170280457],\n",
       "  'jsc_loss': [0.7712263464927673,\n",
       "   0.7712263464927673,\n",
       "   0.7712263464927673,\n",
       "   0.7712263464927673,\n",
       "   0.7712263464927673,\n",
       "   0.7712263464927673,\n",
       "   0.7712263464927673,\n",
       "   0.7373124957084656,\n",
       "   0.7103498578071594,\n",
       "   0.6892164349555969,\n",
       "   0.6680508852005005,\n",
       "   0.6651181578636169,\n",
       "   0.6604356169700623,\n",
       "   0.656772255897522,\n",
       "   0.6539852023124695,\n",
       "   0.6325825452804565,\n",
       "   0.6148619055747986,\n",
       "   0.6015691757202148,\n",
       "   0.5973424315452576,\n",
       "   0.5897487998008728,\n",
       "   0.5809611082077026,\n",
       "   0.5728514790534973,\n",
       "   0.5696913003921509,\n",
       "   0.5593416094779968,\n",
       "   0.5537589192390442,\n",
       "   0.5554227232933044,\n",
       "   0.5549919009208679,\n",
       "   0.5551810264587402,\n",
       "   0.5535399913787842,\n",
       "   0.5558205842971802,\n",
       "   0.5503512024879456,\n",
       "   0.5475321412086487,\n",
       "   0.5463632345199585,\n",
       "   0.5445756316184998,\n",
       "   0.5393005609512329,\n",
       "   0.539730966091156,\n",
       "   0.5372405648231506,\n",
       "   0.540771484375,\n",
       "   0.5395253896713257,\n",
       "   0.535632312297821,\n",
       "   0.5372865200042725,\n",
       "   0.5322132706642151,\n",
       "   0.5321516990661621,\n",
       "   0.5332030653953552,\n",
       "   0.5270840525627136,\n",
       "   0.5256609320640564,\n",
       "   0.5169855356216431,\n",
       "   0.5110670924186707,\n",
       "   0.5109931230545044,\n",
       "   0.5174488425254822,\n",
       "   0.5183802843093872,\n",
       "   0.5218818783760071,\n",
       "   0.5198128819465637,\n",
       "   0.5261937379837036,\n",
       "   0.5195013880729675,\n",
       "   0.5207300782203674,\n",
       "   0.5166662335395813,\n",
       "   0.5166003108024597,\n",
       "   0.5193998217582703,\n",
       "   0.5103519558906555,\n",
       "   0.5063099265098572,\n",
       "   0.5118518471717834,\n",
       "   0.5054870843887329,\n",
       "   0.5153053402900696,\n",
       "   0.5097970366477966,\n",
       "   0.5092413425445557,\n",
       "   0.510322093963623,\n",
       "   0.5125836730003357,\n",
       "   0.5123001337051392,\n",
       "   0.5101457238197327,\n",
       "   0.511739194393158,\n",
       "   0.5160054564476013,\n",
       "   0.5237711071968079,\n",
       "   0.5105046629905701,\n",
       "   0.5035328269004822,\n",
       "   0.5060098171234131,\n",
       "   0.5029600262641907,\n",
       "   0.5007630586624146,\n",
       "   0.4973681569099426,\n",
       "   0.49774378538131714,\n",
       "   0.49768775701522827,\n",
       "   0.5060917735099792,\n",
       "   0.51063072681427,\n",
       "   0.5147280097007751,\n",
       "   0.5134854316711426,\n",
       "   0.5166288614273071,\n",
       "   0.5138498544692993,\n",
       "   0.5120471715927124,\n",
       "   0.5108593106269836,\n",
       "   0.5193074345588684,\n",
       "   0.5143137574195862,\n",
       "   0.5100879073143005,\n",
       "   0.5114001631736755,\n",
       "   0.5121989846229553,\n",
       "   0.5106769800186157,\n",
       "   0.5151149034500122,\n",
       "   0.5188895463943481,\n",
       "   0.5207728743553162,\n",
       "   0.52242112159729,\n",
       "   0.5218076705932617,\n",
       "   0.516133725643158,\n",
       "   0.5065217018127441,\n",
       "   0.513816773891449,\n",
       "   0.5223948359489441,\n",
       "   0.5325519442558289,\n",
       "   0.5313248038291931,\n",
       "   0.5324153900146484,\n",
       "   0.5239878296852112,\n",
       "   0.5213123559951782,\n",
       "   0.5254780054092407,\n",
       "   0.5300856828689575,\n",
       "   0.5241301655769348,\n",
       "   0.5198612213134766,\n",
       "   0.5148723721504211,\n",
       "   0.5023738741874695,\n",
       "   0.5014042854309082,\n",
       "   0.49782851338386536,\n",
       "   0.5055770874023438,\n",
       "   0.5045629143714905,\n",
       "   0.5109761953353882,\n",
       "   0.5067344307899475,\n",
       "   0.5078031420707703,\n",
       "   0.5122028589248657,\n",
       "   0.5172353982925415,\n",
       "   0.5132275223731995,\n",
       "   0.5144888758659363,\n",
       "   0.5175549387931824,\n",
       "   0.513844907283783,\n",
       "   0.5204296708106995,\n",
       "   0.5203317999839783,\n",
       "   0.5126942992210388,\n",
       "   0.4984448254108429,\n",
       "   0.4994506239891052,\n",
       "   0.505825400352478,\n",
       "   0.5114965438842773,\n",
       "   0.5173578262329102,\n",
       "   0.520698606967926,\n",
       "   0.5212584137916565,\n",
       "   0.5167199373245239,\n",
       "   0.50681072473526,\n",
       "   0.5137180089950562,\n",
       "   0.5108732581138611,\n",
       "   0.5225750207901001,\n",
       "   0.5252470374107361,\n",
       "   0.5362733006477356,\n",
       "   0.5271608233451843,\n",
       "   0.5239315629005432,\n",
       "   0.5244049429893494,\n",
       "   0.5313523411750793,\n",
       "   0.5271777510643005,\n",
       "   0.5262236595153809,\n",
       "   0.5262622833251953,\n",
       "   0.5230856537818909,\n",
       "   0.5258283019065857,\n",
       "   0.527007520198822,\n",
       "   0.5289052128791809,\n",
       "   0.5286518335342407,\n",
       "   0.5255851149559021,\n",
       "   0.5226622223854065,\n",
       "   0.5184842944145203,\n",
       "   0.5234573483467102,\n",
       "   0.5243309140205383,\n",
       "   0.5217883586883545,\n",
       "   0.5329183340072632,\n",
       "   0.5134055018424988,\n",
       "   0.5172083973884583,\n",
       "   0.5226967930793762,\n",
       "   0.5202645659446716,\n",
       "   0.5179295539855957,\n",
       "   0.5198214650154114,\n",
       "   0.514292299747467,\n",
       "   0.512999951839447,\n",
       "   0.515413224697113,\n",
       "   0.5193390250205994,\n",
       "   0.5159085988998413,\n",
       "   0.5269239544868469,\n",
       "   0.5319470167160034,\n",
       "   0.526671290397644,\n",
       "   0.5233920812606812,\n",
       "   0.5261853337287903,\n",
       "   0.5268768668174744,\n",
       "   0.524776816368103,\n",
       "   0.5256952047348022,\n",
       "   0.5307273268699646,\n",
       "   0.5448535084724426,\n",
       "   0.5390323996543884,\n",
       "   0.5284007787704468,\n",
       "   0.5220375061035156,\n",
       "   0.5235951542854309,\n",
       "   0.5262869596481323,\n",
       "   0.5379196405410767,\n",
       "   0.5403386354446411,\n",
       "   0.5452479720115662,\n",
       "   0.5487045645713806,\n",
       "   0.5401080846786499,\n",
       "   0.5378512740135193,\n",
       "   0.5315839052200317,\n",
       "   0.5276867747306824,\n",
       "   0.5231932997703552,\n",
       "   0.530331015586853,\n",
       "   0.5345252752304077,\n",
       "   0.5348052382469177,\n",
       "   0.5275558829307556,\n",
       "   0.5301470160484314,\n",
       "   0.5240763425827026,\n",
       "   0.5218496322631836,\n",
       "   0.5127390027046204,\n",
       "   0.516810417175293,\n",
       "   0.504554271697998,\n",
       "   0.506786048412323,\n",
       "   0.507217526435852,\n",
       "   0.5045424699783325,\n",
       "   0.5134305953979492,\n",
       "   0.516191840171814,\n",
       "   0.510447084903717,\n",
       "   0.5032040476799011,\n",
       "   0.5089111924171448,\n",
       "   0.5172300934791565,\n",
       "   0.5221542119979858,\n",
       "   0.5216938853263855,\n",
       "   0.5148578882217407,\n",
       "   0.5236630439758301,\n",
       "   0.5216966271400452,\n",
       "   0.5181969404220581,\n",
       "   0.5217379331588745,\n",
       "   0.5155988931655884,\n",
       "   0.520684540271759,\n",
       "   0.5103058218955994,\n",
       "   0.5204623341560364,\n",
       "   0.5225241780281067,\n",
       "   0.5285460948944092,\n",
       "   0.5316506028175354,\n",
       "   0.5365210771560669,\n",
       "   0.5332726240158081,\n",
       "   0.5397764444351196,\n",
       "   0.5390565991401672,\n",
       "   0.5377398133277893,\n",
       "   0.5378232598304749,\n",
       "   0.5378971099853516,\n",
       "   0.5320138335227966,\n",
       "   0.5288016200065613,\n",
       "   0.5248056650161743,\n",
       "   0.5184349417686462,\n",
       "   0.5174201130867004,\n",
       "   0.5170249938964844,\n",
       "   0.5214046835899353,\n",
       "   0.5247696042060852,\n",
       "   0.5236255526542664,\n",
       "   0.5295357704162598,\n",
       "   0.5255079865455627,\n",
       "   0.5277569890022278,\n",
       "   0.526712954044342,\n",
       "   0.5228685736656189,\n",
       "   0.5226021409034729,\n",
       "   0.5096882581710815,\n",
       "   0.508125364780426,\n",
       "   0.5038045644760132,\n",
       "   0.511117696762085,\n",
       "   0.5224023461341858,\n",
       "   0.5220766663551331,\n",
       "   0.527361273765564,\n",
       "   0.5246283411979675,\n",
       "   0.5245192050933838,\n",
       "   0.5384267568588257,\n",
       "   0.5397018194198608,\n",
       "   0.5309493541717529,\n",
       "   0.5275812745094299,\n",
       "   0.5280553698539734,\n",
       "   0.5327035188674927,\n",
       "   0.531887412071228,\n",
       "   0.5236091017723083,\n",
       "   0.5227482914924622,\n",
       "   0.5187815427780151,\n",
       "   0.516640841960907,\n",
       "   0.5085544586181641,\n",
       "   0.5010799169540405,\n",
       "   0.5078154802322388,\n",
       "   0.5063202977180481,\n",
       "   0.500803530216217,\n",
       "   0.49843457341194153,\n",
       "   0.4976441562175751,\n",
       "   0.49085113406181335,\n",
       "   0.4918975830078125,\n",
       "   0.49317947030067444,\n",
       "   0.4902186989784241,\n",
       "   0.4869069755077362,\n",
       "   0.490236759185791,\n",
       "   0.4910163879394531,\n",
       "   0.4934167265892029,\n",
       "   0.4964115023612976,\n",
       "   0.5032038688659668,\n",
       "   0.5031257271766663,\n",
       "   0.502083420753479,\n",
       "   0.4969487190246582,\n",
       "   0.5028992891311646,\n",
       "   0.5003016591072083,\n",
       "   0.4968484938144684,\n",
       "   0.5048145651817322,\n",
       "   0.5098543167114258,\n",
       "   0.5120834708213806,\n",
       "   0.5045519471168518,\n",
       "   0.5061855912208557,\n",
       "   0.5056400895118713,\n",
       "   0.5048884153366089,\n",
       "   0.502684473991394,\n",
       "   0.49886202812194824,\n",
       "   0.5089665055274963,\n",
       "   0.5035305023193359,\n",
       "   0.5001009702682495,\n",
       "   0.5120363831520081,\n",
       "   0.5177683234214783,\n",
       "   0.5145733952522278,\n",
       "   0.5042256116867065,\n",
       "   0.5027732253074646,\n",
       "   0.4998479187488556,\n",
       "   0.4953037202358246,\n",
       "   0.504481852054596,\n",
       "   0.5126621127128601,\n",
       "   0.5160287618637085,\n",
       "   0.503801167011261,\n",
       "   0.5035216212272644,\n",
       "   0.5083094835281372,\n",
       "   0.5136248469352722,\n",
       "   0.5126002430915833,\n",
       "   0.515216052532196,\n",
       "   0.5233325362205505,\n",
       "   0.5202988982200623,\n",
       "   0.516106903553009,\n",
       "   0.5166804194450378,\n",
       "   0.5120863318443298,\n",
       "   0.5208081007003784,\n",
       "   0.5192074179649353,\n",
       "   0.5236709713935852,\n",
       "   0.5241672396659851,\n",
       "   0.5205250978469849,\n",
       "   0.5126717686653137,\n",
       "   0.5246656537055969,\n",
       "   0.5261565446853638,\n",
       "   0.5294089913368225,\n",
       "   0.5382691621780396,\n",
       "   0.5377689599990845,\n",
       "   0.5394918322563171,\n",
       "   0.5431765913963318,\n",
       "   0.5517724752426147,\n",
       "   0.5520868897438049,\n",
       "   0.554939329624176,\n",
       "   0.5403501987457275,\n",
       "   0.5379177927970886,\n",
       "   0.5362001061439514,\n",
       "   0.5278951525688171,\n",
       "   0.528001606464386,\n",
       "   0.5334578156471252,\n",
       "   0.5319167971611023,\n",
       "   0.5340964198112488,\n",
       "   0.529468834400177,\n",
       "   0.535453200340271,\n",
       "   0.5288861393928528,\n",
       "   0.5361446738243103,\n",
       "   0.5464811325073242,\n",
       "   0.5416108965873718,\n",
       "   0.5376209616661072,\n",
       "   0.5236769914627075,\n",
       "   0.520392894744873,\n",
       "   0.5203235745429993,\n",
       "   0.5232737064361572,\n",
       "   0.5130707621574402,\n",
       "   0.5081000924110413,\n",
       "   0.5082190036773682,\n",
       "   0.5169912576675415,\n",
       "   0.5081732273101807,\n",
       "   0.5060617327690125,\n",
       "   0.5156782865524292,\n",
       "   0.5148056149482727,\n",
       "   0.5123754739761353,\n",
       "   0.5062654614448547,\n",
       "   0.5058265328407288,\n",
       "   0.510250985622406,\n",
       "   0.5115501284599304,\n",
       "   0.518166184425354,\n",
       "   0.500598132610321,\n",
       "   0.4969411790370941,\n",
       "   0.49175137281417847,\n",
       "   0.4911361336708069,\n",
       "   0.4921247661113739,\n",
       "   0.487407922744751,\n",
       "   0.4968006908893585,\n",
       "   0.4961698055267334,\n",
       "   0.49414461851119995,\n",
       "   0.5006855726242065,\n",
       "   0.4970555305480957,\n",
       "   0.49836307764053345,\n",
       "   0.48913559317588806,\n",
       "   0.4949501156806946,\n",
       "   0.4890633225440979,\n",
       "   0.49521932005882263,\n",
       "   0.49812647700309753,\n",
       "   0.49152857065200806,\n",
       "   0.49424082040786743,\n",
       "   0.49380430579185486,\n",
       "   0.4930124282836914,\n",
       "   0.49310582876205444,\n",
       "   0.5062050819396973,\n",
       "   0.5096849203109741,\n",
       "   0.5035887360572815,\n",
       "   0.5023160576820374,\n",
       "   0.5043284893035889,\n",
       "   0.4961197078227997,\n",
       "   0.4941175878047943,\n",
       "   0.49962857365608215,\n",
       "   0.4965081214904785,\n",
       "   0.5052692294120789,\n",
       "   0.5078299641609192,\n",
       "   0.5038079619407654,\n",
       "   0.4984341561794281,\n",
       "   0.5101842880249023,\n",
       "   0.5127468705177307,\n",
       "   0.5224757790565491,\n",
       "   0.5184780955314636,\n",
       "   0.5248311161994934,\n",
       "   0.5216614603996277,\n",
       "   0.5301510095596313,\n",
       "   0.5314689874649048,\n",
       "   0.5303948521614075,\n",
       "   0.5410762429237366,\n",
       "   0.5254436731338501,\n",
       "   0.5235769748687744,\n",
       "   0.5099721550941467,\n",
       "   0.5134134292602539,\n",
       "   0.503116250038147,\n",
       "   0.5020328164100647,\n",
       "   0.4990510046482086,\n",
       "   0.5018885135650635,\n",
       "   0.4991179406642914,\n",
       "   0.5042089223861694,\n",
       "   0.513302206993103,\n",
       "   0.5095289945602417,\n",
       "   0.5193146467208862,\n",
       "   0.5221600532531738,\n",
       "   0.5187221169471741,\n",
       "   0.5186986327171326,\n",
       "   0.5219154357910156,\n",
       "   0.5178573131561279,\n",
       "   0.5139021873474121,\n",
       "   0.5263882875442505,\n",
       "   0.5254857540130615,\n",
       "   0.5264175534248352,\n",
       "   0.524222195148468,\n",
       "   0.5254827737808228,\n",
       "   0.5199188590049744,\n",
       "   0.5283194780349731,\n",
       "   0.530491054058075,\n",
       "   0.5260823965072632,\n",
       "   0.531685471534729,\n",
       "   0.5188033580780029,\n",
       "   0.5217012166976929,\n",
       "   0.5238986611366272,\n",
       "   0.5269650220870972,\n",
       "   0.5230286717414856,\n",
       "   0.5263209342956543,\n",
       "   0.5240039825439453,\n",
       "   0.5275663733482361,\n",
       "   0.5156790018081665,\n",
       "   0.5123388171195984,\n",
       "   0.5131924748420715,\n",
       "   0.5047899484634399,\n",
       "   0.5054369568824768,\n",
       "   0.5057210326194763,\n",
       "   0.5118034482002258,\n",
       "   0.5160629153251648,\n",
       "   0.5108721256256104,\n",
       "   0.5078955292701721,\n",
       "   0.49851560592651367,\n",
       "   0.502121090888977,\n",
       "   0.49488160014152527,\n",
       "   0.4969238340854645,\n",
       "   0.5033998489379883,\n",
       "   0.49956032633781433,\n",
       "   0.504843533039093,\n",
       "   0.5128320455551147,\n",
       "   0.5076115727424622,\n",
       "   0.5168481469154358,\n",
       "   0.5212071537971497,\n",
       "   0.5181018710136414,\n",
       "   0.5253401398658752,\n",
       "   0.529140055179596,\n",
       "   0.5282784104347229,\n",
       "   0.5291708111763,\n",
       "   0.5248733758926392,\n",
       "   0.5157904028892517,\n",
       "   0.5205758810043335,\n",
       "   0.5084360241889954,\n",
       "   0.5071142911911011,\n",
       "   0.5125459432601929,\n",
       "   0.5054905414581299,\n",
       "   0.5159702897071838,\n",
       "   0.5163069367408752,\n",
       "   0.5181310772895813,\n",
       "   0.5070822238922119,\n",
       "   0.5123959183692932,\n",
       "   0.5112054347991943],\n",
       "  'ff_loss': [0.8359650373458862,\n",
       "   0.8359650373458862,\n",
       "   0.666193425655365,\n",
       "   0.5250270962715149,\n",
       "   0.3837142586708069,\n",
       "   0.33898404240608215,\n",
       "   0.2883140444755554,\n",
       "   0.21001850068569183,\n",
       "   0.19067084789276123,\n",
       "   0.15686218440532684,\n",
       "   0.12737888097763062,\n",
       "   0.12474320083856583,\n",
       "   0.10768799483776093,\n",
       "   0.09960845857858658,\n",
       "   0.08115317672491074,\n",
       "   0.06579490751028061,\n",
       "   0.06073582172393799,\n",
       "   0.04667225107550621,\n",
       "   0.04638573154807091,\n",
       "   0.039011694490909576,\n",
       "   0.04278271645307541,\n",
       "   0.04632262513041496,\n",
       "   0.04584329575300217,\n",
       "   0.03298806771636009,\n",
       "   0.0365380123257637,\n",
       "   0.03354579582810402,\n",
       "   0.03240863233804703,\n",
       "   0.030284004285931587,\n",
       "   0.03154507279396057,\n",
       "   0.03028571978211403,\n",
       "   0.02664969302713871,\n",
       "   0.026391874998807907,\n",
       "   0.028470857068896294,\n",
       "   0.027285587042570114,\n",
       "   0.024874404072761536,\n",
       "   0.02264654077589512,\n",
       "   0.020773548632860184,\n",
       "   0.01961522176861763,\n",
       "   0.022214053198695183,\n",
       "   0.023738887161016464,\n",
       "   0.023426270112395287,\n",
       "   0.021874427795410156,\n",
       "   0.02411740832030773,\n",
       "   0.026405932381749153,\n",
       "   0.024034282192587852,\n",
       "   0.024596082046628,\n",
       "   0.02302873693406582,\n",
       "   0.022446149960160255,\n",
       "   0.022841516882181168,\n",
       "   0.01942317746579647,\n",
       "   0.018242482095956802,\n",
       "   0.017457742244005203,\n",
       "   0.01770351268351078,\n",
       "   0.017975911498069763,\n",
       "   0.01666782610118389,\n",
       "   0.017468931153416634,\n",
       "   0.017182527109980583,\n",
       "   0.01725502498447895,\n",
       "   0.018955932930111885,\n",
       "   0.01933412440121174,\n",
       "   0.018895933404564857,\n",
       "   0.017699632793664932,\n",
       "   0.0175296813249588,\n",
       "   0.01862533763051033,\n",
       "   0.018110765144228935,\n",
       "   0.018997861072421074,\n",
       "   0.019786229357123375,\n",
       "   0.019914522767066956,\n",
       "   0.02207547426223755,\n",
       "   0.022581571713089943,\n",
       "   0.023577842861413956,\n",
       "   0.024509401991963387,\n",
       "   0.024021785706281662,\n",
       "   0.024682536721229553,\n",
       "   0.021604685112833977,\n",
       "   0.022209972143173218,\n",
       "   0.020865222439169884,\n",
       "   0.02026147022843361,\n",
       "   0.020009692758321762,\n",
       "   0.021257150918245316,\n",
       "   0.022188972681760788,\n",
       "   0.01989670656621456,\n",
       "   0.021820101886987686,\n",
       "   0.02319909818470478,\n",
       "   0.025719614699482918,\n",
       "   0.020884066820144653,\n",
       "   0.020008211955428123,\n",
       "   0.01887679286301136,\n",
       "   0.022566309198737144,\n",
       "   0.021779676899313927,\n",
       "   0.02190692164003849,\n",
       "   0.024641837924718857,\n",
       "   0.021663252264261246,\n",
       "   0.024906637147068977,\n",
       "   0.02352602407336235,\n",
       "   0.02363976463675499,\n",
       "   0.01838047429919243,\n",
       "   0.021762665361166,\n",
       "   0.020583713427186012,\n",
       "   0.02453463338315487,\n",
       "   0.024799248203635216,\n",
       "   0.02974376082420349,\n",
       "   0.026725443080067635,\n",
       "   0.02629665844142437,\n",
       "   0.02500462345778942,\n",
       "   0.029956432059407234,\n",
       "   0.02387434057891369,\n",
       "   0.023523729294538498,\n",
       "   0.023922504857182503,\n",
       "   0.023759430274367332,\n",
       "   0.02230493724346161,\n",
       "   0.021671144291758537,\n",
       "   0.022490788251161575,\n",
       "   0.02102404274046421,\n",
       "   0.027179403230547905,\n",
       "   0.02751619555056095,\n",
       "   0.030887356027960777,\n",
       "   0.027531424537301064,\n",
       "   0.02886369824409485,\n",
       "   0.024783315137028694,\n",
       "   0.024100910872220993,\n",
       "   0.024019623175263405,\n",
       "   0.022036539390683174,\n",
       "   0.020062172785401344,\n",
       "   0.02300974540412426,\n",
       "   0.022015562281012535,\n",
       "   0.021849174052476883,\n",
       "   0.018124273046851158,\n",
       "   0.01782253198325634,\n",
       "   0.018430979922413826,\n",
       "   0.017980294302105904,\n",
       "   0.017913559451699257,\n",
       "   0.020524591207504272,\n",
       "   0.01932096853852272,\n",
       "   0.023828784003853798,\n",
       "   0.01961902715265751,\n",
       "   0.01885438896715641,\n",
       "   0.017074156552553177,\n",
       "   0.017866311594843864,\n",
       "   0.01971624232828617,\n",
       "   0.020109567791223526,\n",
       "   0.021760838106274605,\n",
       "   0.0279486533254385,\n",
       "   0.030396560207009315,\n",
       "   0.028295312076807022,\n",
       "   0.026958152651786804,\n",
       "   0.03310740366578102,\n",
       "   0.031778767704963684,\n",
       "   0.029041944071650505,\n",
       "   0.027320338413119316,\n",
       "   0.027955660596489906,\n",
       "   0.026014264672994614,\n",
       "   0.02566659078001976,\n",
       "   0.026456506922841072,\n",
       "   0.02360450103878975,\n",
       "   0.01929580233991146,\n",
       "   0.02154582552611828,\n",
       "   0.01929055154323578,\n",
       "   0.024254480376839638,\n",
       "   0.030625537037849426,\n",
       "   0.026421334594488144,\n",
       "   0.02730460837483406,\n",
       "   0.027248388156294823,\n",
       "   0.034587521106004715,\n",
       "   0.029777903109788895,\n",
       "   0.026032516732811928,\n",
       "   0.023542361333966255,\n",
       "   0.027031827718019485,\n",
       "   0.025280175730586052,\n",
       "   0.02348099648952484,\n",
       "   0.028387801721692085,\n",
       "   0.025765830650925636,\n",
       "   0.03260685130953789,\n",
       "   0.031830210238695145,\n",
       "   0.03104010410606861,\n",
       "   0.031398527324199677,\n",
       "   0.02421964332461357,\n",
       "   0.024848997592926025,\n",
       "   0.023676972836256027,\n",
       "   0.022973408922553062,\n",
       "   0.021228963509202003,\n",
       "   0.01965772546827793,\n",
       "   0.02131620980799198,\n",
       "   0.020892536267638206,\n",
       "   0.022585423663258553,\n",
       "   0.021573422476649284,\n",
       "   0.020681124180555344,\n",
       "   0.01989869773387909,\n",
       "   0.018427202478051186,\n",
       "   0.018584946170449257,\n",
       "   0.01976955309510231,\n",
       "   0.017574384808540344,\n",
       "   0.01745198480784893,\n",
       "   0.01865014247596264,\n",
       "   0.0171536635607481,\n",
       "   0.017844241112470627,\n",
       "   0.019080594182014465,\n",
       "   0.018572956323623657,\n",
       "   0.017959972843527794,\n",
       "   0.018117709085345268,\n",
       "   0.01685035228729248,\n",
       "   0.017233313992619514,\n",
       "   0.01668604649603367,\n",
       "   0.017784079536795616,\n",
       "   0.020778793841600418,\n",
       "   0.019904520362615585,\n",
       "   0.019456420093774796,\n",
       "   0.017511971294879913,\n",
       "   0.01741182617843151,\n",
       "   0.017146186903119087,\n",
       "   0.017461806535720825,\n",
       "   0.017331909388303757,\n",
       "   0.017020925879478455,\n",
       "   0.017020374536514282,\n",
       "   0.017135562375187874,\n",
       "   0.01717166043817997,\n",
       "   0.0167409535497427,\n",
       "   0.01834939792752266,\n",
       "   0.018983447924256325,\n",
       "   0.021596817299723625,\n",
       "   0.02294107899069786,\n",
       "   0.02248704992234707,\n",
       "   0.021319594234228134,\n",
       "   0.020627517253160477,\n",
       "   0.019512813538312912,\n",
       "   0.024737799540162086,\n",
       "   0.022874338552355766,\n",
       "   0.02382611110806465,\n",
       "   0.022566918283700943,\n",
       "   0.023337913677096367,\n",
       "   0.02253573015332222,\n",
       "   0.022562427446246147,\n",
       "   0.023588363081216812,\n",
       "   0.024070316925644875,\n",
       "   0.02716687135398388,\n",
       "   0.02215486951172352,\n",
       "   0.019795935600996017,\n",
       "   0.020385054871439934,\n",
       "   0.020734012126922607,\n",
       "   0.021665271371603012,\n",
       "   0.023772990331053734,\n",
       "   0.024533551186323166,\n",
       "   0.022610561922192574,\n",
       "   0.02368890680372715,\n",
       "   0.024309789761900902,\n",
       "   0.02580411545932293,\n",
       "   0.025227604433894157,\n",
       "   0.030862560495734215,\n",
       "   0.0366191640496254,\n",
       "   0.03942079469561577,\n",
       "   0.03631152957677841,\n",
       "   0.034943729639053345,\n",
       "   0.031972888857126236,\n",
       "   0.02754512056708336,\n",
       "   0.03122536838054657,\n",
       "   0.028760787099599838,\n",
       "   0.02331528067588806,\n",
       "   0.024673789739608765,\n",
       "   0.022410886362195015,\n",
       "   0.02189764752984047,\n",
       "   0.022818582132458687,\n",
       "   0.023635614663362503,\n",
       "   0.021520406007766724,\n",
       "   0.017523791640996933,\n",
       "   0.017530521377921104,\n",
       "   0.016399282962083817,\n",
       "   0.02094688080251217,\n",
       "   0.01913805864751339,\n",
       "   0.02554147318005562,\n",
       "   0.026750756427645683,\n",
       "   0.027238879352808,\n",
       "   0.023558035492897034,\n",
       "   0.022668924182653427,\n",
       "   0.021113261580467224,\n",
       "   0.018854964524507523,\n",
       "   0.017823608592152596,\n",
       "   0.01782355271279812,\n",
       "   0.018452778458595276,\n",
       "   0.017948409542441368,\n",
       "   0.018236558884382248,\n",
       "   0.017550021409988403,\n",
       "   0.018420929089188576,\n",
       "   0.01864772103726864,\n",
       "   0.01818230375647545,\n",
       "   0.017834657803177834,\n",
       "   0.01726461574435234,\n",
       "   0.016365671530365944,\n",
       "   0.015894973650574684,\n",
       "   0.015706758946180344,\n",
       "   0.01534352358430624,\n",
       "   0.015341043472290039,\n",
       "   0.015226861461997032,\n",
       "   0.016251565888524055,\n",
       "   0.01709902472794056,\n",
       "   0.016708970069885254,\n",
       "   0.017604049295186996,\n",
       "   0.01612403616309166,\n",
       "   0.016228290274739265,\n",
       "   0.01722380891442299,\n",
       "   0.01674634777009487,\n",
       "   0.016894089058041573,\n",
       "   0.016487078741192818,\n",
       "   0.016991065815091133,\n",
       "   0.01760498620569706,\n",
       "   0.017400192096829414,\n",
       "   0.019703026860952377,\n",
       "   0.019667403772473335,\n",
       "   0.01989082619547844,\n",
       "   0.01927163265645504,\n",
       "   0.01892784610390663,\n",
       "   0.01748853549361229,\n",
       "   0.02004377171397209,\n",
       "   0.01994100585579872,\n",
       "   0.024619562551379204,\n",
       "   0.024757711216807365,\n",
       "   0.03534647449851036,\n",
       "   0.03776438906788826,\n",
       "   0.038412801921367645,\n",
       "   0.03279548138380051,\n",
       "   0.02823575958609581,\n",
       "   0.025617267936468124,\n",
       "   0.025678353384137154,\n",
       "   0.018471738323569298,\n",
       "   0.017037106677889824,\n",
       "   0.016493527218699455,\n",
       "   0.016651589423418045,\n",
       "   0.017035283148288727,\n",
       "   0.01648990996181965,\n",
       "   0.017284657806158066,\n",
       "   0.01827789656817913,\n",
       "   0.01609821431338787,\n",
       "   0.017009636387228966,\n",
       "   0.017609456554055214,\n",
       "   0.016651399433612823,\n",
       "   0.015482546761631966,\n",
       "   0.015380462631583214,\n",
       "   0.015417777001857758,\n",
       "   0.015339302830398083,\n",
       "   0.015841033309698105,\n",
       "   0.016467299312353134,\n",
       "   0.016447393223643303,\n",
       "   0.01649617962539196,\n",
       "   0.017788656055927277,\n",
       "   0.017123326659202576,\n",
       "   0.018154770135879517,\n",
       "   0.01791982911527157,\n",
       "   0.018743274733424187,\n",
       "   0.01888313516974449,\n",
       "   0.018110737204551697,\n",
       "   0.018609050661325455,\n",
       "   0.019064446911215782,\n",
       "   0.01960235834121704,\n",
       "   0.02037927135825157,\n",
       "   0.01993691734969616,\n",
       "   0.020282598212361336,\n",
       "   0.01980823278427124,\n",
       "   0.018074044957756996,\n",
       "   0.017121655866503716,\n",
       "   0.01675892435014248,\n",
       "   0.016711974516510963,\n",
       "   0.01759026013314724,\n",
       "   0.016640007495880127,\n",
       "   0.015629807487130165,\n",
       "   0.015731671825051308,\n",
       "   0.014801779761910439,\n",
       "   0.015092717483639717,\n",
       "   0.01489547174423933,\n",
       "   0.01573239639401436,\n",
       "   0.01602127030491829,\n",
       "   0.016851531341671944,\n",
       "   0.016536740586161613,\n",
       "   0.017242923378944397,\n",
       "   0.016710933297872543,\n",
       "   0.016809191554784775,\n",
       "   0.016146345064044,\n",
       "   0.016916759312152863,\n",
       "   0.01595817133784294,\n",
       "   0.015329835936427116,\n",
       "   0.014944476075470448,\n",
       "   0.014794950373470783,\n",
       "   0.014973022043704987,\n",
       "   0.016059422865509987,\n",
       "   0.016203096136450768,\n",
       "   0.015238973312079906,\n",
       "   0.01571718230843544,\n",
       "   0.017053013667464256,\n",
       "   0.017368631437420845,\n",
       "   0.016661986708641052,\n",
       "   0.01749391295015812,\n",
       "   0.017287811264395714,\n",
       "   0.016930310055613518,\n",
       "   0.016238462179899216,\n",
       "   0.015661532059311867,\n",
       "   0.015454160049557686,\n",
       "   0.01536022312939167,\n",
       "   0.016353856772184372,\n",
       "   0.01621941290795803,\n",
       "   0.015432517044246197,\n",
       "   0.015197333879768848,\n",
       "   0.014642886817455292,\n",
       "   0.014921250753104687,\n",
       "   0.015512382611632347,\n",
       "   0.015585439279675484,\n",
       "   0.015530958771705627,\n",
       "   0.016310423612594604,\n",
       "   0.01652815192937851,\n",
       "   0.015663396567106247,\n",
       "   0.01590074598789215,\n",
       "   0.016077756881713867,\n",
       "   0.015772130340337753,\n",
       "   0.015034499578177929,\n",
       "   0.014439686201512814,\n",
       "   0.013810831122100353,\n",
       "   0.015434535220265388,\n",
       "   0.01535630039870739,\n",
       "   0.01590825989842415,\n",
       "   0.015919914469122887,\n",
       "   0.015540109016001225,\n",
       "   0.014994241297245026,\n",
       "   0.013539179228246212,\n",
       "   0.014654025435447693,\n",
       "   0.014174223877489567,\n",
       "   0.013715332373976707,\n",
       "   0.013745561242103577,\n",
       "   0.015399379655718803,\n",
       "   0.014677805826067924,\n",
       "   0.014693266712129116,\n",
       "   0.01478680968284607,\n",
       "   0.014839483425021172,\n",
       "   0.01502961665391922,\n",
       "   0.016419410705566406,\n",
       "   0.018655743449926376,\n",
       "   0.018274102360010147,\n",
       "   0.01818467490375042,\n",
       "   0.01698850654065609,\n",
       "   0.01755744032561779,\n",
       "   0.018972618505358696,\n",
       "   0.01752587780356407,\n",
       "   0.017626773566007614,\n",
       "   0.016584446653723717,\n",
       "   0.015495863743126392,\n",
       "   0.014999537728726864,\n",
       "   0.015509212389588356,\n",
       "   0.01707705296576023,\n",
       "   0.016479436308145523,\n",
       "   0.016573019325733185,\n",
       "   0.015957875177264214,\n",
       "   0.016298936679959297,\n",
       "   0.017744239419698715,\n",
       "   0.018326962366700172,\n",
       "   0.017144637182354927,\n",
       "   0.01923641562461853,\n",
       "   0.01826772280037403,\n",
       "   0.02058902382850647,\n",
       "   0.02561773546040058,\n",
       "   0.029275942593812943,\n",
       "   0.028755197301506996,\n",
       "   0.021551331505179405,\n",
       "   0.01874123513698578,\n",
       "   0.016816597431898117,\n",
       "   0.01567491702735424,\n",
       "   0.01586754620075226,\n",
       "   0.015469766221940517,\n",
       "   0.015547729097306728,\n",
       "   0.016831135377287865,\n",
       "   0.016228949651122093,\n",
       "   0.016408074647188187,\n",
       "   0.016401231288909912,\n",
       "   0.017171813175082207,\n",
       "   0.018878815695643425,\n",
       "   0.0169235672801733,\n",
       "   0.01593700796365738,\n",
       "   0.016224967315793037,\n",
       "   0.015136471949517727,\n",
       "   0.014433733187615871,\n",
       "   0.014218821190297604,\n",
       "   0.014686798676848412,\n",
       "   0.014701000414788723,\n",
       "   0.01486875582486391,\n",
       "   0.016725368797779083,\n",
       "   0.016439925879240036,\n",
       "   0.01624133251607418,\n",
       "   0.016527462750673294,\n",
       "   0.016822177916765213,\n",
       "   0.016718411818146706,\n",
       "   0.01640079729259014,\n",
       "   0.017116159200668335,\n",
       "   0.017219817265868187,\n",
       "   0.01758219674229622,\n",
       "   0.018775135278701782,\n",
       "   0.017892824485898018,\n",
       "   0.018571380525827408,\n",
       "   0.01756138540804386,\n",
       "   0.017069675028324127,\n",
       "   0.018208283931016922,\n",
       "   0.017673587426543236,\n",
       "   0.017848370596766472,\n",
       "   0.018950102850794792,\n",
       "   0.02115505561232567,\n",
       "   0.02069733664393425],\n",
       "  'test_losses': [4.659166097640991,\n",
       "   3.937362551689148,\n",
       "   3.301075756549835,\n",
       "   2.817754864692688,\n",
       "   2.5098947882652283,\n",
       "   2.324975460767746,\n",
       "   2.2069600224494934,\n",
       "   2.0412362962961197,\n",
       "   1.9759157299995422,\n",
       "   1.9023787826299667,\n",
       "   1.8375924229621887,\n",
       "   1.8078342154622078,\n",
       "   1.7416291683912277,\n",
       "   1.693521924316883,\n",
       "   1.6607793793082237,\n",
       "   1.6162185594439507,\n",
       "   1.5987926721572876,\n",
       "   1.5435795150697231,\n",
       "   1.5077571831643581,\n",
       "   1.474116213619709,\n",
       "   1.441853515803814,\n",
       "   1.4299115352332592,\n",
       "   1.4347810968756676,\n",
       "   1.3986715041100979,\n",
       "   1.3933279290795326,\n",
       "   1.3870477117598057,\n",
       "   1.372456319630146,\n",
       "   1.3911593426018953,\n",
       "   1.3902975916862488,\n",
       "   1.3861379958689213,\n",
       "   1.4064313266426325,\n",
       "   1.396918248385191,\n",
       "   1.3744467329233885,\n",
       "   1.3732763044536114,\n",
       "   1.396938517689705,\n",
       "   1.3954807166010141,\n",
       "   1.406182873994112,\n",
       "   1.4261565990746021,\n",
       "   1.4032064657658339,\n",
       "   1.4089120589196682,\n",
       "   1.4109251257032156,\n",
       "   1.4071799516677856,\n",
       "   1.4202347677201033,\n",
       "   1.4141677636653185,\n",
       "   1.4132651295512915,\n",
       "   1.4203494060784578,\n",
       "   1.4182062204927206,\n",
       "   1.4077264610677958,\n",
       "   1.3874647952616215,\n",
       "   1.3734525348991156,\n",
       "   1.3998985327780247,\n",
       "   1.4063229747116566,\n",
       "   1.3939580116420984,\n",
       "   1.4042579382658005,\n",
       "   1.4041341971606016,\n",
       "   1.402193846181035,\n",
       "   1.4152645450085402,\n",
       "   1.4084091652184725,\n",
       "   1.4179841447621584,\n",
       "   1.4018551278859377,\n",
       "   1.407324830070138,\n",
       "   1.4008110202848911,\n",
       "   1.399720922112465,\n",
       "   1.3641888685524464,\n",
       "   1.3637370597571135,\n",
       "   1.3650834988802671,\n",
       "   1.3534696605056524,\n",
       "   1.3605318814516068,\n",
       "   1.3402140140533447,\n",
       "   1.3633841816335917,\n",
       "   1.3558886982500553,\n",
       "   1.3625716287642717,\n",
       "   1.3651572279632092,\n",
       "   1.3502754122018814,\n",
       "   1.3263829033821821,\n",
       "   1.3335609436035156,\n",
       "   1.321978947147727,\n",
       "   1.2685718275606632,\n",
       "   1.27345996722579,\n",
       "   1.2983111925423145,\n",
       "   1.3051417879760265,\n",
       "   1.2924361433833838,\n",
       "   1.287548515945673,\n",
       "   1.2855657208710909,\n",
       "   1.2456323821097612,\n",
       "   1.235308200120926,\n",
       "   1.2281227763742208,\n",
       "   1.2199888247996569,\n",
       "   1.1892828848212957,\n",
       "   1.1916163470596075,\n",
       "   1.1580204460769892,\n",
       "   1.1646767817437649,\n",
       "   1.168588001281023,\n",
       "   1.1615885961800814,\n",
       "   1.18649847432971,\n",
       "   1.1768868528306484,\n",
       "   1.1818695925176144,\n",
       "   1.1865913830697536,\n",
       "   1.1927557829767466,\n",
       "   1.1787730362266302,\n",
       "   1.1747484114021063,\n",
       "   1.159393236041069,\n",
       "   1.1662318129092455,\n",
       "   1.1798932831734419,\n",
       "   1.1988893281668425,\n",
       "   1.1875008810311556,\n",
       "   1.1912942808121443,\n",
       "   1.1819436810910702,\n",
       "   1.1959273237735033,\n",
       "   1.2131057139486074,\n",
       "   1.2306164354085922,\n",
       "   1.2253354471176863,\n",
       "   1.2355835922062397,\n",
       "   1.2216914240270853,\n",
       "   1.2178090792149305,\n",
       "   1.2291344348341227,\n",
       "   1.2363973315805197,\n",
       "   1.2353794369846582,\n",
       "   1.2211328148841858,\n",
       "   1.2212571073323488,\n",
       "   1.220390435308218,\n",
       "   1.206552604213357,\n",
       "   1.188860820606351,\n",
       "   1.2011343333870173,\n",
       "   1.195271922275424,\n",
       "   1.195885231718421,\n",
       "   1.2207083143293858,\n",
       "   1.2159789111465216,\n",
       "   1.2097433041781187,\n",
       "   1.2111600060015917,\n",
       "   1.199552284553647,\n",
       "   1.180196726694703,\n",
       "   1.1897563934326172,\n",
       "   1.181961763650179,\n",
       "   1.2297130916267633,\n",
       "   1.2231924030929804,\n",
       "   1.2085248176008463,\n",
       "   1.2425196655094624,\n",
       "   1.1950477939099073,\n",
       "   1.145853603258729,\n",
       "   1.1855266727507114,\n",
       "   1.1675692666321993,\n",
       "   1.1624651905149221,\n",
       "   1.190431946888566,\n",
       "   1.2188500203192234,\n",
       "   1.2249814122915268,\n",
       "   1.2411288358271122,\n",
       "   1.2660477310419083,\n",
       "   1.2770038228482008,\n",
       "   1.2479553055018187,\n",
       "   1.2458178494125605,\n",
       "   1.2560936473309994,\n",
       "   1.242681559175253,\n",
       "   1.2527821976691484,\n",
       "   1.2127511538565159,\n",
       "   1.2086194213479757,\n",
       "   1.1905459146946669,\n",
       "   1.2271890491247177,\n",
       "   1.2557704839855433,\n",
       "   1.2293254882097244,\n",
       "   1.1969714276492596,\n",
       "   1.192679587751627,\n",
       "   1.1915340032428503,\n",
       "   1.1992864049971104,\n",
       "   1.183149579912424,\n",
       "   1.1652915123850107,\n",
       "   1.1857110876590014,\n",
       "   1.1729065962135792,\n",
       "   1.1753213834017515,\n",
       "   1.1803783625364304,\n",
       "   1.2031969893723726,\n",
       "   1.2063092980533838,\n",
       "   1.259440790861845,\n",
       "   1.2919040210545063,\n",
       "   1.2490026373416185,\n",
       "   1.2663345262408257,\n",
       "   1.2701307646930218,\n",
       "   1.281965047121048,\n",
       "   1.2795110158622265,\n",
       "   1.292432090267539,\n",
       "   1.2861189488321543,\n",
       "   1.3005995992571115,\n",
       "   1.2886821124702692,\n",
       "   1.2383953984826803,\n",
       "   1.2730846423655748,\n",
       "   1.2658895533531904,\n",
       "   1.2395605854690075,\n",
       "   1.2607542723417282,\n",
       "   1.251952299848199,\n",
       "   1.2511842641979456,\n",
       "   1.3103664256632328,\n",
       "   1.3129840046167374,\n",
       "   1.3431198131293058,\n",
       "   1.3533178251236677,\n",
       "   1.3579432796686888,\n",
       "   1.3438675217330456,\n",
       "   1.3526412397623062,\n",
       "   1.3260985016822815,\n",
       "   1.3444182183593512,\n",
       "   1.3753734659403563,\n",
       "   1.3202072381973267,\n",
       "   1.3430453855544329,\n",
       "   1.3411749247461557,\n",
       "   1.273779472336173,\n",
       "   1.267564881592989,\n",
       "   1.2704447768628597,\n",
       "   1.2569005228579044,\n",
       "   1.270436979830265,\n",
       "   1.2494939249008894,\n",
       "   1.294704930856824,\n",
       "   1.311922699213028,\n",
       "   1.3053875379264355,\n",
       "   1.3262649029493332,\n",
       "   1.3010832369327545,\n",
       "   1.2869449276477098,\n",
       "   1.2785328719764948,\n",
       "   1.257832681760192,\n",
       "   1.260121125727892,\n",
       "   1.2490172628313303,\n",
       "   1.2351901810616255,\n",
       "   1.2405559681355953,\n",
       "   1.2302472051233053,\n",
       "   1.2353556118905544,\n",
       "   1.2315424270927906,\n",
       "   1.221376996487379,\n",
       "   1.2089320663362741,\n",
       "   1.2145618926733732,\n",
       "   1.2104244716465473,\n",
       "   1.2101289071142673,\n",
       "   1.1872443091124296,\n",
       "   1.2013806588947773,\n",
       "   1.224969370290637,\n",
       "   1.2624262310564518,\n",
       "   1.2455171290785074,\n",
       "   1.2883417587727308,\n",
       "   1.2985408920794725,\n",
       "   1.2892248146235943,\n",
       "   1.2900511901825666,\n",
       "   1.3067682087421417,\n",
       "   1.3048785589635372,\n",
       "   1.2984196301549673,\n",
       "   1.3110837750136852,\n",
       "   1.3040798176079988,\n",
       "   1.2745056096464396,\n",
       "   1.277886277064681,\n",
       "   1.26445166580379,\n",
       "   1.306545166298747,\n",
       "   1.3149738814681768,\n",
       "   1.3544342294335365,\n",
       "   1.3751750402152538,\n",
       "   1.3740464970469475,\n",
       "   1.3657000064849854,\n",
       "   1.375761453062296,\n",
       "   1.3973924182355404,\n",
       "   1.3956381529569626,\n",
       "   1.399207916110754,\n",
       "   1.3840998411178589,\n",
       "   1.372443825006485,\n",
       "   1.368228005245328,\n",
       "   1.3542166985571384,\n",
       "   1.3750843275338411,\n",
       "   1.3808174915611744,\n",
       "   1.346063882112503,\n",
       "   1.3292787335813046,\n",
       "   1.3094353284686804,\n",
       "   1.31039671972394,\n",
       "   1.3123341109603643,\n",
       "   1.293308636173606,\n",
       "   1.32675277069211,\n",
       "   1.3348013889044523,\n",
       "   1.3398982621729374,\n",
       "   1.355964794754982,\n",
       "   1.3348337970674038,\n",
       "   1.3516626507043839,\n",
       "   1.351116020232439,\n",
       "   1.3463351149111986,\n",
       "   1.3826248403638601,\n",
       "   1.3885134607553482,\n",
       "   1.3745959904044867,\n",
       "   1.4027991704642773,\n",
       "   1.3975870609283447,\n",
       "   1.3679458741098642,\n",
       "   1.351646175608039,\n",
       "   1.355069573968649,\n",
       "   1.3608692232519388,\n",
       "   1.3549250178039074,\n",
       "   1.324279511347413,\n",
       "   1.3126290682703257,\n",
       "   1.3118704073131084,\n",
       "   1.3359362846240401,\n",
       "   1.3214472532272339,\n",
       "   1.3150229956954718,\n",
       "   1.2996034044772387,\n",
       "   1.2661511618644,\n",
       "   1.2903580069541931,\n",
       "   1.3091951869428158,\n",
       "   1.2802468501031399,\n",
       "   1.3025217968970537,\n",
       "   1.3418909944593906,\n",
       "   1.31155208311975,\n",
       "   1.2854045499116182,\n",
       "   1.2928830850869417,\n",
       "   1.2939920891076326,\n",
       "   1.300380865111947,\n",
       "   1.2635469306260347,\n",
       "   1.2697688601911068,\n",
       "   1.277787046507001,\n",
       "   1.2801032476127148,\n",
       "   1.2920810785144567,\n",
       "   1.3235843516886234,\n",
       "   1.3321898840367794,\n",
       "   1.3385228924453259,\n",
       "   1.3240557946264744,\n",
       "   1.331226510927081,\n",
       "   1.3253290932625532,\n",
       "   1.3289091251790524,\n",
       "   1.3558240123093128,\n",
       "   1.3398175165057182,\n",
       "   1.3110336884856224,\n",
       "   1.2755948267877102,\n",
       "   1.2799394391477108,\n",
       "   1.2926626969128847,\n",
       "   1.2662780489772558,\n",
       "   1.2658141385763884,\n",
       "   1.2578589003533125,\n",
       "   1.305707324296236,\n",
       "   1.2773415818810463,\n",
       "   1.3106874991208315,\n",
       "   1.332120891660452,\n",
       "   1.3578830193728209,\n",
       "   1.368679178878665,\n",
       "   1.3637712206691504,\n",
       "   1.3694844041019678,\n",
       "   1.3789101913571358,\n",
       "   1.3693242687731981,\n",
       "   1.340375980362296,\n",
       "   1.3348812386393547,\n",
       "   1.3598877964541316,\n",
       "   1.3755708895623684,\n",
       "   1.3488830290734768,\n",
       "   1.3642982225865126,\n",
       "   1.3600448947399855,\n",
       "   1.3835752084851265,\n",
       "   1.3955140560865402,\n",
       "   1.3628023266792297,\n",
       "   1.3869121577590704,\n",
       "   1.3859285693615675,\n",
       "   1.3427356146275997,\n",
       "   1.3319061249494553,\n",
       "   1.3210802115499973,\n",
       "   1.3062819745391607,\n",
       "   1.3483968377113342,\n",
       "   1.3320801816880703,\n",
       "   1.3177294414490461,\n",
       "   1.3210375625640154,\n",
       "   1.3400277495384216,\n",
       "   1.3522219751030207,\n",
       "   1.3601036500185728,\n",
       "   1.3915094789117575,\n",
       "   1.3647516313940287,\n",
       "   1.3761504050344229,\n",
       "   1.3632508516311646,\n",
       "   1.3471514377743006,\n",
       "   1.3626023028045893,\n",
       "   1.3447623644024134,\n",
       "   1.3489004839211702,\n",
       "   1.3460351852700114,\n",
       "   1.3384665735065937,\n",
       "   1.3373627252876759,\n",
       "   1.3274072278290987,\n",
       "   1.2958855014294386,\n",
       "   1.3193193525075912,\n",
       "   1.306306567043066,\n",
       "   1.3069407902657986,\n",
       "   1.3095495831221342,\n",
       "   1.281810201704502,\n",
       "   1.2815739326179028,\n",
       "   1.278595356270671,\n",
       "   1.2658684374764562,\n",
       "   1.2662189053371549,\n",
       "   1.260917104780674,\n",
       "   1.257759565487504,\n",
       "   1.2514158096164465,\n",
       "   1.2683470575138927,\n",
       "   1.255714539438486,\n",
       "   1.257310664281249,\n",
       "   1.2442208100110292,\n",
       "   1.2692571133375168,\n",
       "   1.2594521138817072,\n",
       "   1.2539348807185888,\n",
       "   1.2633667271584272,\n",
       "   1.2411301247775555,\n",
       "   1.2530309800058603,\n",
       "   1.2480461169034243,\n",
       "   1.2230067420750856,\n",
       "   1.2618352584540844,\n",
       "   1.217533951625228,\n",
       "   1.2122643990442157,\n",
       "   1.2187871607020497,\n",
       "   1.2109156474471092,\n",
       "   1.1951651005074382,\n",
       "   1.2263050135225058,\n",
       "   1.2323354948312044,\n",
       "   1.2283984571695328,\n",
       "   1.2285238802433014,\n",
       "   1.2558574303984642,\n",
       "   1.2552538327872753,\n",
       "   1.257254496216774,\n",
       "   1.2558206915855408,\n",
       "   1.2638563178479671,\n",
       "   1.2939595011994243,\n",
       "   1.2658786615356803,\n",
       "   1.2630887208506465,\n",
       "   1.2542521972209215,\n",
       "   1.2643381636589766,\n",
       "   1.269854623824358,\n",
       "   1.2866869177669287,\n",
       "   1.3048213878646493,\n",
       "   1.293087936937809,\n",
       "   1.2850107094272971,\n",
       "   1.281098648905754,\n",
       "   1.2399229118600488,\n",
       "   1.2432716246694326,\n",
       "   1.255221202969551,\n",
       "   1.227537913247943,\n",
       "   1.2238518428057432,\n",
       "   1.2109115784987807,\n",
       "   1.2213627099990845,\n",
       "   1.2275813650339842,\n",
       "   1.2152189686894417,\n",
       "   1.2416780591011047,\n",
       "   1.2318742834031582,\n",
       "   1.2042604126036167,\n",
       "   1.198752773925662,\n",
       "   1.20472495816648,\n",
       "   1.2236891370266676,\n",
       "   1.237066401168704,\n",
       "   1.2126856707036495,\n",
       "   1.2049904875457287,\n",
       "   1.2244825270026922,\n",
       "   1.2268977435305715,\n",
       "   1.234554230235517,\n",
       "   1.2182856146246195,\n",
       "   1.2378652039915323,\n",
       "   1.2471880950033665,\n",
       "   1.2578475549817085,\n",
       "   1.2807988654822111,\n",
       "   1.2729836571961641,\n",
       "   1.3039363883435726,\n",
       "   1.2994096223264933,\n",
       "   1.2875185180455446,\n",
       "   1.2831552922725677,\n",
       "   1.2693090457469225,\n",
       "   1.242500752210617,\n",
       "   1.2700712848454714,\n",
       "   1.2897683568298817,\n",
       "   1.2834910545498133,\n",
       "   1.290943494066596,\n",
       "   1.302505150437355,\n",
       "   1.2734241895377636,\n",
       "   1.281744061037898,\n",
       "   1.27993606030941,\n",
       "   1.2479453450068831,\n",
       "   1.2562364703044295,\n",
       "   1.270731931552291,\n",
       "   1.306065296754241,\n",
       "   1.3256381638348103,\n",
       "   1.3470744788646698,\n",
       "   1.3319607805460691,\n",
       "   1.331399915739894,\n",
       "   1.3356820549815893,\n",
       "   1.32994095236063,\n",
       "   1.3354182709008455,\n",
       "   1.3075943207368255,\n",
       "   1.2978254174813628,\n",
       "   1.3019369384273887,\n",
       "   1.288775211200118,\n",
       "   1.285436532460153,\n",
       "   1.2958943443372846,\n",
       "   1.2956532314419746,\n",
       "   1.3039317838847637,\n",
       "   1.3094158377498388,\n",
       "   1.3022714741528034,\n",
       "   1.296018410474062,\n",
       "   1.2982174698263407,\n",
       "   1.288208169862628,\n",
       "   1.2729317843914032,\n",
       "   1.2865910287946463,\n",
       "   1.2515082247555256,\n",
       "   1.2372316718101501,\n",
       "   1.2555589247494936,\n",
       "   1.2584823109209538,\n",
       "   1.2472375724464655,\n",
       "   1.2178413942456245,\n",
       "   1.2706671543419361,\n",
       "   1.3026881869882345,\n",
       "   1.2792048435658216,\n",
       "   1.3171665947884321,\n",
       "   1.338356252759695,\n",
       "   1.3289447240531445],\n",
       "  'pce_acc': [142.23631286621094,\n",
       "   118.49625396728516,\n",
       "   97.64664459228516,\n",
       "   79.24691772460938,\n",
       "   71.4427490234375,\n",
       "   64.33985900878906,\n",
       "   59.807491302490234,\n",
       "   55.606407165527344,\n",
       "   55.381752014160156,\n",
       "   56.923919677734375,\n",
       "   59.020362854003906,\n",
       "   61.60464859008789,\n",
       "   64.10321044921875,\n",
       "   62.588375091552734,\n",
       "   63.46391296386719,\n",
       "   63.361148834228516,\n",
       "   63.66730880737305,\n",
       "   63.0289192199707,\n",
       "   62.480735778808594,\n",
       "   62.66706466674805,\n",
       "   61.37665939331055,\n",
       "   61.82545471191406,\n",
       "   63.18488311767578,\n",
       "   62.463043212890625,\n",
       "   63.36783218383789,\n",
       "   63.315887451171875,\n",
       "   62.757728576660156,\n",
       "   64.48040771484375,\n",
       "   64.77083587646484,\n",
       "   64.14875793457031,\n",
       "   67.2905044555664,\n",
       "   67.01660919189453,\n",
       "   64.73899841308594,\n",
       "   64.82561492919922,\n",
       "   67.82197570800781,\n",
       "   67.55298614501953,\n",
       "   68.66913604736328,\n",
       "   69.54029083251953,\n",
       "   67.8029556274414,\n",
       "   67.80835723876953,\n",
       "   68.20189666748047,\n",
       "   68.22633361816406,\n",
       "   68.1412582397461,\n",
       "   68.09982299804688,\n",
       "   68.70951843261719,\n",
       "   68.75028228759766,\n",
       "   69.04804229736328,\n",
       "   68.90676879882812,\n",
       "   68.05284881591797,\n",
       "   66.68338775634766,\n",
       "   68.55186462402344,\n",
       "   69.18427276611328,\n",
       "   69.00032806396484,\n",
       "   69.47541809082031,\n",
       "   71.04811096191406,\n",
       "   71.64116668701172,\n",
       "   72.30036926269531,\n",
       "   71.90029907226562,\n",
       "   73.07141876220703,\n",
       "   72.55455017089844,\n",
       "   73.2149429321289,\n",
       "   72.79618072509766,\n",
       "   73.90094757080078,\n",
       "   70.81856536865234,\n",
       "   70.81523895263672,\n",
       "   72.62743377685547,\n",
       "   72.52565002441406,\n",
       "   72.8370590209961,\n",
       "   70.41939544677734,\n",
       "   72.31471252441406,\n",
       "   72.65311431884766,\n",
       "   73.77208709716797,\n",
       "   74.74276733398438,\n",
       "   74.53364562988281,\n",
       "   73.36312103271484,\n",
       "   73.50888061523438,\n",
       "   73.49906158447266,\n",
       "   70.20405578613281,\n",
       "   70.42249298095703,\n",
       "   72.9127426147461,\n",
       "   72.2264404296875,\n",
       "   72.71820068359375,\n",
       "   71.55689239501953,\n",
       "   71.61478424072266,\n",
       "   68.87934875488281,\n",
       "   69.70179748535156,\n",
       "   69.7299575805664,\n",
       "   69.19829559326172,\n",
       "   67.0975570678711,\n",
       "   66.92656707763672,\n",
       "   64.50180053710938,\n",
       "   65.46653747558594,\n",
       "   66.32505798339844,\n",
       "   65.35292053222656,\n",
       "   67.20089721679688,\n",
       "   66.73684692382812,\n",
       "   68.22672271728516,\n",
       "   69.09998321533203,\n",
       "   69.98310852050781,\n",
       "   68.70287322998047,\n",
       "   68.84912109375,\n",
       "   67.63700866699219,\n",
       "   67.65141296386719,\n",
       "   68.4329605102539,\n",
       "   69.03077697753906,\n",
       "   67.15702056884766,\n",
       "   67.9142074584961,\n",
       "   67.60201263427734,\n",
       "   68.80794525146484,\n",
       "   68.90229797363281,\n",
       "   69.84799194335938,\n",
       "   70.229736328125,\n",
       "   69.96768188476562,\n",
       "   71.59019470214844,\n",
       "   72.36814880371094,\n",
       "   72.91499328613281,\n",
       "   72.65570831298828,\n",
       "   72.66195678710938,\n",
       "   71.8058090209961,\n",
       "   72.84890747070312,\n",
       "   71.71353149414062,\n",
       "   71.87089538574219,\n",
       "   70.8414535522461,\n",
       "   72.1482162475586,\n",
       "   71.0853500366211,\n",
       "   71.70496368408203,\n",
       "   74.38308715820312,\n",
       "   75.82474517822266,\n",
       "   75.60009002685547,\n",
       "   75.89095306396484,\n",
       "   75.42454528808594,\n",
       "   74.34986114501953,\n",
       "   74.27009582519531,\n",
       "   73.478271484375,\n",
       "   74.48632049560547,\n",
       "   74.3994140625,\n",
       "   72.2413101196289,\n",
       "   74.00151824951172,\n",
       "   72.43708038330078,\n",
       "   69.09795379638672,\n",
       "   71.33650207519531,\n",
       "   70.4367904663086,\n",
       "   68.52763366699219,\n",
       "   69.73295593261719,\n",
       "   70.52277374267578,\n",
       "   70.49444580078125,\n",
       "   71.9314956665039,\n",
       "   71.1173095703125,\n",
       "   71.99899291992188,\n",
       "   70.93626403808594,\n",
       "   68.63518524169922,\n",
       "   71.91011047363281,\n",
       "   70.51715087890625,\n",
       "   70.92597198486328,\n",
       "   68.38358306884766,\n",
       "   69.6238021850586,\n",
       "   68.08797454833984,\n",
       "   70.86431121826172,\n",
       "   70.2364501953125,\n",
       "   70.17914581298828,\n",
       "   68.78924560546875,\n",
       "   68.53748321533203,\n",
       "   66.78620147705078,\n",
       "   68.38036346435547,\n",
       "   69.55962371826172,\n",
       "   69.20506286621094,\n",
       "   70.71134185791016,\n",
       "   69.67174530029297,\n",
       "   68.21588897705078,\n",
       "   68.38480377197266,\n",
       "   70.97821044921875,\n",
       "   69.89750671386719,\n",
       "   71.58675384521484,\n",
       "   72.5928726196289,\n",
       "   70.07147979736328,\n",
       "   71.01899719238281,\n",
       "   70.47351837158203,\n",
       "   72.6661148071289,\n",
       "   70.664306640625,\n",
       "   72.75753784179688,\n",
       "   71.31626892089844,\n",
       "   73.47026824951172,\n",
       "   71.68309020996094,\n",
       "   67.66778564453125,\n",
       "   69.14154052734375,\n",
       "   67.8598403930664,\n",
       "   66.81632232666016,\n",
       "   67.7688217163086,\n",
       "   66.66816711425781,\n",
       "   68.07818603515625,\n",
       "   67.39299774169922,\n",
       "   68.43230438232422,\n",
       "   68.23248291015625,\n",
       "   67.78936767578125,\n",
       "   70.30907440185547,\n",
       "   70.10945892333984,\n",
       "   69.00078582763672,\n",
       "   69.35335540771484,\n",
       "   72.26795196533203,\n",
       "   72.25777435302734,\n",
       "   69.78990173339844,\n",
       "   71.8993148803711,\n",
       "   70.27783203125,\n",
       "   68.48030853271484,\n",
       "   70.12430572509766,\n",
       "   70.34520721435547,\n",
       "   70.10350036621094,\n",
       "   72.181884765625,\n",
       "   70.86727142333984,\n",
       "   73.4136962890625,\n",
       "   74.5180435180664,\n",
       "   74.02957153320312,\n",
       "   73.49073028564453,\n",
       "   73.18367004394531,\n",
       "   71.7907485961914,\n",
       "   70.28569793701172,\n",
       "   69.25968170166016,\n",
       "   69.274658203125,\n",
       "   68.9361572265625,\n",
       "   67.030029296875,\n",
       "   68.09231567382812,\n",
       "   68.35955047607422,\n",
       "   69.80279541015625,\n",
       "   69.65338897705078,\n",
       "   69.54444122314453,\n",
       "   69.98967742919922,\n",
       "   70.1283187866211,\n",
       "   71.4906997680664,\n",
       "   70.9868392944336,\n",
       "   67.8055191040039,\n",
       "   68.18682861328125,\n",
       "   70.08748626708984,\n",
       "   71.87468719482422,\n",
       "   70.62251281738281,\n",
       "   74.04638671875,\n",
       "   75.38310241699219,\n",
       "   74.74871063232422,\n",
       "   75.56924438476562,\n",
       "   75.38272857666016,\n",
       "   76.72665405273438,\n",
       "   75.98274230957031,\n",
       "   76.08030700683594,\n",
       "   76.20660400390625,\n",
       "   73.40585327148438,\n",
       "   73.84031677246094,\n",
       "   71.12771606445312,\n",
       "   73.35808563232422,\n",
       "   74.26661682128906,\n",
       "   73.58873748779297,\n",
       "   74.78553771972656,\n",
       "   75.68206787109375,\n",
       "   76.46442413330078,\n",
       "   77.31510162353516,\n",
       "   78.34686279296875,\n",
       "   77.7702407836914,\n",
       "   78.16438293457031,\n",
       "   76.91795349121094,\n",
       "   77.3211669921875,\n",
       "   76.620849609375,\n",
       "   77.33219909667969,\n",
       "   77.74261474609375,\n",
       "   77.21490478515625,\n",
       "   76.5802230834961,\n",
       "   75.14832305908203,\n",
       "   73.56886291503906,\n",
       "   73.20523071289062,\n",
       "   71.77871704101562,\n",
       "   71.08200073242188,\n",
       "   72.96024322509766,\n",
       "   72.90904998779297,\n",
       "   73.04524230957031,\n",
       "   74.98661804199219,\n",
       "   73.93151092529297,\n",
       "   74.48836517333984,\n",
       "   76.11223602294922,\n",
       "   75.39891052246094,\n",
       "   76.14188385009766,\n",
       "   76.29439544677734,\n",
       "   76.3636245727539,\n",
       "   76.04576873779297,\n",
       "   75.4735336303711,\n",
       "   74.7849349975586,\n",
       "   75.40646362304688,\n",
       "   73.96160888671875,\n",
       "   74.99915313720703,\n",
       "   74.55313873291016,\n",
       "   71.89324188232422,\n",
       "   71.66393280029297,\n",
       "   73.87187957763672,\n",
       "   75.97431182861328,\n",
       "   74.21430969238281,\n",
       "   75.4807357788086,\n",
       "   75.24848937988281,\n",
       "   72.24008178710938,\n",
       "   73.54283905029297,\n",
       "   75.50743103027344,\n",
       "   74.03193664550781,\n",
       "   74.3994140625,\n",
       "   77.02132415771484,\n",
       "   77.13861083984375,\n",
       "   76.14342498779297,\n",
       "   76.5740966796875,\n",
       "   74.22941589355469,\n",
       "   75.72726440429688,\n",
       "   73.62440490722656,\n",
       "   74.4605484008789,\n",
       "   74.31202697753906,\n",
       "   74.5123291015625,\n",
       "   76.4108657836914,\n",
       "   77.1986083984375,\n",
       "   77.34650421142578,\n",
       "   77.17469024658203,\n",
       "   76.80918884277344,\n",
       "   76.68243408203125,\n",
       "   77.00215911865234,\n",
       "   76.697509765625,\n",
       "   76.50041961669922,\n",
       "   74.95587158203125,\n",
       "   73.54804992675781,\n",
       "   72.71446990966797,\n",
       "   73.4130859375,\n",
       "   74.19707489013672,\n",
       "   72.1115493774414,\n",
       "   70.76679992675781,\n",
       "   70.26642608642578,\n",
       "   73.50375366210938,\n",
       "   71.48239135742188,\n",
       "   74.52578735351562,\n",
       "   74.73455810546875,\n",
       "   76.48857879638672,\n",
       "   77.13052368164062,\n",
       "   75.9066162109375,\n",
       "   75.39106750488281,\n",
       "   73.922119140625,\n",
       "   72.94942474365234,\n",
       "   72.37591552734375,\n",
       "   73.06367492675781,\n",
       "   72.61138916015625,\n",
       "   74.04878997802734,\n",
       "   71.53768920898438,\n",
       "   71.74873352050781,\n",
       "   72.74441528320312,\n",
       "   74.7545166015625,\n",
       "   76.02111053466797,\n",
       "   73.3223648071289,\n",
       "   74.42485046386719,\n",
       "   75.48541259765625,\n",
       "   72.83161926269531,\n",
       "   72.597900390625,\n",
       "   71.76102447509766,\n",
       "   70.21485900878906,\n",
       "   73.53239440917969,\n",
       "   73.40209197998047,\n",
       "   71.22167205810547,\n",
       "   72.2055435180664,\n",
       "   74.19900512695312,\n",
       "   77.00127410888672,\n",
       "   77.43327331542969,\n",
       "   77.81935119628906,\n",
       "   76.70885467529297,\n",
       "   76.66578674316406,\n",
       "   76.32608032226562,\n",
       "   75.8084487915039,\n",
       "   76.75603485107422,\n",
       "   74.96025848388672,\n",
       "   75.64610290527344,\n",
       "   75.90743255615234,\n",
       "   75.3298110961914,\n",
       "   76.32377624511719,\n",
       "   77.1524429321289,\n",
       "   75.81734466552734,\n",
       "   77.10224914550781,\n",
       "   75.35323333740234,\n",
       "   76.11801147460938,\n",
       "   76.7811508178711,\n",
       "   73.23778533935547,\n",
       "   73.75904083251953,\n",
       "   73.70311737060547,\n",
       "   71.58452606201172,\n",
       "   73.80936431884766,\n",
       "   72.70927429199219,\n",
       "   72.58068084716797,\n",
       "   72.8998031616211,\n",
       "   73.17542266845703,\n",
       "   72.27970123291016,\n",
       "   69.82781219482422,\n",
       "   69.98825073242188,\n",
       "   71.43455505371094,\n",
       "   70.65070343017578,\n",
       "   70.24332427978516,\n",
       "   72.23808288574219,\n",
       "   70.31498718261719,\n",
       "   73.01252746582031,\n",
       "   73.19849395751953,\n",
       "   69.46662902832031,\n",
       "   73.43592071533203,\n",
       "   70.57494354248047,\n",
       "   68.9048843383789,\n",
       "   69.81581115722656,\n",
       "   69.39374542236328,\n",
       "   68.98886108398438,\n",
       "   70.86640930175781,\n",
       "   71.29820251464844,\n",
       "   71.67005920410156,\n",
       "   71.43180847167969,\n",
       "   73.76485443115234,\n",
       "   74.22017669677734,\n",
       "   74.89863586425781,\n",
       "   74.29629516601562,\n",
       "   75.65999603271484,\n",
       "   76.03816986083984,\n",
       "   73.63159942626953,\n",
       "   74.04647064208984,\n",
       "   72.69037628173828,\n",
       "   73.43334197998047,\n",
       "   73.64555358886719,\n",
       "   74.42414093017578,\n",
       "   76.88617706298828,\n",
       "   74.87926483154297,\n",
       "   74.0615005493164,\n",
       "   73.03392028808594,\n",
       "   68.35579681396484,\n",
       "   69.59009552001953,\n",
       "   68.8084487915039,\n",
       "   68.0394515991211,\n",
       "   68.17417907714844,\n",
       "   68.66571044921875,\n",
       "   70.50818634033203,\n",
       "   71.92973327636719,\n",
       "   70.33502197265625,\n",
       "   73.3786849975586,\n",
       "   72.05937194824219,\n",
       "   69.91410827636719,\n",
       "   69.3597412109375,\n",
       "   69.36485290527344,\n",
       "   71.2131118774414,\n",
       "   70.76214599609375,\n",
       "   69.68314361572266,\n",
       "   69.38599395751953,\n",
       "   71.59500885009766,\n",
       "   71.12804412841797,\n",
       "   72.4979019165039,\n",
       "   69.64230346679688,\n",
       "   68.58855438232422,\n",
       "   70.81896209716797,\n",
       "   72.42506408691406,\n",
       "   74.79041290283203,\n",
       "   73.8857192993164,\n",
       "   76.84390258789062,\n",
       "   76.29728698730469,\n",
       "   74.7716064453125,\n",
       "   75.37054443359375,\n",
       "   73.13236236572266,\n",
       "   70.0115966796875,\n",
       "   71.41138458251953,\n",
       "   72.47599792480469,\n",
       "   71.29254150390625,\n",
       "   73.02999877929688,\n",
       "   75.20565032958984,\n",
       "   73.40947723388672,\n",
       "   74.48213958740234,\n",
       "   75.45502471923828,\n",
       "   72.58623504638672,\n",
       "   72.98554992675781,\n",
       "   75.47549438476562,\n",
       "   78.86046600341797,\n",
       "   80.29856872558594,\n",
       "   80.20340728759766,\n",
       "   78.8078384399414,\n",
       "   79.89364624023438,\n",
       "   80.37454223632812,\n",
       "   81.22623443603516,\n",
       "   80.75241088867188,\n",
       "   80.5973892211914,\n",
       "   79.98570251464844,\n",
       "   79.5076675415039,\n",
       "   79.02881622314453,\n",
       "   78.1530532836914,\n",
       "   78.7251205444336,\n",
       "   78.70091247558594,\n",
       "   78.03880310058594,\n",
       "   79.10093688964844,\n",
       "   78.15541076660156,\n",
       "   76.87264251708984,\n",
       "   75.930908203125,\n",
       "   75.41854095458984,\n",
       "   72.74810791015625,\n",
       "   73.96167755126953,\n",
       "   71.15686798095703,\n",
       "   68.72957611083984,\n",
       "   70.7703857421875,\n",
       "   71.64520263671875,\n",
       "   70.15863037109375,\n",
       "   69.23455810546875,\n",
       "   72.88438415527344,\n",
       "   75.21060180664062,\n",
       "   72.40348815917969,\n",
       "   75.64415740966797,\n",
       "   77.21357727050781,\n",
       "   76.55695343017578],\n",
       "  'voc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.699462890625,\n",
       "   96.29888916015625,\n",
       "   93.60427856445312,\n",
       "   91.16828918457031,\n",
       "   88.92549896240234,\n",
       "   86.15311431884766,\n",
       "   85.38127899169922,\n",
       "   83.85106658935547,\n",
       "   82.04332733154297,\n",
       "   80.90647888183594,\n",
       "   78.89128112792969,\n",
       "   77.4197006225586,\n",
       "   75.72199249267578,\n",
       "   74.24899291992188,\n",
       "   73.37158966064453,\n",
       "   71.49385070800781,\n",
       "   72.4966049194336,\n",
       "   70.73993682861328,\n",
       "   70.02322387695312,\n",
       "   68.18030548095703,\n",
       "   67.09827423095703,\n",
       "   66.28241729736328,\n",
       "   66.5128402709961,\n",
       "   66.81629180908203,\n",
       "   68.27743530273438,\n",
       "   68.07433319091797,\n",
       "   69.1285171508789,\n",
       "   68.82880401611328,\n",
       "   69.88142395019531,\n",
       "   69.9716567993164,\n",
       "   70.21631622314453,\n",
       "   71.4358901977539,\n",
       "   70.6243667602539,\n",
       "   70.63835906982422,\n",
       "   71.5492935180664,\n",
       "   72.21367645263672,\n",
       "   71.92680358886719,\n",
       "   71.12728881835938,\n",
       "   71.0631103515625,\n",
       "   71.5807876586914,\n",
       "   71.47520446777344,\n",
       "   70.55955505371094,\n",
       "   70.18925476074219,\n",
       "   69.44380187988281,\n",
       "   68.65550231933594,\n",
       "   69.32318878173828,\n",
       "   69.39830780029297,\n",
       "   68.81768035888672,\n",
       "   68.23636627197266,\n",
       "   68.16002655029297,\n",
       "   68.09722137451172,\n",
       "   67.41519927978516,\n",
       "   66.207275390625,\n",
       "   66.35089874267578,\n",
       "   65.4559326171875,\n",
       "   64.70496368408203,\n",
       "   64.8423080444336,\n",
       "   64.4891357421875,\n",
       "   65.00102233886719,\n",
       "   63.502017974853516,\n",
       "   63.234371185302734,\n",
       "   62.52918243408203,\n",
       "   62.79165267944336,\n",
       "   62.21300506591797,\n",
       "   62.51286315917969,\n",
       "   62.02410888671875,\n",
       "   59.88508987426758,\n",
       "   60.51185607910156,\n",
       "   60.082489013671875,\n",
       "   61.091739654541016,\n",
       "   59.12209701538086,\n",
       "   59.35744094848633,\n",
       "   58.06481170654297,\n",
       "   56.54288101196289,\n",
       "   55.250091552734375,\n",
       "   54.807342529296875,\n",
       "   54.28938293457031,\n",
       "   53.863136291503906,\n",
       "   54.36557388305664,\n",
       "   54.13658142089844,\n",
       "   53.60371398925781,\n",
       "   53.0899658203125,\n",
       "   52.94028091430664,\n",
       "   53.38176345825195,\n",
       "   52.9836311340332,\n",
       "   52.165306091308594,\n",
       "   51.49845504760742,\n",
       "   50.9566650390625,\n",
       "   50.49789047241211,\n",
       "   50.077430725097656,\n",
       "   49.939544677734375,\n",
       "   50.359230041503906,\n",
       "   50.33030700683594,\n",
       "   50.704444885253906,\n",
       "   50.76284408569336,\n",
       "   50.73261260986328,\n",
       "   50.743709564208984,\n",
       "   50.80301284790039,\n",
       "   50.87847137451172,\n",
       "   51.31624221801758,\n",
       "   50.88845443725586,\n",
       "   51.06568145751953,\n",
       "   49.28544235229492,\n",
       "   48.856258392333984,\n",
       "   49.07643508911133,\n",
       "   49.5595588684082,\n",
       "   48.83843231201172,\n",
       "   49.126705169677734,\n",
       "   48.1790885925293,\n",
       "   48.511192321777344,\n",
       "   48.143638610839844,\n",
       "   47.202659606933594,\n",
       "   46.56431579589844,\n",
       "   46.98134994506836,\n",
       "   46.26292419433594,\n",
       "   45.16782760620117,\n",
       "   44.704524993896484,\n",
       "   43.96770095825195,\n",
       "   43.6478385925293,\n",
       "   43.29368591308594,\n",
       "   43.264244079589844,\n",
       "   43.47004699707031,\n",
       "   42.93149185180664,\n",
       "   45.760135650634766,\n",
       "   44.78361511230469,\n",
       "   44.87644958496094,\n",
       "   46.40021514892578,\n",
       "   43.530818939208984,\n",
       "   42.506919860839844,\n",
       "   43.26384735107422,\n",
       "   42.34205627441406,\n",
       "   42.24845886230469,\n",
       "   42.69529724121094,\n",
       "   43.69059371948242,\n",
       "   44.78044891357422,\n",
       "   45.00588607788086,\n",
       "   47.631927490234375,\n",
       "   47.50874710083008,\n",
       "   45.9759521484375,\n",
       "   49.129859924316406,\n",
       "   46.58470916748047,\n",
       "   46.93602752685547,\n",
       "   47.34162521362305,\n",
       "   45.90456008911133,\n",
       "   44.91359329223633,\n",
       "   44.9842414855957,\n",
       "   46.416385650634766,\n",
       "   51.64533233642578,\n",
       "   46.835227966308594,\n",
       "   44.986488342285156,\n",
       "   44.512184143066406,\n",
       "   46.98943328857422,\n",
       "   43.414451599121094,\n",
       "   43.22350311279297,\n",
       "   41.64628219604492,\n",
       "   42.198978424072266,\n",
       "   42.76704788208008,\n",
       "   44.51945877075195,\n",
       "   44.780094146728516,\n",
       "   44.67603302001953,\n",
       "   45.79822540283203,\n",
       "   48.174076080322266,\n",
       "   49.55222702026367,\n",
       "   48.73990249633789,\n",
       "   48.657958984375,\n",
       "   50.18437957763672,\n",
       "   50.06877136230469,\n",
       "   53.089927673339844,\n",
       "   51.342193603515625,\n",
       "   52.51063537597656,\n",
       "   51.840675354003906,\n",
       "   52.02689743041992,\n",
       "   51.13674545288086,\n",
       "   51.803035736083984,\n",
       "   52.84382247924805,\n",
       "   52.795284271240234,\n",
       "   54.283905029296875,\n",
       "   54.29346466064453,\n",
       "   53.22209548950195,\n",
       "   57.436092376708984,\n",
       "   56.92958068847656,\n",
       "   59.475196838378906,\n",
       "   60.62955093383789,\n",
       "   59.314144134521484,\n",
       "   58.304405212402344,\n",
       "   60.8989372253418,\n",
       "   57.37665939331055,\n",
       "   56.852046966552734,\n",
       "   59.733028411865234,\n",
       "   56.68069839477539,\n",
       "   56.79338455200195,\n",
       "   57.88542175292969,\n",
       "   54.735355377197266,\n",
       "   53.8827018737793,\n",
       "   54.21195602416992,\n",
       "   54.0235595703125,\n",
       "   53.41553497314453,\n",
       "   53.92552185058594,\n",
       "   54.955345153808594,\n",
       "   55.30341339111328,\n",
       "   55.51215362548828,\n",
       "   56.65963363647461,\n",
       "   55.045326232910156,\n",
       "   55.76889419555664,\n",
       "   56.311458587646484,\n",
       "   55.52368927001953,\n",
       "   55.30583190917969,\n",
       "   54.40512466430664,\n",
       "   54.45555877685547,\n",
       "   54.826473236083984,\n",
       "   53.19871139526367,\n",
       "   53.26350784301758,\n",
       "   53.3438606262207,\n",
       "   52.21928787231445,\n",
       "   51.22081756591797,\n",
       "   51.63737487792969,\n",
       "   50.60034942626953,\n",
       "   50.5638427734375,\n",
       "   50.79453659057617,\n",
       "   51.3449592590332,\n",
       "   51.53433609008789,\n",
       "   52.6817741394043,\n",
       "   52.62759780883789,\n",
       "   52.50041961669922,\n",
       "   52.82109832763672,\n",
       "   52.996009826660156,\n",
       "   52.72527313232422,\n",
       "   54.177345275878906,\n",
       "   53.05447006225586,\n",
       "   53.43459701538086,\n",
       "   54.72614288330078,\n",
       "   54.611915588378906,\n",
       "   54.34296798706055,\n",
       "   54.27421188354492,\n",
       "   54.729347229003906,\n",
       "   56.28062057495117,\n",
       "   56.19827651977539,\n",
       "   58.71974182128906,\n",
       "   59.33341979980469,\n",
       "   58.6334228515625,\n",
       "   57.52889633178711,\n",
       "   58.21065139770508,\n",
       "   59.108680725097656,\n",
       "   59.94233322143555,\n",
       "   60.15992736816406,\n",
       "   60.575260162353516,\n",
       "   59.154693603515625,\n",
       "   58.79010009765625,\n",
       "   57.374210357666016,\n",
       "   58.10286331176758,\n",
       "   58.73904800415039,\n",
       "   57.08746337890625,\n",
       "   56.132022857666016,\n",
       "   55.60356521606445,\n",
       "   56.70956802368164,\n",
       "   57.82368850708008,\n",
       "   56.86643600463867,\n",
       "   57.097023010253906,\n",
       "   57.78228759765625,\n",
       "   58.402950286865234,\n",
       "   58.603878021240234,\n",
       "   58.13336944580078,\n",
       "   59.2646484375,\n",
       "   58.770751953125,\n",
       "   59.16877365112305,\n",
       "   60.523681640625,\n",
       "   61.09326934814453,\n",
       "   60.595428466796875,\n",
       "   62.61709976196289,\n",
       "   62.49932098388672,\n",
       "   61.49005126953125,\n",
       "   60.099857330322266,\n",
       "   61.179466247558594,\n",
       "   61.327789306640625,\n",
       "   61.34958267211914,\n",
       "   60.982852935791016,\n",
       "   60.526405334472656,\n",
       "   59.16566848754883,\n",
       "   59.19816970825195,\n",
       "   59.29295349121094,\n",
       "   58.02824020385742,\n",
       "   56.79925537109375,\n",
       "   56.840736389160156,\n",
       "   57.41094207763672,\n",
       "   57.478065490722656,\n",
       "   57.362823486328125,\n",
       "   57.50020217895508,\n",
       "   57.78953552246094,\n",
       "   57.894752502441406,\n",
       "   57.6119499206543,\n",
       "   57.619136810302734,\n",
       "   58.14667892456055,\n",
       "   58.17383575439453,\n",
       "   58.416324615478516,\n",
       "   58.054237365722656,\n",
       "   58.02739715576172,\n",
       "   57.831607818603516,\n",
       "   57.259273529052734,\n",
       "   57.89830017089844,\n",
       "   57.90098190307617,\n",
       "   58.19623947143555,\n",
       "   58.23869705200195,\n",
       "   57.94831085205078,\n",
       "   57.7664680480957,\n",
       "   58.17316818237305,\n",
       "   58.100807189941406,\n",
       "   58.04776382446289,\n",
       "   57.78216552734375,\n",
       "   58.188377380371094,\n",
       "   57.888065338134766,\n",
       "   58.05324935913086,\n",
       "   57.96025848388672,\n",
       "   58.27167510986328,\n",
       "   58.54045104980469,\n",
       "   59.203548431396484,\n",
       "   58.51531219482422,\n",
       "   59.16222381591797,\n",
       "   59.1453742980957,\n",
       "   59.43600082397461,\n",
       "   59.8024787902832,\n",
       "   60.130821228027344,\n",
       "   60.638587951660156,\n",
       "   61.667823791503906,\n",
       "   62.08892059326172,\n",
       "   60.98054122924805,\n",
       "   60.71221160888672,\n",
       "   61.19932556152344,\n",
       "   61.18901443481445,\n",
       "   60.713619232177734,\n",
       "   61.30918884277344,\n",
       "   60.31166458129883,\n",
       "   60.14894104003906,\n",
       "   59.67070388793945,\n",
       "   59.282936096191406,\n",
       "   59.868370056152344,\n",
       "   59.641849517822266,\n",
       "   58.54574966430664,\n",
       "   58.18519592285156,\n",
       "   58.70686721801758,\n",
       "   59.03663635253906,\n",
       "   59.66363525390625,\n",
       "   59.65303039550781,\n",
       "   59.65869903564453,\n",
       "   59.3271598815918,\n",
       "   59.15655517578125,\n",
       "   58.363468170166016,\n",
       "   58.41960906982422,\n",
       "   60.06375503540039,\n",
       "   59.45089340209961,\n",
       "   59.54579162597656,\n",
       "   59.335411071777344,\n",
       "   59.01409912109375,\n",
       "   59.10820770263672,\n",
       "   59.30789566040039,\n",
       "   59.23954391479492,\n",
       "   59.495704650878906,\n",
       "   59.491600036621094,\n",
       "   59.514530181884766,\n",
       "   58.95212173461914,\n",
       "   58.549644470214844,\n",
       "   58.47966003417969,\n",
       "   58.438270568847656,\n",
       "   58.44807052612305,\n",
       "   58.56260681152344,\n",
       "   58.95553207397461,\n",
       "   58.79570770263672,\n",
       "   58.42995071411133,\n",
       "   57.93803405761719,\n",
       "   57.95116424560547,\n",
       "   58.105224609375,\n",
       "   58.07482147216797,\n",
       "   57.96214294433594,\n",
       "   58.10371780395508,\n",
       "   58.32758712768555,\n",
       "   58.763092041015625,\n",
       "   58.070068359375,\n",
       "   58.03660583496094,\n",
       "   57.753639221191406,\n",
       "   57.73280715942383,\n",
       "   57.663326263427734,\n",
       "   57.71712112426758,\n",
       "   57.523250579833984,\n",
       "   57.547752380371094,\n",
       "   57.647884368896484,\n",
       "   57.380592346191406,\n",
       "   57.51228332519531,\n",
       "   57.51673126220703,\n",
       "   57.82207107543945,\n",
       "   57.694923400878906,\n",
       "   57.76618194580078,\n",
       "   58.58435821533203,\n",
       "   58.27273178100586,\n",
       "   58.05131149291992,\n",
       "   58.10292434692383,\n",
       "   58.40099334716797,\n",
       "   58.7181396484375,\n",
       "   58.57294845581055,\n",
       "   58.33330154418945,\n",
       "   58.1573371887207,\n",
       "   58.77610397338867,\n",
       "   58.355438232421875,\n",
       "   58.30091857910156,\n",
       "   58.29905700683594,\n",
       "   58.09064483642578,\n",
       "   57.9964714050293,\n",
       "   58.06008529663086,\n",
       "   58.075462341308594,\n",
       "   57.94071960449219,\n",
       "   58.94266891479492,\n",
       "   58.97510528564453,\n",
       "   58.69318771362305,\n",
       "   57.992637634277344,\n",
       "   57.924598693847656,\n",
       "   57.43097686767578,\n",
       "   57.37549591064453,\n",
       "   57.308685302734375,\n",
       "   56.848777770996094,\n",
       "   57.37250518798828,\n",
       "   57.10794448852539,\n",
       "   56.689613342285156,\n",
       "   56.634681701660156,\n",
       "   56.851810455322266,\n",
       "   57.242435455322266,\n",
       "   57.02250671386719,\n",
       "   57.21755599975586,\n",
       "   57.70510482788086,\n",
       "   57.00643539428711,\n",
       "   56.673370361328125,\n",
       "   57.253665924072266,\n",
       "   57.6080207824707,\n",
       "   57.31402587890625,\n",
       "   58.074378967285156,\n",
       "   58.18905258178711,\n",
       "   57.84735107421875,\n",
       "   58.34650421142578,\n",
       "   57.97441482543945,\n",
       "   57.897621154785156,\n",
       "   57.91862487792969,\n",
       "   57.44580078125,\n",
       "   57.663124084472656,\n",
       "   57.688270568847656,\n",
       "   57.95044708251953,\n",
       "   58.400474548339844,\n",
       "   58.353759765625,\n",
       "   58.68573760986328,\n",
       "   58.76498794555664,\n",
       "   59.23811721801758,\n",
       "   58.94228744506836,\n",
       "   58.7695426940918,\n",
       "   58.42680358886719,\n",
       "   58.5632209777832,\n",
       "   58.66455841064453,\n",
       "   58.70781707763672,\n",
       "   57.651580810546875,\n",
       "   57.836307525634766,\n",
       "   57.828765869140625,\n",
       "   57.98540115356445,\n",
       "   57.87569808959961,\n",
       "   57.646751403808594,\n",
       "   58.47895812988281,\n",
       "   58.43861770629883,\n",
       "   59.11769104003906,\n",
       "   58.51520919799805,\n",
       "   58.234012603759766,\n",
       "   58.02823257446289,\n",
       "   57.897239685058594,\n",
       "   57.79804992675781,\n",
       "   57.67323303222656,\n",
       "   58.20597839355469,\n",
       "   58.23584747314453,\n",
       "   58.16560363769531,\n",
       "   58.37127685546875,\n",
       "   58.12033462524414,\n",
       "   58.249969482421875,\n",
       "   57.99296569824219,\n",
       "   58.4277458190918,\n",
       "   58.5006103515625,\n",
       "   58.334266662597656,\n",
       "   58.65286636352539,\n",
       "   58.922176361083984,\n",
       "   58.60945510864258,\n",
       "   58.64419174194336,\n",
       "   58.55460739135742,\n",
       "   59.073387145996094,\n",
       "   58.96586227416992,\n",
       "   59.12870788574219,\n",
       "   59.620635986328125,\n",
       "   59.282474517822266,\n",
       "   59.18445587158203],\n",
       "  'jsc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   98.1895980834961,\n",
       "   96.53627014160156,\n",
       "   95.0376205444336,\n",
       "   93.24343872070312,\n",
       "   92.95987701416016,\n",
       "   92.48252868652344,\n",
       "   92.08419799804688,\n",
       "   91.76582336425781,\n",
       "   90.36979675292969,\n",
       "   89.07479858398438,\n",
       "   88.01812744140625,\n",
       "   87.66352844238281,\n",
       "   86.87004089355469,\n",
       "   85.98648071289062,\n",
       "   85.10894012451172,\n",
       "   84.65865325927734,\n",
       "   83.37378692626953,\n",
       "   82.64118194580078,\n",
       "   82.85919189453125,\n",
       "   82.7919921875,\n",
       "   82.84452819824219,\n",
       "   82.64923858642578,\n",
       "   82.9512939453125,\n",
       "   82.16690063476562,\n",
       "   81.77420043945312,\n",
       "   81.61810302734375,\n",
       "   81.3441162109375,\n",
       "   80.45042419433594,\n",
       "   80.53006744384766,\n",
       "   80.15250396728516,\n",
       "   80.76673126220703,\n",
       "   80.55620574951172,\n",
       "   79.83138275146484,\n",
       "   80.16083526611328,\n",
       "   79.19258117675781,\n",
       "   79.20915222167969,\n",
       "   79.31880950927734,\n",
       "   78.61064910888672,\n",
       "   78.2178726196289,\n",
       "   77.28699493408203,\n",
       "   76.78059387207031,\n",
       "   76.95321655273438,\n",
       "   77.67430877685547,\n",
       "   78.02655029296875,\n",
       "   78.5219955444336,\n",
       "   78.18754577636719,\n",
       "   79.00213623046875,\n",
       "   78.75479125976562,\n",
       "   78.59967041015625,\n",
       "   78.24946594238281,\n",
       "   78.39386749267578,\n",
       "   78.84563446044922,\n",
       "   77.78690338134766,\n",
       "   77.22224426269531,\n",
       "   78.0779800415039,\n",
       "   77.46392822265625,\n",
       "   78.43631744384766,\n",
       "   77.55152893066406,\n",
       "   77.56959533691406,\n",
       "   77.5008773803711,\n",
       "   77.73646545410156,\n",
       "   77.61769104003906,\n",
       "   77.4683837890625,\n",
       "   77.37954711914062,\n",
       "   77.74944305419922,\n",
       "   78.91852569580078,\n",
       "   77.35645294189453,\n",
       "   76.56536102294922,\n",
       "   77.09725952148438,\n",
       "   77.22819519042969,\n",
       "   76.90676879882812,\n",
       "   76.25090789794922,\n",
       "   76.25426483154297,\n",
       "   76.2847900390625,\n",
       "   77.62264251708984,\n",
       "   78.11446380615234,\n",
       "   78.62672424316406,\n",
       "   78.3572998046875,\n",
       "   78.96640014648438,\n",
       "   78.64567565917969,\n",
       "   78.62425231933594,\n",
       "   78.59547424316406,\n",
       "   79.33399963378906,\n",
       "   78.76032257080078,\n",
       "   78.35011291503906,\n",
       "   78.16890716552734,\n",
       "   78.42304992675781,\n",
       "   78.27925109863281,\n",
       "   78.92660522460938,\n",
       "   79.67803192138672,\n",
       "   79.48111724853516,\n",
       "   79.4171142578125,\n",
       "   79.3104019165039,\n",
       "   79.0113296508789,\n",
       "   78.11726379394531,\n",
       "   78.8763198852539,\n",
       "   80.00016784667969,\n",
       "   81.16512298583984,\n",
       "   80.82870483398438,\n",
       "   81.04073333740234,\n",
       "   80.1137466430664,\n",
       "   79.88558959960938,\n",
       "   80.26822662353516,\n",
       "   80.9259262084961,\n",
       "   80.47948455810547,\n",
       "   80.14339447021484,\n",
       "   79.33557891845703,\n",
       "   77.97643280029297,\n",
       "   77.56207275390625,\n",
       "   76.93490600585938,\n",
       "   78.01702880859375,\n",
       "   77.85057830810547,\n",
       "   78.47320556640625,\n",
       "   77.809326171875,\n",
       "   77.88540649414062,\n",
       "   78.18244934082031,\n",
       "   78.56430053710938,\n",
       "   78.48819732666016,\n",
       "   78.67728424072266,\n",
       "   78.96319580078125,\n",
       "   78.52961730957031,\n",
       "   79.31014251708984,\n",
       "   79.36968994140625,\n",
       "   78.28339385986328,\n",
       "   76.86540222167969,\n",
       "   76.90491485595703,\n",
       "   77.60282135009766,\n",
       "   78.5142593383789,\n",
       "   79.53510284423828,\n",
       "   79.89126586914062,\n",
       "   79.75555419921875,\n",
       "   78.80615234375,\n",
       "   77.63391876220703,\n",
       "   78.4075927734375,\n",
       "   78.01960754394531,\n",
       "   79.23152160644531,\n",
       "   79.37785339355469,\n",
       "   80.46355438232422,\n",
       "   79.7372817993164,\n",
       "   79.3453598022461,\n",
       "   79.33119201660156,\n",
       "   79.86663818359375,\n",
       "   79.50634002685547,\n",
       "   79.2128677368164,\n",
       "   79.64938354492188,\n",
       "   79.51472473144531,\n",
       "   79.9336166381836,\n",
       "   80.04080200195312,\n",
       "   80.36360931396484,\n",
       "   80.27997589111328,\n",
       "   80.2739486694336,\n",
       "   80.20501708984375,\n",
       "   79.60295867919922,\n",
       "   80.14026641845703,\n",
       "   79.92326354980469,\n",
       "   79.46887969970703,\n",
       "   80.58103942871094,\n",
       "   78.56163024902344,\n",
       "   79.11836242675781,\n",
       "   79.86311340332031,\n",
       "   79.46475982666016,\n",
       "   79.16001892089844,\n",
       "   79.00289916992188,\n",
       "   78.3973388671875,\n",
       "   78.85833740234375,\n",
       "   79.05908203125,\n",
       "   79.41890716552734,\n",
       "   78.7450942993164,\n",
       "   80.04121398925781,\n",
       "   80.63562774658203,\n",
       "   80.1803970336914,\n",
       "   79.7920913696289,\n",
       "   79.82683563232422,\n",
       "   80.06108093261719,\n",
       "   79.81026458740234,\n",
       "   79.83272552490234,\n",
       "   80.09291076660156,\n",
       "   81.57844543457031,\n",
       "   80.92858123779297,\n",
       "   80.14346313476562,\n",
       "   79.50321960449219,\n",
       "   79.7623062133789,\n",
       "   79.90275573730469,\n",
       "   81.45032501220703,\n",
       "   81.71504211425781,\n",
       "   82.33290100097656,\n",
       "   82.5234146118164,\n",
       "   81.55988311767578,\n",
       "   81.4052505493164,\n",
       "   80.79452514648438,\n",
       "   80.66796875,\n",
       "   80.22848510742188,\n",
       "   81.07644653320312,\n",
       "   81.35408020019531,\n",
       "   81.17091369628906,\n",
       "   80.2925796508789,\n",
       "   80.4303970336914,\n",
       "   79.89111328125,\n",
       "   79.22035217285156,\n",
       "   77.80997467041016,\n",
       "   78.29376220703125,\n",
       "   76.99364471435547,\n",
       "   77.31148529052734,\n",
       "   77.64572143554688,\n",
       "   77.43435668945312,\n",
       "   78.35746765136719,\n",
       "   78.6063461303711,\n",
       "   77.78181457519531,\n",
       "   76.84761810302734,\n",
       "   77.54884338378906,\n",
       "   78.83080291748047,\n",
       "   79.42842864990234,\n",
       "   79.2000732421875,\n",
       "   78.50999450683594,\n",
       "   79.4981460571289,\n",
       "   79.49756622314453,\n",
       "   79.24661254882812,\n",
       "   79.78990936279297,\n",
       "   79.2237777709961,\n",
       "   79.53532409667969,\n",
       "   78.40889739990234,\n",
       "   79.08802032470703,\n",
       "   79.0964126586914,\n",
       "   79.9822006225586,\n",
       "   80.22784423828125,\n",
       "   80.27334594726562,\n",
       "   80.06748962402344,\n",
       "   80.84265899658203,\n",
       "   80.73418426513672,\n",
       "   80.49956512451172,\n",
       "   80.50407409667969,\n",
       "   80.5240707397461,\n",
       "   79.75212860107422,\n",
       "   79.12706756591797,\n",
       "   79.0898208618164,\n",
       "   78.55208587646484,\n",
       "   78.39920806884766,\n",
       "   78.25245666503906,\n",
       "   79.20349884033203,\n",
       "   79.44111633300781,\n",
       "   79.293701171875,\n",
       "   80.0660171508789,\n",
       "   79.62857818603516,\n",
       "   79.364501953125,\n",
       "   79.29811096191406,\n",
       "   79.20394897460938,\n",
       "   78.97571563720703,\n",
       "   77.49504089355469,\n",
       "   77.4119644165039,\n",
       "   77.15912628173828,\n",
       "   77.96815490722656,\n",
       "   79.30441284179688,\n",
       "   79.44318389892578,\n",
       "   79.84429931640625,\n",
       "   79.18519592285156,\n",
       "   78.84255981445312,\n",
       "   80.62042236328125,\n",
       "   80.81987762451172,\n",
       "   80.06685638427734,\n",
       "   79.99502563476562,\n",
       "   80.13579559326172,\n",
       "   80.83372497558594,\n",
       "   80.65937042236328,\n",
       "   79.88935089111328,\n",
       "   79.78304290771484,\n",
       "   79.38318634033203,\n",
       "   79.16932678222656,\n",
       "   78.25199890136719,\n",
       "   77.29557037353516,\n",
       "   78.13190460205078,\n",
       "   77.97319030761719,\n",
       "   77.523681640625,\n",
       "   77.36138153076172,\n",
       "   77.38092803955078,\n",
       "   76.49433898925781,\n",
       "   76.67832946777344,\n",
       "   77.2485122680664,\n",
       "   76.79398345947266,\n",
       "   76.54585266113281,\n",
       "   76.87862396240234,\n",
       "   76.7524185180664,\n",
       "   77.07360076904297,\n",
       "   77.39556121826172,\n",
       "   78.28723907470703,\n",
       "   78.36722564697266,\n",
       "   78.321533203125,\n",
       "   77.62511444091797,\n",
       "   78.38029479980469,\n",
       "   78.03153228759766,\n",
       "   77.0696792602539,\n",
       "   78.36688995361328,\n",
       "   78.85897827148438,\n",
       "   79.51136779785156,\n",
       "   78.76445007324219,\n",
       "   79.0456314086914,\n",
       "   79.0196762084961,\n",
       "   78.82038116455078,\n",
       "   78.56446838378906,\n",
       "   78.00888061523438,\n",
       "   79.07908630371094,\n",
       "   78.49620056152344,\n",
       "   77.8100814819336,\n",
       "   78.9886474609375,\n",
       "   79.46460723876953,\n",
       "   78.95339965820312,\n",
       "   77.85958099365234,\n",
       "   77.66207122802734,\n",
       "   77.5394058227539,\n",
       "   76.97419738769531,\n",
       "   78.10462951660156,\n",
       "   78.86930084228516,\n",
       "   79.49359893798828,\n",
       "   78.2739486694336,\n",
       "   78.1598892211914,\n",
       "   78.59119415283203,\n",
       "   79.37187957763672,\n",
       "   79.36602783203125,\n",
       "   79.75765228271484,\n",
       "   80.62857818603516,\n",
       "   80.20421600341797,\n",
       "   79.40751647949219,\n",
       "   79.1651840209961,\n",
       "   78.78810119628906,\n",
       "   79.59352111816406,\n",
       "   79.54324340820312,\n",
       "   80.06669616699219,\n",
       "   79.68929290771484,\n",
       "   79.63301849365234,\n",
       "   78.5865478515625,\n",
       "   80.04229736328125,\n",
       "   80.33704376220703,\n",
       "   80.58219909667969,\n",
       "   80.88642883300781,\n",
       "   80.58094787597656,\n",
       "   80.85432434082031,\n",
       "   81.45875549316406,\n",
       "   82.71842956542969,\n",
       "   82.88060760498047,\n",
       "   83.16030883789062,\n",
       "   82.00369262695312,\n",
       "   81.78839874267578,\n",
       "   81.28450775146484,\n",
       "   80.21660614013672,\n",
       "   80.22808074951172,\n",
       "   80.81788635253906,\n",
       "   80.5537109375,\n",
       "   80.44981384277344,\n",
       "   80.1772689819336,\n",
       "   80.8171157836914,\n",
       "   80.33824157714844,\n",
       "   81.09468841552734,\n",
       "   82.09232330322266,\n",
       "   81.38580322265625,\n",
       "   80.63859558105469,\n",
       "   79.76800537109375,\n",
       "   79.34010314941406,\n",
       "   79.5882797241211,\n",
       "   80.01392364501953,\n",
       "   78.80228424072266,\n",
       "   78.25236511230469,\n",
       "   78.25909423828125,\n",
       "   79.41259765625,\n",
       "   78.3055419921875,\n",
       "   77.96637725830078,\n",
       "   79.06641387939453,\n",
       "   79.01628875732422,\n",
       "   78.80899047851562,\n",
       "   78.14215087890625,\n",
       "   77.97755432128906,\n",
       "   78.10042572021484,\n",
       "   78.3912124633789,\n",
       "   79.04344940185547,\n",
       "   77.08565521240234,\n",
       "   77.03235626220703,\n",
       "   76.58832550048828,\n",
       "   76.59228515625,\n",
       "   76.76670837402344,\n",
       "   76.0201187133789,\n",
       "   77.03861236572266,\n",
       "   77.0681381225586,\n",
       "   76.70996856689453,\n",
       "   77.68765258789062,\n",
       "   77.15438842773438,\n",
       "   77.2708740234375,\n",
       "   76.2005386352539,\n",
       "   76.96635437011719,\n",
       "   76.62431335449219,\n",
       "   77.34457397460938,\n",
       "   77.7861099243164,\n",
       "   76.99140930175781,\n",
       "   77.25428771972656,\n",
       "   77.04251098632812,\n",
       "   76.52462768554688,\n",
       "   76.32652282714844,\n",
       "   77.7140121459961,\n",
       "   77.94528198242188,\n",
       "   77.38368225097656,\n",
       "   77.06027221679688,\n",
       "   77.29029846191406,\n",
       "   76.517822265625,\n",
       "   76.37661743164062,\n",
       "   77.08283996582031,\n",
       "   76.66913604736328,\n",
       "   77.7563705444336,\n",
       "   78.1229476928711,\n",
       "   77.84620666503906,\n",
       "   77.3444595336914,\n",
       "   78.84383392333984,\n",
       "   79.06159973144531,\n",
       "   79.98868560791016,\n",
       "   79.44539642333984,\n",
       "   80.19932556152344,\n",
       "   79.75277709960938,\n",
       "   80.82687377929688,\n",
       "   81.00682067871094,\n",
       "   80.68207550048828,\n",
       "   81.76708221435547,\n",
       "   80.27279663085938,\n",
       "   80.04736328125,\n",
       "   78.90672302246094,\n",
       "   79.25487518310547,\n",
       "   78.10879516601562,\n",
       "   77.91317749023438,\n",
       "   77.24815368652344,\n",
       "   77.72830963134766,\n",
       "   77.56150817871094,\n",
       "   77.9754638671875,\n",
       "   78.81925964355469,\n",
       "   78.51441192626953,\n",
       "   79.72775268554688,\n",
       "   80.07662963867188,\n",
       "   79.59113311767578,\n",
       "   79.6355209350586,\n",
       "   79.88996887207031,\n",
       "   79.45603942871094,\n",
       "   79.11309814453125,\n",
       "   80.54638671875,\n",
       "   80.26372528076172,\n",
       "   80.37203216552734,\n",
       "   80.12213897705078,\n",
       "   80.44705963134766,\n",
       "   79.94652557373047,\n",
       "   81.000244140625,\n",
       "   81.00521087646484,\n",
       "   80.55028533935547,\n",
       "   81.08328247070312,\n",
       "   80.0044937133789,\n",
       "   80.4862060546875,\n",
       "   80.4471435546875,\n",
       "   80.43247985839844,\n",
       "   80.2403564453125,\n",
       "   80.62509155273438,\n",
       "   80.45994567871094,\n",
       "   80.59143829345703,\n",
       "   79.65332794189453,\n",
       "   79.22827911376953,\n",
       "   79.00830078125,\n",
       "   78.15106964111328,\n",
       "   78.38260650634766,\n",
       "   78.53738403320312,\n",
       "   79.25648498535156,\n",
       "   79.70348358154297,\n",
       "   79.00892639160156,\n",
       "   78.52030944824219,\n",
       "   77.40943908691406,\n",
       "   77.76836395263672,\n",
       "   76.94829559326172,\n",
       "   77.3382797241211,\n",
       "   78.04893493652344,\n",
       "   77.45854187011719,\n",
       "   77.9668960571289,\n",
       "   79.0838394165039,\n",
       "   78.55626678466797,\n",
       "   79.343017578125,\n",
       "   79.92428588867188,\n",
       "   79.60748291015625,\n",
       "   80.48979949951172,\n",
       "   80.66598510742188,\n",
       "   80.7823486328125,\n",
       "   80.93090057373047,\n",
       "   80.42534637451172,\n",
       "   79.39139556884766,\n",
       "   80.25922393798828,\n",
       "   79.22928619384766,\n",
       "   79.16847229003906,\n",
       "   79.7193374633789,\n",
       "   78.98272705078125,\n",
       "   80.04725646972656,\n",
       "   79.734375,\n",
       "   79.93667602539062,\n",
       "   78.64629364013672,\n",
       "   79.33651733398438,\n",
       "   79.17906951904297],\n",
       "  'ff_acc': [100.0,\n",
       "   100.0,\n",
       "   88.23333740234375,\n",
       "   77.66407012939453,\n",
       "   65.35981750488281,\n",
       "   61.34027862548828,\n",
       "   56.0753288269043,\n",
       "   46.289344787597656,\n",
       "   44.06959533691406,\n",
       "   39.1772346496582,\n",
       "   34.433834075927734,\n",
       "   34.355865478515625,\n",
       "   31.268661499023438,\n",
       "   30.260601043701172,\n",
       "   27.707660675048828,\n",
       "   25.464969635009766,\n",
       "   24.672325134277344,\n",
       "   22.219987869262695,\n",
       "   22.17047119140625,\n",
       "   20.4455509185791,\n",
       "   21.308486938476562,\n",
       "   21.985639572143555,\n",
       "   21.872041702270508,\n",
       "   18.371152877807617,\n",
       "   19.622169494628906,\n",
       "   18.710071563720703,\n",
       "   18.68062973022461,\n",
       "   17.994924545288086,\n",
       "   17.430862426757812,\n",
       "   16.982744216918945,\n",
       "   16.051496505737305,\n",
       "   16.030105590820312,\n",
       "   16.652427673339844,\n",
       "   16.53713035583496,\n",
       "   15.395784378051758,\n",
       "   14.455856323242188,\n",
       "   13.49778938293457,\n",
       "   13.528573989868164,\n",
       "   15.274622917175293,\n",
       "   15.87533187866211,\n",
       "   15.80164623260498,\n",
       "   15.323038101196289,\n",
       "   16.034626007080078,\n",
       "   16.76131248474121,\n",
       "   16.166643142700195,\n",
       "   16.347030639648438,\n",
       "   15.856952667236328,\n",
       "   15.737936019897461,\n",
       "   15.81106948852539,\n",
       "   14.553650856018066,\n",
       "   13.959161758422852,\n",
       "   12.998761177062988,\n",
       "   12.443544387817383,\n",
       "   13.060179710388184,\n",
       "   12.485767364501953,\n",
       "   13.540159225463867,\n",
       "   13.177397727966309,\n",
       "   11.856988906860352,\n",
       "   13.480517387390137,\n",
       "   12.804283142089844,\n",
       "   12.607504844665527,\n",
       "   11.903188705444336,\n",
       "   11.55602741241455,\n",
       "   11.934329986572266,\n",
       "   12.590042114257812,\n",
       "   12.787860870361328,\n",
       "   13.120015144348145,\n",
       "   12.745525360107422,\n",
       "   14.115972518920898,\n",
       "   13.857202529907227,\n",
       "   13.748323440551758,\n",
       "   14.003613471984863,\n",
       "   13.796147346496582,\n",
       "   13.960151672363281,\n",
       "   12.93906307220459,\n",
       "   13.129130363464355,\n",
       "   13.550551414489746,\n",
       "   13.599178314208984,\n",
       "   13.898563385009766,\n",
       "   14.937214851379395,\n",
       "   15.430438995361328,\n",
       "   14.624096870422363,\n",
       "   15.335972785949707,\n",
       "   15.979461669921875,\n",
       "   16.766895294189453,\n",
       "   14.888998985290527,\n",
       "   14.496498107910156,\n",
       "   13.858957290649414,\n",
       "   15.605165481567383,\n",
       "   15.30186653137207,\n",
       "   15.369626998901367,\n",
       "   16.106748580932617,\n",
       "   15.11432933807373,\n",
       "   16.199323654174805,\n",
       "   15.796165466308594,\n",
       "   15.677855491638184,\n",
       "   13.87485122680664,\n",
       "   15.35997200012207,\n",
       "   14.76170825958252,\n",
       "   16.246749877929688,\n",
       "   16.282535552978516,\n",
       "   17.661611557006836,\n",
       "   16.842510223388672,\n",
       "   16.697450637817383,\n",
       "   16.220401763916016,\n",
       "   17.504777908325195,\n",
       "   16.112180709838867,\n",
       "   16.10564422607422,\n",
       "   16.280014038085938,\n",
       "   16.16756820678711,\n",
       "   15.601722717285156,\n",
       "   15.33059024810791,\n",
       "   15.616217613220215,\n",
       "   15.101705551147461,\n",
       "   17.140228271484375,\n",
       "   17.167890548706055,\n",
       "   17.999025344848633,\n",
       "   17.238147735595703,\n",
       "   17.73128318786621,\n",
       "   16.48461151123047,\n",
       "   16.318002700805664,\n",
       "   16.258773803710938,\n",
       "   15.5348539352417,\n",
       "   14.455522537231445,\n",
       "   15.597089767456055,\n",
       "   15.495522499084473,\n",
       "   15.259268760681152,\n",
       "   13.397576332092285,\n",
       "   13.441024780273438,\n",
       "   13.989392280578613,\n",
       "   13.42777156829834,\n",
       "   13.657853126525879,\n",
       "   15.047268867492676,\n",
       "   14.518692970275879,\n",
       "   16.19504737854004,\n",
       "   14.552728652954102,\n",
       "   14.25166130065918,\n",
       "   13.321402549743652,\n",
       "   13.659015655517578,\n",
       "   14.629261016845703,\n",
       "   14.851890563964844,\n",
       "   15.449155807495117,\n",
       "   17.38054847717285,\n",
       "   18.03212547302246,\n",
       "   17.34077262878418,\n",
       "   16.8714599609375,\n",
       "   18.505590438842773,\n",
       "   18.242259979248047,\n",
       "   17.51838493347168,\n",
       "   16.88628387451172,\n",
       "   17.03339385986328,\n",
       "   16.543609619140625,\n",
       "   16.407094955444336,\n",
       "   16.798295974731445,\n",
       "   15.907514572143555,\n",
       "   14.321764945983887,\n",
       "   15.175321578979492,\n",
       "   14.30626106262207,\n",
       "   16.168325424194336,\n",
       "   18.098459243774414,\n",
       "   16.963382720947266,\n",
       "   17.190244674682617,\n",
       "   17.284557342529297,\n",
       "   19.08807373046875,\n",
       "   17.97260284423828,\n",
       "   16.747739791870117,\n",
       "   15.877182960510254,\n",
       "   17.078102111816406,\n",
       "   16.54678726196289,\n",
       "   15.68490219116211,\n",
       "   17.514209747314453,\n",
       "   16.732160568237305,\n",
       "   18.712678909301758,\n",
       "   18.504074096679688,\n",
       "   18.31634521484375,\n",
       "   18.41000747680664,\n",
       "   16.04956817626953,\n",
       "   16.196704864501953,\n",
       "   15.788188934326172,\n",
       "   15.048663139343262,\n",
       "   13.888883590698242,\n",
       "   12.898198127746582,\n",
       "   13.283197402954102,\n",
       "   13.180615425109863,\n",
       "   13.80455493927002,\n",
       "   13.311280250549316,\n",
       "   13.002530097961426,\n",
       "   12.699821472167969,\n",
       "   12.00882339477539,\n",
       "   11.983083724975586,\n",
       "   12.620784759521484,\n",
       "   11.520249366760254,\n",
       "   11.513354301452637,\n",
       "   11.787766456604004,\n",
       "   11.653579711914062,\n",
       "   12.151395797729492,\n",
       "   13.320450782775879,\n",
       "   13.230731010437012,\n",
       "   12.204894065856934,\n",
       "   12.377285957336426,\n",
       "   11.57040786743164,\n",
       "   11.812830924987793,\n",
       "   11.720986366271973,\n",
       "   12.538363456726074,\n",
       "   14.597976684570312,\n",
       "   14.155922889709473,\n",
       "   13.464700698852539,\n",
       "   12.242330551147461,\n",
       "   12.202228546142578,\n",
       "   12.10011100769043,\n",
       "   12.418771743774414,\n",
       "   12.22237777709961,\n",
       "   11.56469440460205,\n",
       "   11.55575942993164,\n",
       "   12.08277416229248,\n",
       "   12.2595853805542,\n",
       "   11.981637954711914,\n",
       "   13.264581680297852,\n",
       "   13.73263931274414,\n",
       "   15.01164436340332,\n",
       "   15.507176399230957,\n",
       "   15.372546195983887,\n",
       "   14.562542915344238,\n",
       "   14.025293350219727,\n",
       "   13.555747032165527,\n",
       "   16.36123275756836,\n",
       "   15.42402458190918,\n",
       "   16.055702209472656,\n",
       "   14.86643123626709,\n",
       "   14.963205337524414,\n",
       "   14.236286163330078,\n",
       "   14.425432205200195,\n",
       "   15.586244583129883,\n",
       "   15.752789497375488,\n",
       "   17.21272087097168,\n",
       "   15.236936569213867,\n",
       "   14.263382911682129,\n",
       "   13.861960411071777,\n",
       "   14.467742919921875,\n",
       "   14.74036693572998,\n",
       "   15.599322319030762,\n",
       "   15.850107192993164,\n",
       "   15.226600646972656,\n",
       "   15.91379451751709,\n",
       "   16.141267776489258,\n",
       "   16.611143112182617,\n",
       "   16.395498275756836,\n",
       "   17.81308364868164,\n",
       "   18.858013153076172,\n",
       "   19.33942222595215,\n",
       "   18.872312545776367,\n",
       "   18.562477111816406,\n",
       "   17.98409080505371,\n",
       "   16.98879051208496,\n",
       "   18.02471923828125,\n",
       "   17.37190818786621,\n",
       "   15.747769355773926,\n",
       "   16.2821102142334,\n",
       "   15.479768753051758,\n",
       "   15.486433029174805,\n",
       "   15.723225593566895,\n",
       "   15.994413375854492,\n",
       "   15.168205261230469,\n",
       "   13.43694019317627,\n",
       "   13.227893829345703,\n",
       "   12.13646125793457,\n",
       "   15.058636665344238,\n",
       "   14.162217140197754,\n",
       "   16.505016326904297,\n",
       "   16.849021911621094,\n",
       "   16.869766235351562,\n",
       "   15.62324333190918,\n",
       "   15.314200401306152,\n",
       "   14.635555267333984,\n",
       "   12.542954444885254,\n",
       "   12.017374038696289,\n",
       "   12.083990097045898,\n",
       "   12.47058391571045,\n",
       "   12.554849624633789,\n",
       "   12.483237266540527,\n",
       "   12.472818374633789,\n",
       "   12.503568649291992,\n",
       "   12.590442657470703,\n",
       "   12.413396835327148,\n",
       "   12.215866088867188,\n",
       "   12.187986373901367,\n",
       "   11.954742431640625,\n",
       "   11.814361572265625,\n",
       "   11.667156219482422,\n",
       "   11.795385360717773,\n",
       "   11.821718215942383,\n",
       "   11.632025718688965,\n",
       "   12.38658332824707,\n",
       "   13.459393501281738,\n",
       "   12.664151191711426,\n",
       "   12.761007308959961,\n",
       "   12.192307472229004,\n",
       "   12.053857803344727,\n",
       "   12.59605884552002,\n",
       "   12.336453437805176,\n",
       "   12.38980770111084,\n",
       "   12.209052085876465,\n",
       "   12.378788948059082,\n",
       "   12.603646278381348,\n",
       "   12.569448471069336,\n",
       "   13.40291976928711,\n",
       "   12.996459007263184,\n",
       "   12.787970542907715,\n",
       "   12.868806838989258,\n",
       "   12.7792329788208,\n",
       "   12.257166862487793,\n",
       "   12.738016128540039,\n",
       "   12.760723114013672,\n",
       "   14.246834754943848,\n",
       "   14.60997200012207,\n",
       "   18.182491302490234,\n",
       "   18.99677848815918,\n",
       "   19.022777557373047,\n",
       "   17.472885131835938,\n",
       "   15.731098175048828,\n",
       "   14.79510498046875,\n",
       "   14.954204559326172,\n",
       "   12.225321769714355,\n",
       "   12.02441692352295,\n",
       "   11.834299087524414,\n",
       "   11.825050354003906,\n",
       "   11.965025901794434,\n",
       "   11.934311866760254,\n",
       "   12.306900024414062,\n",
       "   12.449994087219238,\n",
       "   12.000472068786621,\n",
       "   12.274617195129395,\n",
       "   12.609489440917969,\n",
       "   12.374005317687988,\n",
       "   11.98007583618164,\n",
       "   11.888760566711426,\n",
       "   11.866421699523926,\n",
       "   11.79173755645752,\n",
       "   12.206255912780762,\n",
       "   12.726299285888672,\n",
       "   12.71850872039795,\n",
       "   12.688589096069336,\n",
       "   13.212953567504883,\n",
       "   12.886634826660156,\n",
       "   13.38836669921875,\n",
       "   13.277290344238281,\n",
       "   13.622194290161133,\n",
       "   13.600927352905273,\n",
       "   13.254281997680664,\n",
       "   13.387161254882812,\n",
       "   13.395519256591797,\n",
       "   13.655363082885742,\n",
       "   14.073410987854004,\n",
       "   13.824807167053223,\n",
       "   13.806594848632812,\n",
       "   13.94039535522461,\n",
       "   13.004583358764648,\n",
       "   12.49128532409668,\n",
       "   12.263493537902832,\n",
       "   12.163888931274414,\n",
       "   12.682092666625977,\n",
       "   12.273125648498535,\n",
       "   11.991425514221191,\n",
       "   12.033164978027344,\n",
       "   11.63657283782959,\n",
       "   11.867433547973633,\n",
       "   11.547833442687988,\n",
       "   11.941628456115723,\n",
       "   12.162785530090332,\n",
       "   12.380305290222168,\n",
       "   12.233614921569824,\n",
       "   12.626649856567383,\n",
       "   12.529439926147461,\n",
       "   12.627138137817383,\n",
       "   12.113471984863281,\n",
       "   12.300670623779297,\n",
       "   11.939852714538574,\n",
       "   11.641148567199707,\n",
       "   11.277477264404297,\n",
       "   11.269701957702637,\n",
       "   11.255876541137695,\n",
       "   11.771472930908203,\n",
       "   11.744277000427246,\n",
       "   11.400375366210938,\n",
       "   11.817593574523926,\n",
       "   12.642701148986816,\n",
       "   12.625856399536133,\n",
       "   12.252928733825684,\n",
       "   12.615105628967285,\n",
       "   12.515695571899414,\n",
       "   12.905131340026855,\n",
       "   12.521794319152832,\n",
       "   12.087644577026367,\n",
       "   12.10883617401123,\n",
       "   12.018866539001465,\n",
       "   12.549999237060547,\n",
       "   12.348711013793945,\n",
       "   11.924628257751465,\n",
       "   11.882266998291016,\n",
       "   11.711289405822754,\n",
       "   11.768837928771973,\n",
       "   12.463730812072754,\n",
       "   12.290762901306152,\n",
       "   12.334001541137695,\n",
       "   12.770710945129395,\n",
       "   13.059881210327148,\n",
       "   12.458905220031738,\n",
       "   12.531961441040039,\n",
       "   12.508310317993164,\n",
       "   12.343998908996582,\n",
       "   12.164116859436035,\n",
       "   11.750831604003906,\n",
       "   11.439449310302734,\n",
       "   12.454923629760742,\n",
       "   12.468756675720215,\n",
       "   12.641629219055176,\n",
       "   12.780176162719727,\n",
       "   12.509526252746582,\n",
       "   11.995954513549805,\n",
       "   11.377684593200684,\n",
       "   11.690046310424805,\n",
       "   11.363740921020508,\n",
       "   11.373231887817383,\n",
       "   11.285818099975586,\n",
       "   12.309452056884766,\n",
       "   11.988698959350586,\n",
       "   11.94589900970459,\n",
       "   11.853217124938965,\n",
       "   12.147272109985352,\n",
       "   11.783097267150879,\n",
       "   12.596254348754883,\n",
       "   13.904415130615234,\n",
       "   13.809005737304688,\n",
       "   13.57847785949707,\n",
       "   13.02370834350586,\n",
       "   13.333441734313965,\n",
       "   13.651426315307617,\n",
       "   13.128293991088867,\n",
       "   13.004332542419434,\n",
       "   12.618255615234375,\n",
       "   12.18763542175293,\n",
       "   11.652361869812012,\n",
       "   11.89023208618164,\n",
       "   12.630614280700684,\n",
       "   12.434171676635742,\n",
       "   12.464017868041992,\n",
       "   12.500749588012695,\n",
       "   12.40446662902832,\n",
       "   12.889315605163574,\n",
       "   13.063240051269531,\n",
       "   12.687370300292969,\n",
       "   13.527339935302734,\n",
       "   13.036331176757812,\n",
       "   13.789985656738281,\n",
       "   15.793089866638184,\n",
       "   16.946426391601562,\n",
       "   16.816879272460938,\n",
       "   14.270697593688965,\n",
       "   13.148183822631836,\n",
       "   12.406169891357422,\n",
       "   11.965835571289062,\n",
       "   11.982479095458984,\n",
       "   11.889843940734863,\n",
       "   11.975610733032227,\n",
       "   12.466957092285156,\n",
       "   12.871810913085938,\n",
       "   13.169363021850586,\n",
       "   13.160808563232422,\n",
       "   13.48419189453125,\n",
       "   14.174863815307617,\n",
       "   13.322281837463379,\n",
       "   12.865480422973633,\n",
       "   13.066521644592285,\n",
       "   12.630738258361816,\n",
       "   12.125539779663086,\n",
       "   11.752080917358398,\n",
       "   12.480400085449219,\n",
       "   12.217459678649902,\n",
       "   12.441868782043457,\n",
       "   13.431458473205566,\n",
       "   12.594943046569824,\n",
       "   12.990802764892578,\n",
       "   12.900386810302734,\n",
       "   12.738897323608398,\n",
       "   13.268759727478027,\n",
       "   13.006505012512207,\n",
       "   13.372352600097656,\n",
       "   13.674203872680664,\n",
       "   13.920206069946289,\n",
       "   13.916863441467285,\n",
       "   13.581748008728027,\n",
       "   13.98486614227295,\n",
       "   13.769891738891602,\n",
       "   13.499983787536621,\n",
       "   14.234704971313477,\n",
       "   13.878266334533691,\n",
       "   13.336464881896973,\n",
       "   13.171789169311523,\n",
       "   14.097627639770508,\n",
       "   14.0747652053833],\n",
       "  'test_accs': [442.23631286621094,\n",
       "   418.49625396728516,\n",
       "   385.8799819946289,\n",
       "   356.9109878540039,\n",
       "   336.8025665283203,\n",
       "   325.68013763427734,\n",
       "   315.88282012939453,\n",
       "   300.0853500366211,\n",
       "   295.9876174926758,\n",
       "   291.1387748718262,\n",
       "   286.69763565063477,\n",
       "   288.6198539733887,\n",
       "   284.1532897949219,\n",
       "   278.5374526977539,\n",
       "   274.10568618774414,\n",
       "   268.1214141845703,\n",
       "   263.5675468444824,\n",
       "   258.64831352233887,\n",
       "   256.1658020019531,\n",
       "   252.0259838104248,\n",
       "   249.57810592651367,\n",
       "   247.81131553649902,\n",
       "   247.13527870178223,\n",
       "   239.92997550964355,\n",
       "   239.88017654418945,\n",
       "   238.25674057006836,\n",
       "   235.72420120239258,\n",
       "   237.81646537780762,\n",
       "   235.59087371826172,\n",
       "   234.10601997375488,\n",
       "   233.68920707702637,\n",
       "   231.919189453125,\n",
       "   229.2919464111328,\n",
       "   229.21970176696777,\n",
       "   230.48447608947754,\n",
       "   230.81634521484375,\n",
       "   230.39376258850098,\n",
       "   232.96411323547363,\n",
       "   232.4625883102417,\n",
       "   233.3964958190918,\n",
       "   234.13603496551514,\n",
       "   232.9582691192627,\n",
       "   234.82092666625977,\n",
       "   234.80431175231934,\n",
       "   234.12516975402832,\n",
       "   234.8644790649414,\n",
       "   234.40566635131836,\n",
       "   233.3521022796631,\n",
       "   231.9444236755371,\n",
       "   229.9744577407837,\n",
       "   232.11836433410645,\n",
       "   232.1802339553833,\n",
       "   230.19097328186035,\n",
       "   231.72698879241943,\n",
       "   231.73247146606445,\n",
       "   232.43649864196777,\n",
       "   233.05042171478271,\n",
       "   231.54946327209473,\n",
       "   234.2152509689331,\n",
       "   231.3821029663086,\n",
       "   231.20471858978271,\n",
       "   230.87457084655762,\n",
       "   230.33610248565674,\n",
       "   227.39648818969727,\n",
       "   227.30770874023438,\n",
       "   228.44082260131836,\n",
       "   227.85150623321533,\n",
       "   228.16135787963867,\n",
       "   226.6421947479248,\n",
       "   228.64132118225098,\n",
       "   227.28300285339355,\n",
       "   228.75951480865479,\n",
       "   229.98662281036377,\n",
       "   228.64190292358398,\n",
       "   225.08055019378662,\n",
       "   226.2481336593628,\n",
       "   226.30191707611084,\n",
       "   220.5950927734375,\n",
       "   221.08382034301758,\n",
       "   224.18671131134033,\n",
       "   225.03340911865234,\n",
       "   224.08703708648682,\n",
       "   224.3647699356079,\n",
       "   224.28578186035156,\n",
       "   220.54642486572266,\n",
       "   218.80728816986084,\n",
       "   217.67947387695312,\n",
       "   215.97088813781738,\n",
       "   215.16133308410645,\n",
       "   215.9280071258545,\n",
       "   212.76833152770996,\n",
       "   213.52711296081543,\n",
       "   212.698260307312,\n",
       "   212.91557502746582,\n",
       "   214.65807723999023,\n",
       "   214.3249387741089,\n",
       "   213.9449119567871,\n",
       "   215.43952751159668,\n",
       "   215.11859607696533,\n",
       "   214.75791549682617,\n",
       "   214.22041702270508,\n",
       "   213.3554286956787,\n",
       "   213.72947311401367,\n",
       "   215.4608860015869,\n",
       "   217.12074661254883,\n",
       "   216.2533473968506,\n",
       "   215.7997341156006,\n",
       "   214.56511306762695,\n",
       "   215.77656173706055,\n",
       "   216.2165641784668,\n",
       "   217.6918830871582,\n",
       "   216.92826557159424,\n",
       "   216.79297542572021,\n",
       "   215.31292152404785,\n",
       "   216.34106826782227,\n",
       "   216.72139167785645,\n",
       "   217.1491985321045,\n",
       "   216.75556564331055,\n",
       "   216.5143756866455,\n",
       "   215.98581314086914,\n",
       "   214.35205268859863,\n",
       "   214.1587142944336,\n",
       "   211.7614164352417,\n",
       "   211.73235511779785,\n",
       "   212.15198707580566,\n",
       "   212.1406946182251,\n",
       "   213.7733793258667,\n",
       "   212.45646381378174,\n",
       "   212.3189582824707,\n",
       "   212.897873878479,\n",
       "   210.4293966293335,\n",
       "   208.13736057281494,\n",
       "   209.69232654571533,\n",
       "   208.53127765655518,\n",
       "   214.95576286315918,\n",
       "   213.27086067199707,\n",
       "   211.26068687438965,\n",
       "   213.4786901473999,\n",
       "   208.43306732177734,\n",
       "   203.8680534362793,\n",
       "   207.85983276367188,\n",
       "   206.2476100921631,\n",
       "   207.38816261291504,\n",
       "   209.83823204040527,\n",
       "   212.0176944732666,\n",
       "   211.88363647460938,\n",
       "   214.78833198547363,\n",
       "   216.32268905639648,\n",
       "   216.89276313781738,\n",
       "   213.30484008789062,\n",
       "   214.0113067626953,\n",
       "   214.68781280517578,\n",
       "   213.37499809265137,\n",
       "   214.99950981140137,\n",
       "   210.23645973205566,\n",
       "   209.22276973724365,\n",
       "   208.52751350402832,\n",
       "   211.86090660095215,\n",
       "   218.25512504577637,\n",
       "   214.7157917022705,\n",
       "   210.8793830871582,\n",
       "   210.16317558288574,\n",
       "   210.52907180786133,\n",
       "   211.46392822265625,\n",
       "   209.3173599243164,\n",
       "   206.7174472808838,\n",
       "   208.650616645813,\n",
       "   208.9816551208496,\n",
       "   208.44215393066406,\n",
       "   207.85269927978516,\n",
       "   211.56579208374023,\n",
       "   211.28623008728027,\n",
       "   217.53259086608887,\n",
       "   220.0680809020996,\n",
       "   215.87282180786133,\n",
       "   218.12817764282227,\n",
       "   217.3430938720703,\n",
       "   219.11198806762695,\n",
       "   219.33451461791992,\n",
       "   218.97523021697998,\n",
       "   217.77686882019043,\n",
       "   218.01940631866455,\n",
       "   216.8259105682373,\n",
       "   212.07805728912354,\n",
       "   216.32757663726807,\n",
       "   214.94352436065674,\n",
       "   212.75759983062744,\n",
       "   214.25576782226562,\n",
       "   212.73276138305664,\n",
       "   213.18612098693848,\n",
       "   218.90019989013672,\n",
       "   218.59717655181885,\n",
       "   221.55393505096436,\n",
       "   222.73009967803955,\n",
       "   222.8366813659668,\n",
       "   221.9705104827881,\n",
       "   224.01469898223877,\n",
       "   220.6287145614624,\n",
       "   221.55337810516357,\n",
       "   225.44453525543213,\n",
       "   219.39508819580078,\n",
       "   221.6764440536499,\n",
       "   220.17681980133057,\n",
       "   216.1844244003296,\n",
       "   218.49609756469727,\n",
       "   217.93343830108643,\n",
       "   215.40173530578613,\n",
       "   216.13351249694824,\n",
       "   213.98866653442383,\n",
       "   217.78063774108887,\n",
       "   219.88595008850098,\n",
       "   219.19845962524414,\n",
       "   220.07252597808838,\n",
       "   218.3911018371582,\n",
       "   217.42423152923584,\n",
       "   215.70436000823975,\n",
       "   214.31385231018066,\n",
       "   216.675874710083,\n",
       "   216.50234985351562,\n",
       "   215.6973056793213,\n",
       "   216.935959815979,\n",
       "   216.42895412445068,\n",
       "   217.1264123916626,\n",
       "   216.26915550231934,\n",
       "   215.10938549041748,\n",
       "   216.79550552368164,\n",
       "   216.72504234313965,\n",
       "   216.55564880371094,\n",
       "   215.50513362884521,\n",
       "   212.6596736907959,\n",
       "   213.75027465820312,\n",
       "   216.27509880065918,\n",
       "   220.41605186462402,\n",
       "   219.07038974761963,\n",
       "   224.60218620300293,\n",
       "   224.1753215789795,\n",
       "   222.50766849517822,\n",
       "   222.6605520248413,\n",
       "   224.55188751220703,\n",
       "   224.27361965179443,\n",
       "   224.1437292098999,\n",
       "   225.7463779449463,\n",
       "   224.59720611572266,\n",
       "   222.06182384490967,\n",
       "   222.50825309753418,\n",
       "   221.67170524597168,\n",
       "   225.47532081604004,\n",
       "   227.5716781616211,\n",
       "   231.2325096130371,\n",
       "   233.08695793151855,\n",
       "   232.55230522155762,\n",
       "   231.85390853881836,\n",
       "   232.71379280090332,\n",
       "   233.4200496673584,\n",
       "   233.2323341369629,\n",
       "   233.1081829071045,\n",
       "   230.40010929107666,\n",
       "   230.7261257171631,\n",
       "   230.19513130187988,\n",
       "   229.6360263824463,\n",
       "   231.41300296783447,\n",
       "   231.1335620880127,\n",
       "   227.67845153808594,\n",
       "   225.33770847320557,\n",
       "   223.22019958496094,\n",
       "   222.11811637878418,\n",
       "   224.65606784820557,\n",
       "   222.24644947052002,\n",
       "   227.3960075378418,\n",
       "   228.1997299194336,\n",
       "   228.2073097229004,\n",
       "   228.99678230285645,\n",
       "   226.76226711273193,\n",
       "   227.5578956604004,\n",
       "   225.67794132232666,\n",
       "   223.88062858581543,\n",
       "   226.88146018981934,\n",
       "   227.8314390182495,\n",
       "   227.03758430480957,\n",
       "   228.5074872970581,\n",
       "   227.82660102844238,\n",
       "   225.27289390563965,\n",
       "   224.77509307861328,\n",
       "   224.8029842376709,\n",
       "   225.3367919921875,\n",
       "   224.63656044006348,\n",
       "   221.7094612121582,\n",
       "   220.75711822509766,\n",
       "   221.77830505371094,\n",
       "   224.36342811584473,\n",
       "   223.61622047424316,\n",
       "   223.50822734832764,\n",
       "   222.75586128234863,\n",
       "   220.16532611846924,\n",
       "   221.9982271194458,\n",
       "   223.7780361175537,\n",
       "   220.65674686431885,\n",
       "   222.3203639984131,\n",
       "   226.26589679718018,\n",
       "   226.8811845779419,\n",
       "   224.9096326828003,\n",
       "   225.4479169845581,\n",
       "   223.7745599746704,\n",
       "   225.32512760162354,\n",
       "   223.17464637756348,\n",
       "   223.92658615112305,\n",
       "   224.4149694442749,\n",
       "   223.62810802459717,\n",
       "   224.349027633667,\n",
       "   226.86478900909424,\n",
       "   226.96926021575928,\n",
       "   227.06234550476074,\n",
       "   225.6681900024414,\n",
       "   226.53965091705322,\n",
       "   226.91800498962402,\n",
       "   230.0273666381836,\n",
       "   231.70263481140137,\n",
       "   230.89571380615234,\n",
       "   228.29669952392578,\n",
       "   224.90789413452148,\n",
       "   224.25614547729492,\n",
       "   225.79572296142578,\n",
       "   221.6690092086792,\n",
       "   220.4289197921753,\n",
       "   220.39882850646973,\n",
       "   225.16093063354492,\n",
       "   222.1669454574585,\n",
       "   225.02983951568604,\n",
       "   225.3520164489746,\n",
       "   227.16267490386963,\n",
       "   228.5269956588745,\n",
       "   227.85529804229736,\n",
       "   228.70584106445312,\n",
       "   227.65324115753174,\n",
       "   226.65143966674805,\n",
       "   223.83176517486572,\n",
       "   225.6846055984497,\n",
       "   225.93949604034424,\n",
       "   228.02625942230225,\n",
       "   225.8640365600586,\n",
       "   226.35737895965576,\n",
       "   226.5989933013916,\n",
       "   229.5751667022705,\n",
       "   231.29687881469727,\n",
       "   228.87427520751953,\n",
       "   230.73081970214844,\n",
       "   230.75314903259277,\n",
       "   226.766695022583,\n",
       "   225.32188606262207,\n",
       "   224.07165908813477,\n",
       "   222.87509536743164,\n",
       "   227.66927909851074,\n",
       "   227.68224430084229,\n",
       "   225.15499210357666,\n",
       "   225.5165672302246,\n",
       "   228.1130714416504,\n",
       "   228.70756721496582,\n",
       "   229.43885612487793,\n",
       "   232.23892307281494,\n",
       "   229.70944023132324,\n",
       "   229.5322666168213,\n",
       "   227.70262241363525,\n",
       "   226.1540765762329,\n",
       "   227.48568725585938,\n",
       "   225.91865062713623,\n",
       "   225.55536460876465,\n",
       "   225.20333576202393,\n",
       "   225.02213382720947,\n",
       "   227.41368961334229,\n",
       "   226.79041194915771,\n",
       "   224.5669813156128,\n",
       "   227.2749729156494,\n",
       "   225.33723258972168,\n",
       "   226.00221061706543,\n",
       "   225.59938049316406,\n",
       "   222.47154235839844,\n",
       "   222.59502696990967,\n",
       "   222.1654291152954,\n",
       "   219.84348678588867,\n",
       "   220.1158857345581,\n",
       "   219.1027317047119,\n",
       "   219.01530075073242,\n",
       "   219.19850826263428,\n",
       "   219.44622421264648,\n",
       "   218.44500064849854,\n",
       "   218.27221775054932,\n",
       "   217.7523136138916,\n",
       "   218.4340581893921,\n",
       "   218.7071008682251,\n",
       "   217.64621543884277,\n",
       "   220.07741451263428,\n",
       "   216.7544412612915,\n",
       "   219.58977699279785,\n",
       "   219.47939586639404,\n",
       "   216.47795391082764,\n",
       "   221.1526222229004,\n",
       "   217.42734718322754,\n",
       "   215.60053157806396,\n",
       "   216.56266021728516,\n",
       "   215.32458591461182,\n",
       "   214.85040378570557,\n",
       "   219.6285104751587,\n",
       "   219.80697917938232,\n",
       "   219.43905448913574,\n",
       "   219.36571598052979,\n",
       "   222.51602745056152,\n",
       "   221.91504383087158,\n",
       "   222.38016319274902,\n",
       "   222.22074699401855,\n",
       "   222.8304681777954,\n",
       "   224.73476123809814,\n",
       "   221.8608169555664,\n",
       "   221.6330451965332,\n",
       "   220.78881645202637,\n",
       "   222.8365774154663,\n",
       "   223.34525394439697,\n",
       "   225.25308799743652,\n",
       "   226.9165620803833,\n",
       "   225.0152645111084,\n",
       "   224.1346311569214,\n",
       "   224.52594566345215,\n",
       "   219.41954612731934,\n",
       "   219.63804054260254,\n",
       "   219.78594779968262,\n",
       "   218.05267715454102,\n",
       "   217.58573722839355,\n",
       "   216.82701778411865,\n",
       "   218.46505641937256,\n",
       "   219.55830574035645,\n",
       "   217.1392412185669,\n",
       "   219.91270637512207,\n",
       "   220.32677841186523,\n",
       "   218.13643264770508,\n",
       "   218.15611839294434,\n",
       "   218.23032760620117,\n",
       "   220.27852153778076,\n",
       "   221.8464298248291,\n",
       "   219.8945026397705,\n",
       "   218.65482997894287,\n",
       "   221.1024513244629,\n",
       "   220.8136692047119,\n",
       "   220.9203290939331,\n",
       "   218.72001266479492,\n",
       "   219.954607963562,\n",
       "   221.36421012878418,\n",
       "   223.60761833190918,\n",
       "   225.38771629333496,\n",
       "   224.63486671447754,\n",
       "   227.59836864471436,\n",
       "   227.80657196044922,\n",
       "   226.12731170654297,\n",
       "   227.1364402770996,\n",
       "   225.20242309570312,\n",
       "   222.20655059814453,\n",
       "   226.04444026947021,\n",
       "   228.55530548095703,\n",
       "   227.30688858032227,\n",
       "   226.77917003631592,\n",
       "   227.9212131500244,\n",
       "   225.04513549804688,\n",
       "   225.46621704101562,\n",
       "   225.654052734375,\n",
       "   222.36891651153564,\n",
       "   222.67727851867676,\n",
       "   223.74510192871094,\n",
       "   227.95119094848633,\n",
       "   229.83408164978027,\n",
       "   230.6061019897461,\n",
       "   229.87121200561523,\n",
       "   230.72418785095215,\n",
       "   230.6960916519165,\n",
       "   229.93977165222168,\n",
       "   230.70498752593994,\n",
       "   228.691632270813,\n",
       "   227.68353462219238,\n",
       "   227.33691596984863,\n",
       "   226.86499786376953,\n",
       "   226.13545894622803,\n",
       "   227.92406177520752,\n",
       "   228.89461612701416,\n",
       "   228.2126111984253,\n",
       "   230.1816291809082,\n",
       "   229.0345573425293,\n",
       "   228.2216739654541,\n",
       "   228.11562252044678,\n",
       "   227.20036029815674,\n",
       "   225.47910690307617,\n",
       "   226.5618381500244,\n",
       "   222.80273628234863,\n",
       "   221.5585298538208,\n",
       "   222.50359630584717,\n",
       "   223.40799617767334,\n",
       "   222.29205131530762,\n",
       "   220.27187633514404,\n",
       "   226.23973274230957,\n",
       "   227.78910541534424,\n",
       "   224.80533695220947,\n",
       "   227.08287620544434,\n",
       "   229.93019676208496,\n",
       "   228.99524402618408],\n",
       "  'pce_r2': [-1.5117319718002018,\n",
       "   -1.2153308354620398,\n",
       "   -0.8049916615726673,\n",
       "   -0.4773450639177783,\n",
       "   -0.32648207721746125,\n",
       "   -0.08994867247965432,\n",
       "   0.005824277149417134,\n",
       "   0.11973897384055965,\n",
       "   0.17807465539764356,\n",
       "   0.18312893276162023,\n",
       "   0.17048251990376861,\n",
       "   0.12759394540115743,\n",
       "   0.02979200196588072,\n",
       "   -0.005644206639387361,\n",
       "   -0.11751200098986558,\n",
       "   -0.1996918310082032,\n",
       "   -0.3888928389049857,\n",
       "   -0.4125638491607566,\n",
       "   -0.49294377845808635,\n",
       "   -0.6455980540415183,\n",
       "   -0.5752271627228798,\n",
       "   -0.7044405005229579,\n",
       "   -0.8472106373747492,\n",
       "   -0.7968950187133328,\n",
       "   -0.8920506270202764,\n",
       "   -0.8086042781208373,\n",
       "   -0.8322385276928368,\n",
       "   -1.0152926455794198,\n",
       "   -1.1489415041187616,\n",
       "   -1.1536954520776588,\n",
       "   -1.3338195761349407,\n",
       "   -1.2604299104811574,\n",
       "   -1.0565322857193347,\n",
       "   -0.9566731359532641,\n",
       "   -1.1429814325424208,\n",
       "   -1.1975015048709539,\n",
       "   -1.2404180405264755,\n",
       "   -1.3328789070874354,\n",
       "   -1.312442798890351,\n",
       "   -1.4051285662004638,\n",
       "   -1.4393720894379518,\n",
       "   -1.4702753658983805,\n",
       "   -1.3748737309689263,\n",
       "   -1.3783515618063777,\n",
       "   -1.4767990340418247,\n",
       "   -1.5606510906887174,\n",
       "   -1.4662465252830592,\n",
       "   -1.5459742798909404,\n",
       "   -1.6283425102694662,\n",
       "   -1.5740408661994088,\n",
       "   -1.8155027368495755,\n",
       "   -1.689136461685535,\n",
       "   -1.798713892149438,\n",
       "   -1.9804035018335426,\n",
       "   -2.0534720269937616,\n",
       "   -2.1774591713923868,\n",
       "   -2.1513337624230227,\n",
       "   -2.1582325775792652,\n",
       "   -2.2721571279172093,\n",
       "   -2.1663325187249773,\n",
       "   -2.1542705780697924,\n",
       "   -2.235938345928609,\n",
       "   -2.342847097291103,\n",
       "   -2.0529911932054463,\n",
       "   -1.9947217894649758,\n",
       "   -2.2125972610765157,\n",
       "   -2.3156128885144587,\n",
       "   -2.2864084356596215,\n",
       "   -2.001073214059883,\n",
       "   -2.1500926106175546,\n",
       "   -2.097400740254275,\n",
       "   -2.347414774994586,\n",
       "   -2.6267802026687597,\n",
       "   -2.6258033490960924,\n",
       "   -2.3897978833092575,\n",
       "   -2.5142330939074906,\n",
       "   -2.6394977869779495,\n",
       "   -2.33066854822252,\n",
       "   -2.3188797083491104,\n",
       "   -2.4549387102630438,\n",
       "   -2.3572438123391986,\n",
       "   -2.428196351312326,\n",
       "   -2.402660777383465,\n",
       "   -2.3119117739603907,\n",
       "   -2.1524544673257235,\n",
       "   -2.107592016294947,\n",
       "   -2.007563734279379,\n",
       "   -1.9491076927191195,\n",
       "   -1.8182291467798715,\n",
       "   -1.8248810280846492,\n",
       "   -1.7240590180234667,\n",
       "   -1.8807692890857712,\n",
       "   -1.9308790834757588,\n",
       "   -1.8325909464350776,\n",
       "   -1.9927665991772865,\n",
       "   -1.9753279100058925,\n",
       "   -2.183510410453134,\n",
       "   -2.2906727180079445,\n",
       "   -2.2907187499541615,\n",
       "   -2.2471231588116005,\n",
       "   -2.180902348330672,\n",
       "   -2.0456976627404773,\n",
       "   -1.9704977499569707,\n",
       "   -2.1106357971327387,\n",
       "   -2.1431174786414857,\n",
       "   -1.9571578090654738,\n",
       "   -1.9614956998174287,\n",
       "   -1.8197544681839108,\n",
       "   -2.080138321478768,\n",
       "   -2.0446842197658706,\n",
       "   -2.031912253700978,\n",
       "   -2.0641346809458327,\n",
       "   -1.996428809041232,\n",
       "   -2.186805013452117,\n",
       "   -2.248489385592988,\n",
       "   -2.1919124211065526,\n",
       "   -2.0998181957845357,\n",
       "   -2.0674748449403313,\n",
       "   -1.8725134249401791,\n",
       "   -2.095379914864195,\n",
       "   -1.9584932342653656,\n",
       "   -2.042162939690826,\n",
       "   -1.944187659730361,\n",
       "   -2.0610372105784758,\n",
       "   -1.973782378803259,\n",
       "   -2.138590023064482,\n",
       "   -2.4164976804151066,\n",
       "   -2.800821856247237,\n",
       "   -2.7195409306351195,\n",
       "   -2.7774804863541247,\n",
       "   -2.651467176395419,\n",
       "   -2.4224482089386252,\n",
       "   -2.501581307660579,\n",
       "   -2.428886211215365,\n",
       "   -2.605575546881543,\n",
       "   -2.56660488887505,\n",
       "   -2.3156427439936014,\n",
       "   -2.519937072312221,\n",
       "   -2.287258558782714,\n",
       "   -2.006980235486975,\n",
       "   -2.1538287799802447,\n",
       "   -2.0232199551108687,\n",
       "   -1.7344684799425005,\n",
       "   -1.9108842829798354,\n",
       "   -1.9403496107196543,\n",
       "   -1.913271843924119,\n",
       "   -2.0992188101173923,\n",
       "   -1.9001663381931486,\n",
       "   -1.9651461354539603,\n",
       "   -1.788433276150501,\n",
       "   -1.6633023231093853,\n",
       "   -2.065106061624626,\n",
       "   -1.9568821385569404,\n",
       "   -1.9978295632172127,\n",
       "   -1.848564270156293,\n",
       "   -2.0235081135899033,\n",
       "   -2.0243790846988485,\n",
       "   -2.1068637599183755,\n",
       "   -2.1453515937690057,\n",
       "   -2.096992996774094,\n",
       "   -2.102871296227042,\n",
       "   -1.996911958658694,\n",
       "   -1.8113466515862378,\n",
       "   -1.7661673727829266,\n",
       "   -1.9299586204614076,\n",
       "   -1.8993884992225802,\n",
       "   -2.085673585488115,\n",
       "   -2.230898071389335,\n",
       "   -2.081574425985669,\n",
       "   -2.0514315228959528,\n",
       "   -2.2101877102589116,\n",
       "   -1.9975775499536081,\n",
       "   -2.1479112817570534,\n",
       "   -2.1290860748632783,\n",
       "   -2.0257706127892683,\n",
       "   -2.1716571708669985,\n",
       "   -2.1429497924451146,\n",
       "   -2.480469058407287,\n",
       "   -2.256769469722997,\n",
       "   -2.4689302499349037,\n",
       "   -2.371846155788166,\n",
       "   -2.5701019031760493,\n",
       "   -2.322270900381109,\n",
       "   -1.9840687809807518,\n",
       "   -2.12085102778873,\n",
       "   -1.9768542913805827,\n",
       "   -1.9217570468392418,\n",
       "   -2.030798030028635,\n",
       "   -1.8643285882876697,\n",
       "   -1.9190668296005913,\n",
       "   -1.852762145832612,\n",
       "   -1.9982154422204914,\n",
       "   -1.7785050595678085,\n",
       "   -1.8287127819017175,\n",
       "   -2.004248276160037,\n",
       "   -2.0952807805803397,\n",
       "   -1.8147010677307183,\n",
       "   -1.8356984421025175,\n",
       "   -2.0417893887037812,\n",
       "   -2.087972161978268,\n",
       "   -1.9281325014507247,\n",
       "   -2.078844106068304,\n",
       "   -1.928365723198913,\n",
       "   -1.7930600717481728,\n",
       "   -2.071776200661601,\n",
       "   -2.0773933586979587,\n",
       "   -2.083736187820594,\n",
       "   -2.242560536751714,\n",
       "   -2.2091000655400004,\n",
       "   -2.3622128194682226,\n",
       "   -2.4992405164640066,\n",
       "   -2.4223107484511033,\n",
       "   -2.375529405083864,\n",
       "   -2.3698714039333124,\n",
       "   -2.4439173415384547,\n",
       "   -2.200377082541075,\n",
       "   -2.2547545224907393,\n",
       "   -2.3631909186841003,\n",
       "   -2.3188158904220226,\n",
       "   -2.10485061722385,\n",
       "   -2.2341743756450607,\n",
       "   -2.2359605069508093,\n",
       "   -2.5775347159773188,\n",
       "   -2.5762348421562997,\n",
       "   -2.5375105871890526,\n",
       "   -2.6498659946687857,\n",
       "   -2.890494392236497,\n",
       "   -2.83925339680958,\n",
       "   -2.9279727607827795,\n",
       "   -2.509347287237934,\n",
       "   -2.4652313566114548,\n",
       "   -2.6429275147647266,\n",
       "   -2.8126019188967337,\n",
       "   -2.7442833499521417,\n",
       "   -2.9977726131849716,\n",
       "   -3.268498640285972,\n",
       "   -3.2053106136385088,\n",
       "   -3.4006450369307686,\n",
       "   -3.398005277311136,\n",
       "   -3.5000422111097533,\n",
       "   -3.266459126970717,\n",
       "   -3.4508910933267734,\n",
       "   -3.432248734049277,\n",
       "   -3.023716799410935,\n",
       "   -3.113820430343731,\n",
       "   -2.773475426843452,\n",
       "   -3.054335935592057,\n",
       "   -3.2762122312963378,\n",
       "   -3.091974375497027,\n",
       "   -3.14654567072981,\n",
       "   -3.2223045768685346,\n",
       "   -3.409853711206753,\n",
       "   -3.553057678170908,\n",
       "   -3.753350706661762,\n",
       "   -3.534993136596703,\n",
       "   -3.6600111087606324,\n",
       "   -3.294729319745244,\n",
       "   -3.4867143934490814,\n",
       "   -3.3450270194891223,\n",
       "   -3.5374123205935435,\n",
       "   -3.683164542772377,\n",
       "   -3.4013612376704856,\n",
       "   -3.319355963270638,\n",
       "   -3.092792247087943,\n",
       "   -2.9015507010922477,\n",
       "   -2.957777398601097,\n",
       "   -2.845364723580338,\n",
       "   -2.6091690919628383,\n",
       "   -2.862284172460519,\n",
       "   -2.8943120641871745,\n",
       "   -2.772937587167669,\n",
       "   -3.0802541148606677,\n",
       "   -2.9787106995106836,\n",
       "   -3.1172252702436793,\n",
       "   -3.302006318608992,\n",
       "   -3.1751245570379094,\n",
       "   -3.2511707386661417,\n",
       "   -3.3975342030748648,\n",
       "   -3.3624152098484776,\n",
       "   -3.2469232010942592,\n",
       "   -3.0520002771834625,\n",
       "   -2.966746255219086,\n",
       "   -2.95966571996053,\n",
       "   -2.7768681090121836,\n",
       "   -2.97506972576234,\n",
       "   -3.027052925292166,\n",
       "   -2.8045453221404117,\n",
       "   -2.7256378985784084,\n",
       "   -3.0362996558149806,\n",
       "   -3.3198782820851704,\n",
       "   -3.3139640286760503,\n",
       "   -3.4727201242032955,\n",
       "   -3.184734012057958,\n",
       "   -2.954774865550062,\n",
       "   -3.1561872295207403,\n",
       "   -3.4524930054090808,\n",
       "   -3.3429290498873794,\n",
       "   -3.2305370796893085,\n",
       "   -3.5481995374833897,\n",
       "   -3.6058837705170257,\n",
       "   -3.492945154217919,\n",
       "   -3.4638455934862264,\n",
       "   -3.0792058906162554,\n",
       "   -3.1786132228279973,\n",
       "   -2.9588618154386324,\n",
       "   -3.110309689382844,\n",
       "   -3.182481964320931,\n",
       "   -3.085563445476181,\n",
       "   -3.3408423085189938,\n",
       "   -3.3992893066061667,\n",
       "   -3.497465282040208,\n",
       "   -3.488898370429234,\n",
       "   -3.3870052538333058,\n",
       "   -3.2045297769482257,\n",
       "   -3.3509044194169135,\n",
       "   -3.244814922174294,\n",
       "   -3.139392916166451,\n",
       "   -2.8554214781721896,\n",
       "   -2.711674404162892,\n",
       "   -2.5944783154912403,\n",
       "   -2.6639244851677146,\n",
       "   -2.8683521178718334,\n",
       "   -2.818977400465517,\n",
       "   -2.6578402495863918,\n",
       "   -2.705918973028481,\n",
       "   -3.1595796234642863,\n",
       "   -2.842298954888622,\n",
       "   -3.280991276091327,\n",
       "   -3.283221081456084,\n",
       "   -3.4925259146767367,\n",
       "   -3.7065598812253535,\n",
       "   -3.6787073950663673,\n",
       "   -3.4973086611863256,\n",
       "   -3.2299641303817674,\n",
       "   -3.195201758877614,\n",
       "   -3.0571023262578896,\n",
       "   -3.27414100177068,\n",
       "   -3.2337828524671357,\n",
       "   -3.5430238655852895,\n",
       "   -3.2363585629264424,\n",
       "   -3.0901471714348894,\n",
       "   -3.219210339511461,\n",
       "   -3.577149210786745,\n",
       "   -3.7031080942310277,\n",
       "   -3.5073677160214025,\n",
       "   -3.589013826775476,\n",
       "   -3.46804240856625,\n",
       "   -3.079499714417679,\n",
       "   -3.072879287605832,\n",
       "   -2.9743830007078453,\n",
       "   -2.814727258618883,\n",
       "   -3.3002049042887807,\n",
       "   -3.19535825183414,\n",
       "   -3.106605289458229,\n",
       "   -3.2378732602646147,\n",
       "   -3.3718028652906904,\n",
       "   -3.6498034270570834,\n",
       "   -3.8205466767668366,\n",
       "   -3.775998461295787,\n",
       "   -3.488035725814627,\n",
       "   -3.388726209204486,\n",
       "   -3.3381398654798753,\n",
       "   -3.334209255438834,\n",
       "   -3.49118781303247,\n",
       "   -3.0924566410550467,\n",
       "   -3.2479908127666874,\n",
       "   -3.289856193564262,\n",
       "   -3.3322340381902986,\n",
       "   -3.49147665191395,\n",
       "   -3.5846398733981193,\n",
       "   -3.6448758847914497,\n",
       "   -3.707280231754652,\n",
       "   -3.2898493047054886,\n",
       "   -3.578855325552417,\n",
       "   -3.723918270755795,\n",
       "   -3.3173350614959407,\n",
       "   -3.4490576051823103,\n",
       "   -3.503539945619301,\n",
       "   -3.1073282305052423,\n",
       "   -3.2658308118290247,\n",
       "   -3.1656018589376886,\n",
       "   -3.1329940346757654,\n",
       "   -3.2342729860325212,\n",
       "   -3.369420875254545,\n",
       "   -3.0671121943073247,\n",
       "   -2.826190577337215,\n",
       "   -2.911312248681741,\n",
       "   -3.067886184858307,\n",
       "   -2.889029380525316,\n",
       "   -2.7971903827841627,\n",
       "   -3.397408346636932,\n",
       "   -3.1024694730163676,\n",
       "   -3.554489862207685,\n",
       "   -3.5200829961792817,\n",
       "   -3.0555210125690566,\n",
       "   -3.419347057113079,\n",
       "   -3.0058352177160614,\n",
       "   -2.7853574050410765,\n",
       "   -2.800385572367595,\n",
       "   -2.825835682918864,\n",
       "   -2.8332490810076947,\n",
       "   -3.159310589929417,\n",
       "   -3.2427339637493953,\n",
       "   -3.518355468757191,\n",
       "   -3.478693979601556,\n",
       "   -3.9188161250606823,\n",
       "   -4.023409163735644,\n",
       "   -4.091056205792214,\n",
       "   -3.970844869643515,\n",
       "   -4.057069259175354,\n",
       "   -4.12174749786203,\n",
       "   -4.091623946067301,\n",
       "   -4.141820984015381,\n",
       "   -4.156831099615924,\n",
       "   -4.049325792148143,\n",
       "   -3.8745325184095725,\n",
       "   -4.172817999718398,\n",
       "   -4.581567556628015,\n",
       "   -4.192371332049105,\n",
       "   -4.194007336411486,\n",
       "   -4.279121329187673,\n",
       "   -3.4424316860434274,\n",
       "   -3.674503067128499,\n",
       "   -3.6233089478262475,\n",
       "   -3.399703120019109,\n",
       "   -3.173915503130382,\n",
       "   -3.1410849545166126,\n",
       "   -3.2812591452467235,\n",
       "   -3.450105646964264,\n",
       "   -3.0338565996013536,\n",
       "   -3.4961947815279713,\n",
       "   -3.500816600735246,\n",
       "   -3.4644311093485793,\n",
       "   -3.5400061823490097,\n",
       "   -3.449775391690987,\n",
       "   -3.7476266399254907,\n",
       "   -3.7515680956440853,\n",
       "   -3.4230190691677223,\n",
       "   -3.331752714807328,\n",
       "   -3.8881340209335837,\n",
       "   -3.4964337608811658,\n",
       "   -3.5799804581202554,\n",
       "   -3.2480368130438073,\n",
       "   -3.135893904719304,\n",
       "   -3.5938120330646317,\n",
       "   -4.013808861581603,\n",
       "   -4.061274009751367,\n",
       "   -3.879274797019904,\n",
       "   -4.072453966134344,\n",
       "   -3.939527431463957,\n",
       "   -3.784076728076717,\n",
       "   -4.013653532107315,\n",
       "   -3.550799575884393,\n",
       "   -3.11182635550196,\n",
       "   -3.207682458069785,\n",
       "   -3.3892589786762075,\n",
       "   -3.302412221219612,\n",
       "   -3.3367373604290576,\n",
       "   -3.6781698488467285,\n",
       "   -3.32030819235072,\n",
       "   -3.384589328566916,\n",
       "   -3.599294252572081,\n",
       "   -3.212626492728397,\n",
       "   -3.453623781977793,\n",
       "   -3.9604438525744303,\n",
       "   -4.394092917615377,\n",
       "   -4.886799937326825,\n",
       "   -4.861260989377637,\n",
       "   -4.28053488982954,\n",
       "   -4.876926346424847,\n",
       "   -4.860932852155558,\n",
       "   -5.299360011781449,\n",
       "   -5.145491714712976,\n",
       "   -5.2681920760331415,\n",
       "   -4.910194872394013,\n",
       "   -4.512198062671556,\n",
       "   -4.255144534707053,\n",
       "   -3.9432304919952132,\n",
       "   -4.171004150969095,\n",
       "   -4.266098320792991,\n",
       "   -4.109396006309414,\n",
       "   -4.415949768043323,\n",
       "   -4.21557467501397,\n",
       "   -3.860549701517966,\n",
       "   -3.7648700320152795,\n",
       "   -3.772546024773158,\n",
       "   -3.4980966566648917,\n",
       "   -3.86784501035575,\n",
       "   -3.685364798190154,\n",
       "   -3.2531709634666663,\n",
       "   -3.2212142777581114,\n",
       "   -3.455038096567371,\n",
       "   -3.162606047913213,\n",
       "   -3.130471988003479,\n",
       "   -3.619592771243534,\n",
       "   -4.092793564537901,\n",
       "   -3.828037311434553,\n",
       "   -4.376478268903555,\n",
       "   -4.726042716566252,\n",
       "   -4.805613431619481],\n",
       "  'voc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -19390.561072154273,\n",
       "   -119.74497657985167,\n",
       "   -38.01489187986265,\n",
       "   -18.974460237607413,\n",
       "   -11.518426336147897,\n",
       "   -6.945870313273747,\n",
       "   -4.96460477309495,\n",
       "   -3.8259427161990356,\n",
       "   -2.8641940383120335,\n",
       "   -2.3256803844107217,\n",
       "   -1.6766996962767964,\n",
       "   -1.3104084669645952,\n",
       "   -1.017750463408826,\n",
       "   -0.9159901711680241,\n",
       "   -0.8364823117403697,\n",
       "   -0.6855447221440856,\n",
       "   -0.7611335375505353,\n",
       "   -0.629998372368106,\n",
       "   -0.5584600069692836,\n",
       "   -0.4828185899488808,\n",
       "   -0.44255676300965097,\n",
       "   -0.3993832403025588,\n",
       "   -0.3835063335624689,\n",
       "   -0.35813253022466074,\n",
       "   -0.2871816670657712,\n",
       "   -0.28722791910346634,\n",
       "   -0.2595295372291764,\n",
       "   -0.26201362547458573,\n",
       "   -0.2281241205925293,\n",
       "   -0.2033643928690354,\n",
       "   -0.1997618409786761,\n",
       "   -0.1767052364694366,\n",
       "   -0.18745623095519992,\n",
       "   -0.18154848762504505,\n",
       "   -0.14681391452380188,\n",
       "   -0.1120785957582715,\n",
       "   -0.1036375754251595,\n",
       "   -0.12656960966131026,\n",
       "   -0.14407226083692648,\n",
       "   -0.13739428693678457,\n",
       "   -0.13538833277029583,\n",
       "   -0.1302689775792234,\n",
       "   -0.1347347776346235,\n",
       "   -0.13505806200976744,\n",
       "   -0.13700391829948577,\n",
       "   -0.11888648254142598,\n",
       "   -0.1253722588416879,\n",
       "   -0.13085534455194336,\n",
       "   -0.1491172877321636,\n",
       "   -0.11736274671222846,\n",
       "   -0.1085442347728729,\n",
       "   -0.1221619438030066,\n",
       "   -0.0985858101078414,\n",
       "   -0.10598701531044474,\n",
       "   -0.0899791089401012,\n",
       "   -0.08276890741249043,\n",
       "   -0.07252544124714566,\n",
       "   -0.07477168731645167,\n",
       "   -0.08995766383117987,\n",
       "   -0.07690974194078248,\n",
       "   -0.08483534745851307,\n",
       "   -0.0731400379987941,\n",
       "   -0.07141550775276606,\n",
       "   -0.07514603360255268,\n",
       "   -0.08061580118200462,\n",
       "   -0.08568741804978819,\n",
       "   -0.07335414637914206,\n",
       "   -0.09108413629138634,\n",
       "   -0.09961047428103775,\n",
       "   -0.12278535128194568,\n",
       "   -0.08126312237039057,\n",
       "   -0.08840162970934329,\n",
       "   -0.0791730550445986,\n",
       "   -0.07874138216100213,\n",
       "   -0.07270639986421346,\n",
       "   -0.07820842392794014,\n",
       "   -0.0739733474523061,\n",
       "   -0.07662177100241419,\n",
       "   -0.10086226452852443,\n",
       "   -0.10967265980115126,\n",
       "   -0.10285937033773807,\n",
       "   -0.1373132406253239,\n",
       "   -0.14898343439276918,\n",
       "   -0.20073202646713684,\n",
       "   -0.14649369166539206,\n",
       "   -0.14652112834850084,\n",
       "   -0.13589898964106362,\n",
       "   -0.14294830499751687,\n",
       "   -0.14351927319537228,\n",
       "   -0.10999677733078927,\n",
       "   -0.0959482048379563,\n",
       "   -0.137527934806994,\n",
       "   -0.11712321016470928,\n",
       "   -0.13996476827245052,\n",
       "   -0.13380598043011638,\n",
       "   -0.16912996869679975,\n",
       "   -0.20667034445297316,\n",
       "   -0.2853311566182881,\n",
       "   -0.3773585628227205,\n",
       "   -0.3529271162334222,\n",
       "   -0.38739788331366665,\n",
       "   -0.5488857100507367,\n",
       "   -0.4778694242914334,\n",
       "   -0.4648358718313814,\n",
       "   -0.46901400621821177,\n",
       "   -0.5228147976931872,\n",
       "   -0.5112773874363781,\n",
       "   -0.4023774066620338,\n",
       "   -0.3889918228178213,\n",
       "   -0.5175839818397658,\n",
       "   -0.3980284827101781,\n",
       "   -0.41001318749017956,\n",
       "   -0.40558585018366244,\n",
       "   -0.45346935038313485,\n",
       "   -0.48450149970719636,\n",
       "   -0.5535451644671356,\n",
       "   -0.530772312013712,\n",
       "   -0.46177603612118867,\n",
       "   -0.470054464512053,\n",
       "   -0.5071748095556834,\n",
       "   -0.5534631174817115,\n",
       "   -0.6529788537026631,\n",
       "   -0.6560535358570243,\n",
       "   -0.89584065721627,\n",
       "   -0.7806214119993411,\n",
       "   -0.8528182664888782,\n",
       "   -1.0968273964085324,\n",
       "   -0.8455934676888517,\n",
       "   -0.7564425135578094,\n",
       "   -0.8565194680731927,\n",
       "   -0.7739669117842942,\n",
       "   -0.8197549160100255,\n",
       "   -0.821765954222035,\n",
       "   -0.9482081804487419,\n",
       "   -1.0429024892574659,\n",
       "   -1.096163079533948,\n",
       "   -1.5525599924847344,\n",
       "   -1.5662967693330518,\n",
       "   -1.3627660053586763,\n",
       "   -1.6661962895855371,\n",
       "   -1.4568603644409932,\n",
       "   -1.4848840342987444,\n",
       "   -1.5748112232663862,\n",
       "   -1.4821680713558045,\n",
       "   -1.4670185724123832,\n",
       "   -1.5133654749209149,\n",
       "   -1.6653695997515094,\n",
       "   -2.0420673515664536,\n",
       "   -1.6209678486251895,\n",
       "   -1.475647501813825,\n",
       "   -1.4888664365162891,\n",
       "   -1.7448802616279302,\n",
       "   -1.3674490259984369,\n",
       "   -1.307300591344792,\n",
       "   -1.133800383178178,\n",
       "   -1.1934719204454884,\n",
       "   -1.1391662761058856,\n",
       "   -1.2687066064197756,\n",
       "   -1.1990106046244309,\n",
       "   -1.0783460223929295,\n",
       "   -1.1929107394325165,\n",
       "   -1.3398179128388565,\n",
       "   -1.5785270555181303,\n",
       "   -1.6274464443842307,\n",
       "   -1.6065686241244692,\n",
       "   -1.7179725165091808,\n",
       "   -1.7025119236769832,\n",
       "   -1.9100409415929098,\n",
       "   -1.7161327701670084,\n",
       "   -1.7716324267188646,\n",
       "   -1.6382509523733897,\n",
       "   -1.6238926747480051,\n",
       "   -1.4065443867375338,\n",
       "   -1.41340219821718,\n",
       "   -1.463071879169676,\n",
       "   -1.380434965811872,\n",
       "   -1.5694628492667162,\n",
       "   -1.5571349034261646,\n",
       "   -1.4046176965893058,\n",
       "   -1.9634632811656378,\n",
       "   -1.9338764769104326,\n",
       "   -2.0343181331486737,\n",
       "   -2.1192936532213063,\n",
       "   -2.0360283541850452,\n",
       "   -1.9668736196778887,\n",
       "   -2.1527267339842657,\n",
       "   -1.8910330571456484,\n",
       "   -1.7516764542135679,\n",
       "   -2.0442290309256057,\n",
       "   -1.5668713130416996,\n",
       "   -1.616535616418036,\n",
       "   -1.8155844447564875,\n",
       "   -1.24320461990319,\n",
       "   -1.0925975737788454,\n",
       "   -1.0559353168606815,\n",
       "   -1.0995972065059467,\n",
       "   -1.029192739991848,\n",
       "   -1.0508122440321324,\n",
       "   -1.2835070713121337,\n",
       "   -1.3754479574474776,\n",
       "   -1.3118077265634973,\n",
       "   -1.4463553289848567,\n",
       "   -1.2278532540858418,\n",
       "   -1.2710496456282354,\n",
       "   -1.4366573859071465,\n",
       "   -1.329797023028687,\n",
       "   -1.3155975088406677,\n",
       "   -1.225436690296895,\n",
       "   -1.1465065827499266,\n",
       "   -1.0802110395765365,\n",
       "   -0.8662329781116744,\n",
       "   -0.8466787319706943,\n",
       "   -0.8446730194503873,\n",
       "   -0.7311444206845676,\n",
       "   -0.6097617141733807,\n",
       "   -0.7606172567682781,\n",
       "   -0.6504284765637962,\n",
       "   -0.6390805171503084,\n",
       "   -0.6697557841492074,\n",
       "   -0.7346798674687527,\n",
       "   -0.6877114005117702,\n",
       "   -0.7464057510717568,\n",
       "   -0.7525601569910958,\n",
       "   -0.7450141102240675,\n",
       "   -0.7213664328675118,\n",
       "   -0.6768881098723083,\n",
       "   -0.6221141834266852,\n",
       "   -0.7300910032770365,\n",
       "   -0.658404609072696,\n",
       "   -0.6201690215188114,\n",
       "   -0.7365561589782765,\n",
       "   -0.6899970457405709,\n",
       "   -0.6862191757909903,\n",
       "   -0.6715025568298978,\n",
       "   -0.6749070775040364,\n",
       "   -0.7955063236276152,\n",
       "   -0.7811972422735303,\n",
       "   -0.9515024090563518,\n",
       "   -1.000242599166854,\n",
       "   -0.865962087827598,\n",
       "   -0.8231462522663688,\n",
       "   -0.8879418119149678,\n",
       "   -1.024099254513808,\n",
       "   -1.0481488308790063,\n",
       "   -1.1356631723653323,\n",
       "   -1.1830652220484263,\n",
       "   -1.0299181222348843,\n",
       "   -1.0269188116771786,\n",
       "   -0.8889093913165089,\n",
       "   -1.0520228198210435,\n",
       "   -1.092375929418354,\n",
       "   -0.9273424224874989,\n",
       "   -0.8399255823453275,\n",
       "   -0.8135416967107816,\n",
       "   -0.9850173265320643,\n",
       "   -1.0680005967333739,\n",
       "   -0.9191821338039581,\n",
       "   -1.0736427830597313,\n",
       "   -1.1230043187299104,\n",
       "   -1.1873216241276414,\n",
       "   -1.2176631358036611,\n",
       "   -1.176928864374256,\n",
       "   -1.3115381392679395,\n",
       "   -1.2804797291843122,\n",
       "   -1.4789904196378592,\n",
       "   -1.6444353305192028,\n",
       "   -1.6176532325192308,\n",
       "   -1.5207750883117068,\n",
       "   -1.7028839241629519,\n",
       "   -1.7302467715119114,\n",
       "   -1.582756336093254,\n",
       "   -1.4013055946730524,\n",
       "   -1.4871373680241007,\n",
       "   -1.4388191219325028,\n",
       "   -1.541554135361543,\n",
       "   -1.4511838924236309,\n",
       "   -1.2843028164125037,\n",
       "   -1.0953641523995752,\n",
       "   -1.1431700062812378,\n",
       "   -1.0516781325596725,\n",
       "   -0.9313558355500486,\n",
       "   -0.8169303039394915,\n",
       "   -0.8418320311343286,\n",
       "   -0.8394991529090463,\n",
       "   -0.7891082164248582,\n",
       "   -0.6278024462018494,\n",
       "   -0.6661017809052099,\n",
       "   -0.7024960342900837,\n",
       "   -0.4312843489291349,\n",
       "   -0.39214448663402757,\n",
       "   -0.39230288828462667,\n",
       "   -0.532567844923121,\n",
       "   -0.4568447515001357,\n",
       "   -0.33670555752356135,\n",
       "   -0.3657835494756789,\n",
       "   -0.3724253545968854,\n",
       "   -0.4106913925048623,\n",
       "   -0.43602669890068113,\n",
       "   -0.45895383852802696,\n",
       "   -0.4935087001262559,\n",
       "   -0.5418049309357267,\n",
       "   -0.5281970950165724,\n",
       "   -0.5605306541231079,\n",
       "   -0.5444157480737166,\n",
       "   -0.5057315861238396,\n",
       "   -0.6253799044488466,\n",
       "   -0.5645868716444058,\n",
       "   -0.5002965788292526,\n",
       "   -0.3771309607857709,\n",
       "   -0.4070300253088963,\n",
       "   -0.41515325697860495,\n",
       "   -0.4179093521633017,\n",
       "   -0.4896819941834125,\n",
       "   -0.447988444899883,\n",
       "   -0.46672458446799747,\n",
       "   -0.46882746302952727,\n",
       "   -0.48748352063727696,\n",
       "   -0.6049862438781293,\n",
       "   -0.6558750059060154,\n",
       "   -0.6216076698985893,\n",
       "   -0.6508087243234904,\n",
       "   -0.6346420992180921,\n",
       "   -0.7484502191366895,\n",
       "   -0.768298041336481,\n",
       "   -0.6792994231408336,\n",
       "   -0.5543370115584434,\n",
       "   -0.7170882481345453,\n",
       "   -0.7063476515005984,\n",
       "   -0.6647012869260358,\n",
       "   -0.7553573592086837,\n",
       "   -0.6725346651498496,\n",
       "   -0.7221782036330806,\n",
       "   -0.653997812459215,\n",
       "   -0.6633479440028254,\n",
       "   -0.7658764716183666,\n",
       "   -0.7994746663964825,\n",
       "   -0.6800625815318575,\n",
       "   -0.6708256782866198,\n",
       "   -0.6632923300927487,\n",
       "   -0.6379536282575482,\n",
       "   -0.6192748290971879,\n",
       "   -0.5077914219786619,\n",
       "   -0.5680822378728643,\n",
       "   -0.588683520005056,\n",
       "   -0.5515514020095102,\n",
       "   -0.560834771875778,\n",
       "   -0.5430429461943078,\n",
       "   -0.4963626332624107,\n",
       "   -0.4728412348431281,\n",
       "   -0.5566288425900832,\n",
       "   -0.6138202969907061,\n",
       "   -0.6191842399968555,\n",
       "   -0.6464161002288775,\n",
       "   -0.5972010817833853,\n",
       "   -0.6614726890448719,\n",
       "   -0.6358991030435317,\n",
       "   -0.6293183673336313,\n",
       "   -0.4846964032418579,\n",
       "   -0.4457761443595536,\n",
       "   -0.39304651788754796,\n",
       "   -0.3862989131405321,\n",
       "   -0.4206859800641276,\n",
       "   -0.4029418284398587,\n",
       "   -0.4087822421241729,\n",
       "   -0.45067490437739255,\n",
       "   -0.40219831282664864,\n",
       "   -0.4143462474595043,\n",
       "   -0.46210408545310466,\n",
       "   -0.41294661534823174,\n",
       "   -0.47880218865574276,\n",
       "   -0.5017500520436644,\n",
       "   -0.4458953408288815,\n",
       "   -0.5557608193178201,\n",
       "   -0.5205294017572792,\n",
       "   -0.5964941459787243,\n",
       "   -0.5544561128547609,\n",
       "   -0.6602196707071335,\n",
       "   -0.6040442822557444,\n",
       "   -0.6128336566146757,\n",
       "   -0.5844166981204626,\n",
       "   -0.6263864496099281,\n",
       "   -0.5102029696801875,\n",
       "   -0.4833225916695829,\n",
       "   -0.5049384652964841,\n",
       "   -0.49042810969807427,\n",
       "   -0.39981339122687687,\n",
       "   -0.45976363391073427,\n",
       "   -0.4146715094986997,\n",
       "   -0.41546385805960817,\n",
       "   -0.31050306111646786,\n",
       "   -0.26988579104618227,\n",
       "   -0.2887564904128619,\n",
       "   -0.3247204790686553,\n",
       "   -0.3355617007603604,\n",
       "   -0.3313686705110406,\n",
       "   -0.33821849043525765,\n",
       "   -0.3245584068086489,\n",
       "   -0.3425118751586975,\n",
       "   -0.3178454120234637,\n",
       "   -0.40891908362751583,\n",
       "   -0.4231494789196131,\n",
       "   -0.4083654114389228,\n",
       "   -0.49809415583692096,\n",
       "   -0.43529429614725124,\n",
       "   -0.42161055082793397,\n",
       "   -0.42935888061313743,\n",
       "   -0.4116763818341016,\n",
       "   -0.44593617607665426,\n",
       "   -0.4027273721667506,\n",
       "   -0.39670886706600506,\n",
       "   -0.4299040325233656,\n",
       "   -0.4488577226097594,\n",
       "   -0.5151595955521471,\n",
       "   -0.5012445710394069,\n",
       "   -0.4558803937614373,\n",
       "   -0.44158704588314257,\n",
       "   -0.39069784908031746,\n",
       "   -0.35890259242419753,\n",
       "   -0.38613349065945934,\n",
       "   -0.4161500937150213,\n",
       "   -0.4094652935467915,\n",
       "   -0.39437240240001525,\n",
       "   -0.33591788529765365,\n",
       "   -0.31983644691013513,\n",
       "   -0.34614540073036726,\n",
       "   -0.3638579858520128,\n",
       "   -0.28225470348107273,\n",
       "   -0.2999815771090606,\n",
       "   -0.2572752997607468,\n",
       "   -0.22868634054312897,\n",
       "   -0.2532333868450374,\n",
       "   -0.29639280827098213,\n",
       "   -0.4024359228398626,\n",
       "   -0.35569952814232586,\n",
       "   -0.28774652921841404,\n",
       "   -0.31543575401989665,\n",
       "   -0.32263749839304223,\n",
       "   -0.35134690905911703,\n",
       "   -0.3302995095247625,\n",
       "   -0.3269860422527968,\n",
       "   -0.27621009451422185,\n",
       "   -0.27630891230676835,\n",
       "   -0.34399938093845916,\n",
       "   -0.3906303003533509,\n",
       "   -0.39531625920865676,\n",
       "   -0.4038682511215468,\n",
       "   -0.3575117980978424,\n",
       "   -0.3039251440674948,\n",
       "   -0.255252131701736,\n",
       "   -0.23286783430124136,\n",
       "   -0.24055547597123428,\n",
       "   -0.2359013338658984,\n",
       "   -0.2822013278859923,\n",
       "   -0.35661174253487493,\n",
       "   -0.3238470875827477,\n",
       "   -0.35213173224440086,\n",
       "   -0.46652474597543003,\n",
       "   -0.43098828623388274,\n",
       "   -0.4119760374921133,\n",
       "   -0.3417943155683705,\n",
       "   -0.3153857376750555,\n",
       "   -0.3194231844302855,\n",
       "   -0.2544372485901494,\n",
       "   -0.22847955377295537,\n",
       "   -0.2412591155570476,\n",
       "   -0.19375241796770037,\n",
       "   -0.20721459710622692,\n",
       "   -0.18925954943982815,\n",
       "   -0.19114339004429293,\n",
       "   -0.2470903426757345,\n",
       "   -0.1594984802925976,\n",
       "   -0.18890316492400383,\n",
       "   -0.18160054909118206,\n",
       "   -0.2496788460412882,\n",
       "   -0.24816201869895305,\n",
       "   -0.3014400476650183,\n",
       "   -0.35773655232664403,\n",
       "   -0.3966022560677489,\n",
       "   -0.38583288200956734,\n",
       "   -0.4325316995078803,\n",
       "   -0.43889138457167043,\n",
       "   -0.4155156868156391,\n",
       "   -0.3305858907780337,\n",
       "   -0.33920655783686504,\n",
       "   -0.42688721105122074,\n",
       "   -0.4376653277152456,\n",
       "   -0.5155313977186791,\n",
       "   -0.5360496894248563,\n",
       "   -0.5495938511934371],\n",
       "  'jsc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -373.92927966062905,\n",
       "   -97.68054464143106,\n",
       "   -45.647119675461866,\n",
       "   -23.38972130183134,\n",
       "   -21.36591479293267,\n",
       "   -18.47760034967983,\n",
       "   -16.469213399879198,\n",
       "   -15.085878030913427,\n",
       "   -14.36341090037258,\n",
       "   -13.067659865714445,\n",
       "   -11.859622069085614,\n",
       "   -11.440505163326636,\n",
       "   -10.051663127432798,\n",
       "   -8.941241390113898,\n",
       "   -7.9482522594263045,\n",
       "   -7.350019235479397,\n",
       "   -6.2001769233871755,\n",
       "   -5.656896611177158,\n",
       "   -5.8083544758584456,\n",
       "   -5.75189486614383,\n",
       "   -5.812285551086216,\n",
       "   -5.692393839566057,\n",
       "   -5.909455453190029,\n",
       "   -5.332286672191072,\n",
       "   -5.089299543966273,\n",
       "   -5.002764357601243,\n",
       "   -4.835921957139974,\n",
       "   -4.3201530220373465,\n",
       "   -4.364645433765938,\n",
       "   -4.1897483168318175,\n",
       "   -4.519483797806146,\n",
       "   -4.4037303978008175,\n",
       "   -4.020130969052428,\n",
       "   -4.194046143890325,\n",
       "   -3.7291506281320554,\n",
       "   -3.7386019055873385,\n",
       "   -3.766497668283783,\n",
       "   -3.5481263822328435,\n",
       "   -3.3739700935854033,\n",
       "   -3.1510572505067573,\n",
       "   -3.0672859202502556,\n",
       "   -3.1531869171212428,\n",
       "   -3.3356217091511127,\n",
       "   -3.50082839582108,\n",
       "   -3.677122905542581,\n",
       "   -3.544097270494123,\n",
       "   -3.8232492686930106,\n",
       "   -3.90910376800941,\n",
       "   -3.764985580391884,\n",
       "   -3.6931739885881667,\n",
       "   -3.785618308268492,\n",
       "   -3.9875394469727574,\n",
       "   -3.6315121681388494,\n",
       "   -3.433534693273468,\n",
       "   -3.759363241484585,\n",
       "   -3.60288314782375,\n",
       "   -3.8664268739100622,\n",
       "   -3.5094526683737612,\n",
       "   -3.534533152792501,\n",
       "   -3.460566539224039,\n",
       "   -3.52097472685979,\n",
       "   -3.459964412957717,\n",
       "   -3.443262120360478,\n",
       "   -3.3482868664527574,\n",
       "   -3.4177697419363913,\n",
       "   -3.8531553371397944,\n",
       "   -3.374596045932443,\n",
       "   -3.167820761842421,\n",
       "   -3.3707584322690636,\n",
       "   -3.541619661632521,\n",
       "   -3.424346785163876,\n",
       "   -3.17732690745704,\n",
       "   -3.1686890098047806,\n",
       "   -3.1900234339047104,\n",
       "   -3.676738932005401,\n",
       "   -3.8288010660565597,\n",
       "   -4.024391107757449,\n",
       "   -3.883012599016322,\n",
       "   -4.194065001124024,\n",
       "   -4.070212281379573,\n",
       "   -4.133067158868436,\n",
       "   -4.158770137453523,\n",
       "   -4.358935608991666,\n",
       "   -4.129243074062973,\n",
       "   -4.005960399695935,\n",
       "   -3.8263907408123217,\n",
       "   -3.9706731105111253,\n",
       "   -3.9286351616444657,\n",
       "   -4.219556323670535,\n",
       "   -4.678088415631011,\n",
       "   -4.409407024268891,\n",
       "   -4.26835697788415,\n",
       "   -4.21314396832626,\n",
       "   -4.233115339326431,\n",
       "   -3.9772114369634455,\n",
       "   -4.234919231086582,\n",
       "   -4.793987315800404,\n",
       "   -5.403335360157629,\n",
       "   -5.101573223673503,\n",
       "   -5.265014424351752,\n",
       "   -4.8078391562820215,\n",
       "   -4.737263336266734,\n",
       "   -4.875865956085085,\n",
       "   -5.303099057907233,\n",
       "   -5.1785574356373125,\n",
       "   -5.090754899301454,\n",
       "   -4.575814347611923,\n",
       "   -4.056973173185635,\n",
       "   -3.809655414134748,\n",
       "   -3.542043236495771,\n",
       "   -3.960843265080155,\n",
       "   -3.8868931209736797,\n",
       "   -4.068633458905446,\n",
       "   -3.7762610460015393,\n",
       "   -3.7845661026528994,\n",
       "   -3.8162861280665012,\n",
       "   -3.882588838679852,\n",
       "   -3.9857379236485944,\n",
       "   -4.070893413590349,\n",
       "   -4.154099548086087,\n",
       "   -3.9906261150516142,\n",
       "   -4.293305904478943,\n",
       "   -4.344776120488131,\n",
       "   -3.8622911259433614,\n",
       "   -3.48319295035001,\n",
       "   -3.4779710557063668,\n",
       "   -3.6774874521837013,\n",
       "   -4.074893064588398,\n",
       "   -4.632447395944379,\n",
       "   -4.782125492629484,\n",
       "   -4.6301695563878145,\n",
       "   -4.075438049111212,\n",
       "   -3.6666587918906783,\n",
       "   -3.914872272239303,\n",
       "   -3.7623512791000326,\n",
       "   -4.1356780076383375,\n",
       "   -4.134408997673313,\n",
       "   -4.503584139127129,\n",
       "   -4.333815827019529,\n",
       "   -4.173512016955435,\n",
       "   -4.143829518174884,\n",
       "   -4.245423640002158,\n",
       "   -4.148521638462348,\n",
       "   -3.9832437939574215,\n",
       "   -4.302092906751062,\n",
       "   -4.344895955845916,\n",
       "   -4.564240707304132,\n",
       "   -4.599933096389408,\n",
       "   -4.790372397474802,\n",
       "   -4.726481399253783,\n",
       "   -4.890943291095454,\n",
       "   -4.990619473265611,\n",
       "   -4.6428597045592,\n",
       "   -4.875096089680288,\n",
       "   -4.628076881344736,\n",
       "   -4.36937063874744,\n",
       "   -4.78279664796552,\n",
       "   -4.039774329561901,\n",
       "   -4.300692285962033,\n",
       "   -4.671840414909524,\n",
       "   -4.448566260717523,\n",
       "   -4.303048352932191,\n",
       "   -4.099221918145992,\n",
       "   -3.892857483258724,\n",
       "   -4.287186382694115,\n",
       "   -4.339753783807305,\n",
       "   -4.457891207863303,\n",
       "   -4.075820559447797,\n",
       "   -4.608760773448994,\n",
       "   -4.893154154422365,\n",
       "   -4.744183775203285,\n",
       "   -4.568565758518408,\n",
       "   -4.456412709720177,\n",
       "   -4.6267272237125345,\n",
       "   -4.511234777498088,\n",
       "   -4.482217937633538,\n",
       "   -4.455167875139104,\n",
       "   -5.090516542101813,\n",
       "   -4.783531106455202,\n",
       "   -4.618796989913365,\n",
       "   -4.383985218780907,\n",
       "   -4.529024776634872,\n",
       "   -4.519956539010954,\n",
       "   -5.406714643901697,\n",
       "   -5.559812324102626,\n",
       "   -6.001827160338415,\n",
       "   -5.9697149122424555,\n",
       "   -5.371344479320121,\n",
       "   -5.3481771922793815,\n",
       "   -5.073668845574453,\n",
       "   -5.208915598430963,\n",
       "   -5.019685490017001,\n",
       "   -5.505025437402421,\n",
       "   -5.538591911850819,\n",
       "   -5.29329548589829,\n",
       "   -4.813192901122849,\n",
       "   -4.801966794371215,\n",
       "   -4.625393036536528,\n",
       "   -4.178047378747947,\n",
       "   -3.5797828618881224,\n",
       "   -3.7339871446227093,\n",
       "   -3.3753209061824547,\n",
       "   -3.4814103715670335,\n",
       "   -3.666239623598857,\n",
       "   -3.629307852008025,\n",
       "   -3.8928588540616493,\n",
       "   -3.956065583547443,\n",
       "   -3.6351955597771077,\n",
       "   -3.3357364330032233,\n",
       "   -3.550190199232534,\n",
       "   -4.080226838456858,\n",
       "   -4.325508159786957,\n",
       "   -4.167132822996238,\n",
       "   -3.9482327230388634,\n",
       "   -4.308359500022666,\n",
       "   -4.399138558988092,\n",
       "   -4.359915944420683,\n",
       "   -4.652464469725002,\n",
       "   -4.468142858346247,\n",
       "   -4.484984089621173,\n",
       "   -4.06873699481286,\n",
       "   -4.139447561062497,\n",
       "   -4.06195161485867,\n",
       "   -4.48889186113438,\n",
       "   -4.546366063318608,\n",
       "   -4.349788695472019,\n",
       "   -4.335592046621012,\n",
       "   -4.676695034284543,\n",
       "   -4.618985374077808,\n",
       "   -4.480130792711207,\n",
       "   -4.4782222683928525,\n",
       "   -4.492724025858404,\n",
       "   -4.14566550879201,\n",
       "   -3.835157578438082,\n",
       "   -3.9618292575692555,\n",
       "   -3.8424612407948358,\n",
       "   -3.777862007213237,\n",
       "   -3.698895066765248,\n",
       "   -4.181676462561854,\n",
       "   -4.219403128653316,\n",
       "   -4.158360199810688,\n",
       "   -4.504036735042005,\n",
       "   -4.3352566628623155,\n",
       "   -4.0361998347066645,\n",
       "   -4.0323675284255485,\n",
       "   -4.120469163695288,\n",
       "   -3.96429356519309,\n",
       "   -3.4925046029257327,\n",
       "   -3.50039224512033,\n",
       "   -3.496989444119687,\n",
       "   -3.7362839836545785,\n",
       "   -4.225470352890804,\n",
       "   -4.352145904669579,\n",
       "   -4.431300286849194,\n",
       "   -4.041457516549128,\n",
       "   -3.8124326868862326,\n",
       "   -4.549965483652925,\n",
       "   -4.6581254339729625,\n",
       "   -4.438046967591775,\n",
       "   -4.544518675599499,\n",
       "   -4.645774620533636,\n",
       "   -5.060109193064524,\n",
       "   -4.929812484584821,\n",
       "   -4.655479465716375,\n",
       "   -4.606613735388375,\n",
       "   -4.457112288572001,\n",
       "   -4.379907161411785,\n",
       "   -4.029625466543364,\n",
       "   -3.6737405530343787,\n",
       "   -3.9756845172750426,\n",
       "   -3.923119946331589,\n",
       "   -3.8316686934555637,\n",
       "   -3.8115107150612912,\n",
       "   -3.858416701779957,\n",
       "   -3.542684292017535,\n",
       "   -3.619286959603037,\n",
       "   -3.9490075148707113,\n",
       "   -3.760790996831382,\n",
       "   -3.7231339347266337,\n",
       "   -3.8070604459317208,\n",
       "   -3.6926529212828987,\n",
       "   -3.812824079952483,\n",
       "   -3.916568150045168,\n",
       "   -4.298731036024583,\n",
       "   -4.382366169796453,\n",
       "   -4.399588500266245,\n",
       "   -4.076687641869798,\n",
       "   -4.38938248370764,\n",
       "   -4.227850458647944,\n",
       "   -3.6865358498038407,\n",
       "   -4.292036561936393,\n",
       "   -4.466377266091585,\n",
       "   -4.958006418186468,\n",
       "   -4.655776828546906,\n",
       "   -4.8281515551364045,\n",
       "   -4.826107465850653,\n",
       "   -4.676784351333278,\n",
       "   -4.559268243807156,\n",
       "   -4.269819525867964,\n",
       "   -4.688776800071427,\n",
       "   -4.448489716894736,\n",
       "   -4.059092877271705,\n",
       "   -4.444705231687178,\n",
       "   -4.571464566104775,\n",
       "   -4.298845252923716,\n",
       "   -3.9261160513780426,\n",
       "   -3.850104204117513,\n",
       "   -3.8837170563671393,\n",
       "   -3.6815050995728473,\n",
       "   -4.094519108300789,\n",
       "   -4.326372372060637,\n",
       "   -4.702703358937873,\n",
       "   -4.259295849843443,\n",
       "   -4.18462315828005,\n",
       "   -4.310785541339383,\n",
       "   -4.721078038720128,\n",
       "   -4.767944043106961,\n",
       "   -5.006644952667739,\n",
       "   -5.453303734378619,\n",
       "   -5.165646538589918,\n",
       "   -4.619485092488417,\n",
       "   -4.382381787665628,\n",
       "   -4.298650872325155,\n",
       "   -4.55641249075625,\n",
       "   -4.588086676092162,\n",
       "   -4.831054694927568,\n",
       "   -4.459181077601095,\n",
       "   -4.591899527929292,\n",
       "   -4.104409446484138,\n",
       "   -4.7428763080145675,\n",
       "   -4.9508963469146385,\n",
       "   -5.0150533816672045,\n",
       "   -4.812643529691837,\n",
       "   -4.561734695042934,\n",
       "   -4.713640573990807,\n",
       "   -5.0993542257806075,\n",
       "   -6.014047715959318,\n",
       "   -6.219405179724948,\n",
       "   -6.394344883152898,\n",
       "   -5.945458413444819,\n",
       "   -5.839594835570264,\n",
       "   -5.341637253952625,\n",
       "   -4.7326899377977645,\n",
       "   -4.736331069526207,\n",
       "   -5.005096196660799,\n",
       "   -4.830362171495575,\n",
       "   -4.621365897124985,\n",
       "   -4.616763231174009,\n",
       "   -4.896604020951127,\n",
       "   -4.80072729014034,\n",
       "   -5.148003868224438,\n",
       "   -5.6069880546317465,\n",
       "   -5.126604490122364,\n",
       "   -4.623847952922073,\n",
       "   -4.5616519584353385,\n",
       "   -4.357004085893027,\n",
       "   -4.5803969352894525,\n",
       "   -4.8141419262439475,\n",
       "   -4.266129253052109,\n",
       "   -4.06156620821999,\n",
       "   -4.062528300581984,\n",
       "   -4.591526278367156,\n",
       "   -4.100798264992939,\n",
       "   -3.940421911608433,\n",
       "   -4.350595060662785,\n",
       "   -4.350661397615139,\n",
       "   -4.300126873007521,\n",
       "   -4.058894296146688,\n",
       "   -3.9582404662202126,\n",
       "   -3.86984993275035,\n",
       "   -4.022008714226892,\n",
       "   -4.22523994051692,\n",
       "   -3.574662775397999,\n",
       "   -3.6714851445958363,\n",
       "   -3.5782845617254067,\n",
       "   -3.5996989387042255,\n",
       "   -3.6735532304005414,\n",
       "   -3.3918809077038885,\n",
       "   -3.6747179788073403,\n",
       "   -3.7173220781783103,\n",
       "   -3.567948931307728,\n",
       "   -3.9622747805429706,\n",
       "   -3.7430498816502205,\n",
       "   -3.7710707978266926,\n",
       "   -3.4369789079362247,\n",
       "   -3.700407497536606,\n",
       "   -3.706401744919754,\n",
       "   -3.9502235946987216,\n",
       "   -4.1575175894670755,\n",
       "   -3.857879039789382,\n",
       "   -3.9314891176661257,\n",
       "   -3.8052143264192075,\n",
       "   -3.5022714155405827,\n",
       "   -3.387314402171393,\n",
       "   -3.7737493451948376,\n",
       "   -3.797872063854551,\n",
       "   -3.6671772451737255,\n",
       "   -3.510014664346361,\n",
       "   -3.57732671313149,\n",
       "   -3.4003033610175413,\n",
       "   -3.3858050602236247,\n",
       "   -3.6087644888802455,\n",
       "   -3.4706795611920684,\n",
       "   -3.835307806480725,\n",
       "   -3.9849875663277547,\n",
       "   -3.952337952442293,\n",
       "   -3.8184548040178496,\n",
       "   -4.4474682251959665,\n",
       "   -4.507045190762805,\n",
       "   -4.840519888205382,\n",
       "   -4.548584688269189,\n",
       "   -4.916979397159242,\n",
       "   -4.668054475858912,\n",
       "   -5.254623821898879,\n",
       "   -5.368773367209875,\n",
       "   -5.077239630469205,\n",
       "   -5.6112238952315,\n",
       "   -4.950913309877669,\n",
       "   -4.831387942630898,\n",
       "   -4.510062667013198,\n",
       "   -4.640688033581347,\n",
       "   -4.178436511996277,\n",
       "   -4.0755178311168905,\n",
       "   -3.734389133451155,\n",
       "   -3.948837235869658,\n",
       "   -3.9399950095534217,\n",
       "   -4.028435456482735,\n",
       "   -4.27200625265256,\n",
       "   -4.203265632326102,\n",
       "   -4.761400047203324,\n",
       "   -4.9386829085536075,\n",
       "   -4.664489964483853,\n",
       "   -4.70591323064027,\n",
       "   -4.7702825941994025,\n",
       "   -4.583873638470327,\n",
       "   -4.484974224381919,\n",
       "   -5.176483988182921,\n",
       "   -4.935615562195742,\n",
       "   -4.988801682442451,\n",
       "   -4.863222565871257,\n",
       "   -5.12534692408115,\n",
       "   -4.944296690615241,\n",
       "   -5.574894535876975,\n",
       "   -5.432762703033345,\n",
       "   -5.204191629535808,\n",
       "   -5.4356876837345185,\n",
       "   -5.069570941250549,\n",
       "   -5.419163291569686,\n",
       "   -5.22765931838341,\n",
       "   -5.017851290159511,\n",
       "   -5.058353248778822,\n",
       "   -5.267432168243677,\n",
       "   -5.237568259735547,\n",
       "   -5.142364078734644,\n",
       "   -4.890220502686858,\n",
       "   -4.6678287551062825,\n",
       "   -4.428469005140419,\n",
       "   -4.133450261928999,\n",
       "   -4.287075766342915,\n",
       "   -4.3997853991637,\n",
       "   -4.722254986832753,\n",
       "   -4.915694354436717,\n",
       "   -4.551458921303004,\n",
       "   -4.28023654367078,\n",
       "   -3.8611186717125436,\n",
       "   -3.9675922971546047,\n",
       "   -3.6917700008182157,\n",
       "   -3.8734613029560103,\n",
       "   -4.125620198122183,\n",
       "   -3.8598639021281613,\n",
       "   -4.0003647148727675,\n",
       "   -4.51946393512401,\n",
       "   -4.330229198283636,\n",
       "   -4.546842405663415,\n",
       "   -4.856859791310386,\n",
       "   -4.7210413847171395,\n",
       "   -5.193220261611966,\n",
       "   -5.141553807701629,\n",
       "   -5.3318288301529435,\n",
       "   -5.439979264021063,\n",
       "   -5.154655160465342,\n",
       "   -4.645991888325587,\n",
       "   -5.248087468732268,\n",
       "   -4.90768725129252,\n",
       "   -4.920723714286871,\n",
       "   -5.160514876670548,\n",
       "   -4.825426368802006,\n",
       "   -5.296654089758823,\n",
       "   -4.937967772539428,\n",
       "   -5.043760915399661,\n",
       "   -4.433360036005412,\n",
       "   -4.780767147808323,\n",
       "   -4.700401072146128],\n",
       "  'ff_r2': [0.0,\n",
       "   0.0,\n",
       "   -93.22023802260938,\n",
       "   -68.53104716092913,\n",
       "   -48.083991224581645,\n",
       "   -49.45729139715636,\n",
       "   -41.25447967307775,\n",
       "   -24.138320586632904,\n",
       "   -25.84929103922657,\n",
       "   -21.2125035058771,\n",
       "   -17.353244655132563,\n",
       "   -19.137744640421268,\n",
       "   -14.464453444385168,\n",
       "   -18.39416153646284,\n",
       "   -15.481191887354704,\n",
       "   -14.288651720341182,\n",
       "   -13.610145997325104,\n",
       "   -9.91225919494019,\n",
       "   -7.957994727156644,\n",
       "   -5.959608149965474,\n",
       "   -7.35519044334246,\n",
       "   -8.0146879592774,\n",
       "   -7.994687933478783,\n",
       "   -3.866940893078497,\n",
       "   -5.507237136285713,\n",
       "   -4.620554596029994,\n",
       "   -5.8564839435852605,\n",
       "   -5.3967082373504836,\n",
       "   -3.0739906763306495,\n",
       "   -2.9911482632364623,\n",
       "   -3.265889009701074,\n",
       "   -3.348999518160517,\n",
       "   -3.2646066401414098,\n",
       "   -3.6680823332003545,\n",
       "   -3.1857393831828746,\n",
       "   -3.1688570176343296,\n",
       "   -3.1313197637035977,\n",
       "   -3.5585406715470835,\n",
       "   -5.11768775121887,\n",
       "   -5.484135833972622,\n",
       "   -5.38907071378351,\n",
       "   -5.272057852182055,\n",
       "   -5.6827482136841985,\n",
       "   -6.2736950140637555,\n",
       "   -6.414477285508802,\n",
       "   -6.4592921990267165,\n",
       "   -6.274780951000905,\n",
       "   -6.66522960487828,\n",
       "   -6.124118793218352,\n",
       "   -5.632815646398192,\n",
       "   -5.303123259137435,\n",
       "   -4.5659160377576375,\n",
       "   -4.021459873593676,\n",
       "   -4.667453504752488,\n",
       "   -4.891779282626874,\n",
       "   -5.664695671376122,\n",
       "   -5.285978324019862,\n",
       "   -4.117102182622271,\n",
       "   -4.610229838277319,\n",
       "   -3.8672247499330803,\n",
       "   -3.9096725821998097,\n",
       "   -3.787869050181973,\n",
       "   -3.452991377260963,\n",
       "   -3.1294809973919655,\n",
       "   -3.8820557821720465,\n",
       "   -3.66137300715123,\n",
       "   -3.3118919295671025,\n",
       "   -2.866603390767917,\n",
       "   -3.11651417553413,\n",
       "   -2.756132944721135,\n",
       "   -2.3098255819892137,\n",
       "   -2.191123084012265,\n",
       "   -2.235824300526454,\n",
       "   -2.2544555857003936,\n",
       "   -2.559158950589485,\n",
       "   -2.526546100999133,\n",
       "   -3.3348675935258694,\n",
       "   -3.7625513422444303,\n",
       "   -4.2779228951596515,\n",
       "   -5.717529700727658,\n",
       "   -6.733594948030096,\n",
       "   -6.7511026490526636,\n",
       "   -7.02724728918454,\n",
       "   -8.583020421763765,\n",
       "   -9.205599118739737,\n",
       "   -6.582241447717611,\n",
       "   -6.002151956296579,\n",
       "   -5.5663349997911835,\n",
       "   -6.8788490683675745,\n",
       "   -6.5967005627399296,\n",
       "   -6.4771806164224035,\n",
       "   -6.236375765549422,\n",
       "   -5.091077841890295,\n",
       "   -6.1653386638997665,\n",
       "   -5.564672849771225,\n",
       "   -5.085177823785519,\n",
       "   -4.783683151899143,\n",
       "   -6.5698377042189895,\n",
       "   -5.955915995084323,\n",
       "   -7.366987436659032,\n",
       "   -7.475435679152232,\n",
       "   -8.221813686890012,\n",
       "   -7.43121662482951,\n",
       "   -7.510895754587256,\n",
       "   -6.669695546275294,\n",
       "   -7.635776655668321,\n",
       "   -7.515209830748379,\n",
       "   -8.859394795284778,\n",
       "   -8.915359089619374,\n",
       "   -8.070306437899662,\n",
       "   -7.411901353870753,\n",
       "   -6.882801862518428,\n",
       "   -7.018679701043348,\n",
       "   -6.188900402611642,\n",
       "   -8.16887266141933,\n",
       "   -8.008433026550323,\n",
       "   -8.688532259752115,\n",
       "   -7.735125578591189,\n",
       "   -9.193836779700254,\n",
       "   -7.10921571826848,\n",
       "   -7.817991636517133,\n",
       "   -8.322206587520373,\n",
       "   -7.67099537296423,\n",
       "   -6.062415621627621,\n",
       "   -6.057983627847049,\n",
       "   -7.964723064958271,\n",
       "   -6.652971287428835,\n",
       "   -5.89976141689447,\n",
       "   -6.743386494657248,\n",
       "   -8.19224498660437,\n",
       "   -7.001427315765882,\n",
       "   -7.52031841057433,\n",
       "   -8.704114491562011,\n",
       "   -9.124391444234572,\n",
       "   -11.028084749293496,\n",
       "   -9.032445888898243,\n",
       "   -8.495130015996613,\n",
       "   -7.932986566308964,\n",
       "   -7.953025220975455,\n",
       "   -8.194366343226797,\n",
       "   -9.53160522754542,\n",
       "   -9.64100922247202,\n",
       "   -10.550328957156776,\n",
       "   -9.720812837072634,\n",
       "   -8.331030961079842,\n",
       "   -8.03606248236918,\n",
       "   -9.968490489464733,\n",
       "   -9.132003979257105,\n",
       "   -8.23212295201762,\n",
       "   -7.260737027287954,\n",
       "   -7.315235590654151,\n",
       "   -7.540958500022894,\n",
       "   -6.805598472437697,\n",
       "   -8.573380840680779,\n",
       "   -7.811200084232434,\n",
       "   -7.00558879804934,\n",
       "   -7.059043965989675,\n",
       "   -6.9293511220234185,\n",
       "   -8.91213779547386,\n",
       "   -12.251664071173442,\n",
       "   -9.693091679464901,\n",
       "   -10.84860177283794,\n",
       "   -12.464611148967407,\n",
       "   -15.128214253991004,\n",
       "   -11.85935195974158,\n",
       "   -9.351673575940268,\n",
       "   -8.745938888848416,\n",
       "   -9.70793183996082,\n",
       "   -9.267413551615405,\n",
       "   -6.658231063834823,\n",
       "   -9.54528693820327,\n",
       "   -8.255106171455976,\n",
       "   -12.36000162182849,\n",
       "   -12.501399986596201,\n",
       "   -9.783751064964777,\n",
       "   -8.923281240803892,\n",
       "   -6.658048780539949,\n",
       "   -5.826972789039384,\n",
       "   -5.677415152392281,\n",
       "   -4.2316190923440296,\n",
       "   -3.654934682803294,\n",
       "   -3.136643126086832,\n",
       "   -2.6998109604028193,\n",
       "   -2.851797214173799,\n",
       "   -2.783732035762328,\n",
       "   -2.9058435191922056,\n",
       "   -2.9204334194115162,\n",
       "   -3.1196460380908615,\n",
       "   -3.4624840669082113,\n",
       "   -3.5241310957755214,\n",
       "   -3.142901541343006,\n",
       "   -3.4358052141088615,\n",
       "   -3.5750461518949503,\n",
       "   -3.5774297950464575,\n",
       "   -3.846409511202343,\n",
       "   -4.119646788224417,\n",
       "   -4.774812109454831,\n",
       "   -4.964621929825707,\n",
       "   -3.90509333050297,\n",
       "   -3.8525874808432192,\n",
       "   -3.7882752724887245,\n",
       "   -3.930718692285037,\n",
       "   -4.142681469188395,\n",
       "   -4.583014673857707,\n",
       "   -6.141058310104187,\n",
       "   -5.663810467290326,\n",
       "   -4.580872399138851,\n",
       "   -4.257251072286747,\n",
       "   -4.34766318629789,\n",
       "   -4.3426116445037035,\n",
       "   -4.872403516352549,\n",
       "   -4.4557115525883235,\n",
       "   -4.433051467325185,\n",
       "   -4.0920755411338545,\n",
       "   -4.789988908547161,\n",
       "   -4.976545564378052,\n",
       "   -5.0157209634830915,\n",
       "   -5.263824456753423,\n",
       "   -5.900091639677409,\n",
       "   -6.149658686420546,\n",
       "   -6.440737777965552,\n",
       "   -6.586852197894328,\n",
       "   -5.381105987848055,\n",
       "   -4.633302042459539,\n",
       "   -4.829065102551349,\n",
       "   -7.726017086078208,\n",
       "   -6.01746209997057,\n",
       "   -7.49244528672064,\n",
       "   -4.662503013701716,\n",
       "   -4.309029760400483,\n",
       "   -2.7595921547702016,\n",
       "   -3.909109040360046,\n",
       "   -6.645204449866964,\n",
       "   -6.6257948756922795,\n",
       "   -10.915692329463678,\n",
       "   -7.451201405689819,\n",
       "   -7.5314583740854975,\n",
       "   -5.408853740052954,\n",
       "   -6.669049861496011,\n",
       "   -6.576029792968462,\n",
       "   -6.805239869903012,\n",
       "   -6.664544056138575,\n",
       "   -7.0963678764668,\n",
       "   -9.669870422566968,\n",
       "   -9.720414972625212,\n",
       "   -11.151506953725384,\n",
       "   -10.930581948440432,\n",
       "   -12.043800675983704,\n",
       "   -10.588566059461575,\n",
       "   -10.644103340785092,\n",
       "   -11.23671117394221,\n",
       "   -11.504887396089764,\n",
       "   -11.633487722859758,\n",
       "   -11.032873164302341,\n",
       "   -12.70526503045169,\n",
       "   -11.909617281012661,\n",
       "   -9.389695705032677,\n",
       "   -10.132778692002375,\n",
       "   -9.143459388974824,\n",
       "   -11.782739199087851,\n",
       "   -11.738877395480598,\n",
       "   -11.866877058831252,\n",
       "   -9.220685303523526,\n",
       "   -8.33793033807703,\n",
       "   -7.882983800145116,\n",
       "   -6.793379907074734,\n",
       "   -10.986599454837997,\n",
       "   -8.60920207483738,\n",
       "   -10.810459328738355,\n",
       "   -9.209039564432178,\n",
       "   -8.879495379887494,\n",
       "   -7.321482276059493,\n",
       "   -6.972459686029159,\n",
       "   -6.064833690875865,\n",
       "   -3.9421787435643303,\n",
       "   -3.855418193570557,\n",
       "   -3.9441372810817334,\n",
       "   -3.8154526377062075,\n",
       "   -4.439631458378828,\n",
       "   -4.206933111080724,\n",
       "   -5.394004180810322,\n",
       "   -3.924865136363282,\n",
       "   -3.8626300134168527,\n",
       "   -4.122566625812793,\n",
       "   -4.297540058675326,\n",
       "   -5.010699617241772,\n",
       "   -5.760326844709573,\n",
       "   -6.133180090305762,\n",
       "   -5.804500682751621,\n",
       "   -7.3275369894317155,\n",
       "   -7.535883935430311,\n",
       "   -6.438600879988584,\n",
       "   -7.4863659242725,\n",
       "   -10.720440261122407,\n",
       "   -7.494831815830079,\n",
       "   -6.327140602479702,\n",
       "   -6.127799684143461,\n",
       "   -5.221218687786399,\n",
       "   -4.7501874177294,\n",
       "   -5.136785594862947,\n",
       "   -5.153659629452858,\n",
       "   -5.203345442038832,\n",
       "   -4.806507601110676,\n",
       "   -4.82897234577922,\n",
       "   -3.829726510752246,\n",
       "   -3.590250563489735,\n",
       "   -4.217813019629084,\n",
       "   -4.160102100269603,\n",
       "   -3.6621292471554527,\n",
       "   -3.436277891026969,\n",
       "   -3.8072111960105097,\n",
       "   -3.7997940231534963,\n",
       "   -3.9730389298667763,\n",
       "   -4.144216511505843,\n",
       "   -4.749760821660834,\n",
       "   -4.2944973292328985,\n",
       "   -4.4696834094289315,\n",
       "   -4.61467588398204,\n",
       "   -4.730779956731257,\n",
       "   -4.386196250288112,\n",
       "   -4.556110027835815,\n",
       "   -5.369000690696684,\n",
       "   -5.272591876222051,\n",
       "   -4.75228965249455,\n",
       "   -4.548900590907784,\n",
       "   -4.22428560099717,\n",
       "   -4.350978072705167,\n",
       "   -4.527342216352685,\n",
       "   -4.670528665176058,\n",
       "   -4.996930724129853,\n",
       "   -4.822105938411005,\n",
       "   -4.288880207498389,\n",
       "   -4.432650394354573,\n",
       "   -4.638232694211669,\n",
       "   -5.5099780532539135,\n",
       "   -5.841026441408335,\n",
       "   -5.698881205112962,\n",
       "   -5.672824287148819,\n",
       "   -5.565246239856691,\n",
       "   -4.967843581186761,\n",
       "   -5.207076203274401,\n",
       "   -5.020138455396752,\n",
       "   -6.900963298342284,\n",
       "   -5.104709148993517,\n",
       "   -6.386418300842368,\n",
       "   -5.492332257136447,\n",
       "   -6.489025640686408,\n",
       "   -7.190416546374035,\n",
       "   -7.517386660187473,\n",
       "   -6.878553979602209,\n",
       "   -5.682839347489765,\n",
       "   -6.067609557741164,\n",
       "   -7.190096775190828,\n",
       "   -6.828547892756872,\n",
       "   -6.867939347079899,\n",
       "   -8.21016587910697,\n",
       "   -7.492255761650892,\n",
       "   -6.95028856206772,\n",
       "   -5.663968273103425,\n",
       "   -5.438175508349468,\n",
       "   -5.637574740878659,\n",
       "   -5.633942457265521,\n",
       "   -6.295138466066855,\n",
       "   -6.578310657805979,\n",
       "   -7.400621659843155,\n",
       "   -7.188688283750132,\n",
       "   -5.531419938924832,\n",
       "   -4.940822158886594,\n",
       "   -5.176672607341556,\n",
       "   -3.9678882459308085,\n",
       "   -3.825556863281972,\n",
       "   -3.3741336671820914,\n",
       "   -3.9379550288189327,\n",
       "   -3.939855892675844,\n",
       "   -4.435455971938228,\n",
       "   -4.234068048950144,\n",
       "   -4.781726287768642,\n",
       "   -5.32619026392937,\n",
       "   -4.570949904697243,\n",
       "   -4.276824444127724,\n",
       "   -4.402607592871537,\n",
       "   -4.137385891055607,\n",
       "   -4.353094507416338,\n",
       "   -4.646059904017993,\n",
       "   -4.646213381168263,\n",
       "   -4.437857877962053,\n",
       "   -4.448484292295725,\n",
       "   -4.87011316212762,\n",
       "   -4.725489137569739,\n",
       "   -4.676432365583958,\n",
       "   -4.537837401857615,\n",
       "   -4.706696344515568,\n",
       "   -4.940659151735486,\n",
       "   -5.056949030720583,\n",
       "   -4.865219708279164,\n",
       "   -4.691132084599116,\n",
       "   -4.751060182495081,\n",
       "   -5.263235133458801,\n",
       "   -5.866075444814734,\n",
       "   -6.43128812154125,\n",
       "   -5.714736723969632,\n",
       "   -4.970765263048133,\n",
       "   -5.2168653378467145,\n",
       "   -4.991301950682136,\n",
       "   -4.427085565541934,\n",
       "   -4.1211471425982475,\n",
       "   -5.292570334661776,\n",
       "   -5.341533563595806,\n",
       "   -5.806910198494162,\n",
       "   -6.504817763217735,\n",
       "   -5.0992783922605085,\n",
       "   -5.211488823043131,\n",
       "   -5.866542395025076,\n",
       "   -5.148109526456333,\n",
       "   -5.124217641019159,\n",
       "   -4.974959396028932,\n",
       "   -4.839910160812848,\n",
       "   -5.132966271212121,\n",
       "   -5.629751235891012,\n",
       "   -5.979997356215631,\n",
       "   -5.514596901911091,\n",
       "   -6.158103718559533,\n",
       "   -6.4035600807300455,\n",
       "   -6.928192029682445,\n",
       "   -5.836748960578357,\n",
       "   -5.652853982898448,\n",
       "   -5.134957136885804,\n",
       "   -5.157984597683966,\n",
       "   -5.196768408993201,\n",
       "   -5.2950946295108805,\n",
       "   -4.4840506667563815,\n",
       "   -3.298502992319161,\n",
       "   -3.39143927038789,\n",
       "   -3.206953695263814,\n",
       "   -3.467543783221374,\n",
       "   -3.1738166442043827,\n",
       "   -3.232149241000025,\n",
       "   -3.158654744447359,\n",
       "   -3.47487676717507,\n",
       "   -3.6258129027412105,\n",
       "   -3.8088892986332636,\n",
       "   -4.244449740379106,\n",
       "   -4.470580749848342,\n",
       "   -5.242110836021702,\n",
       "   -5.360269694002587,\n",
       "   -4.643928250648036,\n",
       "   -4.54530081594097,\n",
       "   -4.465807939350742,\n",
       "   -4.962995760781899,\n",
       "   -5.9333622871919,\n",
       "   -4.931218565829776,\n",
       "   -6.562289519798423,\n",
       "   -4.98782826579187,\n",
       "   -5.688833545244318,\n",
       "   -7.734634305212433,\n",
       "   -9.594098389448291,\n",
       "   -8.645708408850913,\n",
       "   -5.979996284649264,\n",
       "   -4.418382753129934,\n",
       "   -4.622276998560786,\n",
       "   -5.04036234466775,\n",
       "   -4.585869431583435,\n",
       "   -4.56505288039149,\n",
       "   -4.017669230893284,\n",
       "   -4.192926705933508,\n",
       "   -3.375751917391854,\n",
       "   -3.48670148615634,\n",
       "   -3.6361682641967947,\n",
       "   -3.2642200580208307,\n",
       "   -2.921334835439062,\n",
       "   -3.466013030203622,\n",
       "   -3.7878817229368886,\n",
       "   -3.930954641618178,\n",
       "   -4.09630737058031,\n",
       "   -4.419510469589034,\n",
       "   -4.811728735011805,\n",
       "   -4.55362082248576,\n",
       "   -4.681806953319312,\n",
       "   -4.329799074155448,\n",
       "   -3.411324469905588,\n",
       "   -4.949855236852313,\n",
       "   -4.046543452290561,\n",
       "   -4.190269019807062,\n",
       "   -4.427649745518737,\n",
       "   -3.7625629613248526,\n",
       "   -3.6012552256400863,\n",
       "   -3.575401962247005,\n",
       "   -3.584529393565975,\n",
       "   -3.205313117387557,\n",
       "   -3.3418903664639865,\n",
       "   -3.403664520753628,\n",
       "   -3.134959435336526,\n",
       "   -2.9934024852114782,\n",
       "   -3.054881712093369,\n",
       "   -2.95395673777947,\n",
       "   -3.0602784134859675,\n",
       "   -3.435871318013535,\n",
       "   -4.827953120149513,\n",
       "   -5.880824087225645,\n",
       "   -6.9829152959142275],\n",
       "  'test_r2s': [-1.5117319718002018,\n",
       "   -1.2153308354620398,\n",
       "   -94.02522968418205,\n",
       "   -69.00839222484692,\n",
       "   -48.41047330179911,\n",
       "   -49.54724006963602,\n",
       "   -41.24865539592834,\n",
       "   -397.9478612734214,\n",
       "   -123.35176102525999,\n",
       "   -66.67649424857734,\n",
       "   -40.572483437060136,\n",
       "   -19430.937137642224,\n",
       "   -152.65723837195077,\n",
       "   -72.88391102284407,\n",
       "   -49.65904215686541,\n",
       "   -40.370180787869856,\n",
       "   -34.01256901521828,\n",
       "   -27.14904988628151,\n",
       "   -23.7173863851404,\n",
       "   -19.521063369751822,\n",
       "   -19.19733938058996,\n",
       "   -18.34408041550346,\n",
       "   -17.502326273297527,\n",
       "   -11.881763298587831,\n",
       "   -12.972174545651171,\n",
       "   -12.073995661749645,\n",
       "   -13.126162059566013,\n",
       "   -12.985419971566653,\n",
       "   -10.545324392383574,\n",
       "   -10.612759175473434,\n",
       "   -10.414813847975967,\n",
       "   -10.141285735617599,\n",
       "   -9.723286523764546,\n",
       "   -9.844183759856062,\n",
       "   -9.007006367987302,\n",
       "   -9.018185623336993,\n",
       "   -8.848714040165357,\n",
       "   -9.670432913669842,\n",
       "   -11.095874573384624,\n",
       "   -11.137519489818043,\n",
       "   -11.225853339980823,\n",
       "   -10.671245687191167,\n",
       "   -10.9729290867099,\n",
       "   -11.606000475109116,\n",
       "   -11.620951189408515,\n",
       "   -11.540727297824638,\n",
       "   -11.004163322548994,\n",
       "   -11.382127380444636,\n",
       "   -11.03221783027037,\n",
       "   -10.68655048258564,\n",
       "   -10.756848678744877,\n",
       "   -10.06756373775605,\n",
       "   -9.49454001381646,\n",
       "   -10.605841052913664,\n",
       "   -10.989413139639812,\n",
       "   -11.74414434145988,\n",
       "   -11.249372557572478,\n",
       "   -10.186325327311717,\n",
       "   -11.000781757719228,\n",
       "   -9.81418672452907,\n",
       "   -9.614840600255299,\n",
       "   -9.89171487236804,\n",
       "   -9.520883566178822,\n",
       "   -9.147484874615316,\n",
       "   -9.49221725532123,\n",
       "   -9.498482529960349,\n",
       "   -9.170840264718091,\n",
       "   -8.746511994534474,\n",
       "   -8.65232348986818,\n",
       "   -8.439445339530348,\n",
       "   -7.832422930637028,\n",
       "   -8.041142948401756,\n",
       "   -8.788899878333803,\n",
       "   -8.326270488481695,\n",
       "   -8.191923629343716,\n",
       "   -8.49215342835769,\n",
       "   -9.601672460186128,\n",
       "   -9.590920822009968,\n",
       "   -9.865213647257189,\n",
       "   -11.44076789507652,\n",
       "   -12.403647545555952,\n",
       "   -12.937301054740782,\n",
       "   -13.347110762333909,\n",
       "   -14.998496358526204,\n",
       "   -15.319807567242785,\n",
       "   -12.956604865000795,\n",
       "   -12.15813639588347,\n",
       "   -11.722483198831046,\n",
       "   -12.932470123603384,\n",
       "   -12.881379464344768,\n",
       "   -12.440155368309995,\n",
       "   -12.225964824668866,\n",
       "   -10.985660906803698,\n",
       "   -12.117586155238738,\n",
       "   -11.686806637060114,\n",
       "   -11.426555749127338,\n",
       "   -11.79180310633179,\n",
       "   -13.405816436136888,\n",
       "   -12.657940027920151,\n",
       "   -13.970773836992265,\n",
       "   -13.999450144140123,\n",
       "   -14.340670991431892,\n",
       "   -13.774161540680057,\n",
       "   -14.532642077685107,\n",
       "   -14.35611315334686,\n",
       "   -14.828313668837414,\n",
       "   -14.910849923614359,\n",
       "   -15.693658764203684,\n",
       "   -16.018091903983162,\n",
       "   -15.368215176573338,\n",
       "   -15.099839781712387,\n",
       "   -14.512891862415241,\n",
       "   -14.654749119436772,\n",
       "   -13.429389187967114,\n",
       "   -14.939171092029333,\n",
       "   -14.479014868009836,\n",
       "   -14.85320848972561,\n",
       "   -14.274721076048053,\n",
       "   -15.355620732276147,\n",
       "   -13.662220914855943,\n",
       "   -14.070329898623804,\n",
       "   -14.546964112574276,\n",
       "   -13.841482348251272,\n",
       "   -12.411627521069612,\n",
       "   -12.470973280682038,\n",
       "   -14.6587080013203,\n",
       "   -13.777113680397164,\n",
       "   -13.221981700207033,\n",
       "   -14.218009365892499,\n",
       "   -15.784556057958678,\n",
       "   -14.022360427660345,\n",
       "   -13.979422687344677,\n",
       "   -15.33664570863162,\n",
       "   -15.886818643490663,\n",
       "   -18.604394017979708,\n",
       "   -17.012119585717013,\n",
       "   -16.445716519108576,\n",
       "   -16.179920591417535,\n",
       "   -15.161315296558232,\n",
       "   -14.62444788416226,\n",
       "   -16.45682574783816,\n",
       "   -16.200547368467216,\n",
       "   -17.24023036074764,\n",
       "   -16.587872071947817,\n",
       "   -15.723172891375366,\n",
       "   -15.326052642570295,\n",
       "   -17.337384396071506,\n",
       "   -16.728559828109873,\n",
       "   -16.00898949680679,\n",
       "   -14.56045794725948,\n",
       "   -14.627977997306495,\n",
       "   -15.365017832839575,\n",
       "   -14.592260601139298,\n",
       "   -16.71026233446851,\n",
       "   -15.741865522133939,\n",
       "   -15.286487881526428,\n",
       "   -15.32326992486322,\n",
       "   -15.592527772788756,\n",
       "   -18.09017621407493,\n",
       "   -20.612484621131927,\n",
       "   -18.146706567186058,\n",
       "   -18.96245704935766,\n",
       "   -20.390208700929016,\n",
       "   -23.044627300737886,\n",
       "   -19.13638550110968,\n",
       "   -16.68555474430306,\n",
       "   -16.696924809691545,\n",
       "   -17.526562448173564,\n",
       "   -16.92074293695304,\n",
       "   -14.007895109501199,\n",
       "   -16.726678154113834,\n",
       "   -15.732780843536215,\n",
       "   -20.187484600231706,\n",
       "   -20.666904324840914,\n",
       "   -17.512788681586073,\n",
       "   -17.31026780924435,\n",
       "   -15.41212524391661,\n",
       "   -14.754137546326941,\n",
       "   -14.412791322226596,\n",
       "   -12.87309482216612,\n",
       "   -12.425140489022859,\n",
       "   -11.856230759134359,\n",
       "   -11.128192473165472,\n",
       "   -10.69757825703119,\n",
       "   -11.408501803870053,\n",
       "   -11.129300796197665,\n",
       "   -10.841422421975995,\n",
       "   -11.10389213616712,\n",
       "   -11.412972335256917,\n",
       "   -11.367772160976372,\n",
       "   -12.365841612242953,\n",
       "   -12.92770945734241,\n",
       "   -13.389696504949848,\n",
       "   -13.495151142411936,\n",
       "   -13.258030620867547,\n",
       "   -13.529978380762026,\n",
       "   -13.815908756744268,\n",
       "   -13.900269027504836,\n",
       "   -12.71824466343732,\n",
       "   -13.489814111149514,\n",
       "   -12.821870998831969,\n",
       "   -12.919393900669668,\n",
       "   -12.699824538266647,\n",
       "   -12.421246159880285,\n",
       "   -13.930825121081162,\n",
       "   -12.975186521596914,\n",
       "   -11.343988655353515,\n",
       "   -11.262991493653018,\n",
       "   -10.982896402052477,\n",
       "   -11.469741906851093,\n",
       "   -12.41333161386289,\n",
       "   -11.819137879610949,\n",
       "   -12.147795055455553,\n",
       "   -11.64586578270045,\n",
       "   -12.140151455490958,\n",
       "   -11.949316465829497,\n",
       "   -12.150462708235052,\n",
       "   -13.02283972273505,\n",
       "   -13.769852380183284,\n",
       "   -13.56814870939056,\n",
       "   -13.703355916226013,\n",
       "   -13.997405182979477,\n",
       "   -13.20445799478416,\n",
       "   -12.414125848486908,\n",
       "   -12.750184580149972,\n",
       "   -15.453787653266621,\n",
       "   -14.153557838596516,\n",
       "   -15.050864154906876,\n",
       "   -12.3690038526973,\n",
       "   -11.550084446646295,\n",
       "   -10.448395239984789,\n",
       "   -11.78611401895515,\n",
       "   -14.554000815307473,\n",
       "   -14.45823042925653,\n",
       "   -19.33517408715726,\n",
       "   -16.060051852921113,\n",
       "   -15.893787890307522,\n",
       "   -13.90983522880326,\n",
       "   -15.289870167942588,\n",
       "   -14.88014212194292,\n",
       "   -14.527025596830622,\n",
       "   -14.813820566012879,\n",
       "   -15.061074897051483,\n",
       "   -17.15766840498213,\n",
       "   -17.20463302656409,\n",
       "   -18.781565920634726,\n",
       "   -18.999827336313423,\n",
       "   -20.25957034936426,\n",
       "   -19.13607957905696,\n",
       "   -19.12614827354407,\n",
       "   -19.361177673345004,\n",
       "   -19.770254887988436,\n",
       "   -20.194956376640924,\n",
       "   -19.774616690671003,\n",
       "   -20.78091160085313,\n",
       "   -20.205683807258957,\n",
       "   -17.364479690946034,\n",
       "   -18.385695191340922,\n",
       "   -17.74087557303193,\n",
       "   -20.561206815667482,\n",
       "   -20.905365044923215,\n",
       "   -20.402071742469218,\n",
       "   -17.279816376167894,\n",
       "   -16.820613651163224,\n",
       "   -16.25620163192111,\n",
       "   -15.174221599799669,\n",
       "   -19.444483450751207,\n",
       "   -16.78332792113781,\n",
       "   -19.806495477323132,\n",
       "   -18.156168431934084,\n",
       "   -17.49523405689918,\n",
       "   -16.226013262112197,\n",
       "   -15.585211538486101,\n",
       "   -14.873504261799269,\n",
       "   -12.554290257900998,\n",
       "   -12.183273723280704,\n",
       "   -12.81542786754212,\n",
       "   -12.75376001963189,\n",
       "   -13.154490449994576,\n",
       "   -12.968250951399225,\n",
       "   -14.034667931285652,\n",
       "   -12.017052019693157,\n",
       "   -11.842888287653473,\n",
       "   -12.33557961771979,\n",
       "   -12.47221990320155,\n",
       "   -13.302440612622114,\n",
       "   -13.823116505205336,\n",
       "   -13.835773726579573,\n",
       "   -13.748988570918659,\n",
       "   -15.707153427843293,\n",
       "   -16.200257132690616,\n",
       "   -15.225043009538382,\n",
       "   -15.887618740536194,\n",
       "   -18.593734799676596,\n",
       "   -15.879900681967506,\n",
       "   -14.796592282961583,\n",
       "   -13.785067030036531,\n",
       "   -13.40989411031731,\n",
       "   -13.467260255594457,\n",
       "   -14.131960132495575,\n",
       "   -13.69452609885171,\n",
       "   -13.88764547894609,\n",
       "   -13.244388802500705,\n",
       "   -13.141214671440629,\n",
       "   -11.684562127521595,\n",
       "   -11.336163328216223,\n",
       "   -12.461497138618327,\n",
       "   -12.104846655145382,\n",
       "   -11.498091131846833,\n",
       "   -11.739226267848341,\n",
       "   -12.369649744281748,\n",
       "   -12.129342577442173,\n",
       "   -11.814357330094698,\n",
       "   -11.75938114669469,\n",
       "   -12.528798045518602,\n",
       "   -11.72654893710388,\n",
       "   -12.328975338345018,\n",
       "   -12.361056605859272,\n",
       "   -12.645454298661274,\n",
       "   -11.617101376408566,\n",
       "   -11.811687696592475,\n",
       "   -12.963291606886505,\n",
       "   -13.230556667570998,\n",
       "   -12.667755939371315,\n",
       "   -12.709452961503889,\n",
       "   -13.303893543308074,\n",
       "   -12.827751029213236,\n",
       "   -12.915302105569705,\n",
       "   -12.9411177781759,\n",
       "   -13.44398251703776,\n",
       "   -13.706685980291198,\n",
       "   -13.20648300298041,\n",
       "   -13.395655849686557,\n",
       "   -13.07582812133122,\n",
       "   -14.065377381397301,\n",
       "   -13.681837637291196,\n",
       "   -14.270235526456652,\n",
       "   -14.57459173466514,\n",
       "   -14.829671138609783,\n",
       "   -13.681546960731076,\n",
       "   -13.614315428960907,\n",
       "   -13.625524034048869,\n",
       "   -16.299644938542716,\n",
       "   -15.47586277164308,\n",
       "   -16.776539140591545,\n",
       "   -16.241567438683187,\n",
       "   -16.702001129093958,\n",
       "   -16.789573677893834,\n",
       "   -16.60272888003255,\n",
       "   -15.248919248200568,\n",
       "   -13.871851303892402,\n",
       "   -14.992185487787932,\n",
       "   -15.723608620499204,\n",
       "   -15.12460131721295,\n",
       "   -15.311259358523579,\n",
       "   -17.0301241673583,\n",
       "   -16.503621250724095,\n",
       "   -16.4618820532533,\n",
       "   -15.54331742229337,\n",
       "   -14.525656959129588,\n",
       "   -14.2067777455953,\n",
       "   -14.147554578171441,\n",
       "   -14.60553604739557,\n",
       "   -15.29631150635678,\n",
       "   -15.904421308925535,\n",
       "   -15.3642810386138,\n",
       "   -13.518741443752614,\n",
       "   -12.964902864992506,\n",
       "   -13.744371940864518,\n",
       "   -12.09910252868142,\n",
       "   -11.803901177569402,\n",
       "   -11.81830787274006,\n",
       "   -11.999151711203687,\n",
       "   -12.22177991967564,\n",
       "   -12.627050780964883,\n",
       "   -11.96031848104369,\n",
       "   -12.50283213852795,\n",
       "   -13.26608517123507,\n",
       "   -12.36562216117251,\n",
       "   -11.53026464670298,\n",
       "   -11.718496785060804,\n",
       "   -11.350414539500445,\n",
       "   -11.632961772981966,\n",
       "   -12.2447948289909,\n",
       "   -11.625735884936756,\n",
       "   -11.535260580085332,\n",
       "   -11.631574732010538,\n",
       "   -12.166167949000789,\n",
       "   -12.180837580893769,\n",
       "   -11.829506286633016,\n",
       "   -12.290733244441704,\n",
       "   -11.872531175078088,\n",
       "   -12.705759481159964,\n",
       "   -12.7667563634892,\n",
       "   -12.375902780843425,\n",
       "   -12.758424840877343,\n",
       "   -12.014587831227402,\n",
       "   -12.439845290076738,\n",
       "   -12.886346853100235,\n",
       "   -13.174859078060305,\n",
       "   -12.245803268265188,\n",
       "   -12.17371098921857,\n",
       "   -12.546227855863522,\n",
       "   -12.501555143681706,\n",
       "   -11.751355910250211,\n",
       "   -11.94865865130146,\n",
       "   -13.054501349850218,\n",
       "   -13.142953236420293,\n",
       "   -13.729031432176619,\n",
       "   -14.35041199560862,\n",
       "   -13.465252780230779,\n",
       "   -13.7112498143578,\n",
       "   -14.369066742921671,\n",
       "   -13.621489585927028,\n",
       "   -14.05630595451052,\n",
       "   -13.778147656029244,\n",
       "   -14.282606929349765,\n",
       "   -14.674794897943427,\n",
       "   -15.185038141176014,\n",
       "   -15.244786540652779,\n",
       "   -15.445050920063647,\n",
       "   -15.399212804336202,\n",
       "   -15.60416050093751,\n",
       "   -16.67788446829234,\n",
       "   -14.688609961514544,\n",
       "   -14.114037822421167,\n",
       "   -13.227691804298757,\n",
       "   -13.470629625592355,\n",
       "   -13.18421316037794,\n",
       "   -12.790602550888583,\n",
       "   -12.13078467545053,\n",
       "   -11.157622122470856,\n",
       "   -11.190237791689906,\n",
       "   -11.111313219393214,\n",
       "   -11.509161874475055,\n",
       "   -11.470854317186342,\n",
       "   -12.108975369699447,\n",
       "   -11.802611425649761,\n",
       "   -11.771101023575312,\n",
       "   -12.47713545407581,\n",
       "   -12.30429199425696,\n",
       "   -12.661537223814726,\n",
       "   -12.49998459554505,\n",
       "   -13.95692465176379,\n",
       "   -14.245396817405286,\n",
       "   -13.934285323890505,\n",
       "   -13.78523314558349,\n",
       "   -13.793067158844838,\n",
       "   -14.331093326590603,\n",
       "   -15.778083764057595,\n",
       "   -14.475044039192635,\n",
       "   -16.05634477595577,\n",
       "   -14.250624437717548,\n",
       "   -14.214230222935287,\n",
       "   -16.752110355205254,\n",
       "   -18.606332945716566,\n",
       "   -17.369840171351584,\n",
       "   -14.732598691954987,\n",
       "   -13.667909914287833,\n",
       "   -13.435405582348789,\n",
       "   -13.80018358627055,\n",
       "   -13.315939662813609,\n",
       "   -12.68140946209207,\n",
       "   -12.18196334589749,\n",
       "   -12.643432562971814,\n",
       "   -12.380767688932893,\n",
       "   -13.125418554891267,\n",
       "   -13.686208986382614,\n",
       "   -12.89143758852097,\n",
       "   -12.761696140659026,\n",
       "   -12.94897674159833,\n",
       "   -13.263746144105937,\n",
       "   -13.363461837916045,\n",
       "   -13.310706696021818,\n",
       "   -13.431646198712013,\n",
       "   -13.69080611136259,\n",
       "   -12.862381677288676,\n",
       "   -12.832616757293522,\n",
       "   -13.209526709688383,\n",
       "   -12.198795379026507,\n",
       "   -13.853183991500876,\n",
       "   -13.478851491936869,\n",
       "   -13.315788244462176,\n",
       "   -13.66302025773985,\n",
       "   -12.91866564708305,\n",
       "   -12.953792099265142,\n",
       "   -12.814917930597977,\n",
       "   -12.964766116713712,\n",
       "   -11.933272059971046,\n",
       "   -12.228981680672486,\n",
       "   -11.96509774931214,\n",
       "   -11.949612630762438,\n",
       "   -11.732039096610878,\n",
       "   -11.341365959676889,\n",
       "   -12.209410156618691,\n",
       "   -12.517926961614517,\n",
       "   -12.745334872562996,\n",
       "   -14.15332282277716,\n",
       "   -15.923683641025075,\n",
       "   -17.038523650873273],\n",
       "  'train_pce_loss': [0.5746474862098694,\n",
       "   0.6829570531845093,\n",
       "   0.6750085949897766,\n",
       "   0.7871534824371338,\n",
       "   0.9784559607505798,\n",
       "   0.5137147903442383,\n",
       "   0.7117196321487427,\n",
       "   0.9034402966499329,\n",
       "   0.6434188485145569,\n",
       "   0.6752991080284119,\n",
       "   0.4702785015106201,\n",
       "   0.6182985305786133,\n",
       "   0.5099064111709595,\n",
       "   0.8926501870155334,\n",
       "   0.4903205633163452,\n",
       "   1.0011324882507324,\n",
       "   0.7497689723968506,\n",
       "   0.5937034487724304,\n",
       "   0.6942434906959534,\n",
       "   1.1937201023101807,\n",
       "   0.5240352749824524,\n",
       "   0.5812059044837952,\n",
       "   0.7148011922836304,\n",
       "   0.6646133661270142,\n",
       "   0.42139801383018494,\n",
       "   0.4610889256000519,\n",
       "   0.5968232154846191,\n",
       "   0.6638157963752747,\n",
       "   0.6419691443443298,\n",
       "   0.5214149355888367,\n",
       "   0.6429277062416077,\n",
       "   0.6033633947372437,\n",
       "   0.7714674472808838,\n",
       "   0.435441792011261,\n",
       "   0.6882688403129578,\n",
       "   0.36481040716171265,\n",
       "   0.5269919037818909,\n",
       "   0.4599514603614807,\n",
       "   0.5398368239402771,\n",
       "   0.5968461632728577,\n",
       "   0.5799974799156189,\n",
       "   0.5138877630233765,\n",
       "   0.5306335687637329,\n",
       "   0.8376507759094238,\n",
       "   0.6488759517669678,\n",
       "   1.0647159814834595,\n",
       "   0.7229564189910889,\n",
       "   0.604823112487793,\n",
       "   0.630354106426239,\n",
       "   0.5512419939041138,\n",
       "   0.5012642741203308,\n",
       "   0.6495415568351746,\n",
       "   0.5106784105300903,\n",
       "   0.7798227667808533,\n",
       "   0.7120667695999146,\n",
       "   0.6361857652664185,\n",
       "   0.5689478516578674,\n",
       "   0.5965529084205627,\n",
       "   0.4748985469341278,\n",
       "   0.901616096496582,\n",
       "   0.6979384422302246,\n",
       "   0.5437799692153931,\n",
       "   0.6103211641311646,\n",
       "   0.5691927075386047,\n",
       "   0.38768333196640015,\n",
       "   0.791748583316803,\n",
       "   0.576848030090332,\n",
       "   0.921583890914917,\n",
       "   0.48198646306991577,\n",
       "   0.509486734867096,\n",
       "   0.561499834060669,\n",
       "   0.6327372193336487,\n",
       "   0.940743625164032,\n",
       "   0.7983304858207703,\n",
       "   0.5848692655563354,\n",
       "   0.8565517067909241,\n",
       "   0.433207631111145,\n",
       "   0.6521171927452087,\n",
       "   0.7405316829681396,\n",
       "   0.8894317150115967,\n",
       "   0.4984970688819885,\n",
       "   0.5580602288246155,\n",
       "   0.4695761799812317,\n",
       "   0.8462674617767334,\n",
       "   0.39474838972091675,\n",
       "   0.4905354082584381,\n",
       "   0.6272612810134888,\n",
       "   0.621570348739624,\n",
       "   0.6012877225875854,\n",
       "   0.5203099250793457,\n",
       "   0.4873199760913849,\n",
       "   0.6237702369689941,\n",
       "   0.45315250754356384,\n",
       "   0.7362940907478333,\n",
       "   0.5540891885757446,\n",
       "   0.8313782811164856,\n",
       "   0.5733475089073181,\n",
       "   0.4617811143398285,\n",
       "   0.33362555503845215,\n",
       "   0.5262445211410522,\n",
       "   0.45837417244911194,\n",
       "   0.390918105840683,\n",
       "   0.9548255801200867,\n",
       "   0.7350761294364929,\n",
       "   0.4789186418056488,\n",
       "   0.6684906482696533,\n",
       "   0.5351191163063049,\n",
       "   0.6716580986976624,\n",
       "   0.5019736886024475,\n",
       "   0.9259206056594849,\n",
       "   0.7475588917732239,\n",
       "   0.6268036961555481,\n",
       "   0.5005176067352295,\n",
       "   0.692741334438324,\n",
       "   0.542508065700531,\n",
       "   0.5988234877586365,\n",
       "   0.9391497373580933,\n",
       "   0.539847195148468,\n",
       "   0.4663919508457184,\n",
       "   0.7128303647041321,\n",
       "   0.5174659490585327,\n",
       "   0.5893128514289856,\n",
       "   0.688732385635376,\n",
       "   0.49304383993148804,\n",
       "   0.583141028881073,\n",
       "   0.7041978240013123,\n",
       "   0.7741525173187256,\n",
       "   0.7281026244163513,\n",
       "   0.6832098960876465,\n",
       "   0.40716415643692017,\n",
       "   0.9120370745658875,\n",
       "   0.8055062890052795,\n",
       "   0.4860917627811432,\n",
       "   0.6228340268135071,\n",
       "   0.6955113410949707,\n",
       "   0.5085079669952393,\n",
       "   0.6141871213912964,\n",
       "   0.507212221622467,\n",
       "   0.5592188239097595,\n",
       "   0.6356930732727051,\n",
       "   0.6832472085952759,\n",
       "   0.6882488131523132,\n",
       "   0.968265950679779,\n",
       "   0.4042680859565735,\n",
       "   0.5170057415962219,\n",
       "   0.5163141489028931,\n",
       "   0.49127066135406494,\n",
       "   1.0092681646347046,\n",
       "   0.3723972737789154,\n",
       "   0.6018324494361877,\n",
       "   0.627169668674469,\n",
       "   0.4306887984275818,\n",
       "   0.35619884729385376,\n",
       "   0.3711654841899872,\n",
       "   0.7004224061965942,\n",
       "   0.5334319472312927,\n",
       "   0.6579405665397644,\n",
       "   0.6262574195861816,\n",
       "   0.661827802658081,\n",
       "   0.3158898949623108,\n",
       "   0.5977708697319031,\n",
       "   0.763085126876831,\n",
       "   0.43645012378692627,\n",
       "   0.5738995671272278,\n",
       "   0.5083428621292114,\n",
       "   0.7451717853546143,\n",
       "   0.5701158046722412,\n",
       "   0.41044193506240845,\n",
       "   0.6171779632568359,\n",
       "   0.4823395013809204,\n",
       "   0.8442443013191223,\n",
       "   0.5206971764564514,\n",
       "   0.6793959140777588,\n",
       "   0.4941157102584839,\n",
       "   0.6349212527275085,\n",
       "   0.42874282598495483,\n",
       "   0.9802446365356445,\n",
       "   0.8882261514663696,\n",
       "   0.5163580179214478,\n",
       "   0.4737609326839447,\n",
       "   0.5724018812179565,\n",
       "   0.5199697017669678,\n",
       "   0.6515285968780518,\n",
       "   0.6801077127456665,\n",
       "   0.3824911117553711,\n",
       "   0.5621745586395264,\n",
       "   0.6192822456359863,\n",
       "   0.7404460310935974,\n",
       "   0.8040797114372253,\n",
       "   0.3917004466056824,\n",
       "   0.6580401062965393,\n",
       "   0.664881706237793,\n",
       "   0.43035876750946045,\n",
       "   0.4914887249469757,\n",
       "   1.1279325485229492,\n",
       "   0.7508130073547363,\n",
       "   0.6739608645439148,\n",
       "   0.38145264983177185,\n",
       "   0.47771865129470825,\n",
       "   0.5992075204849243,\n",
       "   0.5255648493766785,\n",
       "   0.4494553506374359,\n",
       "   0.6955723762512207,\n",
       "   0.5616952776908875,\n",
       "   0.7074947357177734,\n",
       "   0.6731886863708496,\n",
       "   0.5708425045013428,\n",
       "   0.7992222905158997,\n",
       "   0.6333558559417725,\n",
       "   0.6347446441650391,\n",
       "   0.5012131333351135,\n",
       "   0.592116117477417,\n",
       "   0.4512796103954315,\n",
       "   0.4759487807750702,\n",
       "   0.556820273399353,\n",
       "   0.6077094078063965,\n",
       "   0.4567844569683075,\n",
       "   0.5386797189712524,\n",
       "   0.595719039440155,\n",
       "   0.6054683923721313,\n",
       "   0.5564695596694946,\n",
       "   0.6492299437522888,\n",
       "   0.600229799747467,\n",
       "   0.5337790250778198,\n",
       "   0.5189247727394104,\n",
       "   0.48594677448272705,\n",
       "   0.41553232073783875,\n",
       "   0.575661301612854,\n",
       "   0.4847898483276367,\n",
       "   0.5469862818717957,\n",
       "   0.9054031372070312,\n",
       "   0.7799564599990845,\n",
       "   0.5286548733711243,\n",
       "   0.5687072277069092,\n",
       "   0.4075065553188324,\n",
       "   0.4855659008026123,\n",
       "   0.5203854441642761,\n",
       "   0.6267803311347961,\n",
       "   0.48472803831100464,\n",
       "   0.67058265209198,\n",
       "   0.3918435275554657,\n",
       "   0.6646991968154907,\n",
       "   0.5375505685806274,\n",
       "   0.5241284370422363,\n",
       "   0.7758238911628723,\n",
       "   0.37163206934928894,\n",
       "   0.5544890761375427,\n",
       "   0.5243570804595947,\n",
       "   0.7163388729095459,\n",
       "   0.32410353422164917,\n",
       "   0.6993549466133118,\n",
       "   0.5060097575187683,\n",
       "   0.47689124941825867,\n",
       "   0.6555317044258118,\n",
       "   0.4197772741317749,\n",
       "   0.7119113206863403,\n",
       "   0.5800583362579346,\n",
       "   0.41180354356765747,\n",
       "   0.47815030813217163,\n",
       "   0.5747683644294739,\n",
       "   0.6589785218238831,\n",
       "   0.5013659000396729,\n",
       "   0.6762198209762573,\n",
       "   0.44186773896217346,\n",
       "   0.44170063734054565,\n",
       "   0.44294679164886475,\n",
       "   0.4917764365673065,\n",
       "   0.6861132383346558,\n",
       "   0.7423427104949951,\n",
       "   0.5973727703094482,\n",
       "   0.3600471615791321,\n",
       "   0.8995466828346252,\n",
       "   0.6041198372840881,\n",
       "   0.5928427577018738,\n",
       "   0.5456071496009827,\n",
       "   0.7253624200820923,\n",
       "   0.6987441182136536,\n",
       "   0.8231449127197266,\n",
       "   0.5643940567970276,\n",
       "   0.47382354736328125,\n",
       "   0.670573353767395,\n",
       "   0.7988815307617188,\n",
       "   0.4962504506111145,\n",
       "   0.5473235845565796,\n",
       "   0.6158801317214966,\n",
       "   0.6553158164024353,\n",
       "   0.4602515995502472,\n",
       "   0.438114732503891,\n",
       "   0.7447032332420349,\n",
       "   0.4812341630458832,\n",
       "   0.577448308467865,\n",
       "   0.6296146512031555,\n",
       "   0.3478110432624817,\n",
       "   0.6727113723754883,\n",
       "   0.8764877319335938,\n",
       "   0.5819105505943298,\n",
       "   0.5979053378105164,\n",
       "   0.46401265263557434,\n",
       "   0.5941994786262512,\n",
       "   0.727341890335083,\n",
       "   0.5491726398468018,\n",
       "   0.48160460591316223,\n",
       "   0.6222859025001526,\n",
       "   0.683601975440979,\n",
       "   0.5684530138969421,\n",
       "   0.6809476017951965,\n",
       "   0.709415078163147,\n",
       "   0.4988442659378052,\n",
       "   0.5694546699523926,\n",
       "   0.5380479097366333,\n",
       "   0.6857338547706604,\n",
       "   0.5098621249198914,\n",
       "   0.5684963464736938,\n",
       "   0.5693463683128357,\n",
       "   0.7157023549079895,\n",
       "   0.43876299262046814,\n",
       "   0.7638653516769409,\n",
       "   0.5731527209281921,\n",
       "   0.6367858052253723,\n",
       "   0.48124438524246216,\n",
       "   1.0470227003097534,\n",
       "   0.7780981063842773,\n",
       "   0.5917081832885742,\n",
       "   1.05698823928833,\n",
       "   0.48273783922195435,\n",
       "   0.49415475130081177,\n",
       "   0.5945636034011841,\n",
       "   1.0226656198501587,\n",
       "   0.5741525888442993,\n",
       "   0.8666215538978577,\n",
       "   0.4975893199443817,\n",
       "   0.7069604992866516,\n",
       "   0.8433697819709778,\n",
       "   0.49034595489501953,\n",
       "   0.3329896628856659,\n",
       "   0.5970598459243774,\n",
       "   0.5544826984405518,\n",
       "   0.6899140477180481,\n",
       "   0.5460309982299805,\n",
       "   0.398121178150177,\n",
       "   0.38962242007255554,\n",
       "   0.5390483736991882,\n",
       "   0.5375264883041382,\n",
       "   0.5846655368804932,\n",
       "   0.5179450511932373,\n",
       "   0.6163444519042969,\n",
       "   0.5442443490028381,\n",
       "   0.5010012984275818,\n",
       "   0.5043258666992188,\n",
       "   0.5155674815177917,\n",
       "   0.5058170557022095,\n",
       "   0.4581281542778015,\n",
       "   0.4921586215496063,\n",
       "   0.4633941948413849,\n",
       "   0.5131279230117798,\n",
       "   1.1292833089828491,\n",
       "   0.373567134141922,\n",
       "   0.4403690993785858,\n",
       "   0.507458508014679,\n",
       "   0.7327693700790405,\n",
       "   0.7564803957939148,\n",
       "   0.5526825189590454,\n",
       "   0.6657463908195496,\n",
       "   0.715603768825531,\n",
       "   0.5585368275642395,\n",
       "   0.6485475897789001,\n",
       "   0.5136707425117493,\n",
       "   0.49270662665367126,\n",
       "   0.41776156425476074,\n",
       "   0.3827747702598572,\n",
       "   0.6275627017021179,\n",
       "   0.418874055147171,\n",
       "   0.6381188035011292,\n",
       "   0.5141465663909912,\n",
       "   0.39856693148612976,\n",
       "   0.7036909461021423,\n",
       "   0.43211305141448975,\n",
       "   0.5504504442214966,\n",
       "   0.3729342818260193,\n",
       "   0.6287752985954285,\n",
       "   0.68803870677948,\n",
       "   0.4721459448337555,\n",
       "   0.7612656950950623,\n",
       "   0.6409385800361633,\n",
       "   0.6390045285224915,\n",
       "   0.42210906744003296,\n",
       "   0.5453873872756958,\n",
       "   0.49106356501579285,\n",
       "   0.623464822769165,\n",
       "   0.5472401976585388,\n",
       "   0.8357234001159668,\n",
       "   0.46997740864753723,\n",
       "   0.6098131537437439,\n",
       "   0.4806123673915863,\n",
       "   0.7613642811775208,\n",
       "   0.5357930064201355,\n",
       "   0.5757352113723755,\n",
       "   0.9006451368331909,\n",
       "   0.6357408165931702,\n",
       "   0.5231704711914062,\n",
       "   0.559042751789093,\n",
       "   0.4212794601917267,\n",
       "   0.31440213322639465,\n",
       "   0.5094371438026428,\n",
       "   0.5297833681106567,\n",
       "   0.44037169218063354,\n",
       "   0.35584983229637146,\n",
       "   0.8568077683448792,\n",
       "   0.5829609632492065,\n",
       "   0.7997849583625793,\n",
       "   0.481697678565979,\n",
       "   0.43229684233665466,\n",
       "   0.6836176514625549,\n",
       "   0.43470290303230286,\n",
       "   0.5129136443138123,\n",
       "   0.5159005522727966,\n",
       "   0.7178567051887512,\n",
       "   0.42228373885154724,\n",
       "   0.5920198559761047,\n",
       "   0.5832056999206543,\n",
       "   0.5799728035926819,\n",
       "   0.517556369304657,\n",
       "   0.5282113552093506,\n",
       "   0.5540682077407837,\n",
       "   0.47442883253097534,\n",
       "   0.3686157166957855,\n",
       "   0.830269992351532,\n",
       "   0.47391435503959656,\n",
       "   0.531998336315155,\n",
       "   0.5580770373344421,\n",
       "   0.7291931509971619,\n",
       "   0.5470909476280212,\n",
       "   0.5963670015335083,\n",
       "   0.5760383605957031,\n",
       "   0.5471144318580627,\n",
       "   0.3580986559391022,\n",
       "   0.6609709858894348,\n",
       "   0.6701814532279968,\n",
       "   0.7105475664138794,\n",
       "   0.3881882429122925,\n",
       "   0.6651128530502319,\n",
       "   0.5665343403816223,\n",
       "   0.5071942806243896,\n",
       "   0.43350914120674133,\n",
       "   0.5577793717384338,\n",
       "   0.6039817929267883,\n",
       "   0.5338420271873474,\n",
       "   0.5069848895072937,\n",
       "   0.5642927289009094,\n",
       "   0.9595751166343689,\n",
       "   0.8271070718765259,\n",
       "   0.4947223365306854,\n",
       "   1.2870486974716187,\n",
       "   0.4404611885547638,\n",
       "   0.8327752351760864,\n",
       "   0.510497510433197,\n",
       "   0.6338707208633423,\n",
       "   0.47676026821136475,\n",
       "   0.6974141597747803,\n",
       "   0.47361916303634644,\n",
       "   0.4187571704387665,\n",
       "   0.5923404097557068,\n",
       "   0.529971182346344,\n",
       "   0.5938511490821838,\n",
       "   0.5948971509933472,\n",
       "   0.7188659906387329,\n",
       "   0.5572364330291748,\n",
       "   0.5710159540176392,\n",
       "   0.8329363465309143,\n",
       "   0.34864142537117004,\n",
       "   0.3747294843196869,\n",
       "   0.4061836898326874,\n",
       "   0.4529673755168915,\n",
       "   0.5689578056335449,\n",
       "   0.45461738109588623,\n",
       "   0.6055338978767395,\n",
       "   0.5902531743049622,\n",
       "   0.8051123023033142,\n",
       "   0.45597168803215027,\n",
       "   0.4565396010875702,\n",
       "   0.5629880428314209,\n",
       "   0.523408830165863,\n",
       "   0.6981525421142578,\n",
       "   0.8506486415863037,\n",
       "   0.47661668062210083,\n",
       "   0.7442893385887146,\n",
       "   0.6104977130889893,\n",
       "   0.6545993685722351,\n",
       "   0.3750794231891632,\n",
       "   0.4316989779472351,\n",
       "   0.5390625596046448,\n",
       "   0.7302889227867126,\n",
       "   0.5275316834449768,\n",
       "   0.44010186195373535,\n",
       "   0.44522398710250854,\n",
       "   0.7439544200897217,\n",
       "   0.47167733311653137,\n",
       "   0.6842063069343567,\n",
       "   0.5372928977012634,\n",
       "   0.5650533437728882],\n",
       "  'train_voc_loss': [1.9683254957199097,\n",
       "   1.8485445976257324,\n",
       "   1.7414538860321045,\n",
       "   1.7124608755111694,\n",
       "   2.040254592895508,\n",
       "   2.039905548095703,\n",
       "   2.0282957553863525,\n",
       "   1.5306907892227173,\n",
       "   1.6771399974822998,\n",
       "   1.4373234510421753,\n",
       "   1.3115170001983643,\n",
       "   1.610030174255371,\n",
       "   1.2426666021347046,\n",
       "   1.5555413961410522,\n",
       "   1.6823415756225586,\n",
       "   1.5661934614181519,\n",
       "   1.5461360216140747,\n",
       "   1.3896819353103638,\n",
       "   1.567206859588623,\n",
       "   1.9278056621551514,\n",
       "   1.2848281860351562,\n",
       "   1.82277512550354,\n",
       "   1.3073195219039917,\n",
       "   2.1054067611694336,\n",
       "   1.7270869016647339,\n",
       "   1.3059906959533691,\n",
       "   1.4825494289398193,\n",
       "   1.166218638420105,\n",
       "   1.5095826387405396,\n",
       "   1.8912622928619385,\n",
       "   1.302393913269043,\n",
       "   1.3682552576065063,\n",
       "   1.2146918773651123,\n",
       "   1.0820517539978027,\n",
       "   1.357024073600769,\n",
       "   1.4808688163757324,\n",
       "   1.7502459287643433,\n",
       "   1.5138088464736938,\n",
       "   1.604420781135559,\n",
       "   1.565438985824585,\n",
       "   1.4305334091186523,\n",
       "   1.2745689153671265,\n",
       "   1.4944803714752197,\n",
       "   1.2261559963226318,\n",
       "   1.2974891662597656,\n",
       "   1.659335732460022,\n",
       "   1.5555840730667114,\n",
       "   1.3786723613739014,\n",
       "   1.0435136556625366,\n",
       "   1.1748554706573486,\n",
       "   1.4894901514053345,\n",
       "   1.0055222511291504,\n",
       "   1.4682883024215698,\n",
       "   1.5143736600875854,\n",
       "   1.5993523597717285,\n",
       "   1.6591542959213257,\n",
       "   1.3330737352371216,\n",
       "   1.3007467985153198,\n",
       "   1.1248964071273804,\n",
       "   1.010445237159729,\n",
       "   1.259731650352478,\n",
       "   1.3163889646530151,\n",
       "   1.2027703523635864,\n",
       "   1.055796504020691,\n",
       "   1.0865166187286377,\n",
       "   1.145374059677124,\n",
       "   1.2016950845718384,\n",
       "   1.298404574394226,\n",
       "   0.9777397513389587,\n",
       "   1.1524195671081543,\n",
       "   1.1353181600570679,\n",
       "   1.27065908908844,\n",
       "   1.1967034339904785,\n",
       "   1.2616031169891357,\n",
       "   1.2109490633010864,\n",
       "   1.6393781900405884,\n",
       "   1.4151065349578857,\n",
       "   0.9364504814147949,\n",
       "   1.2054643630981445,\n",
       "   1.8111859560012817,\n",
       "   1.2671993970870972,\n",
       "   1.2185816764831543,\n",
       "   1.1065254211425781,\n",
       "   1.2274149656295776,\n",
       "   0.9482995271682739,\n",
       "   1.0081785917282104,\n",
       "   1.021538496017456,\n",
       "   1.1350098848342896,\n",
       "   1.1459598541259766,\n",
       "   1.1609477996826172,\n",
       "   1.0615029335021973,\n",
       "   1.215708613395691,\n",
       "   1.2260427474975586,\n",
       "   1.2669472694396973,\n",
       "   1.0260374546051025,\n",
       "   0.8727229833602905,\n",
       "   0.9015462398529053,\n",
       "   1.4668498039245605,\n",
       "   0.847905158996582,\n",
       "   0.9271970987319946,\n",
       "   0.8579281568527222,\n",
       "   1.0759772062301636,\n",
       "   1.1835556030273438,\n",
       "   0.996826171875,\n",
       "   1.2878676652908325,\n",
       "   1.014094591140747,\n",
       "   1.0548250675201416,\n",
       "   1.2565745115280151,\n",
       "   0.9730187654495239,\n",
       "   1.2941707372665405,\n",
       "   0.9423704147338867,\n",
       "   1.0055068731307983,\n",
       "   1.1412070989608765,\n",
       "   1.007718563079834,\n",
       "   1.3322727680206299,\n",
       "   0.802998960018158,\n",
       "   1.507598876953125,\n",
       "   1.0623284578323364,\n",
       "   0.921265184879303,\n",
       "   0.8882890343666077,\n",
       "   0.8629993200302124,\n",
       "   1.1375173330307007,\n",
       "   0.847892165184021,\n",
       "   1.1958166360855103,\n",
       "   0.7791954874992371,\n",
       "   1.2540075778961182,\n",
       "   1.2199221849441528,\n",
       "   1.106244444847107,\n",
       "   0.6332293748855591,\n",
       "   0.6919865012168884,\n",
       "   0.9355245232582092,\n",
       "   1.0131473541259766,\n",
       "   1.0147953033447266,\n",
       "   0.8656352162361145,\n",
       "   1.3221884965896606,\n",
       "   1.0570882558822632,\n",
       "   0.9477182030677795,\n",
       "   1.3618152141571045,\n",
       "   0.8925977945327759,\n",
       "   0.8558655977249146,\n",
       "   1.0937060117721558,\n",
       "   0.9556945562362671,\n",
       "   1.2563517093658447,\n",
       "   0.9618879556655884,\n",
       "   0.8774797916412354,\n",
       "   1.1417855024337769,\n",
       "   1.18081533908844,\n",
       "   1.0874741077423096,\n",
       "   1.0354446172714233,\n",
       "   0.8331519365310669,\n",
       "   0.8017674088478088,\n",
       "   0.847062885761261,\n",
       "   1.2817482948303223,\n",
       "   0.8376883864402771,\n",
       "   1.2630783319473267,\n",
       "   0.9789702892303467,\n",
       "   1.281658411026001,\n",
       "   0.9085366129875183,\n",
       "   0.9727864265441895,\n",
       "   0.986139714717865,\n",
       "   0.875315248966217,\n",
       "   0.8697839379310608,\n",
       "   1.02540123462677,\n",
       "   0.9885756373405457,\n",
       "   1.1597718000411987,\n",
       "   0.9625004529953003,\n",
       "   1.0230168104171753,\n",
       "   0.6792938113212585,\n",
       "   1.1439249515533447,\n",
       "   1.1154624223709106,\n",
       "   1.0591182708740234,\n",
       "   0.7338738441467285,\n",
       "   0.9856040477752686,\n",
       "   0.9523488879203796,\n",
       "   0.9011781215667725,\n",
       "   1.2940294742584229,\n",
       "   0.8310645818710327,\n",
       "   1.3057169914245605,\n",
       "   1.1300268173217773,\n",
       "   1.1421890258789062,\n",
       "   0.7968342900276184,\n",
       "   1.160295844078064,\n",
       "   0.8202199935913086,\n",
       "   0.830906867980957,\n",
       "   0.7309756278991699,\n",
       "   1.0563846826553345,\n",
       "   1.1119483709335327,\n",
       "   1.0608174800872803,\n",
       "   1.0213475227355957,\n",
       "   0.9739815592765808,\n",
       "   0.967086672782898,\n",
       "   0.723571240901947,\n",
       "   0.9528481364250183,\n",
       "   1.1752971410751343,\n",
       "   1.211073398590088,\n",
       "   0.6481403708457947,\n",
       "   0.8760543465614319,\n",
       "   1.0812509059906006,\n",
       "   0.9553205966949463,\n",
       "   0.8391122221946716,\n",
       "   1.003423810005188,\n",
       "   0.7786895632743835,\n",
       "   1.2101918458938599,\n",
       "   0.9945505857467651,\n",
       "   1.2718454599380493,\n",
       "   0.9963544607162476,\n",
       "   0.8045486807823181,\n",
       "   0.7758927941322327,\n",
       "   0.9274791479110718,\n",
       "   1.107041597366333,\n",
       "   0.8722119927406311,\n",
       "   0.9202720522880554,\n",
       "   0.9080623984336853,\n",
       "   1.033992886543274,\n",
       "   0.993561863899231,\n",
       "   0.9502905011177063,\n",
       "   1.0072115659713745,\n",
       "   0.5784444808959961,\n",
       "   0.874424397945404,\n",
       "   0.9729948043823242,\n",
       "   1.0877447128295898,\n",
       "   1.101201057434082,\n",
       "   1.058863878250122,\n",
       "   0.5514864921569824,\n",
       "   1.0087504386901855,\n",
       "   1.2381470203399658,\n",
       "   0.64876788854599,\n",
       "   0.8729982376098633,\n",
       "   0.6882399916648865,\n",
       "   1.2194842100143433,\n",
       "   0.7935522198677063,\n",
       "   0.8710499405860901,\n",
       "   0.9938203692436218,\n",
       "   0.9890987277030945,\n",
       "   1.1053422689437866,\n",
       "   0.6208606958389282,\n",
       "   0.8547735810279846,\n",
       "   0.8937300443649292,\n",
       "   0.8954732418060303,\n",
       "   0.8345766067504883,\n",
       "   0.8958337903022766,\n",
       "   1.260640025138855,\n",
       "   1.0378060340881348,\n",
       "   0.8272361755371094,\n",
       "   0.9612981081008911,\n",
       "   0.8419663310050964,\n",
       "   0.9748445153236389,\n",
       "   1.2141883373260498,\n",
       "   0.7038059234619141,\n",
       "   1.2658123970031738,\n",
       "   0.7771232724189758,\n",
       "   0.9284042119979858,\n",
       "   0.9124010801315308,\n",
       "   0.9352827072143555,\n",
       "   0.8999922871589661,\n",
       "   0.7665082812309265,\n",
       "   0.9768656492233276,\n",
       "   0.9420091509819031,\n",
       "   0.978828489780426,\n",
       "   0.8337990641593933,\n",
       "   1.099129557609558,\n",
       "   0.9046844840049744,\n",
       "   0.9407356381416321,\n",
       "   0.9946216344833374,\n",
       "   0.8608030676841736,\n",
       "   0.8192784786224365,\n",
       "   0.9019429087638855,\n",
       "   0.7570082545280457,\n",
       "   0.9706389904022217,\n",
       "   0.75114905834198,\n",
       "   1.0047085285186768,\n",
       "   0.6283395290374756,\n",
       "   1.0335561037063599,\n",
       "   0.8287237882614136,\n",
       "   1.0556505918502808,\n",
       "   0.8041052222251892,\n",
       "   0.9234931468963623,\n",
       "   0.7619414329528809,\n",
       "   1.0253448486328125,\n",
       "   0.8322389125823975,\n",
       "   0.675738513469696,\n",
       "   0.7797162532806396,\n",
       "   0.8344252109527588,\n",
       "   0.8500539660453796,\n",
       "   0.8843940496444702,\n",
       "   0.9079994559288025,\n",
       "   1.2767126560211182,\n",
       "   1.2899929285049438,\n",
       "   1.0876284837722778,\n",
       "   0.9604844450950623,\n",
       "   1.050916075706482,\n",
       "   0.943513810634613,\n",
       "   1.2816359996795654,\n",
       "   1.0413230657577515,\n",
       "   0.8844766616821289,\n",
       "   1.0413306951522827,\n",
       "   0.8270395994186401,\n",
       "   0.8553844690322876,\n",
       "   0.9980257749557495,\n",
       "   0.8635388612747192,\n",
       "   0.7647684216499329,\n",
       "   0.8921670913696289,\n",
       "   1.0128933191299438,\n",
       "   0.744680643081665,\n",
       "   1.05271315574646,\n",
       "   0.8620778918266296,\n",
       "   0.8322820663452148,\n",
       "   1.0860166549682617,\n",
       "   0.9212834239006042,\n",
       "   0.8475087881088257,\n",
       "   1.11231529712677,\n",
       "   1.1020985841751099,\n",
       "   0.69805908203125,\n",
       "   0.9573234915733337,\n",
       "   1.0987530946731567,\n",
       "   0.7984652519226074,\n",
       "   0.9705206155776978,\n",
       "   0.8560587167739868,\n",
       "   0.885915994644165,\n",
       "   0.8981694579124451,\n",
       "   0.7058164477348328,\n",
       "   0.7207136750221252,\n",
       "   0.9565775990486145,\n",
       "   0.8740643858909607,\n",
       "   0.9890300035476685,\n",
       "   1.166684865951538,\n",
       "   1.096104383468628,\n",
       "   0.9710026383399963,\n",
       "   1.3616297245025635,\n",
       "   1.0460505485534668,\n",
       "   1.1721365451812744,\n",
       "   0.946825385093689,\n",
       "   0.8117328882217407,\n",
       "   0.6151800155639648,\n",
       "   0.8616582155227661,\n",
       "   0.8082736730575562,\n",
       "   0.9315339922904968,\n",
       "   0.896559476852417,\n",
       "   0.7870740294456482,\n",
       "   0.7563210725784302,\n",
       "   0.9625817537307739,\n",
       "   1.0321264266967773,\n",
       "   0.9260467886924744,\n",
       "   0.8516845107078552,\n",
       "   0.8446280360221863,\n",
       "   1.081078052520752,\n",
       "   1.2268747091293335,\n",
       "   0.831767201423645,\n",
       "   0.787583589553833,\n",
       "   0.6400005221366882,\n",
       "   0.8553913235664368,\n",
       "   0.9034038186073303,\n",
       "   0.6184225678443909,\n",
       "   0.9229533672332764,\n",
       "   0.8340974450111389,\n",
       "   1.0281139612197876,\n",
       "   1.3301666975021362,\n",
       "   0.9784762263298035,\n",
       "   0.8993932604789734,\n",
       "   0.6032382249832153,\n",
       "   0.7820708155632019,\n",
       "   0.7570930123329163,\n",
       "   0.7289441823959351,\n",
       "   0.7444018721580505,\n",
       "   0.8520572185516357,\n",
       "   0.8123065829277039,\n",
       "   0.8183521032333374,\n",
       "   0.9913497567176819,\n",
       "   0.6875600814819336,\n",
       "   0.8113734126091003,\n",
       "   0.8598513603210449,\n",
       "   0.9267427921295166,\n",
       "   0.6964077949523926,\n",
       "   0.5939767956733704,\n",
       "   0.5993788838386536,\n",
       "   0.914544403553009,\n",
       "   1.0420256853103638,\n",
       "   0.9259949922561646,\n",
       "   0.6939689517021179,\n",
       "   0.7559119462966919,\n",
       "   1.089127779006958,\n",
       "   0.805450975894928,\n",
       "   1.2434008121490479,\n",
       "   1.319029688835144,\n",
       "   0.9658960103988647,\n",
       "   1.097913384437561,\n",
       "   0.7667502760887146,\n",
       "   0.811202347278595,\n",
       "   0.8349339365959167,\n",
       "   0.9707669615745544,\n",
       "   0.7906580567359924,\n",
       "   0.7669655680656433,\n",
       "   0.9251677989959717,\n",
       "   0.7550991773605347,\n",
       "   0.781176745891571,\n",
       "   0.8057861328125,\n",
       "   0.7211723923683167,\n",
       "   0.6092966794967651,\n",
       "   0.672686755657196,\n",
       "   0.6192160844802856,\n",
       "   0.9021896719932556,\n",
       "   0.5927733778953552,\n",
       "   1.1172276735305786,\n",
       "   1.1204710006713867,\n",
       "   1.000220775604248,\n",
       "   0.6765170097351074,\n",
       "   0.9936642646789551,\n",
       "   0.9941509366035461,\n",
       "   0.736944854259491,\n",
       "   0.6899063587188721,\n",
       "   1.0186576843261719,\n",
       "   0.5836395621299744,\n",
       "   1.0030657052993774,\n",
       "   0.8677741289138794,\n",
       "   0.8729197978973389,\n",
       "   0.8372963070869446,\n",
       "   0.7661294937133789,\n",
       "   0.8884261846542358,\n",
       "   0.9473573565483093,\n",
       "   0.4850562810897827,\n",
       "   1.039879322052002,\n",
       "   1.0710840225219727,\n",
       "   0.9251983165740967,\n",
       "   0.6404138207435608,\n",
       "   0.6091939210891724,\n",
       "   0.9437069892883301,\n",
       "   0.8276467323303223,\n",
       "   1.1512031555175781,\n",
       "   0.7017877697944641,\n",
       "   0.8333209753036499,\n",
       "   0.8350247144699097,\n",
       "   0.9852919578552246,\n",
       "   0.8601418733596802,\n",
       "   0.7029648423194885,\n",
       "   0.9645145535469055,\n",
       "   0.6529580950737,\n",
       "   0.6245729923248291,\n",
       "   0.7740992903709412,\n",
       "   1.0601915121078491,\n",
       "   0.7562922239303589,\n",
       "   0.7002360224723816,\n",
       "   0.8573402762413025,\n",
       "   0.8238248229026794,\n",
       "   0.9623505473136902,\n",
       "   0.6633362174034119,\n",
       "   0.5221544504165649,\n",
       "   0.9636216163635254,\n",
       "   0.8709726333618164,\n",
       "   0.8083432912826538,\n",
       "   1.1284140348434448,\n",
       "   0.6621360182762146,\n",
       "   1.1052488088607788,\n",
       "   1.0235368013381958,\n",
       "   1.1468557119369507,\n",
       "   0.8506228923797607,\n",
       "   0.800377368927002,\n",
       "   1.0198935270309448,\n",
       "   1.0108226537704468,\n",
       "   0.7515286803245544,\n",
       "   0.8901488184928894,\n",
       "   0.6641250848770142,\n",
       "   0.686201274394989,\n",
       "   0.8604446053504944,\n",
       "   0.8026770353317261,\n",
       "   0.7630004286766052,\n",
       "   0.8534275889396667,\n",
       "   1.230905532836914,\n",
       "   1.377511978149414,\n",
       "   0.8288300037384033,\n",
       "   0.8943560719490051,\n",
       "   0.7738669514656067,\n",
       "   0.851614773273468,\n",
       "   0.6566601991653442,\n",
       "   0.6450167894363403,\n",
       "   0.8951742649078369,\n",
       "   1.122024416923523,\n",
       "   0.6509303450584412,\n",
       "   0.8706217408180237,\n",
       "   0.6865898370742798,\n",
       "   0.8110659122467041,\n",
       "   0.7311701774597168,\n",
       "   0.5951629281044006,\n",
       "   0.9243084192276001,\n",
       "   0.9590203762054443,\n",
       "   0.9338419437408447,\n",
       "   0.7504920363426208,\n",
       "   0.7315772771835327,\n",
       "   0.6687105298042297,\n",
       "   1.0030986070632935,\n",
       "   0.9796055555343628,\n",
       "   0.730786144733429,\n",
       "   0.7902600765228271,\n",
       "   0.9424278736114502,\n",
       "   0.8922436833381653,\n",
       "   0.7806726694107056,\n",
       "   0.6437512040138245,\n",
       "   0.7451263070106506,\n",
       "   0.8275781273841858,\n",
       "   0.6207119822502136,\n",
       "   0.6128727197647095],\n",
       "  'train_jsc_loss': [0.4012385606765747,\n",
       "   0.38669270277023315,\n",
       "   0.46217426657676697,\n",
       "   0.44045957922935486,\n",
       "   0.43182018399238586,\n",
       "   0.4890371561050415,\n",
       "   0.5255961418151855,\n",
       "   0.4810066819190979,\n",
       "   0.39628729224205017,\n",
       "   0.5031587481498718,\n",
       "   0.44679391384124756,\n",
       "   0.47161900997161865,\n",
       "   0.4131975471973419,\n",
       "   0.5074591040611267,\n",
       "   0.4987841248512268,\n",
       "   0.41991376876831055,\n",
       "   0.4221978783607483,\n",
       "   0.43432900309562683,\n",
       "   0.4066295325756073,\n",
       "   0.41545483469963074,\n",
       "   0.46264591813087463,\n",
       "   0.5666466951370239,\n",
       "   0.48827219009399414,\n",
       "   0.4488881230354309,\n",
       "   0.5192398428916931,\n",
       "   0.47637590765953064,\n",
       "   0.4889872670173645,\n",
       "   0.4461042881011963,\n",
       "   0.47741734981536865,\n",
       "   0.45659470558166504,\n",
       "   0.4784396290779114,\n",
       "   0.456186443567276,\n",
       "   0.50369793176651,\n",
       "   0.48672300577163696,\n",
       "   0.49976420402526855,\n",
       "   0.45343640446662903,\n",
       "   0.5784483551979065,\n",
       "   0.3866817057132721,\n",
       "   0.47677794098854065,\n",
       "   0.465637743473053,\n",
       "   0.46714726090431213,\n",
       "   0.582021176815033,\n",
       "   0.4658244252204895,\n",
       "   0.551921546459198,\n",
       "   0.48291510343551636,\n",
       "   0.4935677647590637,\n",
       "   0.49826881289482117,\n",
       "   0.45503729581832886,\n",
       "   0.42925992608070374,\n",
       "   0.45070260763168335,\n",
       "   0.4472137987613678,\n",
       "   0.43657466769218445,\n",
       "   0.42975082993507385,\n",
       "   0.4295942187309265,\n",
       "   0.5117407441139221,\n",
       "   0.47152119874954224,\n",
       "   0.4526520371437073,\n",
       "   0.5347562432289124,\n",
       "   0.4808591604232788,\n",
       "   0.4904254972934723,\n",
       "   0.4609832465648651,\n",
       "   0.4257909655570984,\n",
       "   0.5061102509498596,\n",
       "   0.4129607379436493,\n",
       "   0.5315631031990051,\n",
       "   0.5295005440711975,\n",
       "   0.5099196434020996,\n",
       "   0.5618430376052856,\n",
       "   0.42146363854408264,\n",
       "   0.4489457607269287,\n",
       "   0.4643121659755707,\n",
       "   0.5164875388145447,\n",
       "   0.4177666902542114,\n",
       "   0.5114156603813171,\n",
       "   0.4355967938899994,\n",
       "   0.4483923316001892,\n",
       "   0.635874330997467,\n",
       "   0.4856247305870056,\n",
       "   0.48730504512786865,\n",
       "   0.4445032477378845,\n",
       "   0.4264974594116211,\n",
       "   0.4773978292942047,\n",
       "   0.4263923764228821,\n",
       "   0.4214518964290619,\n",
       "   0.49646708369255066,\n",
       "   0.42394691705703735,\n",
       "   0.4259062707424164,\n",
       "   0.5793100595474243,\n",
       "   0.4969542920589447,\n",
       "   0.43090733885765076,\n",
       "   0.5372801423072815,\n",
       "   0.5296250581741333,\n",
       "   0.472784161567688,\n",
       "   0.550078272819519,\n",
       "   0.46306312084198,\n",
       "   0.5411256551742554,\n",
       "   0.39393019676208496,\n",
       "   0.49005597829818726,\n",
       "   0.518324613571167,\n",
       "   0.5137961506843567,\n",
       "   0.5514139533042908,\n",
       "   0.536566436290741,\n",
       "   0.4453050196170807,\n",
       "   0.38120245933532715,\n",
       "   0.6032959818840027,\n",
       "   0.47965294122695923,\n",
       "   0.6886293888092041,\n",
       "   0.5651043653488159,\n",
       "   0.41176313161849976,\n",
       "   0.536281943321228,\n",
       "   0.42512914538383484,\n",
       "   0.495026171207428,\n",
       "   0.4276140034198761,\n",
       "   0.5241574048995972,\n",
       "   0.5588124990463257,\n",
       "   0.3937467634677887,\n",
       "   0.46880748867988586,\n",
       "   0.4251408278942108,\n",
       "   0.4417665898799896,\n",
       "   0.46818631887435913,\n",
       "   0.446829229593277,\n",
       "   0.441150426864624,\n",
       "   0.5077528953552246,\n",
       "   0.44269776344299316,\n",
       "   0.4008966386318207,\n",
       "   0.39782214164733887,\n",
       "   0.47598743438720703,\n",
       "   0.4625742733478546,\n",
       "   0.4599335491657257,\n",
       "   0.4869791567325592,\n",
       "   0.48997727036476135,\n",
       "   0.47301390767097473,\n",
       "   0.44342803955078125,\n",
       "   0.4156888723373413,\n",
       "   0.44318366050720215,\n",
       "   0.5248576998710632,\n",
       "   0.3877120316028595,\n",
       "   0.612177312374115,\n",
       "   0.5510254502296448,\n",
       "   0.4527892470359802,\n",
       "   0.44307610392570496,\n",
       "   0.48254892230033875,\n",
       "   0.4450127184391022,\n",
       "   0.39900878071784973,\n",
       "   0.46790239214897156,\n",
       "   0.506989061832428,\n",
       "   0.4155241549015045,\n",
       "   0.40923088788986206,\n",
       "   0.47189608216285706,\n",
       "   0.43600061535835266,\n",
       "   0.42130327224731445,\n",
       "   0.43838346004486084,\n",
       "   0.465714693069458,\n",
       "   0.44875988364219666,\n",
       "   0.5153843760490417,\n",
       "   0.4446471333503723,\n",
       "   0.46020808815956116,\n",
       "   0.471845418214798,\n",
       "   0.432513564825058,\n",
       "   0.37295278906822205,\n",
       "   0.42872339487075806,\n",
       "   0.44682103395462036,\n",
       "   0.4697648584842682,\n",
       "   0.4302505552768707,\n",
       "   0.5593122243881226,\n",
       "   0.41296470165252686,\n",
       "   0.4062421917915344,\n",
       "   0.5009151101112366,\n",
       "   0.44971299171447754,\n",
       "   0.480245977640152,\n",
       "   0.5204930305480957,\n",
       "   0.4477521777153015,\n",
       "   0.4781207740306854,\n",
       "   0.40309837460517883,\n",
       "   0.4788137674331665,\n",
       "   0.41987696290016174,\n",
       "   0.43493908643722534,\n",
       "   0.5300794243812561,\n",
       "   0.4643271267414093,\n",
       "   0.48378849029541016,\n",
       "   0.4940398633480072,\n",
       "   0.484098345041275,\n",
       "   0.4865841269493103,\n",
       "   0.4540320634841919,\n",
       "   0.5037797689437866,\n",
       "   0.4212329685688019,\n",
       "   0.43793055415153503,\n",
       "   0.4560057818889618,\n",
       "   0.5583928823471069,\n",
       "   0.4280925989151001,\n",
       "   0.5430894494056702,\n",
       "   0.43655481934547424,\n",
       "   0.45499280095100403,\n",
       "   0.5057559013366699,\n",
       "   0.42792466282844543,\n",
       "   0.4356158971786499,\n",
       "   0.39711353182792664,\n",
       "   0.5438277125358582,\n",
       "   0.40209296345710754,\n",
       "   0.6071301102638245,\n",
       "   0.5728313326835632,\n",
       "   0.5375674366950989,\n",
       "   0.4306847155094147,\n",
       "   0.4407518208026886,\n",
       "   0.4485255181789398,\n",
       "   0.44632187485694885,\n",
       "   0.4234805405139923,\n",
       "   0.5002444386482239,\n",
       "   0.4793483316898346,\n",
       "   0.45068687200546265,\n",
       "   0.4384443163871765,\n",
       "   0.43723562359809875,\n",
       "   0.44205617904663086,\n",
       "   0.414688378572464,\n",
       "   0.46262186765670776,\n",
       "   0.4795342683792114,\n",
       "   0.4961809515953064,\n",
       "   0.3884285092353821,\n",
       "   0.4940955340862274,\n",
       "   0.4838266968727112,\n",
       "   0.47007790207862854,\n",
       "   0.522399365901947,\n",
       "   0.45383942127227783,\n",
       "   0.4705599248409271,\n",
       "   0.4814196228981018,\n",
       "   0.39305222034454346,\n",
       "   0.5079061388969421,\n",
       "   0.4800668954849243,\n",
       "   0.4625774621963501,\n",
       "   0.4564840793609619,\n",
       "   0.42062926292419434,\n",
       "   0.4859551787376404,\n",
       "   0.4675742983818054,\n",
       "   0.41080889105796814,\n",
       "   0.44931715726852417,\n",
       "   0.5016325116157532,\n",
       "   0.5324395298957825,\n",
       "   0.4179287254810333,\n",
       "   0.4403761625289917,\n",
       "   0.42160919308662415,\n",
       "   0.4583502411842346,\n",
       "   0.3902628421783447,\n",
       "   0.3927125930786133,\n",
       "   0.4870010018348694,\n",
       "   0.3927208185195923,\n",
       "   0.5082118511199951,\n",
       "   0.4085151255130768,\n",
       "   0.40827634930610657,\n",
       "   0.4414244294166565,\n",
       "   0.6249464154243469,\n",
       "   0.4790962338447571,\n",
       "   0.4320996105670929,\n",
       "   0.5498518347740173,\n",
       "   0.43510955572128296,\n",
       "   0.5249418020248413,\n",
       "   0.4614781439304352,\n",
       "   0.5123311877250671,\n",
       "   0.4269168972969055,\n",
       "   0.6552894115447998,\n",
       "   0.4907764196395874,\n",
       "   0.439992219209671,\n",
       "   0.4907139539718628,\n",
       "   0.47129401564598083,\n",
       "   0.4148164987564087,\n",
       "   0.5113555788993835,\n",
       "   0.5800454020500183,\n",
       "   0.5896334648132324,\n",
       "   0.4708375334739685,\n",
       "   0.45314157009124756,\n",
       "   0.37746572494506836,\n",
       "   0.41060230135917664,\n",
       "   0.4381433129310608,\n",
       "   0.4558205008506775,\n",
       "   0.4266001880168915,\n",
       "   0.45281165838241577,\n",
       "   0.5027211308479309,\n",
       "   0.5010074973106384,\n",
       "   0.43531346321105957,\n",
       "   0.5102292895317078,\n",
       "   0.4473963677883148,\n",
       "   0.567780077457428,\n",
       "   0.48128485679626465,\n",
       "   0.4051571786403656,\n",
       "   0.41678470373153687,\n",
       "   0.4458863437175751,\n",
       "   0.4083403944969177,\n",
       "   0.3973901867866516,\n",
       "   0.435159832239151,\n",
       "   0.3901779055595398,\n",
       "   0.40593278408050537,\n",
       "   0.4313308000564575,\n",
       "   0.441012978553772,\n",
       "   0.5120640397071838,\n",
       "   0.3823620676994324,\n",
       "   0.4282466769218445,\n",
       "   0.4386390149593353,\n",
       "   0.46732980012893677,\n",
       "   0.45715248584747314,\n",
       "   0.4027648866176605,\n",
       "   0.4118971526622772,\n",
       "   0.4435957074165344,\n",
       "   0.4238775968551636,\n",
       "   0.41053932905197144,\n",
       "   0.39182496070861816,\n",
       "   0.4594990015029907,\n",
       "   0.4749605655670166,\n",
       "   0.4277384281158447,\n",
       "   0.4050357937812805,\n",
       "   0.4299841821193695,\n",
       "   0.4264964759349823,\n",
       "   0.39060667157173157,\n",
       "   0.44624581933021545,\n",
       "   0.44071897864341736,\n",
       "   0.4863888621330261,\n",
       "   0.4795502722263336,\n",
       "   0.3942696154117584,\n",
       "   0.41384807229042053,\n",
       "   0.4620306193828583,\n",
       "   0.4300083518028259,\n",
       "   0.37916022539138794,\n",
       "   0.4335963726043701,\n",
       "   0.4734959304332733,\n",
       "   0.3470795750617981,\n",
       "   0.48000937700271606,\n",
       "   0.4533405900001526,\n",
       "   0.4352867901325226,\n",
       "   0.39708054065704346,\n",
       "   0.4702663719654083,\n",
       "   0.4073982834815979,\n",
       "   0.39167192578315735,\n",
       "   0.41263630986213684,\n",
       "   0.4392797350883484,\n",
       "   0.4461383521556854,\n",
       "   0.4800829291343689,\n",
       "   0.40620964765548706,\n",
       "   0.43520352244377136,\n",
       "   0.39663857221603394,\n",
       "   0.4510628581047058,\n",
       "   0.5115212202072144,\n",
       "   0.43841683864593506,\n",
       "   0.4335009753704071,\n",
       "   0.34130609035491943,\n",
       "   0.491103857755661,\n",
       "   0.4820467233657837,\n",
       "   0.4911231994628906,\n",
       "   0.45768219232559204,\n",
       "   0.49293503165245056,\n",
       "   0.513585090637207,\n",
       "   0.4331737756729126,\n",
       "   0.5100343227386475,\n",
       "   0.41497763991355896,\n",
       "   0.4827168583869934,\n",
       "   0.36866915225982666,\n",
       "   0.4181729853153229,\n",
       "   0.48853445053100586,\n",
       "   0.5294215679168701,\n",
       "   0.4976225793361664,\n",
       "   0.4015260636806488,\n",
       "   0.41957980394363403,\n",
       "   0.4570312201976776,\n",
       "   0.4596919119358063,\n",
       "   0.45008593797683716,\n",
       "   0.3989095389842987,\n",
       "   0.5166522264480591,\n",
       "   0.4711854159832001,\n",
       "   0.46719178557395935,\n",
       "   0.48525190353393555,\n",
       "   0.43268051743507385,\n",
       "   0.46916091442108154,\n",
       "   0.43929585814476013,\n",
       "   0.4732974171638489,\n",
       "   0.4658428728580475,\n",
       "   0.3990176320075989,\n",
       "   0.5139769911766052,\n",
       "   0.4954155385494232,\n",
       "   0.4094008207321167,\n",
       "   0.4262600541114807,\n",
       "   0.5424546599388123,\n",
       "   0.4296783208847046,\n",
       "   0.513923704624176,\n",
       "   0.5395695567131042,\n",
       "   0.4050862193107605,\n",
       "   0.47846296429634094,\n",
       "   0.48229479789733887,\n",
       "   0.5069383978843689,\n",
       "   0.646669864654541,\n",
       "   0.43181154131889343,\n",
       "   0.48260560631752014,\n",
       "   0.6084344387054443,\n",
       "   0.43748319149017334,\n",
       "   0.4496765732765198,\n",
       "   0.4256362020969391,\n",
       "   0.4187060296535492,\n",
       "   0.48217669129371643,\n",
       "   0.5252381563186646,\n",
       "   0.42929431796073914,\n",
       "   0.4253142476081848,\n",
       "   0.49829334020614624,\n",
       "   0.42699187994003296,\n",
       "   0.47585129737854004,\n",
       "   0.40367886424064636,\n",
       "   0.44152215123176575,\n",
       "   0.43158915638923645,\n",
       "   0.3811967670917511,\n",
       "   0.4938998222351074,\n",
       "   0.5126939415931702,\n",
       "   0.41895610094070435,\n",
       "   0.448852002620697,\n",
       "   0.49376600980758667,\n",
       "   0.42737704515457153,\n",
       "   0.42381978034973145,\n",
       "   0.45254164934158325,\n",
       "   0.3924993872642517,\n",
       "   0.42801418900489807,\n",
       "   0.40137603878974915,\n",
       "   0.4220412075519562,\n",
       "   0.5517644882202148,\n",
       "   0.43417787551879883,\n",
       "   0.43380752205848694,\n",
       "   0.3916924297809601,\n",
       "   0.46589767932891846,\n",
       "   0.5533406734466553,\n",
       "   0.5486298203468323,\n",
       "   0.43770408630371094,\n",
       "   0.5576971173286438,\n",
       "   0.4165732264518738,\n",
       "   0.3989327549934387,\n",
       "   0.44347506761550903,\n",
       "   0.3956802189350128,\n",
       "   0.48238784074783325,\n",
       "   0.4615665078163147,\n",
       "   0.387770414352417,\n",
       "   0.41981276869773865,\n",
       "   0.4424685537815094,\n",
       "   0.4465833306312561,\n",
       "   0.44807788729667664,\n",
       "   0.520084798336029,\n",
       "   0.4184839427471161,\n",
       "   0.4680967628955841,\n",
       "   0.5061407089233398,\n",
       "   0.6206468343734741,\n",
       "   0.590817928314209,\n",
       "   0.42024803161621094,\n",
       "   0.47067388892173767,\n",
       "   0.42997661232948303,\n",
       "   0.4661472737789154,\n",
       "   0.41510871052742004,\n",
       "   0.45580023527145386,\n",
       "   0.39329150319099426,\n",
       "   0.5677485466003418,\n",
       "   0.4127098321914673,\n",
       "   0.468782514333725,\n",
       "   0.42713508009910583,\n",
       "   0.44318315386772156,\n",
       "   0.40313050150871277,\n",
       "   0.372492253780365,\n",
       "   0.42612412571907043,\n",
       "   0.42956990003585815,\n",
       "   0.41159379482269287,\n",
       "   0.4780280590057373,\n",
       "   0.4430190622806549,\n",
       "   0.45057836174964905,\n",
       "   0.42199909687042236,\n",
       "   0.43243563175201416,\n",
       "   0.38915079832077026,\n",
       "   0.5893341898918152,\n",
       "   0.40473583340644836,\n",
       "   0.4848777651786804,\n",
       "   0.37468811869621277,\n",
       "   0.40910133719444275,\n",
       "   0.4035269618034363,\n",
       "   0.5088841319084167,\n",
       "   0.4106249213218689,\n",
       "   0.5329418778419495,\n",
       "   0.4468436539173126,\n",
       "   0.38591450452804565,\n",
       "   0.46319350600242615,\n",
       "   0.49017268419265747,\n",
       "   0.3843449056148529,\n",
       "   0.4797748923301697,\n",
       "   0.4703126847743988,\n",
       "   0.4302389919757843,\n",
       "   0.43440482020378113,\n",
       "   0.3876875042915344,\n",
       "   0.639642059803009,\n",
       "   0.4338899850845337,\n",
       "   0.42111462354660034,\n",
       "   0.4198302626609802,\n",
       "   0.49587446451187134,\n",
       "   0.4166201055049896,\n",
       "   0.41084229946136475,\n",
       "   0.4660886824131012,\n",
       "   0.46837371587753296,\n",
       "   0.41387373208999634,\n",
       "   0.4669596552848816,\n",
       "   0.4435461163520813,\n",
       "   0.3869730234146118,\n",
       "   0.46030858159065247,\n",
       "   0.5634137392044067,\n",
       "   0.5295844078063965],\n",
       "  'train_ff_loss': [0.9139850735664368,\n",
       "   0.6861335039138794,\n",
       "   0.5132743716239929,\n",
       "   0.6536004543304443,\n",
       "   0.5740627646446228,\n",
       "   0.586243748664856,\n",
       "   0.8790755867958069,\n",
       "   0.5780368447303772,\n",
       "   0.628463864326477,\n",
       "   0.7580192685127258,\n",
       "   0.5001361966133118,\n",
       "   0.6533856987953186,\n",
       "   0.7389962673187256,\n",
       "   0.6401693224906921,\n",
       "   0.5588725209236145,\n",
       "   0.7449828386306763,\n",
       "   0.6464913487434387,\n",
       "   0.6570213437080383,\n",
       "   0.7092259526252747,\n",
       "   0.587226927280426,\n",
       "   0.6660160422325134,\n",
       "   0.6752064824104309,\n",
       "   0.8292747735977173,\n",
       "   0.5729632377624512,\n",
       "   0.5222464203834534,\n",
       "   0.6726937294006348,\n",
       "   0.8503722548484802,\n",
       "   0.5863068103790283,\n",
       "   0.6972460746765137,\n",
       "   0.84016352891922,\n",
       "   0.6616964340209961,\n",
       "   0.6597496271133423,\n",
       "   0.541454017162323,\n",
       "   0.6727270483970642,\n",
       "   0.7098929286003113,\n",
       "   0.5404314398765564,\n",
       "   0.4220435619354248,\n",
       "   0.7932433485984802,\n",
       "   0.5558652281761169,\n",
       "   0.9486461877822876,\n",
       "   0.5164967775344849,\n",
       "   0.46008607745170593,\n",
       "   0.5460508465766907,\n",
       "   0.5049210786819458,\n",
       "   0.5494444370269775,\n",
       "   0.46445947885513306,\n",
       "   0.5689277052879333,\n",
       "   0.6845861077308655,\n",
       "   0.5040497183799744,\n",
       "   0.49030202627182007,\n",
       "   0.7244164943695068,\n",
       "   0.5891981720924377,\n",
       "   0.7545877695083618,\n",
       "   0.6551951766014099,\n",
       "   0.5714312791824341,\n",
       "   0.7032845616340637,\n",
       "   0.6212620735168457,\n",
       "   0.7514358758926392,\n",
       "   0.6721751093864441,\n",
       "   0.6201521754264832,\n",
       "   0.5930303335189819,\n",
       "   0.6356028914451599,\n",
       "   0.5955822467803955,\n",
       "   0.6859191656112671,\n",
       "   0.4911980628967285,\n",
       "   0.6902416944503784,\n",
       "   0.6319810152053833,\n",
       "   0.881683886051178,\n",
       "   0.8782361745834351,\n",
       "   0.622782826423645,\n",
       "   0.6403681635856628,\n",
       "   0.5265160202980042,\n",
       "   0.5631312131881714,\n",
       "   0.5998978018760681,\n",
       "   0.5499956011772156,\n",
       "   0.5458579063415527,\n",
       "   0.6921757459640503,\n",
       "   0.6367412209510803,\n",
       "   0.604640543460846,\n",
       "   0.5583202838897705,\n",
       "   0.5969489216804504,\n",
       "   0.7074165344238281,\n",
       "   0.5766742825508118,\n",
       "   0.855985701084137,\n",
       "   0.5570560693740845,\n",
       "   0.6193381547927856,\n",
       "   0.5942575931549072,\n",
       "   0.7329462766647339,\n",
       "   0.4639856517314911,\n",
       "   0.7026077508926392,\n",
       "   0.5751808881759644,\n",
       "   0.5272096395492554,\n",
       "   0.5085440278053284,\n",
       "   0.5536144375801086,\n",
       "   0.593837320804596,\n",
       "   0.8106245994567871,\n",
       "   0.6106041669845581,\n",
       "   0.6236424446105957,\n",
       "   0.6028454899787903,\n",
       "   0.56871497631073,\n",
       "   0.5742876529693604,\n",
       "   0.6888130307197571,\n",
       "   0.4534357786178589,\n",
       "   0.510553777217865,\n",
       "   0.4618160128593445,\n",
       "   0.5303966999053955,\n",
       "   0.5758352875709534,\n",
       "   0.5068953633308411,\n",
       "   0.5829008221626282,\n",
       "   0.42878028750419617,\n",
       "   0.4985491931438446,\n",
       "   0.6811795830726624,\n",
       "   0.5425613522529602,\n",
       "   0.559995710849762,\n",
       "   0.841697096824646,\n",
       "   0.5464413166046143,\n",
       "   0.5309339165687561,\n",
       "   0.6693186163902283,\n",
       "   0.5110412836074829,\n",
       "   0.47012919187545776,\n",
       "   0.5138744115829468,\n",
       "   0.719140350818634,\n",
       "   0.5159203410148621,\n",
       "   0.4580996334552765,\n",
       "   0.5573716759681702,\n",
       "   0.5704656839370728,\n",
       "   0.6736791133880615,\n",
       "   0.5593364834785461,\n",
       "   0.5115672945976257,\n",
       "   0.48100993037223816,\n",
       "   0.4015443027019501,\n",
       "   0.490205854177475,\n",
       "   0.5809239149093628,\n",
       "   0.5079513788223267,\n",
       "   0.5862575769424438,\n",
       "   0.6521339416503906,\n",
       "   0.4663490653038025,\n",
       "   0.5128904581069946,\n",
       "   0.559081494808197,\n",
       "   0.6585254073143005,\n",
       "   0.7370421886444092,\n",
       "   0.4326588213443756,\n",
       "   0.5715048909187317,\n",
       "   0.6217370629310608,\n",
       "   0.6105536818504333,\n",
       "   0.531783401966095,\n",
       "   0.5351061224937439,\n",
       "   0.7974656224250793,\n",
       "   0.5057655572891235,\n",
       "   0.47890782356262207,\n",
       "   0.5114729404449463,\n",
       "   0.529600977897644,\n",
       "   0.4846900701522827,\n",
       "   0.5542484521865845,\n",
       "   0.6138301491737366,\n",
       "   0.5859664678573608,\n",
       "   0.4073810279369354,\n",
       "   0.6292622089385986,\n",
       "   0.6175569891929626,\n",
       "   0.6166059374809265,\n",
       "   0.7505260109901428,\n",
       "   0.4185321033000946,\n",
       "   0.6063205003738403,\n",
       "   0.45040395855903625,\n",
       "   0.5949292182922363,\n",
       "   0.561997652053833,\n",
       "   0.4863685071468353,\n",
       "   0.5772954225540161,\n",
       "   0.5848982930183411,\n",
       "   0.4828416109085083,\n",
       "   0.7962599396705627,\n",
       "   0.6935182213783264,\n",
       "   0.6989952325820923,\n",
       "   0.4534990191459656,\n",
       "   0.5804949998855591,\n",
       "   0.48416104912757874,\n",
       "   0.48792538046836853,\n",
       "   0.8605107069015503,\n",
       "   0.5553064942359924,\n",
       "   0.6222459673881531,\n",
       "   0.41523122787475586,\n",
       "   0.6622335314750671,\n",
       "   0.6231098175048828,\n",
       "   0.5040680766105652,\n",
       "   0.41336938738822937,\n",
       "   0.5034913420677185,\n",
       "   0.8023216724395752,\n",
       "   0.7084397673606873,\n",
       "   0.4886689782142639,\n",
       "   0.5558741092681885,\n",
       "   0.5615256428718567,\n",
       "   0.5950765013694763,\n",
       "   0.6262319684028625,\n",
       "   0.5326374173164368,\n",
       "   0.6116920709609985,\n",
       "   0.5550559759140015,\n",
       "   0.5293211340904236,\n",
       "   0.5310497283935547,\n",
       "   0.477783739566803,\n",
       "   0.5666220784187317,\n",
       "   0.524691104888916,\n",
       "   0.5017873644828796,\n",
       "   0.6350731253623962,\n",
       "   0.5312337875366211,\n",
       "   0.5444920063018799,\n",
       "   0.5309146046638489,\n",
       "   0.928202748298645,\n",
       "   0.5392403602600098,\n",
       "   0.5266764760017395,\n",
       "   0.4959462881088257,\n",
       "   0.5765926837921143,\n",
       "   0.49957141280174255,\n",
       "   0.5565375685691833,\n",
       "   0.5739412307739258,\n",
       "   0.42854395508766174,\n",
       "   0.5696872472763062,\n",
       "   0.4835355579853058,\n",
       "   0.46055546402931213,\n",
       "   0.5877718925476074,\n",
       "   0.6078678369522095,\n",
       "   0.6440384984016418,\n",
       "   0.6597995758056641,\n",
       "   0.6181944608688354,\n",
       "   0.5696147084236145,\n",
       "   0.46808314323425293,\n",
       "   0.5919217467308044,\n",
       "   0.6759831309318542,\n",
       "   0.42204511165618896,\n",
       "   0.4913177192211151,\n",
       "   0.8829939961433411,\n",
       "   0.6533975601196289,\n",
       "   0.52406907081604,\n",
       "   0.5216820240020752,\n",
       "   0.7160357236862183,\n",
       "   0.45296305418014526,\n",
       "   0.5356283187866211,\n",
       "   0.5491729974746704,\n",
       "   0.48066115379333496,\n",
       "   0.4720020890235901,\n",
       "   0.6121954917907715,\n",
       "   0.49002763628959656,\n",
       "   0.4032285511493683,\n",
       "   0.5479273200035095,\n",
       "   0.6792595386505127,\n",
       "   0.6157563328742981,\n",
       "   0.6780861616134644,\n",
       "   0.5806301832199097,\n",
       "   0.573538064956665,\n",
       "   0.5944019556045532,\n",
       "   0.453634649515152,\n",
       "   0.4605073928833008,\n",
       "   0.41292113065719604,\n",
       "   0.48510831594467163,\n",
       "   0.45687368512153625,\n",
       "   0.6184446811676025,\n",
       "   0.49680468440055847,\n",
       "   0.5503877401351929,\n",
       "   0.4799923300743103,\n",
       "   0.5099664926528931,\n",
       "   0.5369855761528015,\n",
       "   0.6111956238746643,\n",
       "   0.5554164052009583,\n",
       "   0.5769646167755127,\n",
       "   0.5435225367546082,\n",
       "   0.5119224786758423,\n",
       "   0.5379918217658997,\n",
       "   0.5811460018157959,\n",
       "   0.449186772108078,\n",
       "   0.6176161766052246,\n",
       "   0.5089207291603088,\n",
       "   0.638126790523529,\n",
       "   0.6362550258636475,\n",
       "   0.6031496524810791,\n",
       "   0.5843274593353271,\n",
       "   0.5013030767440796,\n",
       "   0.6327431797981262,\n",
       "   0.5709112882614136,\n",
       "   0.5316464304924011,\n",
       "   0.3969250023365021,\n",
       "   0.5303880572319031,\n",
       "   0.4575325846672058,\n",
       "   0.4527999758720398,\n",
       "   0.5419046878814697,\n",
       "   0.5146121382713318,\n",
       "   0.5660095810890198,\n",
       "   0.6672066450119019,\n",
       "   0.7435916662216187,\n",
       "   0.5385726690292358,\n",
       "   0.6413288712501526,\n",
       "   0.8084607124328613,\n",
       "   0.4897218644618988,\n",
       "   0.5937196016311646,\n",
       "   0.38531872630119324,\n",
       "   0.5666894316673279,\n",
       "   0.5198386311531067,\n",
       "   0.4861341118812561,\n",
       "   0.4874042570590973,\n",
       "   0.5456341505050659,\n",
       "   0.5053039789199829,\n",
       "   0.5057346820831299,\n",
       "   0.42839980125427246,\n",
       "   0.674334704875946,\n",
       "   0.5487114191055298,\n",
       "   0.48846128582954407,\n",
       "   0.38235336542129517,\n",
       "   0.42994964122772217,\n",
       "   0.8303700089454651,\n",
       "   0.5412471890449524,\n",
       "   0.5490736961364746,\n",
       "   0.6894521713256836,\n",
       "   0.5432488322257996,\n",
       "   0.600213885307312,\n",
       "   0.4200763404369354,\n",
       "   0.4960232079029083,\n",
       "   0.4822370707988739,\n",
       "   0.5091882944107056,\n",
       "   0.5800322890281677,\n",
       "   0.5551229119300842,\n",
       "   0.5547793507575989,\n",
       "   0.5910687446594238,\n",
       "   0.5136160850524902,\n",
       "   0.4450649619102478,\n",
       "   0.6226088404655457,\n",
       "   0.6081411838531494,\n",
       "   0.5789448618888855,\n",
       "   0.564699649810791,\n",
       "   0.5899977087974548,\n",
       "   0.5776907801628113,\n",
       "   0.3653371334075928,\n",
       "   0.5482273697853088,\n",
       "   0.6260676980018616,\n",
       "   0.4769545793533325,\n",
       "   0.4225323498249054,\n",
       "   0.5910497307777405,\n",
       "   0.4676249921321869,\n",
       "   0.6246470808982849,\n",
       "   0.656434178352356,\n",
       "   0.6465710401535034,\n",
       "   0.6755921840667725,\n",
       "   0.42968082427978516,\n",
       "   0.3808198869228363,\n",
       "   0.4442204236984253,\n",
       "   0.5469980835914612,\n",
       "   0.48383966088294983,\n",
       "   0.622766375541687,\n",
       "   0.49858707189559937,\n",
       "   0.5074455142021179,\n",
       "   0.5546939373016357,\n",
       "   0.48108214139938354,\n",
       "   0.7484133243560791,\n",
       "   0.42812421917915344,\n",
       "   0.6497378349304199,\n",
       "   0.37150701880455017,\n",
       "   0.4673999547958374,\n",
       "   0.6267754435539246,\n",
       "   0.767241358757019,\n",
       "   0.4408099353313446,\n",
       "   0.38631290197372437,\n",
       "   0.4800322651863098,\n",
       "   0.48397332429885864,\n",
       "   0.5089253187179565,\n",
       "   0.3843717575073242,\n",
       "   0.501854658126831,\n",
       "   0.5813918709754944,\n",
       "   0.5932206511497498,\n",
       "   0.5575631260871887,\n",
       "   0.4691791236400604,\n",
       "   0.45841366052627563,\n",
       "   0.6253863573074341,\n",
       "   0.4607184827327728,\n",
       "   0.5523284077644348,\n",
       "   0.38905465602874756,\n",
       "   0.409438818693161,\n",
       "   0.5710828304290771,\n",
       "   0.6632475852966309,\n",
       "   0.42459794878959656,\n",
       "   0.37263810634613037,\n",
       "   0.5329287648200989,\n",
       "   0.5955272912979126,\n",
       "   0.546517550945282,\n",
       "   0.38652217388153076,\n",
       "   0.4628523886203766,\n",
       "   0.4730750024318695,\n",
       "   0.40203285217285156,\n",
       "   0.4542046785354614,\n",
       "   0.4667084515094757,\n",
       "   0.43648672103881836,\n",
       "   0.46496737003326416,\n",
       "   0.40374672412872314,\n",
       "   0.7120689153671265,\n",
       "   0.5207025408744812,\n",
       "   0.5715049505233765,\n",
       "   0.5572484731674194,\n",
       "   0.4144945740699768,\n",
       "   0.6658796072006226,\n",
       "   0.6276839971542358,\n",
       "   0.48282748460769653,\n",
       "   0.43214091658592224,\n",
       "   0.6065804362297058,\n",
       "   0.40119412541389465,\n",
       "   0.4681762754917145,\n",
       "   0.5176730751991272,\n",
       "   0.5624833703041077,\n",
       "   0.5762589573860168,\n",
       "   0.4593495726585388,\n",
       "   0.6379728317260742,\n",
       "   0.8546478748321533,\n",
       "   0.4316761791706085,\n",
       "   0.46481087803840637,\n",
       "   0.5424947738647461,\n",
       "   0.5584569573402405,\n",
       "   0.39533302187919617,\n",
       "   0.4626319110393524,\n",
       "   0.5092728734016418,\n",
       "   0.39925745129585266,\n",
       "   0.6095612049102783,\n",
       "   0.3973555564880371,\n",
       "   0.44537922739982605,\n",
       "   0.4823451042175293,\n",
       "   0.4171735644340515,\n",
       "   0.44204822182655334,\n",
       "   0.5478754639625549,\n",
       "   0.5220900177955627,\n",
       "   0.5795369744300842,\n",
       "   0.6106845140457153,\n",
       "   0.5308266878128052,\n",
       "   0.48452770709991455,\n",
       "   0.44730037450790405,\n",
       "   0.6208806037902832,\n",
       "   0.6797561049461365,\n",
       "   0.4787510335445404,\n",
       "   0.4266223609447479,\n",
       "   0.7025402784347534,\n",
       "   0.479548841714859,\n",
       "   0.6124406456947327,\n",
       "   0.5454011559486389,\n",
       "   0.489071249961853,\n",
       "   0.620345950126648,\n",
       "   0.4288344979286194,\n",
       "   0.46904003620147705,\n",
       "   0.468238890171051,\n",
       "   0.5492371320724487,\n",
       "   0.7372491955757141,\n",
       "   0.7359151244163513,\n",
       "   0.4358302354812622,\n",
       "   0.5685343742370605,\n",
       "   0.6322718262672424,\n",
       "   0.4369892477989197,\n",
       "   0.5537912845611572,\n",
       "   0.5335025787353516,\n",
       "   0.601355791091919,\n",
       "   0.559535026550293,\n",
       "   0.493184357881546,\n",
       "   0.5395610332489014,\n",
       "   0.5375872850418091,\n",
       "   0.5146540403366089,\n",
       "   0.3726463317871094,\n",
       "   0.5105351209640503,\n",
       "   0.4450579881668091,\n",
       "   0.3929958641529083,\n",
       "   0.46598008275032043,\n",
       "   0.3904884159564972,\n",
       "   0.4465755522251129,\n",
       "   0.4960402846336365,\n",
       "   0.5118791460990906,\n",
       "   0.4431590437889099,\n",
       "   0.40028807520866394,\n",
       "   0.41260501742362976,\n",
       "   0.39359039068222046,\n",
       "   0.3279475271701813,\n",
       "   0.5201953649520874,\n",
       "   0.46705591678619385,\n",
       "   0.5391621589660645,\n",
       "   0.5290255546569824,\n",
       "   0.4679757356643677,\n",
       "   0.7337220311164856,\n",
       "   0.42100030183792114,\n",
       "   0.6538391709327698,\n",
       "   0.5093963742256165,\n",
       "   0.42516574263572693,\n",
       "   0.47941747307777405,\n",
       "   0.44601091742515564,\n",
       "   0.3963691294193268,\n",
       "   0.3288385272026062,\n",
       "   0.6329975128173828,\n",
       "   0.5880573391914368,\n",
       "   0.4522211253643036,\n",
       "   0.5924271941184998,\n",
       "   0.44791221618652344,\n",
       "   0.4741922914981842,\n",
       "   0.5081514716148376,\n",
       "   0.6510410308837891,\n",
       "   0.7217993140220642,\n",
       "   0.43483665585517883,\n",
       "   0.3985070288181305,\n",
       "   0.4097038507461548,\n",
       "   0.44171062111854553,\n",
       "   0.6428006291389465,\n",
       "   0.7269284725189209,\n",
       "   0.5170347690582275]}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./json/20200811_OPVNN2_CV-1.json', 'w') as fp:\n",
    "    json.dump(cv_fits, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lr', 'best_loss_epoch', 'best_acc_epoch', 'best_r2_epoch', 'pce_loss', 'voc_loss', 'jsc_loss', 'ff_loss', 'test_losses', 'pce_acc', 'voc_acc', 'jsc_acc', 'ff_acc', 'test_accs', 'pce_r2', 'voc_r2', 'jsc_r2', 'ff_r2', 'test_r2s', 'train_pce_loss', 'train_voc_loss', 'train_jsc_loss', 'train_ff_loss'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./json/20200811_OPVNN2_CV-1.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_results(fit_dict):\n",
    "    lr = float(fit_dict['lr'])\n",
    "    best_loss_epoch = int(fit_dict['best_loss_epoch'])\n",
    "    best_acc_epoch = int(fit_dict['best_acc_epoch'])\n",
    "    best_r2_epoch = int(fit_dict['best_r2_epoch'])\n",
    "    \n",
    "    test_loss = [float(i) for i in fit_dict['test_losses']]\n",
    "    pce_loss = [float(i) for i in fit_dict['pce_loss']]\n",
    "    voc_loss = [float(i) for i in fit_dict['voc_loss']]\n",
    "    jsc_loss = [float(i) for i in fit_dict['jsc_loss']]\n",
    "    ff_loss = [float(i) for i in fit_dict['ff_loss']]\n",
    "    \n",
    "    test_acc = [float(i) for i in fit_dict['test_accs']]\n",
    "    pce_acc = [float(i) for i in fit_dict['pce_acc']]\n",
    "    voc_acc = [float(i) for i in fit_dict['voc_acc']]\n",
    "    jsc_acc = [float(i) for i in fit_dict['jsc_acc']]\n",
    "    ff_acc = [float(i) for i in fit_dict['ff_acc']]\n",
    "    \n",
    "    test_r2 = [float(i) for i in fit_dict['test_r2s']]\n",
    "    pce_r2 = [float(i) for i in fit_dict['pce_r2']]\n",
    "    voc_r2 = [float(i) for i in fit_dict['voc_r2']]\n",
    "    jsc_r2 = [float(i) for i in fit_dict['jsc_r2']]\n",
    "    ff_r2 = [float(i) for i in fit_dict['ff_r2']]\n",
    "    \n",
    "    train_pce_loss = [float(i) for i in fit_dict['train_pce_loss']]\n",
    "    train_voc_loss = [float(i) for i in fit_dict['train_voc_loss']]\n",
    "    train_jsc_loss = [float(i) for i in fit_dict['train_jsc_loss']]\n",
    "    train_ff_loss = [float(i) for i in fit_dict['train_ff_loss']]\n",
    "\n",
    "    epochs = np.arange(0, (len(test_loss)), 1)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12, 6))\n",
    "    ax1.plot(epochs, pce_loss, c = 'r', label = 'pce loss')\n",
    "    ax1.plot(epochs, voc_loss, c = 'g', label = 'voc loss')\n",
    "    ax1.plot(epochs, jsc_loss, c = 'b', label = 'jsc loss')\n",
    "    ax1.plot(epochs, ff_loss, c = 'c', label = 'ff loss')\n",
    "#     ax1.plot(epochs, test_loss, c = 'k', label = 'total loss')\n",
    "    ax1.plot(epochs, train_pce_loss, c = 'r', linestyle = '-.', label = 'pce train loss')\n",
    "    ax1.plot(epochs, train_voc_loss, c = 'g', linestyle = '-.', label = 'voc train loss')\n",
    "    ax1.plot(epochs, train_jsc_loss, c = 'b', linestyle = '-.', label = 'jsc train loss')\n",
    "    ax1.plot(epochs, train_ff_loss, c = 'c', linestyle = '-.', label = 'ff train loss')\n",
    "#     ax1.scatter(best_loss_epoch, min(test_loss), s = 64, c = 'c')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Mean Squared Error Loss')\n",
    "    ax1.legend(loc = 'best')\n",
    "    ax1.set_title(f'MSE Loss with lr = {lr}')\n",
    "\n",
    "    ax2.plot(epochs, pce_acc, c = 'r', label = 'pce acc')\n",
    "    ax2.plot(epochs, voc_acc, c = 'g', label = 'voc acc')\n",
    "    ax2.plot(epochs, jsc_acc, c = 'b', label = 'jsc acc')\n",
    "    ax2.plot(epochs, ff_acc, c = 'c', label = 'ff acc')\n",
    "#     ax2.plot(epochs, test_acc, c = 'k', label = 'total acc')\n",
    "#     ax2.scatter(best_acc_epoch, min(test_acc), s = 64, c = 'c')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Mean Absolute Percent Error')\n",
    "    ax2.legend(loc = 'best')\n",
    "    ax2.set_title(f'MAPE with lr = {lr}')\n",
    "\n",
    "    ax3.plot(epochs, pce_r2, c = 'r', label = 'pce R$^2$')\n",
    "    ax3.plot(epochs, voc_r2, c = 'g', label = 'voc R$^2$')\n",
    "    ax3.plot(epochs, jsc_r2, c = 'b', label = 'jsc R$^2$')\n",
    "    ax3.plot(epochs, ff_r2, c = 'c', label = 'ff R$^2$')\n",
    "#     ax3.plot(epochs, test_r2, c = 'k', label = 'total R$^2$')\n",
    "#     ax3.scatter(best_r2_epoch, max(test_r2), s = 64, c = 'c')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('R$^2$')\n",
    "    ax3.legend(loc = 'best')\n",
    "    ax3.set_title(f'R$^2$ with lr = {lr}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key, fit_dict in data.items():\n",
    "    plot_fit_results(fit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
