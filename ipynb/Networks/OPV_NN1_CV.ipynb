{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a2af04190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../py-conjugated/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import morphology_networks as net\n",
    "import model_training as train\n",
    "import model_testing as test\n",
    "import physically_informed_loss_functions as pilf\n",
    "import network_utils as nuts\n",
    "\n",
    "torch.manual_seed(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPV NN1\n",
    "\n",
    "### This notebook to uses only OPV processing conditions to predict device performance.\n",
    "\n",
    "\n",
    "# Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_df shape: (366, 9)\n",
      "test_df shape: (10, 60)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PCE</th>\n",
       "      <th>VocL</th>\n",
       "      <th>Jsc</th>\n",
       "      <th>FF</th>\n",
       "      <th>Substrate</th>\n",
       "      <th>Device</th>\n",
       "      <th>Time (min)</th>\n",
       "      <th>Temp (C)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078981</td>\n",
       "      <td>0.066177</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.088229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026882</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.185014</td>\n",
       "      <td>0.934181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.565143</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.187694</td>\n",
       "      <td>0.915305</td>\n",
       "      <td>0.008375</td>\n",
       "      <td>0.570857</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.190771</td>\n",
       "      <td>0.928501</td>\n",
       "      <td>0.058959</td>\n",
       "      <td>0.436489</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       PCE      VocL       Jsc        FF  Substrate  Device  \\\n",
       "0           0  0.000000  0.000000  0.078981  0.066177          1       5   \n",
       "1           1  0.088229  1.000000  0.026882  0.000000          7       5   \n",
       "2           2  0.185014  0.934181  0.000000  0.565143          7       6   \n",
       "3           3  0.187694  0.915305  0.008375  0.570857          7       2   \n",
       "4           4  0.190771  0.928501  0.058959  0.436489          1       1   \n",
       "\n",
       "   Time (min)  Temp (C)  \n",
       "0           0         0  \n",
       "1           0         0  \n",
       "2           0         0  \n",
       "3           0         0  \n",
       "4           0         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take in data as a dataframe for easy preprocessing\n",
    "device_df = pd.read_excel('/Users/wesleytatum/Desktop/py-conjugated/data/normed_OPV_device.xlsx')\n",
    "\n",
    "test_df = pd.read_excel('/Users/wesleytatum/Desktop/py-conjugated/data/normed_OPV_test.xlsx')\n",
    "\n",
    "print (f'device_df shape: {device_df.shape}')\n",
    "print (f'test_df shape: {test_df.shape}')\n",
    "device_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = device_df[['Time (min)', 'Temp (C)']] #input features used to make prediction\n",
    "y_train = device_df[['PCE', 'VocL', 'Jsc', 'FF']] #target features to be predicted\n",
    "\n",
    "x_test = test_df[['Anneal_time', 'Anneal_temp']]\n",
    "y_test = test_df[['PCE', 'VocL', 'Jsc', 'FF']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model with variety of learning rates and epochs to find best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "def fit(model, lr, epochs):\n",
    "    \n",
    "    #define the optimizer\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    #empty list to hold loss per epoch\n",
    "    train_epoch_losses = []\n",
    "    pce_train_epoch_losses = []\n",
    "    voc_train_epoch_losses = []\n",
    "    jsc_train_epoch_losses = []\n",
    "    ff_train_epoch_losses = []\n",
    "\n",
    "    test_epoch_losses = []\n",
    "    pce_test_epoch_losses = []\n",
    "    voc_test_epoch_losses = []\n",
    "    jsc_test_epoch_losses = []\n",
    "    ff_test_epoch_losses = []\n",
    "\n",
    "    pce_test_epoch_accuracies = []\n",
    "    voc_test_epoch_accuracies = []\n",
    "    jsc_test_epoch_accuracies = []\n",
    "    ff_test_epoch_accuracies = []\n",
    "    test_epoch_accuracies = []\n",
    "\n",
    "    pce_test_epoch_r2 = []\n",
    "    voc_test_epoch_r2 = []\n",
    "    jsc_test_epoch_r2 = []\n",
    "    ff_test_epoch_r2 = []\n",
    "    test_epoch_r2s = []\n",
    "\n",
    "    save_epochs = np.arange(0, num_epochs, 5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('On epoch ', epoch)\n",
    "\n",
    "    #     save_dir = \"/Users/wesleytatum/Desktop/model_states/OPV_NN2/\"\n",
    "    #     model_name = \"OPV_NN2\"\n",
    "    #     model_path = save_dir+model_name+'*.pt'\n",
    "    #     if epoch < 10:\n",
    "    #         save_path = save_dir + model_name + '_epoch0' + str(epoch) + '.pt'\n",
    "    #     else:\n",
    "    #         save_path = save_dir + model_name + '_epoch' + str(epoch) + '.pt'\n",
    "\n",
    "    #     if glob.glob(model_path) != []:\n",
    "    #         model_states = glob.glob(model_path)\n",
    "    #         model_states = sorted(model_states)\n",
    "    #         previous_model = model_states[-1]    \n",
    "\n",
    "    #         model, optimizer = nuts.load_trained_model(previous_model, model, optimizer)\n",
    "\n",
    "        model, train_loss, pce_train_loss, voc_train_loss, jsc_train_loss, ff_train_loss = train.train_OPV_df_model(model = model,                                                                                                     training_data_set = training_data_set,\n",
    "                                                                                                             optimizer = optimizer)\n",
    "        train_epoch_losses.append(train_loss)\n",
    "        pce_train_epoch_losses.append(pce_train_loss)\n",
    "        voc_train_epoch_losses.append(voc_train_loss)\n",
    "        jsc_train_epoch_losses.append(jsc_train_loss)\n",
    "        ff_train_epoch_losses.append(ff_train_loss)\n",
    "\n",
    "        test_losses, test_accs, test_r2s = test.eval_OPV_df_model(model = model,\n",
    "                                                                  testing_data_set = testing_data_set)\n",
    "        pce_test_epoch_losses.append(test_losses[0])\n",
    "        voc_test_epoch_losses.append(test_losses[1])\n",
    "        jsc_test_epoch_losses.append(test_losses[2])\n",
    "        ff_test_epoch_losses.append(test_losses[3])\n",
    "\n",
    "        tot_tst_loss = sum(test_losses)\n",
    "        test_epoch_losses.append(tot_tst_loss)\n",
    "\n",
    "        pce_test_epoch_accuracies.append(test_accs[0])\n",
    "        voc_test_epoch_accuracies.append(test_accs[1])\n",
    "        jsc_test_epoch_accuracies.append(test_accs[2])\n",
    "        ff_test_epoch_accuracies.append(test_accs[3])\n",
    "\n",
    "        tot_tst_acc = sum(test_accs)\n",
    "        test_epoch_accuracies.append(tot_tst_acc)\n",
    "\n",
    "        pce_test_epoch_r2.append(test_r2s[0])\n",
    "        voc_test_epoch_r2.append(test_r2s[1])\n",
    "        jsc_test_epoch_r2.append(test_r2s[2])\n",
    "        ff_test_epoch_r2.append(test_r2s[3])\n",
    "\n",
    "        tot_tst_r2 = sum(test_r2s)\n",
    "        test_epoch_r2s.append(tot_tst_r2)\n",
    "\n",
    "        #nuts.save_trained_model(save_path, epoch, model, optimizer)\n",
    "        \n",
    "        print('Finished epoch ', epoch)\n",
    "        \n",
    "    best_loss_indx = test_epoch_losses.index(min(test_epoch_losses))\n",
    "    best_acc_indx = test_epoch_accuracies.index(min(test_epoch_accuracies))\n",
    "    best_r2_indx = test_epoch_r2s.index(max(test_epoch_r2s))\n",
    "    \n",
    "    fit_results = {\n",
    "        'lr': lr,\n",
    "        'best_loss_epoch': best_loss_indx,\n",
    "        'best_acc_epoch': best_acc_indx,\n",
    "        'best_r2_epoch': best_r2_indx,\n",
    "        'pce_loss': pce_test_epoch_losses,\n",
    "        'voc_loss': voc_test_epoch_losses,\n",
    "        'jsc_loss': jsc_test_epoch_losses,\n",
    "        'ff_loss': ff_test_epoch_losses,\n",
    "        'test_losses': test_epoch_losses,        \n",
    "        'pce_acc': pce_test_epoch_accuracies,\n",
    "        'voc_acc': voc_test_epoch_accuracies,\n",
    "        'jsc_acc': jsc_test_epoch_accuracies,\n",
    "        'ff_acc': ff_test_epoch_accuracies,\n",
    "        'test_accs': test_epoch_accuracies,\n",
    "        'pce_r2': pce_test_epoch_r2,\n",
    "        'voc_r2': voc_test_epoch_r2,\n",
    "        'jsc_r2': jsc_test_epoch_r2,\n",
    "        'ff_r2': ff_test_epoch_r2,\n",
    "        'test_r2s': test_epoch_r2s,\n",
    "        'train_pce_loss': pce_train_epoch_losses,\n",
    "        'train_voc_loss': voc_train_epoch_losses,\n",
    "        'train_jsc_loss': jsc_train_epoch_losses,\n",
    "        'train_ff_loss': ff_train_epoch_losses\n",
    "    }\n",
    "\n",
    "    return fit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 500\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Device configuration (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "in_dims = int(x_train.shape[1]) #number of x channels\n",
    "out_dims = y_test.shape[1] #number of predicted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold # 0\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 1442.0770263671875\n",
      "                              Voc = 1365.193115234375\n",
      "                              Jsc = 1888.4403076171875\n",
      "                              FF = 2368.80419921875\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 1439.868408203125\n",
      "                              Voc = 1383.015380859375\n",
      "                              Jsc = 2039.4891357421875\n",
      "                              FF = 2136.444580078125\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 1440.7259521484375\n",
      "                              Voc = 1399.5035400390625\n",
      "                              Jsc = 2101.83056640625\n",
      "                              FF = 1975.4482421875\n",
      "Finished epoch  2\n",
      "On epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1433.288818359375\n",
      "                              Voc = 1406.2271728515625\n",
      "                              Jsc = 2094.412109375\n",
      "                              FF = 1860.9222412109375\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 1441.464599609375\n",
      "                              Voc = 1413.2462158203125\n",
      "                              Jsc = 2150.93115234375\n",
      "                              FF = 1778.608642578125\n",
      "Finished epoch  4\n",
      "On epoch  5\n",
      "Total Epoch Testing MAPE: PCE = 1446.868408203125\n",
      "                              Voc = 1420.8243408203125\n",
      "                              Jsc = 2187.973876953125\n",
      "                              FF = 1713.3079833984375\n",
      "Finished epoch  5\n",
      "On epoch  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1449.507080078125\n",
      "                              Voc = 1425.619873046875\n",
      "                              Jsc = 2201.804443359375\n",
      "                              FF = 1659.285888671875\n",
      "Finished epoch  6\n",
      "On epoch  7\n",
      "Total Epoch Testing MAPE: PCE = 1452.578857421875\n",
      "                              Voc = 1430.8331298828125\n",
      "                              Jsc = 2203.298095703125\n",
      "                              FF = 1614.890625\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 1451.493408203125\n",
      "                              Voc = 1435.99267578125\n",
      "                              Jsc = 2219.6787109375\n",
      "                              FF = 1577.2015380859375\n",
      "Finished epoch  8\n",
      "On epoch  9\n",
      "Total Epoch Testing MAPE: PCE = 1452.533447265625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2235.373046875\n",
      "                              FF = 1548.816162109375\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 1453.7706298828125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2240.796875\n",
      "                              FF = 1524.7596435546875\n",
      "Finished epoch  10\n",
      "On epoch  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1451.4046630859375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2235.49072265625\n",
      "                              FF = 1504.662841796875\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 1446.19091796875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2215.394775390625\n",
      "                              FF = 1488.439697265625\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 1442.3509521484375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2208.91162109375\n",
      "                              FF = 1474.576416015625\n",
      "Finished epoch  13\n",
      "On epoch  14\n",
      "Total Epoch Testing MAPE: PCE = 1438.094482421875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2195.599365234375\n",
      "                              FF = 1462.871826171875\n",
      "Finished epoch  14\n",
      "On epoch  15\n",
      "Total Epoch Testing MAPE: PCE = 1435.580810546875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2203.78955078125\n",
      "                              FF = 1460.6068115234375\n",
      "Finished epoch  15\n",
      "On epoch  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1433.9674072265625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2214.582275390625\n",
      "                              FF = 1459.832275390625\n",
      "Finished epoch  16\n",
      "On epoch  17\n",
      "Total Epoch Testing MAPE: PCE = 1432.5980224609375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2214.567138671875\n",
      "                              FF = 1458.8262939453125\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 1430.7120361328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2207.827392578125\n",
      "                              FF = 1459.6722412109375\n",
      "Finished epoch  18\n",
      "On epoch  19\n",
      "Total Epoch Testing MAPE: PCE = 1428.7833251953125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2207.95849609375\n",
      "                              FF = 1461.6810302734375\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 1428.144775390625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2207.275390625\n",
      "                              FF = 1463.01123046875\n",
      "Finished epoch  20\n",
      "On epoch  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1427.0430908203125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2213.35498046875\n",
      "                              FF = 1464.40380859375\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 1427.513671875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2216.650634765625\n",
      "                              FF = 1465.2210693359375\n",
      "Finished epoch  22\n",
      "On epoch  23\n",
      "Total Epoch Testing MAPE: PCE = 1427.9979248046875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2213.611328125\n",
      "                              FF = 1466.593505859375\n",
      "Finished epoch  23\n",
      "On epoch  24\n",
      "Total Epoch Testing MAPE: PCE = 1425.6690673828125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2217.142333984375\n",
      "                              FF = 1468.3046875\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 1426.379150390625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2216.619140625\n",
      "                              FF = 1470.5245361328125\n",
      "Finished epoch  25\n",
      "On epoch  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1424.8602294921875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2224.137939453125\n",
      "                              FF = 1473.3985595703125\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 1423.539794921875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2221.26171875\n",
      "                              FF = 1476.01318359375\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 1424.8607177734375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2225.107177734375\n",
      "                              FF = 1477.49609375\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 1425.1519775390625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2221.842041015625\n",
      "                              FF = 1478.8719482421875\n",
      "Finished epoch  29\n",
      "On epoch  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1424.9949951171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2218.36865234375\n",
      "                              FF = 1480.571044921875\n",
      "Finished epoch  30\n",
      "On epoch  31\n",
      "Total Epoch Testing MAPE: PCE = 1423.84912109375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2216.33349609375\n",
      "                              FF = 1483.4317626953125\n",
      "Finished epoch  31\n",
      "On epoch  32\n",
      "Total Epoch Testing MAPE: PCE = 1423.7235107421875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2214.442138671875\n",
      "                              FF = 1484.5753173828125\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 1423.3387451171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2206.9140625\n",
      "                              FF = 1486.193603515625\n",
      "Finished epoch  33\n",
      "On epoch  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1422.7266845703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2208.413818359375\n",
      "                              FF = 1487.9593505859375\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 1420.831298828125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2210.145263671875\n",
      "                              FF = 1489.72314453125\n",
      "Finished epoch  35\n",
      "On epoch  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1420.2254638671875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2217.482177734375\n",
      "                              FF = 1491.3094482421875\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 1421.41748046875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2217.9072265625\n",
      "                              FF = 1493.7340087890625\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 1422.45166015625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2218.031005859375\n",
      "                              FF = 1495.702880859375\n",
      "Finished epoch  38\n",
      "On epoch  39\n",
      "Total Epoch Testing MAPE: PCE = 1423.1971435546875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2215.699462890625\n",
      "                              FF = 1497.56884765625\n",
      "Finished epoch  39\n",
      "On epoch  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1427.0535888671875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2205.29638671875\n",
      "                              FF = 1499.490478515625\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 1428.153076171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2211.033447265625\n",
      "                              FF = 1501.579345703125\n",
      "Finished epoch  41\n",
      "On epoch  42\n",
      "Total Epoch Testing MAPE: PCE = 1427.5672607421875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2216.88720703125\n",
      "                              FF = 1502.49072265625\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 1428.4105224609375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2215.151123046875\n",
      "                              FF = 1503.00634765625\n",
      "Finished epoch  43\n",
      "On epoch  44\n",
      "Total Epoch Testing MAPE: PCE = 1428.081298828125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2209.156494140625\n",
      "                              FF = 1505.7362060546875\n",
      "Finished epoch  44\n",
      "On epoch  45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1428.596923828125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2202.857421875\n",
      "                              FF = 1506.815673828125\n",
      "Finished epoch  45\n",
      "On epoch  46\n",
      "Total Epoch Testing MAPE: PCE = 1429.1339111328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2198.279296875\n",
      "                              FF = 1507.7359619140625\n",
      "Finished epoch  46\n",
      "On epoch  47\n",
      "Total Epoch Testing MAPE: PCE = 1430.803466796875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2189.710693359375\n",
      "                              FF = 1508.49658203125\n",
      "Finished epoch  47\n",
      "On epoch  48\n",
      "Total Epoch Testing MAPE: PCE = 1428.835693359375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2190.50244140625\n",
      "                              FF = 1510.3277587890625\n",
      "Finished epoch  48\n",
      "On epoch  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1428.7008056640625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2184.58154296875\n",
      "                              FF = 1511.645263671875\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 1427.8001708984375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2186.092529296875\n",
      "                              FF = 1514.1072998046875\n",
      "Finished epoch  50\n",
      "On epoch  51\n",
      "Total Epoch Testing MAPE: PCE = 1429.144775390625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2175.254638671875\n",
      "                              FF = 1515.5445556640625\n",
      "Finished epoch  51\n",
      "On epoch  52\n",
      "Total Epoch Testing MAPE: PCE = 1428.4970703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2181.974365234375\n",
      "                              FF = 1515.70703125\n",
      "Finished epoch  52\n",
      "On epoch  53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1426.507568359375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2175.2880859375\n",
      "                              FF = 1517.2596435546875\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 1426.71044921875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2172.789306640625\n",
      "                              FF = 1518.90771484375\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 1426.39404296875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2169.2861328125\n",
      "                              FF = 1519.9146728515625\n",
      "Finished epoch  55\n",
      "On epoch  56\n",
      "Total Epoch Testing MAPE: PCE = 1425.5714111328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2167.266845703125\n",
      "                              FF = 1520.9771728515625\n",
      "Finished epoch  56\n",
      "On epoch  57\n",
      "Total Epoch Testing MAPE: PCE = 1428.3101806640625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2168.80078125\n",
      "                              FF = 1522.5482177734375\n",
      "Finished epoch  57\n",
      "On epoch  58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1426.690185546875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2156.664794921875\n",
      "                              FF = 1523.4871826171875\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 1425.644287109375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2148.620361328125\n",
      "                              FF = 1524.3402099609375\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 1427.819580078125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2147.756591796875\n",
      "                              FF = 1525.32666015625\n",
      "Finished epoch  60\n",
      "On epoch  61\n",
      "Total Epoch Testing MAPE: PCE = 1425.4970703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2145.583984375\n",
      "                              FF = 1526.14111328125\n",
      "Finished epoch  61\n",
      "On epoch  62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1426.5245361328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2143.75244140625\n",
      "                              FF = 1526.8365478515625\n",
      "Finished epoch  62\n",
      "On epoch  63\n",
      "Total Epoch Testing MAPE: PCE = 1425.05810546875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2140.749267578125\n",
      "                              FF = 1528.1881103515625\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 1422.1171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2139.1533203125\n",
      "                              FF = 1529.419189453125\n",
      "Finished epoch  64\n",
      "On epoch  65\n",
      "Total Epoch Testing MAPE: PCE = 1420.5543212890625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2136.554443359375\n",
      "                              FF = 1530.599853515625\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 1419.0330810546875\n",
      "                              Voc = 1447.359619140625\n",
      "                              Jsc = 2140.646728515625\n",
      "                              FF = 1531.38232421875\n",
      "Finished epoch  66\n",
      "On epoch  67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1418.9197998046875\n",
      "                              Voc = 1447.134765625\n",
      "                              Jsc = 2138.50390625\n",
      "                              FF = 1530.84814453125\n",
      "Finished epoch  67\n",
      "On epoch  68\n",
      "Total Epoch Testing MAPE: PCE = 1417.1915283203125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2140.61572265625\n",
      "                              FF = 1531.2833251953125\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 1418.4149169921875\n",
      "                              Voc = 1447.4344482421875\n",
      "                              Jsc = 2132.79541015625\n",
      "                              FF = 1532.2647705078125\n",
      "Finished epoch  69\n",
      "On epoch  70\n",
      "Total Epoch Testing MAPE: PCE = 1416.6143798828125\n",
      "                              Voc = 1446.7235107421875\n",
      "                              Jsc = 2132.461669921875\n",
      "                              FF = 1532.8790283203125\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 1414.1888427734375\n",
      "                              Voc = 1447.5289306640625\n",
      "                              Jsc = 2124.489501953125\n",
      "                              FF = 1532.975341796875\n",
      "Finished epoch  71\n",
      "On epoch  72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1413.4012451171875\n",
      "                              Voc = 1447.0543212890625\n",
      "                              Jsc = 2119.478759765625\n",
      "                              FF = 1533.7158203125\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 1412.908935546875\n",
      "                              Voc = 1447.3994140625\n",
      "                              Jsc = 2117.8251953125\n",
      "                              FF = 1533.9100341796875\n",
      "Finished epoch  73\n",
      "On epoch  74\n",
      "Total Epoch Testing MAPE: PCE = 1414.684814453125\n",
      "                              Voc = 1444.417236328125\n",
      "                              Jsc = 2109.007568359375\n",
      "                              FF = 1534.2357177734375\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 1414.681396484375\n",
      "                              Voc = 1444.5579833984375\n",
      "                              Jsc = 2110.3046875\n",
      "                              FF = 1533.5576171875\n",
      "Finished epoch  75\n",
      "On epoch  76\n",
      "Total Epoch Testing MAPE: PCE = 1412.5406494140625\n",
      "                              Voc = 1443.8743896484375\n",
      "                              Jsc = 2112.59130859375\n",
      "                              FF = 1533.068603515625\n",
      "Finished epoch  76\n",
      "On epoch  77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1412.1134033203125\n",
      "                              Voc = 1444.052001953125\n",
      "                              Jsc = 2114.78173828125\n",
      "                              FF = 1534.6390380859375\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 1408.66943359375\n",
      "                              Voc = 1444.791015625\n",
      "                              Jsc = 2115.222412109375\n",
      "                              FF = 1534.927001953125\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 1409.1048583984375\n",
      "                              Voc = 1442.86474609375\n",
      "                              Jsc = 2116.648193359375\n",
      "                              FF = 1534.491943359375\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 1409.7947998046875\n",
      "                              Voc = 1443.4134521484375\n",
      "                              Jsc = 2114.4072265625\n",
      "                              FF = 1534.4625244140625\n",
      "Finished epoch  80\n",
      "On epoch  81\n",
      "Total Epoch Testing MAPE: PCE = 1411.3602294921875\n",
      "                              Voc = 1443.036376953125\n",
      "                              Jsc = 2116.1884765625\n",
      "                              FF = 1534.00830078125\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 1410.5291748046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Voc = 1443.0264892578125\n",
      "                              Jsc = 2122.2099609375\n",
      "                              FF = 1534.0592041015625\n",
      "Finished epoch  82\n",
      "On epoch  83\n",
      "Total Epoch Testing MAPE: PCE = 1410.375732421875\n",
      "                              Voc = 1442.9515380859375\n",
      "                              Jsc = 2122.20068359375\n",
      "                              FF = 1535.05908203125\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 1409.7938232421875\n",
      "                              Voc = 1442.6767578125\n",
      "                              Jsc = 2121.620849609375\n",
      "                              FF = 1535.9281005859375\n",
      "Finished epoch  84\n",
      "On epoch  85\n",
      "Total Epoch Testing MAPE: PCE = 1408.7083740234375\n",
      "                              Voc = 1443.6695556640625\n",
      "                              Jsc = 2124.491455078125\n",
      "                              FF = 1535.4564208984375\n",
      "Finished epoch  85\n",
      "On epoch  86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1410.9990234375\n",
      "                              Voc = 1444.6397705078125\n",
      "                              Jsc = 2121.180419921875\n",
      "                              FF = 1534.7784423828125\n",
      "Finished epoch  86\n",
      "On epoch  87\n",
      "Total Epoch Testing MAPE: PCE = 1410.8748779296875\n",
      "                              Voc = 1444.828369140625\n",
      "                              Jsc = 2116.638427734375\n",
      "                              FF = 1534.638427734375\n",
      "Finished epoch  87\n",
      "On epoch  88\n",
      "Total Epoch Testing MAPE: PCE = 1408.4832763671875\n",
      "                              Voc = 1444.1610107421875\n",
      "                              Jsc = 2113.424072265625\n",
      "                              FF = 1533.6197509765625\n",
      "Finished epoch  88\n",
      "On epoch  89\n",
      "Total Epoch Testing MAPE: PCE = 1408.8890380859375\n",
      "                              Voc = 1442.6153564453125\n",
      "                              Jsc = 2121.862548828125\n",
      "                              FF = 1533.8778076171875\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 1409.4296875\n",
      "                              Voc = 1443.76416015625\n",
      "                              Jsc = 2125.023193359375\n",
      "                              FF = 1534.247802734375\n",
      "Finished epoch  90\n",
      "On epoch  91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1410.8450927734375\n",
      "                              Voc = 1444.195556640625\n",
      "                              Jsc = 2126.726318359375\n",
      "                              FF = 1534.060791015625\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 1409.8858642578125\n",
      "                              Voc = 1443.425537109375\n",
      "                              Jsc = 2127.55859375\n",
      "                              FF = 1534.6185302734375\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 1409.0142822265625\n",
      "                              Voc = 1443.7587890625\n",
      "                              Jsc = 2124.989990234375\n",
      "                              FF = 1535.088134765625\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 1407.40771484375\n",
      "                              Voc = 1443.8131103515625\n",
      "                              Jsc = 2121.222900390625\n",
      "                              FF = 1536.2237548828125\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 1407.722900390625\n",
      "                              Voc = 1444.1217041015625\n",
      "                              Jsc = 2117.089111328125\n",
      "                              FF = 1535.815673828125\n",
      "Finished epoch  95\n",
      "On epoch  96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1407.2412109375\n",
      "                              Voc = 1443.1795654296875\n",
      "                              Jsc = 2116.483642578125\n",
      "                              FF = 1536.0648193359375\n",
      "Finished epoch  96\n",
      "On epoch  97\n",
      "Total Epoch Testing MAPE: PCE = 1408.47509765625\n",
      "                              Voc = 1444.6173095703125\n",
      "                              Jsc = 2116.489501953125\n",
      "                              FF = 1536.4652099609375\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 1409.1982421875\n",
      "                              Voc = 1443.8653564453125\n",
      "                              Jsc = 2115.9150390625\n",
      "                              FF = 1536.9088134765625\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 1408.8045654296875\n",
      "                              Voc = 1444.0816650390625\n",
      "                              Jsc = 2115.981201171875\n",
      "                              FF = 1537.003662109375\n",
      "Finished epoch  99\n",
      "On epoch  100\n",
      "Total Epoch Testing MAPE: PCE = 1407.5528564453125\n",
      "                              Voc = 1443.8372802734375\n",
      "                              Jsc = 2113.822509765625\n",
      "                              FF = 1537.214111328125\n",
      "Finished epoch  100\n",
      "On epoch  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.278076171875\n",
      "                              Voc = 1444.1934814453125\n",
      "                              Jsc = 2109.109375\n",
      "                              FF = 1536.5950927734375\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 1405.2158203125\n",
      "                              Voc = 1444.085693359375\n",
      "                              Jsc = 2105.164306640625\n",
      "                              FF = 1536.1710205078125\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 1405.615478515625\n",
      "                              Voc = 1444.246337890625\n",
      "                              Jsc = 2106.11767578125\n",
      "                              FF = 1537.4193115234375\n",
      "Finished epoch  103\n",
      "On epoch  104\n",
      "Total Epoch Testing MAPE: PCE = 1404.6185302734375\n",
      "                              Voc = 1443.637451171875\n",
      "                              Jsc = 2100.58154296875\n",
      "                              FF = 1537.7447509765625\n",
      "Finished epoch  104\n",
      "On epoch  105\n",
      "Total Epoch Testing MAPE: PCE = 1406.428466796875\n",
      "                              Voc = 1442.1053466796875\n",
      "                              Jsc = 2097.526611328125\n",
      "                              FF = 1538.43359375\n",
      "Finished epoch  105\n",
      "On epoch  106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.87158203125\n",
      "                              Voc = 1442.8134765625\n",
      "                              Jsc = 2095.929931640625\n",
      "                              FF = 1538.3096923828125\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 1403.4007568359375\n",
      "                              Voc = 1442.212158203125\n",
      "                              Jsc = 2095.89111328125\n",
      "                              FF = 1538.7464599609375\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 1405.2998046875\n",
      "                              Voc = 1442.748779296875\n",
      "                              Jsc = 2103.96337890625\n",
      "                              FF = 1538.9139404296875\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 1404.85205078125\n",
      "                              Voc = 1443.326171875\n",
      "                              Jsc = 2099.481689453125\n",
      "                              FF = 1538.3070068359375\n",
      "Finished epoch  109\n",
      "On epoch  110\n",
      "Total Epoch Testing MAPE: PCE = 1403.7020263671875\n",
      "                              Voc = 1442.9552001953125\n",
      "                              Jsc = 2104.233642578125\n",
      "                              FF = 1537.25439453125\n",
      "Finished epoch  110\n",
      "On epoch  111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.2403564453125\n",
      "                              Voc = 1443.1767578125\n",
      "                              Jsc = 2105.260009765625\n",
      "                              FF = 1536.957763671875\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 1405.4886474609375\n",
      "                              Voc = 1442.91064453125\n",
      "                              Jsc = 2118.18017578125\n",
      "                              FF = 1536.8375244140625\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 1405.8486328125\n",
      "                              Voc = 1443.2354736328125\n",
      "                              Jsc = 2122.7275390625\n",
      "                              FF = 1538.581787109375\n",
      "Finished epoch  113\n",
      "On epoch  114\n",
      "Total Epoch Testing MAPE: PCE = 1406.9906005859375\n",
      "                              Voc = 1443.6673583984375\n",
      "                              Jsc = 2125.68017578125\n",
      "                              FF = 1539.1591796875\n",
      "Finished epoch  114\n",
      "On epoch  115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.986572265625\n",
      "                              Voc = 1444.3714599609375\n",
      "                              Jsc = 2124.655517578125\n",
      "                              FF = 1539.350341796875\n",
      "Finished epoch  115\n",
      "On epoch  116\n",
      "Total Epoch Testing MAPE: PCE = 1407.67822265625\n",
      "                              Voc = 1445.0972900390625\n",
      "                              Jsc = 2127.959228515625\n",
      "                              FF = 1540.3353271484375\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 1407.5419921875\n",
      "                              Voc = 1444.56640625\n",
      "                              Jsc = 2127.59716796875\n",
      "                              FF = 1539.572265625\n",
      "Finished epoch  117\n",
      "On epoch  118\n",
      "Total Epoch Testing MAPE: PCE = 1406.689453125\n",
      "                              Voc = 1444.95654296875\n",
      "                              Jsc = 2125.4560546875\n",
      "                              FF = 1539.587646484375\n",
      "Finished epoch  118\n",
      "On epoch  119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1407.583984375\n",
      "                              Voc = 1443.88525390625\n",
      "                              Jsc = 2120.80615234375\n",
      "                              FF = 1540.4814453125\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 1407.98193359375\n",
      "                              Voc = 1444.33544921875\n",
      "                              Jsc = 2117.106201171875\n",
      "                              FF = 1541.064453125\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 1407.4652099609375\n",
      "                              Voc = 1443.8287353515625\n",
      "                              Jsc = 2117.710693359375\n",
      "                              FF = 1541.641845703125\n",
      "Finished epoch  121\n",
      "On epoch  122\n",
      "Total Epoch Testing MAPE: PCE = 1406.7841796875\n",
      "                              Voc = 1444.5689697265625\n",
      "                              Jsc = 2111.76953125\n",
      "                              FF = 1541.9229736328125\n",
      "Finished epoch  122\n",
      "On epoch  123\n",
      "Total Epoch Testing MAPE: PCE = 1407.8870849609375\n",
      "                              Voc = 1445.541259765625\n",
      "                              Jsc = 2108.17578125\n",
      "                              FF = 1541.94970703125\n",
      "Finished epoch  123\n",
      "On epoch  124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.5623779296875\n",
      "                              Voc = 1445.4888916015625\n",
      "                              Jsc = 2106.7587890625\n",
      "                              FF = 1542.39697265625\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 1405.521728515625\n",
      "                              Voc = 1444.842041015625\n",
      "                              Jsc = 2107.830078125\n",
      "                              FF = 1542.9588623046875\n",
      "Finished epoch  125\n",
      "On epoch  126\n",
      "Total Epoch Testing MAPE: PCE = 1403.1038818359375\n",
      "                              Voc = 1445.422119140625\n",
      "                              Jsc = 2111.4765625\n",
      "                              FF = 1542.666015625\n",
      "Finished epoch  126\n",
      "On epoch  127\n",
      "Total Epoch Testing MAPE: PCE = 1402.2784423828125\n",
      "                              Voc = 1444.45166015625\n",
      "                              Jsc = 2112.21630859375\n",
      "                              FF = 1543.2183837890625\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 1401.1668701171875\n",
      "                              Voc = 1445.1822509765625\n",
      "                              Jsc = 2115.822998046875\n",
      "                              FF = 1544.634521484375\n",
      "Finished epoch  128\n",
      "On epoch  129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1398.635498046875\n",
      "                              Voc = 1446.1260986328125\n",
      "                              Jsc = 2117.794921875\n",
      "                              FF = 1543.6231689453125\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 1401.7171630859375\n",
      "                              Voc = 1445.805419921875\n",
      "                              Jsc = 2125.40234375\n",
      "                              FF = 1542.856201171875\n",
      "Finished epoch  130\n",
      "On epoch  131\n",
      "Total Epoch Testing MAPE: PCE = 1402.715576171875\n",
      "                              Voc = 1446.5064697265625\n",
      "                              Jsc = 2131.63330078125\n",
      "                              FF = 1544.8385009765625\n",
      "Finished epoch  131\n",
      "On epoch  132\n",
      "Total Epoch Testing MAPE: PCE = 1402.7015380859375\n",
      "                              Voc = 1445.615234375\n",
      "                              Jsc = 2133.514404296875\n",
      "                              FF = 1545.1522216796875\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 1402.077880859375\n",
      "                              Voc = 1444.8470458984375\n",
      "                              Jsc = 2123.246826171875\n",
      "                              FF = 1545.27001953125\n",
      "Finished epoch  133\n",
      "On epoch  134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1403.4222412109375\n",
      "                              Voc = 1445.1553955078125\n",
      "                              Jsc = 2121.58544921875\n",
      "                              FF = 1544.3280029296875\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 1401.486572265625\n",
      "                              Voc = 1445.7171630859375\n",
      "                              Jsc = 2118.707275390625\n",
      "                              FF = 1544.09033203125\n",
      "Finished epoch  135\n",
      "On epoch  136\n",
      "Total Epoch Testing MAPE: PCE = 1400.9864501953125\n",
      "                              Voc = 1446.81591796875\n",
      "                              Jsc = 2122.041015625\n",
      "                              FF = 1544.063232421875\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 1401.57275390625\n",
      "                              Voc = 1447.2276611328125\n",
      "                              Jsc = 2121.455078125\n",
      "                              FF = 1544.1719970703125\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 1401.562255859375\n",
      "                              Voc = 1446.7904052734375\n",
      "                              Jsc = 2118.529296875\n",
      "                              FF = 1543.34912109375\n",
      "Finished epoch  138\n",
      "On epoch  139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1401.1904296875\n",
      "                              Voc = 1447.0989990234375\n",
      "                              Jsc = 2122.0654296875\n",
      "                              FF = 1542.19091796875\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 1403.0904541015625\n",
      "                              Voc = 1447.6583251953125\n",
      "                              Jsc = 2119.645751953125\n",
      "                              FF = 1543.3992919921875\n",
      "Finished epoch  140\n",
      "On epoch  141\n",
      "Total Epoch Testing MAPE: PCE = 1405.0916748046875\n",
      "                              Voc = 1446.6881103515625\n",
      "                              Jsc = 2121.250244140625\n",
      "                              FF = 1543.8243408203125\n",
      "Finished epoch  141\n",
      "On epoch  142\n",
      "Total Epoch Testing MAPE: PCE = 1404.0848388671875\n",
      "                              Voc = 1447.453369140625\n",
      "                              Jsc = 2123.8056640625\n",
      "                              FF = 1543.365966796875\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 1404.7103271484375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2115.844970703125\n",
      "                              FF = 1544.0169677734375\n",
      "Finished epoch  143\n",
      "On epoch  144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.292724609375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2113.09619140625\n",
      "                              FF = 1543.1585693359375\n",
      "Finished epoch  144\n",
      "On epoch  145\n",
      "Total Epoch Testing MAPE: PCE = 1404.7156982421875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2119.651611328125\n",
      "                              FF = 1542.9610595703125\n",
      "Finished epoch  145\n",
      "On epoch  146\n",
      "Total Epoch Testing MAPE: PCE = 1403.94091796875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2111.29345703125\n",
      "                              FF = 1542.5894775390625\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 1401.6094970703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2108.685791015625\n",
      "                              FF = 1542.7379150390625\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 1400.68359375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2108.504150390625\n",
      "                              FF = 1543.0203857421875\n",
      "Finished epoch  148\n",
      "On epoch  149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1402.1207275390625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2108.814208984375\n",
      "                              FF = 1543.7384033203125\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 1400.6370849609375\n",
      "                              Voc = 1447.342529296875\n",
      "                              Jsc = 2104.946044921875\n",
      "                              FF = 1542.9002685546875\n",
      "Finished epoch  150\n",
      "On epoch  151\n",
      "Total Epoch Testing MAPE: PCE = 1399.4971923828125\n",
      "                              Voc = 1447.0513916015625\n",
      "                              Jsc = 2101.3291015625\n",
      "                              FF = 1543.7398681640625\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 1401.0814208984375\n",
      "                              Voc = 1447.0496826171875\n",
      "                              Jsc = 2092.49560546875\n",
      "                              FF = 1544.3232421875\n",
      "Finished epoch  152\n",
      "On epoch  153\n",
      "Total Epoch Testing MAPE: PCE = 1402.0892333984375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2098.08251953125\n",
      "                              FF = 1545.715576171875\n",
      "Finished epoch  153\n",
      "On epoch  154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1402.6138916015625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2099.631591796875\n",
      "                              FF = 1545.1552734375\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 1402.5565185546875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2098.05126953125\n",
      "                              FF = 1545.8101806640625\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 1403.1968994140625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2093.955810546875\n",
      "                              FF = 1546.0533447265625\n",
      "Finished epoch  156\n",
      "On epoch  157\n",
      "Total Epoch Testing MAPE: PCE = 1405.5010986328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2093.177490234375\n",
      "                              FF = 1544.372314453125\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 1405.290771484375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2100.792236328125\n",
      "                              FF = 1544.67529296875\n",
      "Finished epoch  158\n",
      "On epoch  159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.0638427734375\n",
      "                              Voc = 1447.3802490234375\n",
      "                              Jsc = 2100.64404296875\n",
      "                              FF = 1544.34765625\n",
      "Finished epoch  159\n",
      "On epoch  160\n",
      "Total Epoch Testing MAPE: PCE = 1406.02734375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2102.943359375\n",
      "                              FF = 1544.1064453125\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 1405.1031494140625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2098.8642578125\n",
      "                              FF = 1544.3670654296875\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 1405.338623046875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2099.04833984375\n",
      "                              FF = 1544.6517333984375\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 1407.568359375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2098.7333984375\n",
      "                              FF = 1544.76953125\n",
      "Finished epoch  163\n",
      "On epoch  164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.757080078125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2105.3583984375\n",
      "                              FF = 1545.3780517578125\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 1406.591552734375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2108.91748046875\n",
      "                              FF = 1545.27734375\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 1403.95361328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2106.083984375\n",
      "                              FF = 1545.16259765625\n",
      "Finished epoch  166\n",
      "On epoch  167\n",
      "Total Epoch Testing MAPE: PCE = 1404.7978515625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2106.1552734375\n",
      "                              FF = 1545.7657470703125\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 1406.1170654296875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2106.610595703125\n",
      "                              FF = 1546.99365234375\n",
      "Finished epoch  168\n",
      "On epoch  169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1409.08056640625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2101.956787109375\n",
      "                              FF = 1545.575927734375\n",
      "Finished epoch  169\n",
      "On epoch  170\n",
      "Total Epoch Testing MAPE: PCE = 1407.2105712890625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2097.321044921875\n",
      "                              FF = 1546.02294921875\n",
      "Finished epoch  170\n",
      "On epoch  171\n",
      "Total Epoch Testing MAPE: PCE = 1405.39501953125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2103.19482421875\n",
      "                              FF = 1545.3289794921875\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 1404.8377685546875\n",
      "                              Voc = 1447.0015869140625\n",
      "                              Jsc = 2103.884765625\n",
      "                              FF = 1543.86572265625\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 1405.0733642578125\n",
      "                              Voc = 1446.0374755859375\n",
      "                              Jsc = 2103.635986328125\n",
      "                              FF = 1544.1217041015625\n",
      "Finished epoch  173\n",
      "On epoch  174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.0322265625\n",
      "                              Voc = 1447.240234375\n",
      "                              Jsc = 2098.53271484375\n",
      "                              FF = 1545.2646484375\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 1402.9891357421875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2105.8134765625\n",
      "                              FF = 1545.1514892578125\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 1403.900390625\n",
      "                              Voc = 1446.8255615234375\n",
      "                              Jsc = 2107.796142578125\n",
      "                              FF = 1544.5035400390625\n",
      "Finished epoch  176\n",
      "On epoch  177\n",
      "Total Epoch Testing MAPE: PCE = 1402.693115234375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2116.1337890625\n",
      "                              FF = 1545.593017578125\n",
      "Finished epoch  177\n",
      "On epoch  178\n",
      "Total Epoch Testing MAPE: PCE = 1405.798095703125\n",
      "                              Voc = 1447.2611083984375\n",
      "                              Jsc = 2118.105224609375\n",
      "                              FF = 1545.3746337890625\n",
      "Finished epoch  178\n",
      "On epoch  179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.3909912109375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2114.716064453125\n",
      "                              FF = 1544.790283203125\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 1407.636474609375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2117.815185546875\n",
      "                              FF = 1543.539794921875\n",
      "Finished epoch  180\n",
      "On epoch  181\n",
      "Total Epoch Testing MAPE: PCE = 1406.9407958984375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2116.211669921875\n",
      "                              FF = 1543.5806884765625\n",
      "Finished epoch  181\n",
      "On epoch  182\n",
      "Total Epoch Testing MAPE: PCE = 1406.626953125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2121.26416015625\n",
      "                              FF = 1542.916748046875\n",
      "Finished epoch  182\n",
      "On epoch  183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.35693359375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2121.91064453125\n",
      "                              FF = 1543.9368896484375\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 1406.9373779296875\n",
      "                              Voc = 1447.075439453125\n",
      "                              Jsc = 2121.798828125\n",
      "                              FF = 1544.254150390625\n",
      "Finished epoch  184\n",
      "On epoch  185\n",
      "Total Epoch Testing MAPE: PCE = 1405.9105224609375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2125.726806640625\n",
      "                              FF = 1544.6336669921875\n",
      "Finished epoch  185\n",
      "On epoch  186\n",
      "Total Epoch Testing MAPE: PCE = 1404.5439453125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2119.112060546875\n",
      "                              FF = 1543.47509765625\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 1403.0576171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2118.66650390625\n",
      "                              FF = 1542.8704833984375\n",
      "Finished epoch  187\n",
      "On epoch  188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1403.8765869140625\n",
      "                              Voc = 1447.0924072265625\n",
      "                              Jsc = 2113.525146484375\n",
      "                              FF = 1543.18701171875\n",
      "Finished epoch  188\n",
      "On epoch  189\n",
      "Total Epoch Testing MAPE: PCE = 1403.4407958984375\n",
      "                              Voc = 1446.1087646484375\n",
      "                              Jsc = 2117.69384765625\n",
      "                              FF = 1542.9490966796875\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 1403.6109619140625\n",
      "                              Voc = 1445.8994140625\n",
      "                              Jsc = 2119.2109375\n",
      "                              FF = 1542.656494140625\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 1402.0460205078125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2117.907470703125\n",
      "                              FF = 1541.52685546875\n",
      "Finished epoch  191\n",
      "On epoch  192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.313720703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2121.857666015625\n",
      "                              FF = 1541.946533203125\n",
      "Finished epoch  192\n",
      "On epoch  193\n",
      "Total Epoch Testing MAPE: PCE = 1405.8321533203125\n",
      "                              Voc = 1445.791259765625\n",
      "                              Jsc = 2130.857421875\n",
      "                              FF = 1544.0662841796875\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 1405.7894287109375\n",
      "                              Voc = 1445.71337890625\n",
      "                              Jsc = 2133.623046875\n",
      "                              FF = 1544.426025390625\n",
      "Finished epoch  194\n",
      "On epoch  195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.6942138671875\n",
      "                              Voc = 1444.796875\n",
      "                              Jsc = 2129.63916015625\n",
      "                              FF = 1544.5667724609375\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 1402.22119140625\n",
      "                              Voc = 1445.3111572265625\n",
      "                              Jsc = 2124.31591796875\n",
      "                              FF = 1544.4552001953125\n",
      "Finished epoch  196\n",
      "On epoch  197\n",
      "Total Epoch Testing MAPE: PCE = 1401.2593994140625\n",
      "                              Voc = 1445.765625\n",
      "                              Jsc = 2126.079345703125\n",
      "                              FF = 1544.144287109375\n",
      "Finished epoch  197\n",
      "On epoch  198\n",
      "Total Epoch Testing MAPE: PCE = 1405.374755859375\n",
      "                              Voc = 1446.3076171875\n",
      "                              Jsc = 2122.0146484375\n",
      "                              FF = 1544.50048828125\n",
      "Finished epoch  198\n",
      "On epoch  199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.7197265625\n",
      "                              Voc = 1446.166748046875\n",
      "                              Jsc = 2123.4697265625\n",
      "                              FF = 1544.779052734375\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 1406.353515625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2125.447998046875\n",
      "                              FF = 1545.7718505859375\n",
      "Finished epoch  200\n",
      "On epoch  201\n",
      "Total Epoch Testing MAPE: PCE = 1404.5052490234375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2127.718505859375\n",
      "                              FF = 1545.0205078125\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 1405.5531005859375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2124.822509765625\n",
      "                              FF = 1546.126953125\n",
      "Finished epoch  202\n",
      "On epoch  203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.5072021484375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2125.965087890625\n",
      "                              FF = 1546.4185791015625\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 1405.4266357421875\n",
      "                              Voc = 1444.840576171875\n",
      "                              Jsc = 2127.570068359375\n",
      "                              FF = 1547.0694580078125\n",
      "Finished epoch  204\n",
      "On epoch  205\n",
      "Total Epoch Testing MAPE: PCE = 1404.646728515625\n",
      "                              Voc = 1444.1754150390625\n",
      "                              Jsc = 2122.77783203125\n",
      "                              FF = 1547.1143798828125\n",
      "Finished epoch  205\n",
      "On epoch  206\n",
      "Total Epoch Testing MAPE: PCE = 1406.0069580078125\n",
      "                              Voc = 1442.5089111328125\n",
      "                              Jsc = 2123.606689453125\n",
      "                              FF = 1547.600830078125\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 1408.406494140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Voc = 1443.228271484375\n",
      "                              Jsc = 2113.177001953125\n",
      "                              FF = 1548.281494140625\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 1408.7608642578125\n",
      "                              Voc = 1442.28564453125\n",
      "                              Jsc = 2123.830078125\n",
      "                              FF = 1547.7266845703125\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 1408.572265625\n",
      "                              Voc = 1439.9881591796875\n",
      "                              Jsc = 2127.551513671875\n",
      "                              FF = 1547.45751953125\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 1407.74169921875\n",
      "                              Voc = 1440.39111328125\n",
      "                              Jsc = 2123.6650390625\n",
      "                              FF = 1546.3543701171875\n",
      "Finished epoch  210\n",
      "On epoch  211\n",
      "Total Epoch Testing MAPE: PCE = 1406.9832763671875\n",
      "                              Voc = 1441.039794921875\n",
      "                              Jsc = 2128.138916015625\n",
      "                              FF = 1546.2999267578125\n",
      "Finished epoch  211\n",
      "On epoch  212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1408.739990234375\n",
      "                              Voc = 1441.00439453125\n",
      "                              Jsc = 2127.0146484375\n",
      "                              FF = 1545.6328125\n",
      "Finished epoch  212\n",
      "On epoch  213\n",
      "Total Epoch Testing MAPE: PCE = 1407.1082763671875\n",
      "                              Voc = 1441.272216796875\n",
      "                              Jsc = 2124.30029296875\n",
      "                              FF = 1545.165771484375\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 1407.7801513671875\n",
      "                              Voc = 1441.37158203125\n",
      "                              Jsc = 2123.5126953125\n",
      "                              FF = 1544.661376953125\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 1408.8848876953125\n",
      "                              Voc = 1439.1885986328125\n",
      "                              Jsc = 2122.944091796875\n",
      "                              FF = 1544.843017578125\n",
      "Finished epoch  215\n",
      "On epoch  216\n",
      "Total Epoch Testing MAPE: PCE = 1409.12890625\n",
      "                              Voc = 1438.019287109375\n",
      "                              Jsc = 2132.63671875\n",
      "                              FF = 1544.66796875\n",
      "Finished epoch  216\n",
      "On epoch  217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1410.452880859375\n",
      "                              Voc = 1438.687744140625\n",
      "                              Jsc = 2138.288818359375\n",
      "                              FF = 1544.84619140625\n",
      "Finished epoch  217\n",
      "On epoch  218\n",
      "Total Epoch Testing MAPE: PCE = 1409.7127685546875\n",
      "                              Voc = 1438.85498046875\n",
      "                              Jsc = 2136.92333984375\n",
      "                              FF = 1545.2119140625\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 1409.867919921875\n",
      "                              Voc = 1439.6063232421875\n",
      "                              Jsc = 2138.48193359375\n",
      "                              FF = 1545.340576171875\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 1411.2176513671875\n",
      "                              Voc = 1439.4666748046875\n",
      "                              Jsc = 2134.573974609375\n",
      "                              FF = 1544.5701904296875\n",
      "Finished epoch  220\n",
      "On epoch  221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1411.31884765625\n",
      "                              Voc = 1439.392333984375\n",
      "                              Jsc = 2143.412353515625\n",
      "                              FF = 1544.5474853515625\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 1411.0521240234375\n",
      "                              Voc = 1437.971923828125\n",
      "                              Jsc = 2154.685302734375\n",
      "                              FF = 1545.2408447265625\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 1410.956787109375\n",
      "                              Voc = 1438.846923828125\n",
      "                              Jsc = 2159.57568359375\n",
      "                              FF = 1546.0411376953125\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 1411.84716796875\n",
      "                              Voc = 1440.22216796875\n",
      "                              Jsc = 2166.0576171875\n",
      "                              FF = 1544.8951416015625\n",
      "Finished epoch  224\n",
      "On epoch  225\n",
      "Total Epoch Testing MAPE: PCE = 1411.3392333984375\n",
      "                              Voc = 1441.0294189453125\n",
      "                              Jsc = 2172.8203125\n",
      "                              FF = 1546.21630859375\n",
      "Finished epoch  225\n",
      "On epoch  226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1414.2532958984375\n",
      "                              Voc = 1439.287841796875\n",
      "                              Jsc = 2173.846435546875\n",
      "                              FF = 1545.6685791015625\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 1414.5582275390625\n",
      "                              Voc = 1440.048828125\n",
      "                              Jsc = 2181.25439453125\n",
      "                              FF = 1547.02734375\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 1413.1474609375\n",
      "                              Voc = 1439.116455078125\n",
      "                              Jsc = 2185.72265625\n",
      "                              FF = 1546.16748046875\n",
      "Finished epoch  228\n",
      "On epoch  229\n",
      "Total Epoch Testing MAPE: PCE = 1412.5982666015625\n",
      "                              Voc = 1439.9158935546875\n",
      "                              Jsc = 2178.2197265625\n",
      "                              FF = 1546.078125\n",
      "Finished epoch  229\n",
      "On epoch  230\n",
      "Total Epoch Testing MAPE: PCE = 1412.389892578125\n",
      "                              Voc = 1439.0439453125\n",
      "                              Jsc = 2179.677001953125\n",
      "                              FF = 1546.77099609375\n",
      "Finished epoch  230\n",
      "On epoch  231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1413.0943603515625\n",
      "                              Voc = 1439.3193359375\n",
      "                              Jsc = 2181.537353515625\n",
      "                              FF = 1546.7613525390625\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 1414.63623046875\n",
      "                              Voc = 1438.6976318359375\n",
      "                              Jsc = 2172.48828125\n",
      "                              FF = 1546.3673095703125\n",
      "Finished epoch  232\n",
      "On epoch  233\n",
      "Total Epoch Testing MAPE: PCE = 1413.928466796875\n",
      "                              Voc = 1439.2061767578125\n",
      "                              Jsc = 2170.820556640625\n",
      "                              FF = 1547.5899658203125\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 1413.2802734375\n",
      "                              Voc = 1439.7314453125\n",
      "                              Jsc = 2166.054443359375\n",
      "                              FF = 1547.4951171875\n",
      "Finished epoch  234\n",
      "On epoch  235\n",
      "Total Epoch Testing MAPE: PCE = 1413.2554931640625\n",
      "                              Voc = 1438.53759765625\n",
      "                              Jsc = 2164.05078125\n",
      "                              FF = 1547.6678466796875\n",
      "Finished epoch  235\n",
      "On epoch  236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1414.203369140625\n",
      "                              Voc = 1439.85302734375\n",
      "                              Jsc = 2161.31787109375\n",
      "                              FF = 1547.416259765625\n",
      "Finished epoch  236\n",
      "On epoch  237\n",
      "Total Epoch Testing MAPE: PCE = 1416.51904296875\n",
      "                              Voc = 1438.84326171875\n",
      "                              Jsc = 2165.474853515625\n",
      "                              FF = 1546.981201171875\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 1414.4488525390625\n",
      "                              Voc = 1439.4986572265625\n",
      "                              Jsc = 2166.70849609375\n",
      "                              FF = 1547.93896484375\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 1414.9132080078125\n",
      "                              Voc = 1439.9329833984375\n",
      "                              Jsc = 2168.7451171875\n",
      "                              FF = 1547.4310302734375\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 1415.0797119140625\n",
      "                              Voc = 1439.58447265625\n",
      "                              Jsc = 2163.61083984375\n",
      "                              FF = 1548.1058349609375\n",
      "Finished epoch  240\n",
      "On epoch  241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1415.5904541015625\n",
      "                              Voc = 1440.4915771484375\n",
      "                              Jsc = 2158.150390625\n",
      "                              FF = 1548.8756103515625\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 1417.9246826171875\n",
      "                              Voc = 1440.466064453125\n",
      "                              Jsc = 2156.611572265625\n",
      "                              FF = 1547.570556640625\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 1417.6094970703125\n",
      "                              Voc = 1439.8187255859375\n",
      "                              Jsc = 2161.5947265625\n",
      "                              FF = 1546.52734375\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 1415.9334716796875\n",
      "                              Voc = 1440.7821044921875\n",
      "                              Jsc = 2158.645263671875\n",
      "                              FF = 1546.6722412109375\n",
      "Finished epoch  244\n",
      "On epoch  245\n",
      "Total Epoch Testing MAPE: PCE = 1416.44873046875\n",
      "                              Voc = 1442.2186279296875\n",
      "                              Jsc = 2162.318603515625\n",
      "                              FF = 1545.3436279296875\n",
      "Finished epoch  245\n",
      "On epoch  246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1415.9686279296875\n",
      "                              Voc = 1443.5413818359375\n",
      "                              Jsc = 2164.3642578125\n",
      "                              FF = 1545.327880859375\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 1415.9403076171875\n",
      "                              Voc = 1442.9488525390625\n",
      "                              Jsc = 2172.27734375\n",
      "                              FF = 1545.66064453125\n",
      "Finished epoch  247\n",
      "On epoch  248\n",
      "Total Epoch Testing MAPE: PCE = 1416.5147705078125\n",
      "                              Voc = 1444.5086669921875\n",
      "                              Jsc = 2167.669189453125\n",
      "                              FF = 1546.2967529296875\n",
      "Finished epoch  248\n",
      "On epoch  249\n",
      "Total Epoch Testing MAPE: PCE = 1417.0511474609375\n",
      "                              Voc = 1443.615966796875\n",
      "                              Jsc = 2169.405029296875\n",
      "                              FF = 1546.16650390625\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 1415.270751953125\n",
      "                              Voc = 1444.6221923828125\n",
      "                              Jsc = 2168.619140625\n",
      "                              FF = 1546.822021484375\n",
      "Finished epoch  250\n",
      "On epoch  251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1415.06201171875\n",
      "                              Voc = 1445.3751220703125\n",
      "                              Jsc = 2164.096435546875\n",
      "                              FF = 1546.7135009765625\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 1413.3743896484375\n",
      "                              Voc = 1445.5089111328125\n",
      "                              Jsc = 2159.68212890625\n",
      "                              FF = 1547.57080078125\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 1411.2928466796875\n",
      "                              Voc = 1445.73876953125\n",
      "                              Jsc = 2162.492431640625\n",
      "                              FF = 1548.1168212890625\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 1412.8582763671875\n",
      "                              Voc = 1444.697998046875\n",
      "                              Jsc = 2160.869140625\n",
      "                              FF = 1546.39306640625\n",
      "Finished epoch  254\n",
      "On epoch  255\n",
      "Total Epoch Testing MAPE: PCE = 1411.4718017578125\n",
      "                              Voc = 1444.3453369140625\n",
      "                              Jsc = 2164.615966796875\n",
      "                              FF = 1547.4542236328125\n",
      "Finished epoch  255\n",
      "On epoch  256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1413.481201171875\n",
      "                              Voc = 1444.6688232421875\n",
      "                              Jsc = 2165.021240234375\n",
      "                              FF = 1548.5550537109375\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 1413.4171142578125\n",
      "                              Voc = 1444.658447265625\n",
      "                              Jsc = 2166.556396484375\n",
      "                              FF = 1547.024169921875\n",
      "Finished epoch  257\n",
      "On epoch  258\n",
      "Total Epoch Testing MAPE: PCE = 1412.9622802734375\n",
      "                              Voc = 1444.0772705078125\n",
      "                              Jsc = 2170.26318359375\n",
      "                              FF = 1547.13037109375\n",
      "Finished epoch  258\n",
      "On epoch  259\n",
      "Total Epoch Testing MAPE: PCE = 1414.3106689453125\n",
      "                              Voc = 1444.0904541015625\n",
      "                              Jsc = 2167.368896484375\n",
      "                              FF = 1545.165771484375\n",
      "Finished epoch  259\n",
      "On epoch  260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1414.665283203125\n",
      "                              Voc = 1443.4307861328125\n",
      "                              Jsc = 2166.422119140625\n",
      "                              FF = 1544.4180908203125\n",
      "Finished epoch  260\n",
      "On epoch  261\n",
      "Total Epoch Testing MAPE: PCE = 1414.250732421875\n",
      "                              Voc = 1443.3077392578125\n",
      "                              Jsc = 2168.050048828125\n",
      "                              FF = 1544.472900390625\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 1411.9825439453125\n",
      "                              Voc = 1443.855224609375\n",
      "                              Jsc = 2168.47412109375\n",
      "                              FF = 1545.803466796875\n",
      "Finished epoch  262\n",
      "On epoch  263\n",
      "Total Epoch Testing MAPE: PCE = 1413.56396484375\n",
      "                              Voc = 1444.5706787109375\n",
      "                              Jsc = 2162.862548828125\n",
      "                              FF = 1546.2166748046875\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 1414.9447021484375\n",
      "                              Voc = 1444.790771484375\n",
      "                              Jsc = 2157.722900390625\n",
      "                              FF = 1545.57470703125\n",
      "Finished epoch  264\n",
      "On epoch  265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1414.099609375\n",
      "                              Voc = 1444.1668701171875\n",
      "                              Jsc = 2164.69677734375\n",
      "                              FF = 1545.025634765625\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 1415.706787109375\n",
      "                              Voc = 1443.8707275390625\n",
      "                              Jsc = 2165.090576171875\n",
      "                              FF = 1546.012451171875\n",
      "Finished epoch  266\n",
      "On epoch  267\n",
      "Total Epoch Testing MAPE: PCE = 1417.3037109375\n",
      "                              Voc = 1440.15966796875\n",
      "                              Jsc = 2166.19384765625\n",
      "                              FF = 1545.593505859375\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 1416.8077392578125\n",
      "                              Voc = 1440.3861083984375\n",
      "                              Jsc = 2160.370361328125\n",
      "                              FF = 1546.755859375\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 1414.8267822265625\n",
      "                              Voc = 1439.081298828125\n",
      "                              Jsc = 2158.966552734375\n",
      "                              FF = 1547.58935546875\n",
      "Finished epoch  269\n",
      "On epoch  270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1414.7501220703125\n",
      "                              Voc = 1440.01123046875\n",
      "                              Jsc = 2159.1513671875\n",
      "                              FF = 1547.794189453125\n",
      "Finished epoch  270\n",
      "On epoch  271\n",
      "Total Epoch Testing MAPE: PCE = 1414.8636474609375\n",
      "                              Voc = 1438.5396728515625\n",
      "                              Jsc = 2161.14892578125\n",
      "                              FF = 1547.861328125\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 1416.4810791015625\n",
      "                              Voc = 1439.831298828125\n",
      "                              Jsc = 2154.45751953125\n",
      "                              FF = 1547.196044921875\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 1412.3572998046875\n",
      "                              Voc = 1440.197998046875\n",
      "                              Jsc = 2164.131591796875\n",
      "                              FF = 1546.6285400390625\n",
      "Finished epoch  273\n",
      "On epoch  274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1412.1810302734375\n",
      "                              Voc = 1439.938720703125\n",
      "                              Jsc = 2163.270263671875\n",
      "                              FF = 1545.520263671875\n",
      "Finished epoch  274\n",
      "On epoch  275\n",
      "Total Epoch Testing MAPE: PCE = 1411.578857421875\n",
      "                              Voc = 1439.52783203125\n",
      "                              Jsc = 2155.710693359375\n",
      "                              FF = 1545.9405517578125\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 1410.6771240234375\n",
      "                              Voc = 1439.8297119140625\n",
      "                              Jsc = 2160.714111328125\n",
      "                              FF = 1546.3363037109375\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 1410.348388671875\n",
      "                              Voc = 1439.468017578125\n",
      "                              Jsc = 2166.68212890625\n",
      "                              FF = 1545.6739501953125\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 1408.508056640625\n",
      "                              Voc = 1440.38232421875\n",
      "                              Jsc = 2165.741455078125\n",
      "                              FF = 1545.5059814453125\n",
      "Finished epoch  278\n",
      "On epoch  279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1411.101318359375\n",
      "                              Voc = 1439.8740234375\n",
      "                              Jsc = 2165.5068359375\n",
      "                              FF = 1545.908203125\n",
      "Finished epoch  279\n",
      "On epoch  280\n",
      "Total Epoch Testing MAPE: PCE = 1410.8160400390625\n",
      "                              Voc = 1440.3692626953125\n",
      "                              Jsc = 2170.903076171875\n",
      "                              FF = 1546.2471923828125\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 1409.525634765625\n",
      "                              Voc = 1439.9991455078125\n",
      "                              Jsc = 2174.515869140625\n",
      "                              FF = 1546.13916015625\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 1409.4437255859375\n",
      "                              Voc = 1441.8870849609375\n",
      "                              Jsc = 2177.542724609375\n",
      "                              FF = 1546.4583740234375\n",
      "Finished epoch  282\n",
      "On epoch  283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1410.0233154296875\n",
      "                              Voc = 1442.029052734375\n",
      "                              Jsc = 2173.47265625\n",
      "                              FF = 1545.694091796875\n",
      "Finished epoch  283\n",
      "On epoch  284\n",
      "Total Epoch Testing MAPE: PCE = 1409.390625\n",
      "                              Voc = 1441.692626953125\n",
      "                              Jsc = 2176.164794921875\n",
      "                              FF = 1546.4246826171875\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 1408.6221923828125\n",
      "                              Voc = 1442.4619140625\n",
      "                              Jsc = 2170.39501953125\n",
      "                              FF = 1546.533447265625\n",
      "Finished epoch  285\n",
      "On epoch  286\n",
      "Total Epoch Testing MAPE: PCE = 1410.0224609375\n",
      "                              Voc = 1442.004150390625\n",
      "                              Jsc = 2158.95361328125\n",
      "                              FF = 1545.590087890625\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 1412.0489501953125\n",
      "                              Voc = 1442.0987548828125\n",
      "                              Jsc = 2164.0185546875\n",
      "                              FF = 1545.824951171875\n",
      "Finished epoch  287\n",
      "On epoch  288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1412.3428955078125\n",
      "                              Voc = 1443.326416015625\n",
      "                              Jsc = 2165.1005859375\n",
      "                              FF = 1544.4853515625\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 1412.7303466796875\n",
      "                              Voc = 1444.12890625\n",
      "                              Jsc = 2169.110595703125\n",
      "                              FF = 1544.5166015625\n",
      "Finished epoch  289\n",
      "On epoch  290\n",
      "Total Epoch Testing MAPE: PCE = 1414.205810546875\n",
      "                              Voc = 1442.2379150390625\n",
      "                              Jsc = 2173.396240234375\n",
      "                              FF = 1544.326171875\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 1414.07177734375\n",
      "                              Voc = 1442.3048095703125\n",
      "                              Jsc = 2177.10595703125\n",
      "                              FF = 1543.4931640625\n",
      "Finished epoch  291\n",
      "On epoch  292\n",
      "Total Epoch Testing MAPE: PCE = 1414.7109375\n",
      "                              Voc = 1441.87890625\n",
      "                              Jsc = 2177.57568359375\n",
      "                              FF = 1543.4603271484375\n",
      "Finished epoch  292\n",
      "On epoch  293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1414.1522216796875\n",
      "                              Voc = 1439.4033203125\n",
      "                              Jsc = 2176.635986328125\n",
      "                              FF = 1542.9095458984375\n",
      "Finished epoch  293\n",
      "On epoch  294\n",
      "Total Epoch Testing MAPE: PCE = 1413.77099609375\n",
      "                              Voc = 1440.2025146484375\n",
      "                              Jsc = 2178.486328125\n",
      "                              FF = 1542.6768798828125\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 1413.8828125\n",
      "                              Voc = 1440.30908203125\n",
      "                              Jsc = 2178.258056640625\n",
      "                              FF = 1541.84619140625\n",
      "Finished epoch  295\n",
      "On epoch  296\n",
      "Total Epoch Testing MAPE: PCE = 1411.943603515625\n",
      "                              Voc = 1442.666748046875\n",
      "                              Jsc = 2174.012451171875\n",
      "                              FF = 1542.9334716796875\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 1412.7916259765625\n",
      "                              Voc = 1444.2244873046875\n",
      "                              Jsc = 2180.101806640625\n",
      "                              FF = 1542.093505859375\n",
      "Finished epoch  297\n",
      "On epoch  298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1413.530029296875\n",
      "                              Voc = 1445.1309814453125\n",
      "                              Jsc = 2185.43505859375\n",
      "                              FF = 1542.9071044921875\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 1412.494873046875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2185.4609375\n",
      "                              FF = 1543.39404296875\n",
      "Finished epoch  299\n",
      "On epoch  300\n",
      "Total Epoch Testing MAPE: PCE = 1412.2183837890625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2188.677490234375\n",
      "                              FF = 1543.9522705078125\n",
      "Finished epoch  300\n",
      "On epoch  301\n",
      "Total Epoch Testing MAPE: PCE = 1411.6676025390625\n",
      "                              Voc = 1445.292724609375\n",
      "                              Jsc = 2193.778564453125\n",
      "                              FF = 1543.9217529296875\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 1409.744873046875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2196.259765625\n",
      "                              FF = 1544.079345703125\n",
      "Finished epoch  302\n",
      "On epoch  303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1410.9324951171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2191.18212890625\n",
      "                              FF = 1543.379638671875\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 1412.324951171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2186.89111328125\n",
      "                              FF = 1542.54296875\n",
      "Finished epoch  304\n",
      "On epoch  305\n",
      "Total Epoch Testing MAPE: PCE = 1413.2125244140625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2182.498291015625\n",
      "                              FF = 1543.3739013671875\n",
      "Finished epoch  305\n",
      "On epoch  306\n",
      "Total Epoch Testing MAPE: PCE = 1413.6278076171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2180.701904296875\n",
      "                              FF = 1542.7840576171875\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 1415.65283203125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2181.213623046875\n",
      "                              FF = 1542.3555908203125\n",
      "Finished epoch  307\n",
      "On epoch  308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1414.3060302734375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2177.8544921875\n",
      "                              FF = 1542.352294921875\n",
      "Finished epoch  308\n",
      "On epoch  309\n",
      "Total Epoch Testing MAPE: PCE = 1410.2109375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2180.4482421875\n",
      "                              FF = 1541.9537353515625\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 1410.14111328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2179.292724609375\n",
      "                              FF = 1542.2532958984375\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 1409.7353515625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2183.6669921875\n",
      "                              FF = 1542.083740234375\n",
      "Finished epoch  311\n",
      "On epoch  312\n",
      "Total Epoch Testing MAPE: PCE = 1409.615478515625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2175.873046875\n",
      "                              FF = 1542.737060546875\n",
      "Finished epoch  312\n",
      "On epoch  313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1411.664306640625\n",
      "                              Voc = 1444.878662109375\n",
      "                              Jsc = 2177.2255859375\n",
      "                              FF = 1542.4947509765625\n",
      "Finished epoch  313\n",
      "On epoch  314\n",
      "Total Epoch Testing MAPE: PCE = 1411.4881591796875\n",
      "                              Voc = 1443.9197998046875\n",
      "                              Jsc = 2184.107421875\n",
      "                              FF = 1541.88818359375\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 1410.0919189453125\n",
      "                              Voc = 1444.560791015625\n",
      "                              Jsc = 2188.18408203125\n",
      "                              FF = 1542.8912353515625\n",
      "Finished epoch  315\n",
      "On epoch  316\n",
      "Total Epoch Testing MAPE: PCE = 1411.941162109375\n",
      "                              Voc = 1442.8408203125\n",
      "                              Jsc = 2194.4541015625\n",
      "                              FF = 1542.659423828125\n",
      "Finished epoch  316\n",
      "On epoch  317\n",
      "Total Epoch Testing MAPE: PCE = 1413.862060546875\n",
      "                              Voc = 1443.1016845703125\n",
      "                              Jsc = 2194.472412109375\n",
      "                              FF = 1542.724853515625\n",
      "Finished epoch  317\n",
      "On epoch  318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1412.029052734375\n",
      "                              Voc = 1443.618408203125\n",
      "                              Jsc = 2196.43701171875\n",
      "                              FF = 1541.216064453125\n",
      "Finished epoch  318\n",
      "On epoch  319\n",
      "Total Epoch Testing MAPE: PCE = 1410.046142578125\n",
      "                              Voc = 1444.760498046875\n",
      "                              Jsc = 2192.984619140625\n",
      "                              FF = 1541.84912109375\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 1410.86328125\n",
      "                              Voc = 1444.684814453125\n",
      "                              Jsc = 2194.65380859375\n",
      "                              FF = 1540.9127197265625\n",
      "Finished epoch  320\n",
      "On epoch  321\n",
      "Total Epoch Testing MAPE: PCE = 1409.68896484375\n",
      "                              Voc = 1444.4786376953125\n",
      "                              Jsc = 2194.982421875\n",
      "                              FF = 1540.937255859375\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 1409.2950439453125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2193.875\n",
      "                              FF = 1541.2650146484375\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 1408.2200927734375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2195.647216796875\n",
      "                              FF = 1540.5472412109375\n",
      "Finished epoch  323\n",
      "On epoch  324\n",
      "Total Epoch Testing MAPE: PCE = 1408.646240234375\n",
      "                              Voc = 1443.3272705078125\n",
      "                              Jsc = 2197.40380859375\n",
      "                              FF = 1539.953857421875\n",
      "Finished epoch  324\n",
      "On epoch  325\n",
      "Total Epoch Testing MAPE: PCE = 1409.4139404296875\n",
      "                              Voc = 1444.2987060546875\n",
      "                              Jsc = 2202.739990234375\n",
      "                              FF = 1539.2681884765625\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 1410.2955322265625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2196.077392578125\n",
      "                              FF = 1538.4073486328125\n",
      "Finished epoch  326\n",
      "On epoch  327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1410.7471923828125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2195.79248046875\n",
      "                              FF = 1538.9298095703125\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 1410.485595703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2194.121826171875\n",
      "                              FF = 1538.299560546875\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 1409.89794921875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2190.6689453125\n",
      "                              FF = 1539.177001953125\n",
      "Finished epoch  329\n",
      "On epoch  330\n",
      "Total Epoch Testing MAPE: PCE = 1411.1317138671875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2190.63525390625\n",
      "                              FF = 1539.4334716796875\n",
      "Finished epoch  330\n",
      "On epoch  331\n",
      "Total Epoch Testing MAPE: PCE = 1409.2918701171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2186.768798828125\n",
      "                              FF = 1539.267578125\n",
      "Finished epoch  331\n",
      "On epoch  332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1409.70166015625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2191.0390625\n",
      "                              FF = 1539.931884765625\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 1410.0518798828125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2190.8515625\n",
      "                              FF = 1540.203369140625\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 1409.1195068359375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2187.932861328125\n",
      "                              FF = 1540.6995849609375\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 1406.9132080078125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2189.162109375\n",
      "                              FF = 1540.9002685546875\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 1405.47998046875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2189.38525390625\n",
      "                              FF = 1540.7803955078125\n",
      "Finished epoch  336\n",
      "On epoch  337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.037353515625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2182.25634765625\n",
      "                              FF = 1539.9935302734375\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 1404.959228515625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2172.91162109375\n",
      "                              FF = 1540.23681640625\n",
      "Finished epoch  338\n",
      "On epoch  339\n",
      "Total Epoch Testing MAPE: PCE = 1405.668701171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2173.90673828125\n",
      "                              FF = 1539.17578125\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 1404.0477294921875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2167.032958984375\n",
      "                              FF = 1540.005126953125\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 1404.7232666015625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2165.774169921875\n",
      "                              FF = 1541.427490234375\n",
      "Finished epoch  341\n",
      "On epoch  342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.985595703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2167.136962890625\n",
      "                              FF = 1541.818115234375\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 1406.9791259765625\n",
      "                              Voc = 1444.9468994140625\n",
      "                              Jsc = 2163.727294921875\n",
      "                              FF = 1543.0792236328125\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 1407.24169921875\n",
      "                              Voc = 1445.2955322265625\n",
      "                              Jsc = 2168.504638671875\n",
      "                              FF = 1542.2933349609375\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 1405.8206787109375\n",
      "                              Voc = 1445.0823974609375\n",
      "                              Jsc = 2171.276611328125\n",
      "                              FF = 1542.351806640625\n",
      "Finished epoch  345\n",
      "On epoch  346\n",
      "Total Epoch Testing MAPE: PCE = 1405.2564697265625\n",
      "                              Voc = 1444.4376220703125\n",
      "                              Jsc = 2179.265869140625\n",
      "                              FF = 1540.9293212890625\n",
      "Finished epoch  346\n",
      "On epoch  347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1403.0618896484375\n",
      "                              Voc = 1444.0740966796875\n",
      "                              Jsc = 2181.815673828125\n",
      "                              FF = 1540.6531982421875\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 1404.670654296875\n",
      "                              Voc = 1444.7645263671875\n",
      "                              Jsc = 2176.136474609375\n",
      "                              FF = 1539.7545166015625\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 1405.349609375\n",
      "                              Voc = 1445.8175048828125\n",
      "                              Jsc = 2175.082275390625\n",
      "                              FF = 1540.0533447265625\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 1405.2696533203125\n",
      "                              Voc = 1446.8890380859375\n",
      "                              Jsc = 2180.425537109375\n",
      "                              FF = 1539.8291015625\n",
      "Finished epoch  350\n",
      "On epoch  351\n",
      "Total Epoch Testing MAPE: PCE = 1403.8067626953125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2173.465087890625\n",
      "                              FF = 1539.9813232421875\n",
      "Finished epoch  351\n",
      "On epoch  352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1402.9193115234375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2170.92333984375\n",
      "                              FF = 1539.8751220703125\n",
      "Finished epoch  352\n",
      "On epoch  353\n",
      "Total Epoch Testing MAPE: PCE = 1402.917236328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2172.939453125\n",
      "                              FF = 1539.5489501953125\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 1401.8577880859375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2169.4443359375\n",
      "                              FF = 1538.5634765625\n",
      "Finished epoch  354\n",
      "On epoch  355\n",
      "Total Epoch Testing MAPE: PCE = 1398.796142578125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2175.91943359375\n",
      "                              FF = 1538.895263671875\n",
      "Finished epoch  355\n",
      "On epoch  356\n",
      "Total Epoch Testing MAPE: PCE = 1397.003662109375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2181.73779296875\n",
      "                              FF = 1538.522705078125\n",
      "Finished epoch  356\n",
      "On epoch  357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1398.8992919921875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2186.87158203125\n",
      "                              FF = 1538.0333251953125\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 1398.3282470703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2187.864990234375\n",
      "                              FF = 1537.581298828125\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 1398.9908447265625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2198.754150390625\n",
      "                              FF = 1538.153564453125\n",
      "Finished epoch  359\n",
      "On epoch  360\n",
      "Total Epoch Testing MAPE: PCE = 1397.4580078125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2197.5869140625\n",
      "                              FF = 1537.392822265625\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 1398.3013916015625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2199.43017578125\n",
      "                              FF = 1536.36572265625\n",
      "Finished epoch  361\n",
      "On epoch  362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1399.0281982421875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2202.454345703125\n",
      "                              FF = 1535.1143798828125\n",
      "Finished epoch  362\n",
      "On epoch  363\n",
      "Total Epoch Testing MAPE: PCE = 1398.065185546875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2201.5166015625\n",
      "                              FF = 1533.931884765625\n",
      "Finished epoch  363\n",
      "On epoch  364\n",
      "Total Epoch Testing MAPE: PCE = 1397.512451171875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2204.893798828125\n",
      "                              FF = 1534.283935546875\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 1399.151123046875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2208.818603515625\n",
      "                              FF = 1534.40234375\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 1399.3448486328125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2206.70458984375\n",
      "                              FF = 1535.4669189453125\n",
      "Finished epoch  366\n",
      "On epoch  367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1399.994384765625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2201.619873046875\n",
      "                              FF = 1537.1435546875\n",
      "Finished epoch  367\n",
      "On epoch  368\n",
      "Total Epoch Testing MAPE: PCE = 1400.31103515625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2206.251220703125\n",
      "                              FF = 1537.347412109375\n",
      "Finished epoch  368\n",
      "On epoch  369\n",
      "Total Epoch Testing MAPE: PCE = 1402.108642578125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2208.758056640625\n",
      "                              FF = 1537.84326171875\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 1402.910400390625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2204.012939453125\n",
      "                              FF = 1539.682373046875\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 1404.485595703125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2205.797607421875\n",
      "                              FF = 1540.3177490234375\n",
      "Finished epoch  371\n",
      "On epoch  372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.420654296875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2204.745849609375\n",
      "                              FF = 1540.383544921875\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 1406.7061767578125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2203.88720703125\n",
      "                              FF = 1540.0179443359375\n",
      "Finished epoch  373\n",
      "On epoch  374\n",
      "Total Epoch Testing MAPE: PCE = 1407.3642578125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2202.844482421875\n",
      "                              FF = 1538.3658447265625\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 1408.1170654296875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2204.976318359375\n",
      "                              FF = 1537.463623046875\n",
      "Finished epoch  375\n",
      "On epoch  376\n",
      "Total Epoch Testing MAPE: PCE = 1409.0111083984375\n",
      "                              Voc = nan\n",
      "                              Jsc = 2210.87255859375\n",
      "                              FF = 1537.2396240234375\n",
      "Finished epoch  376\n",
      "On epoch  377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1408.80419921875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2207.36669921875\n",
      "                              FF = 1536.5328369140625\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 1407.962158203125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2203.08154296875\n",
      "                              FF = 1536.978759765625\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 1406.598876953125\n",
      "                              Voc = nan\n",
      "                              Jsc = 2194.081787109375\n",
      "                              FF = 1536.755126953125\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 1405.82373046875\n",
      "                              Voc = 1445.61669921875\n",
      "                              Jsc = 2196.352783203125\n",
      "                              FF = 1536.2481689453125\n",
      "Finished epoch  380\n",
      "On epoch  381\n",
      "Total Epoch Testing MAPE: PCE = 1408.005859375\n",
      "                              Voc = 1445.174072265625\n",
      "                              Jsc = 2197.67333984375\n",
      "                              FF = 1534.30517578125\n",
      "Finished epoch  381\n",
      "On epoch  382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1407.4573974609375\n",
      "                              Voc = 1445.05859375\n",
      "                              Jsc = 2201.19189453125\n",
      "                              FF = 1533.711669921875\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 1407.4931640625\n",
      "                              Voc = 1445.218994140625\n",
      "                              Jsc = 2201.034912109375\n",
      "                              FF = 1534.321533203125\n",
      "Finished epoch  383\n",
      "On epoch  384\n",
      "Total Epoch Testing MAPE: PCE = 1406.0260009765625\n",
      "                              Voc = 1443.9560546875\n",
      "                              Jsc = 2195.332763671875\n",
      "                              FF = 1534.177734375\n",
      "Finished epoch  384\n",
      "On epoch  385\n",
      "Total Epoch Testing MAPE: PCE = 1406.0048828125\n",
      "                              Voc = 1443.586181640625\n",
      "                              Jsc = 2196.359619140625\n",
      "                              FF = 1535.7279052734375\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 1404.1466064453125\n",
      "                              Voc = 1442.4544677734375\n",
      "                              Jsc = 2197.7783203125\n",
      "                              FF = 1534.91796875\n",
      "Finished epoch  386\n",
      "On epoch  387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.92626953125\n",
      "                              Voc = 1443.666015625\n",
      "                              Jsc = 2195.92138671875\n",
      "                              FF = 1534.590576171875\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 1406.0244140625\n",
      "                              Voc = 1443.0849609375\n",
      "                              Jsc = 2193.82275390625\n",
      "                              FF = 1534.634521484375\n",
      "Finished epoch  388\n",
      "On epoch  389\n",
      "Total Epoch Testing MAPE: PCE = 1404.955078125\n",
      "                              Voc = 1443.5662841796875\n",
      "                              Jsc = 2192.88037109375\n",
      "                              FF = 1534.5692138671875\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 1406.1513671875\n",
      "                              Voc = 1443.1795654296875\n",
      "                              Jsc = 2198.216552734375\n",
      "                              FF = 1533.95361328125\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 1405.8822021484375\n",
      "                              Voc = 1444.027587890625\n",
      "                              Jsc = 2198.978515625\n",
      "                              FF = 1534.1378173828125\n",
      "Finished epoch  391\n",
      "On epoch  392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1403.6138916015625\n",
      "                              Voc = 1442.500244140625\n",
      "                              Jsc = 2201.456298828125\n",
      "                              FF = 1534.4219970703125\n",
      "Finished epoch  392\n",
      "On epoch  393\n",
      "Total Epoch Testing MAPE: PCE = 1404.4971923828125\n",
      "                              Voc = 1443.1846923828125\n",
      "                              Jsc = 2211.5986328125\n",
      "                              FF = 1534.7247314453125\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 1404.0565185546875\n",
      "                              Voc = 1442.8812255859375\n",
      "                              Jsc = 2208.658447265625\n",
      "                              FF = 1534.7518310546875\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 1405.1181640625\n",
      "                              Voc = 1442.1500244140625\n",
      "                              Jsc = 2208.037841796875\n",
      "                              FF = 1534.1632080078125\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 1406.2171630859375\n",
      "                              Voc = 1443.466064453125\n",
      "                              Jsc = 2210.3408203125\n",
      "                              FF = 1533.844482421875\n",
      "Finished epoch  396\n",
      "On epoch  397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.1275634765625\n",
      "                              Voc = 1441.697265625\n",
      "                              Jsc = 2207.97900390625\n",
      "                              FF = 1534.552978515625\n",
      "Finished epoch  397\n",
      "On epoch  398\n",
      "Total Epoch Testing MAPE: PCE = 1404.384765625\n",
      "                              Voc = 1441.565185546875\n",
      "                              Jsc = 2206.93408203125\n",
      "                              FF = 1535.2896728515625\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 1404.18505859375\n",
      "                              Voc = 1442.1583251953125\n",
      "                              Jsc = 2205.150146484375\n",
      "                              FF = 1535.937744140625\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 1403.5189208984375\n",
      "                              Voc = 1442.7530517578125\n",
      "                              Jsc = 2196.88134765625\n",
      "                              FF = 1536.935546875\n",
      "Finished epoch  400\n",
      "On epoch  401\n",
      "Total Epoch Testing MAPE: PCE = 1402.978759765625\n",
      "                              Voc = 1443.66748046875\n",
      "                              Jsc = 2192.967041015625\n",
      "                              FF = 1536.8238525390625\n",
      "Finished epoch  401\n",
      "On epoch  402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1403.4735107421875\n",
      "                              Voc = 1445.55615234375\n",
      "                              Jsc = 2195.796630859375\n",
      "                              FF = 1537.0572509765625\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 1403.93994140625\n",
      "                              Voc = 1444.867919921875\n",
      "                              Jsc = 2197.01953125\n",
      "                              FF = 1536.00048828125\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 1404.610107421875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2198.234375\n",
      "                              FF = 1536.5430908203125\n",
      "Finished epoch  404\n",
      "On epoch  405\n",
      "Total Epoch Testing MAPE: PCE = 1404.3463134765625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2193.85888671875\n",
      "                              FF = 1535.6405029296875\n",
      "Finished epoch  405\n",
      "On epoch  406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1402.8709716796875\n",
      "                              Voc = 1444.186767578125\n",
      "                              Jsc = 2189.7958984375\n",
      "                              FF = 1536.2384033203125\n",
      "Finished epoch  406\n",
      "On epoch  407\n",
      "Total Epoch Testing MAPE: PCE = 1403.252197265625\n",
      "                              Voc = nan\n",
      "                              Jsc = 2193.421630859375\n",
      "                              FF = 1536.23388671875\n",
      "Finished epoch  407\n",
      "On epoch  408\n",
      "Total Epoch Testing MAPE: PCE = 1404.398193359375\n",
      "                              Voc = 1443.151611328125\n",
      "                              Jsc = 2189.05078125\n",
      "                              FF = 1534.6181640625\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 1404.6190185546875\n",
      "                              Voc = nan\n",
      "                              Jsc = 2181.771240234375\n",
      "                              FF = 1534.6146240234375\n",
      "Finished epoch  409\n",
      "On epoch  410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.8455810546875\n",
      "                              Voc = 1442.394775390625\n",
      "                              Jsc = 2182.35107421875\n",
      "                              FF = 1533.623779296875\n",
      "Finished epoch  410\n",
      "On epoch  411\n",
      "Total Epoch Testing MAPE: PCE = 1405.42919921875\n",
      "                              Voc = 1441.9072265625\n",
      "                              Jsc = 2181.681640625\n",
      "                              FF = 1532.364501953125\n",
      "Finished epoch  411\n",
      "On epoch  412\n",
      "Total Epoch Testing MAPE: PCE = 1407.214111328125\n",
      "                              Voc = 1441.585205078125\n",
      "                              Jsc = 2177.247314453125\n",
      "                              FF = 1532.7847900390625\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 1407.9775390625\n",
      "                              Voc = 1440.909423828125\n",
      "                              Jsc = 2180.07958984375\n",
      "                              FF = 1533.8736572265625\n",
      "Finished epoch  413\n",
      "On epoch  414\n",
      "Total Epoch Testing MAPE: PCE = 1407.7066650390625\n",
      "                              Voc = 1440.702392578125\n",
      "                              Jsc = 2178.418212890625\n",
      "                              FF = 1533.55859375\n",
      "Finished epoch  414\n",
      "On epoch  415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1407.156494140625\n",
      "                              Voc = 1438.244140625\n",
      "                              Jsc = 2183.572265625\n",
      "                              FF = 1532.885986328125\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 1409.658447265625\n",
      "                              Voc = 1438.8133544921875\n",
      "                              Jsc = 2181.83642578125\n",
      "                              FF = 1532.4544677734375\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 1410.8623046875\n",
      "                              Voc = 1438.7864990234375\n",
      "                              Jsc = 2180.6376953125\n",
      "                              FF = 1532.5179443359375\n",
      "Finished epoch  417\n",
      "On epoch  418\n",
      "Total Epoch Testing MAPE: PCE = 1407.6851806640625\n",
      "                              Voc = 1439.1207275390625\n",
      "                              Jsc = 2184.13134765625\n",
      "                              FF = 1532.09033203125\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 1405.62841796875\n",
      "                              Voc = 1439.3778076171875\n",
      "                              Jsc = 2175.85546875\n",
      "                              FF = 1531.7369384765625\n",
      "Finished epoch  419\n",
      "On epoch  420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.1651611328125\n",
      "                              Voc = 1439.6368408203125\n",
      "                              Jsc = 2171.797119140625\n",
      "                              FF = 1531.76123046875\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 1405.304443359375\n",
      "                              Voc = 1440.0220947265625\n",
      "                              Jsc = 2167.126953125\n",
      "                              FF = 1531.927001953125\n",
      "Finished epoch  421\n",
      "On epoch  422\n",
      "Total Epoch Testing MAPE: PCE = 1406.7430419921875\n",
      "                              Voc = 1439.6712646484375\n",
      "                              Jsc = 2167.445068359375\n",
      "                              FF = 1531.6361083984375\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 1407.7554931640625\n",
      "                              Voc = 1440.035888671875\n",
      "                              Jsc = 2169.25244140625\n",
      "                              FF = 1531.439453125\n",
      "Finished epoch  423\n",
      "On epoch  424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1408.2679443359375\n",
      "                              Voc = 1438.4102783203125\n",
      "                              Jsc = 2161.408447265625\n",
      "                              FF = 1530.761474609375\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 1405.4156494140625\n",
      "                              Voc = 1438.24560546875\n",
      "                              Jsc = 2151.187255859375\n",
      "                              FF = 1530.5162353515625\n",
      "Finished epoch  425\n",
      "On epoch  426\n",
      "Total Epoch Testing MAPE: PCE = 1405.08740234375\n",
      "                              Voc = 1438.0174560546875\n",
      "                              Jsc = 2152.28515625\n",
      "                              FF = 1531.5601806640625\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 1406.0631103515625\n",
      "                              Voc = 1438.3443603515625\n",
      "                              Jsc = 2149.57275390625\n",
      "                              FF = 1531.3743896484375\n",
      "Finished epoch  427\n",
      "On epoch  428\n",
      "Total Epoch Testing MAPE: PCE = 1407.4998779296875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Voc = 1439.8643798828125\n",
      "                              Jsc = 2154.816162109375\n",
      "                              FF = 1530.9500732421875\n",
      "Finished epoch  428\n",
      "On epoch  429\n",
      "Total Epoch Testing MAPE: PCE = 1408.091796875\n",
      "                              Voc = 1440.9134521484375\n",
      "                              Jsc = 2155.12841796875\n",
      "                              FF = 1530.2196044921875\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 1406.73193359375\n",
      "                              Voc = 1440.743408203125\n",
      "                              Jsc = 2164.754150390625\n",
      "                              FF = 1530.2646484375\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 1406.6353759765625\n",
      "                              Voc = 1439.605224609375\n",
      "                              Jsc = 2171.727294921875\n",
      "                              FF = 1531.06591796875\n",
      "Finished epoch  431\n",
      "On epoch  432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.732177734375\n",
      "                              Voc = 1439.30615234375\n",
      "                              Jsc = 2176.065673828125\n",
      "                              FF = 1531.5087890625\n",
      "Finished epoch  432\n",
      "On epoch  433\n",
      "Total Epoch Testing MAPE: PCE = 1406.306884765625\n",
      "                              Voc = 1439.575439453125\n",
      "                              Jsc = 2175.9716796875\n",
      "                              FF = 1531.45703125\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 1406.7838134765625\n",
      "                              Voc = 1440.7825927734375\n",
      "                              Jsc = 2174.52197265625\n",
      "                              FF = 1531.89794921875\n",
      "Finished epoch  434\n",
      "On epoch  435\n",
      "Total Epoch Testing MAPE: PCE = 1406.7730712890625\n",
      "                              Voc = 1441.4400634765625\n",
      "                              Jsc = 2176.231689453125\n",
      "                              FF = 1532.1378173828125\n",
      "Finished epoch  435\n",
      "On epoch  436\n",
      "Total Epoch Testing MAPE: PCE = 1405.208251953125\n",
      "                              Voc = 1442.4354248046875\n",
      "                              Jsc = 2169.363525390625\n",
      "                              FF = 1530.6612548828125\n",
      "Finished epoch  436\n",
      "On epoch  437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.1597900390625\n",
      "                              Voc = 1442.846923828125\n",
      "                              Jsc = 2165.33251953125\n",
      "                              FF = 1530.3636474609375\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 1405.1353759765625\n",
      "                              Voc = 1444.067626953125\n",
      "                              Jsc = 2168.52197265625\n",
      "                              FF = 1529.99365234375\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 1406.6082763671875\n",
      "                              Voc = 1443.21630859375\n",
      "                              Jsc = 2167.412109375\n",
      "                              FF = 1530.0362548828125\n",
      "Finished epoch  439\n",
      "On epoch  440\n",
      "Total Epoch Testing MAPE: PCE = 1408.865966796875\n",
      "                              Voc = 1441.476806640625\n",
      "                              Jsc = 2172.255615234375\n",
      "                              FF = 1530.6043701171875\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 1408.9849853515625\n",
      "                              Voc = 1441.6466064453125\n",
      "                              Jsc = 2173.27099609375\n",
      "                              FF = 1530.354736328125\n",
      "Finished epoch  441\n",
      "On epoch  442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.0513916015625\n",
      "                              Voc = 1442.5467529296875\n",
      "                              Jsc = 2175.102783203125\n",
      "                              FF = 1529.6231689453125\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 1407.4080810546875\n",
      "                              Voc = 1443.6622314453125\n",
      "                              Jsc = 2169.423828125\n",
      "                              FF = 1530.1995849609375\n",
      "Finished epoch  443\n",
      "On epoch  444\n",
      "Total Epoch Testing MAPE: PCE = 1406.217529296875\n",
      "                              Voc = 1443.7415771484375\n",
      "                              Jsc = 2172.7333984375\n",
      "                              FF = 1531.110107421875\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 1406.2667236328125\n",
      "                              Voc = 1443.5535888671875\n",
      "                              Jsc = 2179.51220703125\n",
      "                              FF = 1531.2509765625\n",
      "Finished epoch  445\n",
      "On epoch  446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.556884765625\n",
      "                              Voc = 1443.083740234375\n",
      "                              Jsc = 2187.013427734375\n",
      "                              FF = 1531.89501953125\n",
      "Finished epoch  446\n",
      "On epoch  447\n",
      "Total Epoch Testing MAPE: PCE = 1406.603759765625\n",
      "                              Voc = 1441.338134765625\n",
      "                              Jsc = 2191.810302734375\n",
      "                              FF = 1532.942626953125\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 1405.9500732421875\n",
      "                              Voc = 1442.541748046875\n",
      "                              Jsc = 2196.287109375\n",
      "                              FF = 1532.901611328125\n",
      "Finished epoch  448\n",
      "On epoch  449\n",
      "Total Epoch Testing MAPE: PCE = 1406.3865966796875\n",
      "                              Voc = 1442.2275390625\n",
      "                              Jsc = 2191.995361328125\n",
      "                              FF = 1531.6246337890625\n",
      "Finished epoch  449\n",
      "On epoch  450\n",
      "Total Epoch Testing MAPE: PCE = 1405.4727783203125\n",
      "                              Voc = 1441.4598388671875\n",
      "                              Jsc = 2191.669189453125\n",
      "                              FF = 1532.46435546875\n",
      "Finished epoch  450\n",
      "On epoch  451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.4827880859375\n",
      "                              Voc = 1442.05908203125\n",
      "                              Jsc = 2194.388916015625\n",
      "                              FF = 1532.7957763671875\n",
      "Finished epoch  451\n",
      "On epoch  452\n",
      "Total Epoch Testing MAPE: PCE = 1402.8663330078125\n",
      "                              Voc = 1440.4599609375\n",
      "                              Jsc = 2184.389404296875\n",
      "                              FF = 1532.046630859375\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 1403.912353515625\n",
      "                              Voc = 1441.6240234375\n",
      "                              Jsc = 2187.423828125\n",
      "                              FF = 1532.3138427734375\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 1403.7978515625\n",
      "                              Voc = 1441.6817626953125\n",
      "                              Jsc = 2189.20556640625\n",
      "                              FF = 1532.1541748046875\n",
      "Finished epoch  454\n",
      "On epoch  455\n",
      "Total Epoch Testing MAPE: PCE = 1405.8046875\n",
      "                              Voc = 1441.4727783203125\n",
      "                              Jsc = 2191.140625\n",
      "                              FF = 1532.94091796875\n",
      "Finished epoch  455\n",
      "On epoch  456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.97998046875\n",
      "                              Voc = 1440.996826171875\n",
      "                              Jsc = 2194.166748046875\n",
      "                              FF = 1531.5421142578125\n",
      "Finished epoch  456\n",
      "On epoch  457\n",
      "Total Epoch Testing MAPE: PCE = 1405.5157470703125\n",
      "                              Voc = 1441.56298828125\n",
      "                              Jsc = 2195.313720703125\n",
      "                              FF = 1530.4246826171875\n",
      "Finished epoch  457\n",
      "On epoch  458\n",
      "Total Epoch Testing MAPE: PCE = 1404.2674560546875\n",
      "                              Voc = 1440.1986083984375\n",
      "                              Jsc = 2198.8671875\n",
      "                              FF = 1530.474853515625\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 1403.0533447265625\n",
      "                              Voc = 1440.730712890625\n",
      "                              Jsc = 2201.011962890625\n",
      "                              FF = 1531.401611328125\n",
      "Finished epoch  459\n",
      "On epoch  460\n",
      "Total Epoch Testing MAPE: PCE = 1403.2447509765625\n",
      "                              Voc = 1442.0484619140625\n",
      "                              Jsc = 2199.22705078125\n",
      "                              FF = 1530.3543701171875\n",
      "Finished epoch  460\n",
      "On epoch  461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1403.3175048828125\n",
      "                              Voc = 1440.729248046875\n",
      "                              Jsc = 2197.92431640625\n",
      "                              FF = 1531.357177734375\n",
      "Finished epoch  461\n",
      "On epoch  462\n",
      "Total Epoch Testing MAPE: PCE = 1402.2603759765625\n",
      "                              Voc = 1441.762939453125\n",
      "                              Jsc = 2190.428955078125\n",
      "                              FF = 1531.41845703125\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 1402.8248291015625\n",
      "                              Voc = 1440.066650390625\n",
      "                              Jsc = 2189.789306640625\n",
      "                              FF = 1531.5438232421875\n",
      "Finished epoch  463\n",
      "On epoch  464\n",
      "Total Epoch Testing MAPE: PCE = 1403.6494140625\n",
      "                              Voc = 1441.4376220703125\n",
      "                              Jsc = 2185.083984375\n",
      "                              FF = 1531.085205078125\n",
      "Finished epoch  464\n",
      "On epoch  465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1404.6044921875\n",
      "                              Voc = 1439.5301513671875\n",
      "                              Jsc = 2191.7119140625\n",
      "                              FF = 1530.3721923828125\n",
      "Finished epoch  465\n",
      "On epoch  466\n",
      "Total Epoch Testing MAPE: PCE = 1402.289794921875\n",
      "                              Voc = 1439.97607421875\n",
      "                              Jsc = 2197.07861328125\n",
      "                              FF = 1530.392822265625\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 1403.16162109375\n",
      "                              Voc = 1440.3641357421875\n",
      "                              Jsc = 2197.197021484375\n",
      "                              FF = 1529.6192626953125\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 1402.880615234375\n",
      "                              Voc = 1439.264892578125\n",
      "                              Jsc = 2199.2802734375\n",
      "                              FF = 1530.692138671875\n",
      "Finished epoch  468\n",
      "On epoch  469\n",
      "Total Epoch Testing MAPE: PCE = 1405.8404541015625\n",
      "                              Voc = 1437.8287353515625\n",
      "                              Jsc = 2197.764404296875\n",
      "                              FF = 1530.3363037109375\n",
      "Finished epoch  469\n",
      "On epoch  470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.1064453125\n",
      "                              Voc = 1437.4599609375\n",
      "                              Jsc = 2203.98291015625\n",
      "                              FF = 1531.3505859375\n",
      "Finished epoch  470\n",
      "On epoch  471\n",
      "Total Epoch Testing MAPE: PCE = 1408.142333984375\n",
      "                              Voc = 1436.97802734375\n",
      "                              Jsc = 2201.19921875\n",
      "                              FF = 1532.290771484375\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 1405.276123046875\n",
      "                              Voc = 1437.9293212890625\n",
      "                              Jsc = 2195.859375\n",
      "                              FF = 1533.585205078125\n",
      "Finished epoch  472\n",
      "On epoch  473\n",
      "Total Epoch Testing MAPE: PCE = 1404.8011474609375\n",
      "                              Voc = 1436.645751953125\n",
      "                              Jsc = 2189.862548828125\n",
      "                              FF = 1533.7139892578125\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 1404.08935546875\n",
      "                              Voc = 1437.5411376953125\n",
      "                              Jsc = 2183.999755859375\n",
      "                              FF = 1533.3603515625\n",
      "Finished epoch  474\n",
      "On epoch  475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.9267578125\n",
      "                              Voc = 1438.239013671875\n",
      "                              Jsc = 2196.712158203125\n",
      "                              FF = 1533.1715087890625\n",
      "Finished epoch  475\n",
      "On epoch  476\n",
      "Total Epoch Testing MAPE: PCE = 1405.770263671875\n",
      "                              Voc = 1438.7457275390625\n",
      "                              Jsc = 2202.570068359375\n",
      "                              FF = 1534.149169921875\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 1404.789306640625\n",
      "                              Voc = 1438.115478515625\n",
      "                              Jsc = 2198.7919921875\n",
      "                              FF = 1534.96728515625\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 1407.2021484375\n",
      "                              Voc = 1438.1307373046875\n",
      "                              Jsc = 2208.240966796875\n",
      "                              FF = 1534.4315185546875\n",
      "Finished epoch  478\n",
      "On epoch  479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.7542724609375\n",
      "                              Voc = 1437.9583740234375\n",
      "                              Jsc = 2208.8251953125\n",
      "                              FF = 1533.6754150390625\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 1407.0750732421875\n",
      "                              Voc = 1438.908935546875\n",
      "                              Jsc = 2210.528564453125\n",
      "                              FF = 1533.780517578125\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 1407.4696044921875\n",
      "                              Voc = 1439.477783203125\n",
      "                              Jsc = 2214.35693359375\n",
      "                              FF = 1533.7099609375\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 1408.395263671875\n",
      "                              Voc = 1439.7989501953125\n",
      "                              Jsc = 2213.810302734375\n",
      "                              FF = 1531.6448974609375\n",
      "Finished epoch  482\n",
      "On epoch  483\n",
      "Total Epoch Testing MAPE: PCE = 1407.93408203125\n",
      "                              Voc = 1438.923583984375\n",
      "                              Jsc = 2211.240478515625\n",
      "                              FF = 1532.6180419921875\n",
      "Finished epoch  483\n",
      "On epoch  484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.8116455078125\n",
      "                              Voc = 1438.7808837890625\n",
      "                              Jsc = 2217.261474609375\n",
      "                              FF = 1532.1790771484375\n",
      "Finished epoch  484\n",
      "On epoch  485\n",
      "Total Epoch Testing MAPE: PCE = 1407.1121826171875\n",
      "                              Voc = 1439.2540283203125\n",
      "                              Jsc = 2213.905517578125\n",
      "                              FF = 1531.3653564453125\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 1407.6104736328125\n",
      "                              Voc = 1438.8714599609375\n",
      "                              Jsc = 2209.8115234375\n",
      "                              FF = 1530.3455810546875\n",
      "Finished epoch  486\n",
      "On epoch  487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1406.6861572265625\n",
      "                              Voc = 1437.378173828125\n",
      "                              Jsc = 2210.805908203125\n",
      "                              FF = 1531.771240234375\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 1407.1199951171875\n",
      "                              Voc = 1436.3875732421875\n",
      "                              Jsc = 2211.607177734375\n",
      "                              FF = 1529.83740234375\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 1405.5582275390625\n",
      "                              Voc = 1437.5865478515625\n",
      "                              Jsc = 2219.829833984375\n",
      "                              FF = 1529.4345703125\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 1402.0775146484375\n",
      "                              Voc = 1438.0262451171875\n",
      "                              Jsc = 2221.37255859375\n",
      "                              FF = 1529.8111572265625\n",
      "Finished epoch  490\n",
      "On epoch  491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1402.969970703125\n",
      "                              Voc = 1437.7099609375\n",
      "                              Jsc = 2218.65625\n",
      "                              FF = 1530.46826171875\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 1403.951416015625\n",
      "                              Voc = 1437.689697265625\n",
      "                              Jsc = 2215.72314453125\n",
      "                              FF = 1529.3048095703125\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 1402.6494140625\n",
      "                              Voc = 1437.353515625\n",
      "                              Jsc = 2209.710205078125\n",
      "                              FF = 1529.95361328125\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 1402.224609375\n",
      "                              Voc = 1438.7830810546875\n",
      "                              Jsc = 2209.211669921875\n",
      "                              FF = 1530.5963134765625\n",
      "Finished epoch  494\n",
      "On epoch  495\n",
      "Total Epoch Testing MAPE: PCE = 1404.3255615234375\n",
      "                              Voc = 1439.56201171875\n",
      "                              Jsc = 2207.749755859375\n",
      "                              FF = 1530.5621337890625\n",
      "Finished epoch  495\n",
      "On epoch  496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 1405.3001708984375\n",
      "                              Voc = 1439.575439453125\n",
      "                              Jsc = 2209.65869140625\n",
      "                              FF = 1529.9532470703125\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 1405.7701416015625\n",
      "                              Voc = 1438.4874267578125\n",
      "                              Jsc = 2208.82958984375\n",
      "                              FF = 1529.9034423828125\n",
      "Finished epoch  497\n",
      "On epoch  498\n",
      "Total Epoch Testing MAPE: PCE = 1401.855712890625\n",
      "                              Voc = 1438.9722900390625\n",
      "                              Jsc = 2206.35693359375\n",
      "                              FF = 1529.09033203125\n",
      "Finished epoch  498\n",
      "On epoch  499\n",
      "Total Epoch Testing MAPE: PCE = 1401.35205078125\n",
      "                              Voc = 1440.5870361328125\n",
      "                              Jsc = 2203.47607421875\n",
      "                              FF = 1528.555908203125\n",
      "Finished epoch  499\n",
      "Fold # 1\n",
      "-----------------------------\n",
      "On epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([292])) that is different to the input size (torch.Size([292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 100.0\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 100.0\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 100.0\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 99.81840515136719\n",
      "Finished epoch  2\n",
      "On epoch  3\n",
      "Total Epoch Testing MAPE: PCE = 100.0\n",
      "                              Voc = 99.10810852050781\n",
      "                              Jsc = 100.0\n",
      "                              FF = 99.38652801513672\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 100.0\n",
      "                              Voc = 95.22362518310547\n",
      "                              Jsc = 100.0\n",
      "                              FF = 98.95853424072266\n",
      "Finished epoch  4\n",
      "On epoch  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 100.0\n",
      "                              Voc = 81.32511138916016\n",
      "                              Jsc = 100.0\n",
      "                              FF = 97.71183013916016\n",
      "Finished epoch  5\n",
      "On epoch  6\n",
      "Total Epoch Testing MAPE: PCE = 100.0\n",
      "                              Voc = 61.42805862426758\n",
      "                              Jsc = 100.0\n",
      "                              FF = 96.12283325195312\n",
      "Finished epoch  6\n",
      "On epoch  7\n",
      "Total Epoch Testing MAPE: PCE = 99.593017578125\n",
      "                              Voc = 43.443809509277344\n",
      "                              Jsc = 99.9741439819336\n",
      "                              FF = 95.00473022460938\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 98.80357360839844\n",
      "                              Voc = 29.833295822143555\n",
      "                              Jsc = 99.93478393554688\n",
      "                              FF = 93.84471893310547\n",
      "Finished epoch  8\n",
      "On epoch  9\n",
      "Total Epoch Testing MAPE: PCE = 98.04512786865234\n",
      "                              Voc = 19.211816787719727\n",
      "                              Jsc = 99.8962631225586\n",
      "                              FF = 90.55099487304688\n",
      "Finished epoch  9\n",
      "On epoch  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 96.63422393798828\n",
      "                              Voc = 14.962586402893066\n",
      "                              Jsc = 99.79946899414062\n",
      "                              FF = 86.3533935546875\n",
      "Finished epoch  10\n",
      "On epoch  11\n",
      "Total Epoch Testing MAPE: PCE = 95.47875213623047\n",
      "                              Voc = 14.258062362670898\n",
      "                              Jsc = 99.58987426757812\n",
      "                              FF = 82.91101837158203\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 93.66071319580078\n",
      "                              Voc = 15.484380722045898\n",
      "                              Jsc = 97.96209716796875\n",
      "                              FF = 80.64086151123047\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 91.04043579101562\n",
      "                              Voc = 18.092464447021484\n",
      "                              Jsc = 97.1470947265625\n",
      "                              FF = 78.17166900634766\n",
      "Finished epoch  13\n",
      "On epoch  14\n",
      "Total Epoch Testing MAPE: PCE = 86.67298126220703\n",
      "                              Voc = 22.572877883911133\n",
      "                              Jsc = 97.23344421386719\n",
      "                              FF = 75.97039031982422\n",
      "Finished epoch  14\n",
      "On epoch  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 82.05436706542969\n",
      "                              Voc = 22.714529037475586\n",
      "                              Jsc = 96.3702392578125\n",
      "                              FF = 74.62641906738281\n",
      "Finished epoch  15\n",
      "On epoch  16\n",
      "Total Epoch Testing MAPE: PCE = 77.8738784790039\n",
      "                              Voc = 23.353717803955078\n",
      "                              Jsc = 95.23208618164062\n",
      "                              FF = 74.01013946533203\n",
      "Finished epoch  16\n",
      "On epoch  17\n",
      "Total Epoch Testing MAPE: PCE = 73.97730255126953\n",
      "                              Voc = 25.26613998413086\n",
      "                              Jsc = 94.21966552734375\n",
      "                              FF = 74.1618881225586\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 65.13360595703125\n",
      "                              Voc = 26.74277687072754\n",
      "                              Jsc = 93.6417007446289\n",
      "                              FF = 72.20088195800781\n",
      "Finished epoch  18\n",
      "On epoch  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.76347732543945\n",
      "                              Voc = 25.836933135986328\n",
      "                              Jsc = 92.02835083007812\n",
      "                              FF = 71.76177215576172\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 58.36055374145508\n",
      "                              Voc = 27.042085647583008\n",
      "                              Jsc = 91.09517669677734\n",
      "                              FF = 70.72808074951172\n",
      "Finished epoch  20\n",
      "On epoch  21\n",
      "Total Epoch Testing MAPE: PCE = 55.59914779663086\n",
      "                              Voc = 26.387939453125\n",
      "                              Jsc = 89.8126220703125\n",
      "                              FF = 69.52877044677734\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 51.67924880981445\n",
      "                              Voc = 23.34821891784668\n",
      "                              Jsc = 90.05935668945312\n",
      "                              FF = 68.56534576416016\n",
      "Finished epoch  22\n",
      "On epoch  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.99936294555664\n",
      "                              Voc = 20.62447738647461\n",
      "                              Jsc = 89.72344207763672\n",
      "                              FF = 68.02226257324219\n",
      "Finished epoch  23\n",
      "On epoch  24\n",
      "Total Epoch Testing MAPE: PCE = 47.57164001464844\n",
      "                              Voc = 17.740510940551758\n",
      "                              Jsc = 88.28407287597656\n",
      "                              FF = 68.4730224609375\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 44.9553337097168\n",
      "                              Voc = 15.086639404296875\n",
      "                              Jsc = 87.95135498046875\n",
      "                              FF = 68.03438568115234\n",
      "Finished epoch  25\n",
      "On epoch  26\n",
      "Total Epoch Testing MAPE: PCE = 42.3228759765625\n",
      "                              Voc = 13.434404373168945\n",
      "                              Jsc = 88.4302978515625\n",
      "                              FF = 68.41891479492188\n",
      "Finished epoch  26\n",
      "On epoch  27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 40.68994140625\n",
      "                              Voc = 11.513252258300781\n",
      "                              Jsc = 88.26992797851562\n",
      "                              FF = 68.54104614257812\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 42.17164993286133\n",
      "                              Voc = 9.49150562286377\n",
      "                              Jsc = 88.87136840820312\n",
      "                              FF = 68.91902923583984\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 41.47787857055664\n",
      "                              Voc = 7.062843322753906\n",
      "                              Jsc = 89.0590591430664\n",
      "                              FF = 68.35039520263672\n",
      "Finished epoch  29\n",
      "On epoch  30\n",
      "Total Epoch Testing MAPE: PCE = 41.0718994140625\n",
      "                              Voc = 6.871313571929932\n",
      "                              Jsc = 87.42671966552734\n",
      "                              FF = 68.38601684570312\n",
      "Finished epoch  30\n",
      "On epoch  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 43.005672454833984\n",
      "                              Voc = 6.571547508239746\n",
      "                              Jsc = 86.44449615478516\n",
      "                              FF = 67.91425323486328\n",
      "Finished epoch  31\n",
      "On epoch  32\n",
      "Total Epoch Testing MAPE: PCE = 43.04051208496094\n",
      "                              Voc = 6.140998363494873\n",
      "                              Jsc = 86.09627532958984\n",
      "                              FF = 67.787841796875\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 41.174171447753906\n",
      "                              Voc = 5.548038482666016\n",
      "                              Jsc = 85.39836883544922\n",
      "                              FF = 67.23754119873047\n",
      "Finished epoch  33\n",
      "On epoch  34\n",
      "Total Epoch Testing MAPE: PCE = 40.33378601074219\n",
      "                              Voc = 5.463534832000732\n",
      "                              Jsc = 84.08512115478516\n",
      "                              FF = 67.56492614746094\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 39.18430709838867\n",
      "                              Voc = 5.021236419677734\n",
      "                              Jsc = 81.74076080322266\n",
      "                              FF = 67.38005065917969\n",
      "Finished epoch  35\n",
      "On epoch  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 41.38411331176758\n",
      "                              Voc = 4.992785453796387\n",
      "                              Jsc = 81.6522445678711\n",
      "                              FF = 67.18047332763672\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 41.2113037109375\n",
      "                              Voc = 5.088417053222656\n",
      "                              Jsc = 82.11217498779297\n",
      "                              FF = 67.19622802734375\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 41.03074645996094\n",
      "                              Voc = 5.456381797790527\n",
      "                              Jsc = 82.78203582763672\n",
      "                              FF = 66.84201049804688\n",
      "Finished epoch  38\n",
      "On epoch  39\n",
      "Total Epoch Testing MAPE: PCE = 39.606048583984375\n",
      "                              Voc = 5.957935333251953\n",
      "                              Jsc = 82.59718322753906\n",
      "                              FF = 66.89701080322266\n",
      "Finished epoch  39\n",
      "On epoch  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.42104721069336\n",
      "                              Voc = 6.362461090087891\n",
      "                              Jsc = 80.77568054199219\n",
      "                              FF = 66.98104858398438\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 40.846622467041016\n",
      "                              Voc = 6.9024434089660645\n",
      "                              Jsc = 79.36146545410156\n",
      "                              FF = 67.45883178710938\n",
      "Finished epoch  41\n",
      "On epoch  42\n",
      "Total Epoch Testing MAPE: PCE = 43.39634323120117\n",
      "                              Voc = 7.359724521636963\n",
      "                              Jsc = 79.2438735961914\n",
      "                              FF = 67.78994750976562\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 45.63332748413086\n",
      "                              Voc = 7.238026142120361\n",
      "                              Jsc = 78.68064880371094\n",
      "                              FF = 67.42375183105469\n",
      "Finished epoch  43\n",
      "On epoch  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 46.38202667236328\n",
      "                              Voc = 6.999794960021973\n",
      "                              Jsc = 78.02727508544922\n",
      "                              FF = 67.6196060180664\n",
      "Finished epoch  44\n",
      "On epoch  45\n",
      "Total Epoch Testing MAPE: PCE = 47.614566802978516\n",
      "                              Voc = 6.772452354431152\n",
      "                              Jsc = 79.27345275878906\n",
      "                              FF = 67.07184600830078\n",
      "Finished epoch  45\n",
      "On epoch  46\n",
      "Total Epoch Testing MAPE: PCE = 51.131927490234375\n",
      "                              Voc = 6.414223670959473\n",
      "                              Jsc = 79.45844268798828\n",
      "                              FF = 67.38349914550781\n",
      "Finished epoch  46\n",
      "On epoch  47\n",
      "Total Epoch Testing MAPE: PCE = 51.18329620361328\n",
      "                              Voc = 6.60486364364624\n",
      "                              Jsc = 79.14100646972656\n",
      "                              FF = 68.94445037841797\n",
      "Finished epoch  47\n",
      "On epoch  48\n",
      "Total Epoch Testing MAPE: PCE = 50.254608154296875\n",
      "                              Voc = 6.309930801391602\n",
      "                              Jsc = 79.02291870117188\n",
      "                              FF = 69.1351318359375\n",
      "Finished epoch  48\n",
      "On epoch  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.641441345214844\n",
      "                              Voc = 6.643216133117676\n",
      "                              Jsc = 79.17530059814453\n",
      "                              FF = 70.16603088378906\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 54.719688415527344\n",
      "                              Voc = 6.743905544281006\n",
      "                              Jsc = 78.0190658569336\n",
      "                              FF = 71.26272583007812\n",
      "Finished epoch  50\n",
      "On epoch  51\n",
      "Total Epoch Testing MAPE: PCE = 57.87140655517578\n",
      "                              Voc = 6.696926116943359\n",
      "                              Jsc = 75.68402099609375\n",
      "                              FF = 72.2385025024414\n",
      "Finished epoch  51\n",
      "On epoch  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.5751838684082\n",
      "                              Voc = 6.1360859870910645\n",
      "                              Jsc = 75.6257553100586\n",
      "                              FF = 73.10916900634766\n",
      "Finished epoch  52\n",
      "On epoch  53\n",
      "Total Epoch Testing MAPE: PCE = 60.67554473876953\n",
      "                              Voc = 5.957366943359375\n",
      "                              Jsc = 74.92019653320312\n",
      "                              FF = 74.17221069335938\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 60.51813888549805\n",
      "                              Voc = 5.668439865112305\n",
      "                              Jsc = 74.91127014160156\n",
      "                              FF = 74.5993881225586\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 60.84745407104492\n",
      "                              Voc = 5.648148059844971\n",
      "                              Jsc = 74.27607727050781\n",
      "                              FF = 74.99982452392578\n",
      "Finished epoch  55\n",
      "On epoch  56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.23851013183594\n",
      "                              Voc = 5.50006628036499\n",
      "                              Jsc = 73.13035583496094\n",
      "                              FF = 74.8647232055664\n",
      "Finished epoch  56\n",
      "On epoch  57\n",
      "Total Epoch Testing MAPE: PCE = 61.815547943115234\n",
      "                              Voc = 5.4111809730529785\n",
      "                              Jsc = 70.87873840332031\n",
      "                              FF = 76.01038360595703\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 63.23725509643555\n",
      "                              Voc = 5.710599422454834\n",
      "                              Jsc = 70.51538848876953\n",
      "                              FF = 75.65483093261719\n",
      "Finished epoch  58\n",
      "On epoch  59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 64.38526153564453\n",
      "                              Voc = 5.757628917694092\n",
      "                              Jsc = 68.7463607788086\n",
      "                              FF = 75.21452331542969\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 63.962310791015625\n",
      "                              Voc = 5.621723651885986\n",
      "                              Jsc = 68.96955108642578\n",
      "                              FF = 75.63687133789062\n",
      "Finished epoch  60\n",
      "On epoch  61\n",
      "Total Epoch Testing MAPE: PCE = 63.004432678222656\n",
      "                              Voc = 5.207146167755127\n",
      "                              Jsc = 69.91325378417969\n",
      "                              FF = 75.71725463867188\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 61.80577850341797\n",
      "                              Voc = 5.3386311531066895\n",
      "                              Jsc = 69.6548080444336\n",
      "                              FF = 77.01935577392578\n",
      "Finished epoch  62\n",
      "On epoch  63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 64.2953872680664\n",
      "                              Voc = 5.165896415710449\n",
      "                              Jsc = 70.64642333984375\n",
      "                              FF = 77.45724487304688\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 63.5999870300293\n",
      "                              Voc = 5.285913467407227\n",
      "                              Jsc = 69.13227081298828\n",
      "                              FF = 77.10921478271484\n",
      "Finished epoch  64\n",
      "On epoch  65\n",
      "Total Epoch Testing MAPE: PCE = 62.21993637084961\n",
      "                              Voc = 5.133314609527588\n",
      "                              Jsc = 68.0438232421875\n",
      "                              FF = 76.60060119628906\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 63.436073303222656\n",
      "                              Voc = 5.141606330871582\n",
      "                              Jsc = 66.81482696533203\n",
      "                              FF = 76.1234359741211\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 63.37478256225586\n",
      "                              Voc = 5.046926021575928\n",
      "                              Jsc = 66.76913452148438\n",
      "                              FF = 76.21526336669922\n",
      "Finished epoch  67\n",
      "On epoch  68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 61.54741668701172\n",
      "                              Voc = 5.109879493713379\n",
      "                              Jsc = 67.28541564941406\n",
      "                              FF = 76.65901184082031\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 60.55465316772461\n",
      "                              Voc = 5.25911808013916\n",
      "                              Jsc = 66.44437408447266\n",
      "                              FF = 76.87098693847656\n",
      "Finished epoch  69\n",
      "On epoch  70\n",
      "Total Epoch Testing MAPE: PCE = 58.60254669189453\n",
      "                              Voc = 5.465014457702637\n",
      "                              Jsc = 66.25971984863281\n",
      "                              FF = 76.84831237792969\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 59.48657989501953\n",
      "                              Voc = 5.814783096313477\n",
      "                              Jsc = 64.73355102539062\n",
      "                              FF = 77.47319030761719\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 57.976158142089844\n",
      "                              Voc = 6.0042805671691895\n",
      "                              Jsc = 64.2333755493164\n",
      "                              FF = 76.72947692871094\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 56.00808334350586\n",
      "                              Voc = 5.8988118171691895\n",
      "                              Jsc = 64.26142883300781\n",
      "                              FF = 75.81800842285156\n",
      "Finished epoch  73\n",
      "On epoch  74\n",
      "Total Epoch Testing MAPE: PCE = 55.76586151123047\n",
      "                              Voc = 5.645599365234375\n",
      "                              Jsc = 64.99299621582031\n",
      "                              FF = 76.1032943725586\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 57.6241569519043\n",
      "                              Voc = 5.74378776550293\n",
      "                              Jsc = 64.09333038330078\n",
      "                              FF = 75.52458953857422\n",
      "Finished epoch  75\n",
      "On epoch  76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 59.349910736083984\n",
      "                              Voc = 5.666783809661865\n",
      "                              Jsc = 62.53341293334961\n",
      "                              FF = 75.4088134765625\n",
      "Finished epoch  76\n",
      "On epoch  77\n",
      "Total Epoch Testing MAPE: PCE = 61.293540954589844\n",
      "                              Voc = 6.082457542419434\n",
      "                              Jsc = 62.808048248291016\n",
      "                              FF = 74.73975372314453\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 60.57029342651367\n",
      "                              Voc = 5.928184986114502\n",
      "                              Jsc = 63.48553466796875\n",
      "                              FF = 74.27246856689453\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 60.075157165527344\n",
      "                              Voc = 6.004772186279297\n",
      "                              Jsc = 63.362548828125\n",
      "                              FF = 74.27206420898438\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 60.12748336791992\n",
      "                              Voc = 5.928916931152344\n",
      "                              Jsc = 62.36898422241211\n",
      "                              FF = 73.55903625488281\n",
      "Finished epoch  80\n",
      "On epoch  81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.89006423950195\n",
      "                              Voc = 6.250359058380127\n",
      "                              Jsc = 62.059104919433594\n",
      "                              FF = 73.88601684570312\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 55.992828369140625\n",
      "                              Voc = 6.463336944580078\n",
      "                              Jsc = 61.60710525512695\n",
      "                              FF = 74.20860290527344\n",
      "Finished epoch  82\n",
      "On epoch  83\n",
      "Total Epoch Testing MAPE: PCE = 53.669822692871094\n",
      "                              Voc = 6.429443359375\n",
      "                              Jsc = 60.906673431396484\n",
      "                              FF = 74.2602767944336\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 54.15654754638672\n",
      "                              Voc = 6.54774284362793\n",
      "                              Jsc = 58.82341766357422\n",
      "                              FF = 73.65289306640625\n",
      "Finished epoch  84\n",
      "On epoch  85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.67410659790039\n",
      "                              Voc = 6.739950180053711\n",
      "                              Jsc = 57.184452056884766\n",
      "                              FF = 73.61217498779297\n",
      "Finished epoch  85\n",
      "On epoch  86\n",
      "Total Epoch Testing MAPE: PCE = 53.37603759765625\n",
      "                              Voc = 6.922729969024658\n",
      "                              Jsc = 55.8418083190918\n",
      "                              FF = 72.86996459960938\n",
      "Finished epoch  86\n",
      "On epoch  87\n",
      "Total Epoch Testing MAPE: PCE = 53.02895736694336\n",
      "                              Voc = 6.992962837219238\n",
      "                              Jsc = 55.39377212524414\n",
      "                              FF = 73.31221771240234\n",
      "Finished epoch  87\n",
      "On epoch  88\n",
      "Total Epoch Testing MAPE: PCE = 54.95492172241211\n",
      "                              Voc = 6.643479347229004\n",
      "                              Jsc = 53.80899429321289\n",
      "                              FF = 73.82737731933594\n",
      "Finished epoch  88\n",
      "On epoch  89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.7708740234375\n",
      "                              Voc = 6.803363800048828\n",
      "                              Jsc = 52.44052505493164\n",
      "                              FF = 73.64987182617188\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 55.36239242553711\n",
      "                              Voc = 6.886044979095459\n",
      "                              Jsc = 51.32943344116211\n",
      "                              FF = 73.07312774658203\n",
      "Finished epoch  90\n",
      "On epoch  91\n",
      "Total Epoch Testing MAPE: PCE = 52.96999740600586\n",
      "                              Voc = 6.722312927246094\n",
      "                              Jsc = 51.53916549682617\n",
      "                              FF = 73.71675872802734\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 54.14948272705078\n",
      "                              Voc = 6.552661418914795\n",
      "                              Jsc = 50.49482727050781\n",
      "                              FF = 74.40336608886719\n",
      "Finished epoch  92\n",
      "On epoch  93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.98243713378906\n",
      "                              Voc = 6.729937553405762\n",
      "                              Jsc = 50.22986602783203\n",
      "                              FF = 73.85810852050781\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 52.50421905517578\n",
      "                              Voc = 6.681100845336914\n",
      "                              Jsc = 51.046539306640625\n",
      "                              FF = 74.26449584960938\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 49.70573043823242\n",
      "                              Voc = 6.556455135345459\n",
      "                              Jsc = 49.45317077636719\n",
      "                              FF = 74.95683288574219\n",
      "Finished epoch  95\n",
      "On epoch  96\n",
      "Total Epoch Testing MAPE: PCE = 46.52045822143555\n",
      "                              Voc = 6.295512676239014\n",
      "                              Jsc = 48.00135040283203\n",
      "                              FF = 74.88201904296875\n",
      "Finished epoch  96\n",
      "On epoch  97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 46.0628776550293\n",
      "                              Voc = 5.964085102081299\n",
      "                              Jsc = 47.436378479003906\n",
      "                              FF = 75.15571594238281\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 46.8388557434082\n",
      "                              Voc = 5.796631813049316\n",
      "                              Jsc = 47.67023468017578\n",
      "                              FF = 74.73432159423828\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 45.00222396850586\n",
      "                              Voc = 5.685065746307373\n",
      "                              Jsc = 46.690311431884766\n",
      "                              FF = 74.49817657470703\n",
      "Finished epoch  99\n",
      "On epoch  100\n",
      "Total Epoch Testing MAPE: PCE = 42.91063690185547\n",
      "                              Voc = 5.520218849182129\n",
      "                              Jsc = 46.1884651184082\n",
      "                              FF = 74.2578125\n",
      "Finished epoch  100\n",
      "On epoch  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 43.27516174316406\n",
      "                              Voc = 5.454285144805908\n",
      "                              Jsc = 45.055145263671875\n",
      "                              FF = 73.43018341064453\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 46.290977478027344\n",
      "                              Voc = 5.469971656799316\n",
      "                              Jsc = 44.24415588378906\n",
      "                              FF = 73.91236877441406\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 49.22450637817383\n",
      "                              Voc = 5.526272773742676\n",
      "                              Jsc = 42.985748291015625\n",
      "                              FF = 74.20369720458984\n",
      "Finished epoch  103\n",
      "On epoch  104\n",
      "Total Epoch Testing MAPE: PCE = 49.40953826904297\n",
      "                              Voc = 5.495024681091309\n",
      "                              Jsc = 42.01170349121094\n",
      "                              FF = 74.1310806274414\n",
      "Finished epoch  104\n",
      "On epoch  105\n",
      "Total Epoch Testing MAPE: PCE = 52.14385986328125\n",
      "                              Voc = 5.481142044067383\n",
      "                              Jsc = 41.97103500366211\n",
      "                              FF = 74.66048431396484\n",
      "Finished epoch  105\n",
      "On epoch  106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.04958724975586\n",
      "                              Voc = 5.482917308807373\n",
      "                              Jsc = 43.1747932434082\n",
      "                              FF = 74.31658172607422\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 54.12794876098633\n",
      "                              Voc = 5.535180568695068\n",
      "                              Jsc = 43.11577606201172\n",
      "                              FF = 74.12664031982422\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 54.16387939453125\n",
      "                              Voc = 5.393547058105469\n",
      "                              Jsc = 42.576744079589844\n",
      "                              FF = 74.4426040649414\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 54.70452880859375\n",
      "                              Voc = 5.336018085479736\n",
      "                              Jsc = 42.19673538208008\n",
      "                              FF = 74.97772216796875\n",
      "Finished epoch  109\n",
      "On epoch  110\n",
      "Total Epoch Testing MAPE: PCE = 52.682579040527344\n",
      "                              Voc = 5.24510383605957\n",
      "                              Jsc = 41.63806915283203\n",
      "                              FF = 75.49758911132812\n",
      "Finished epoch  110\n",
      "On epoch  111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.4222297668457\n",
      "                              Voc = 5.229123592376709\n",
      "                              Jsc = 40.36664962768555\n",
      "                              FF = 75.62919616699219\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 55.16367721557617\n",
      "                              Voc = 5.112959861755371\n",
      "                              Jsc = 39.84696960449219\n",
      "                              FF = 75.77828216552734\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 53.58473587036133\n",
      "                              Voc = 5.108956813812256\n",
      "                              Jsc = 39.28057861328125\n",
      "                              FF = 75.36074829101562\n",
      "Finished epoch  113\n",
      "On epoch  114\n",
      "Total Epoch Testing MAPE: PCE = 54.6912841796875\n",
      "                              Voc = 5.268588542938232\n",
      "                              Jsc = 37.62853240966797\n",
      "                              FF = 75.58412170410156\n",
      "Finished epoch  114\n",
      "On epoch  115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.124977111816406\n",
      "                              Voc = 5.354217052459717\n",
      "                              Jsc = 37.321807861328125\n",
      "                              FF = 75.70938110351562\n",
      "Finished epoch  115\n",
      "On epoch  116\n",
      "Total Epoch Testing MAPE: PCE = 54.61698913574219\n",
      "                              Voc = 5.291571140289307\n",
      "                              Jsc = 37.35214614868164\n",
      "                              FF = 75.61532592773438\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 53.948341369628906\n",
      "                              Voc = 5.147136211395264\n",
      "                              Jsc = 37.73102569580078\n",
      "                              FF = 76.22679138183594\n",
      "Finished epoch  117\n",
      "On epoch  118\n",
      "Total Epoch Testing MAPE: PCE = 55.597991943359375\n",
      "                              Voc = 5.127538204193115\n",
      "                              Jsc = 37.107574462890625\n",
      "                              FF = 76.14665985107422\n",
      "Finished epoch  118\n",
      "On epoch  119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.99189376831055\n",
      "                              Voc = 5.135881423950195\n",
      "                              Jsc = 36.8482551574707\n",
      "                              FF = 75.92774200439453\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 55.479740142822266\n",
      "                              Voc = 4.944460868835449\n",
      "                              Jsc = 36.26747512817383\n",
      "                              FF = 75.62715911865234\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 56.27850341796875\n",
      "                              Voc = 4.83804178237915\n",
      "                              Jsc = 36.734771728515625\n",
      "                              FF = 75.04597473144531\n",
      "Finished epoch  121\n",
      "On epoch  122\n",
      "Total Epoch Testing MAPE: PCE = 54.747535705566406\n",
      "                              Voc = 4.822089195251465\n",
      "                              Jsc = 36.6840934753418\n",
      "                              FF = 75.52315521240234\n",
      "Finished epoch  122\n",
      "On epoch  123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.110530853271484\n",
      "                              Voc = 4.7814040184021\n",
      "                              Jsc = 35.687782287597656\n",
      "                              FF = 74.84921264648438\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 54.49726104736328\n",
      "                              Voc = 4.805774688720703\n",
      "                              Jsc = 35.739994049072266\n",
      "                              FF = 75.06233215332031\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 53.03515625\n",
      "                              Voc = 4.799842834472656\n",
      "                              Jsc = 36.31208038330078\n",
      "                              FF = 74.38118743896484\n",
      "Finished epoch  125\n",
      "On epoch  126\n",
      "Total Epoch Testing MAPE: PCE = 52.55643081665039\n",
      "                              Voc = 4.999750137329102\n",
      "                              Jsc = 35.625099182128906\n",
      "                              FF = 73.85442352294922\n",
      "Finished epoch  126\n",
      "On epoch  127\n",
      "Total Epoch Testing MAPE: PCE = 51.19166564941406\n",
      "                              Voc = 5.410690784454346\n",
      "                              Jsc = 35.35767364501953\n",
      "                              FF = 72.88348388671875\n",
      "Finished epoch  127\n",
      "On epoch  128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.474857330322266\n",
      "                              Voc = 5.146890640258789\n",
      "                              Jsc = 35.02641296386719\n",
      "                              FF = 72.06539154052734\n",
      "Finished epoch  128\n",
      "On epoch  129\n",
      "Total Epoch Testing MAPE: PCE = 51.924156188964844\n",
      "                              Voc = 4.8645853996276855\n",
      "                              Jsc = 34.82210922241211\n",
      "                              FF = 71.19515228271484\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 50.771278381347656\n",
      "                              Voc = 5.0823974609375\n",
      "                              Jsc = 34.73525619506836\n",
      "                              FF = 70.43769073486328\n",
      "Finished epoch  130\n",
      "On epoch  131\n",
      "Total Epoch Testing MAPE: PCE = 50.619388580322266\n",
      "                              Voc = 4.8475022315979\n",
      "                              Jsc = 33.922306060791016\n",
      "                              FF = 69.82333374023438\n",
      "Finished epoch  131\n",
      "On epoch  132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.29880905151367\n",
      "                              Voc = 4.859438419342041\n",
      "                              Jsc = 33.576114654541016\n",
      "                              FF = 69.00263977050781\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 49.7965202331543\n",
      "                              Voc = 4.725356578826904\n",
      "                              Jsc = 33.26980209350586\n",
      "                              FF = 67.63440704345703\n",
      "Finished epoch  133\n",
      "On epoch  134\n",
      "Total Epoch Testing MAPE: PCE = 46.928504943847656\n",
      "                              Voc = 4.743608474731445\n",
      "                              Jsc = 32.64867401123047\n",
      "                              FF = 67.92304992675781\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 48.621334075927734\n",
      "                              Voc = 4.708400249481201\n",
      "                              Jsc = 32.16032409667969\n",
      "                              FF = 67.60813903808594\n",
      "Finished epoch  135\n",
      "On epoch  136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.7347526550293\n",
      "                              Voc = 4.721141338348389\n",
      "                              Jsc = 30.989765167236328\n",
      "                              FF = 67.18141174316406\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 49.066383361816406\n",
      "                              Voc = 4.753009796142578\n",
      "                              Jsc = 30.63249969482422\n",
      "                              FF = 67.27527618408203\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 48.912784576416016\n",
      "                              Voc = 4.663568496704102\n",
      "                              Jsc = 30.72086524963379\n",
      "                              FF = 67.50507354736328\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 47.48601150512695\n",
      "                              Voc = 4.768433570861816\n",
      "                              Jsc = 30.417240142822266\n",
      "                              FF = 65.87335968017578\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 48.60540008544922\n",
      "                              Voc = 4.8441691398620605\n",
      "                              Jsc = 29.988929748535156\n",
      "                              FF = 65.68096160888672\n",
      "Finished epoch  140\n",
      "On epoch  141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 47.097007751464844\n",
      "                              Voc = 4.813908576965332\n",
      "                              Jsc = 29.416297912597656\n",
      "                              FF = 65.38302612304688\n",
      "Finished epoch  141\n",
      "On epoch  142\n",
      "Total Epoch Testing MAPE: PCE = 44.25255584716797\n",
      "                              Voc = 4.7156291007995605\n",
      "                              Jsc = 29.41865348815918\n",
      "                              FF = 65.38753509521484\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 45.49492645263672\n",
      "                              Voc = 4.687937259674072\n",
      "                              Jsc = 29.414363861083984\n",
      "                              FF = 65.5765380859375\n",
      "Finished epoch  143\n",
      "On epoch  144\n",
      "Total Epoch Testing MAPE: PCE = 45.41978073120117\n",
      "                              Voc = 4.548623561859131\n",
      "                              Jsc = 28.84431266784668\n",
      "                              FF = 65.49502563476562\n",
      "Finished epoch  144\n",
      "On epoch  145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 45.310142517089844\n",
      "                              Voc = 4.500339508056641\n",
      "                              Jsc = 28.545745849609375\n",
      "                              FF = 65.524658203125\n",
      "Finished epoch  145\n",
      "On epoch  146\n",
      "Total Epoch Testing MAPE: PCE = 46.03028869628906\n",
      "                              Voc = 4.526695251464844\n",
      "                              Jsc = 28.247922897338867\n",
      "                              FF = 64.75212860107422\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 46.804012298583984\n",
      "                              Voc = 4.456513404846191\n",
      "                              Jsc = 28.15970230102539\n",
      "                              FF = 64.41202545166016\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 48.55858612060547\n",
      "                              Voc = 4.6774373054504395\n",
      "                              Jsc = 27.584985733032227\n",
      "                              FF = 64.40526580810547\n",
      "Finished epoch  148\n",
      "On epoch  149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.92781448364258\n",
      "                              Voc = 4.495483875274658\n",
      "                              Jsc = 27.05105209350586\n",
      "                              FF = 64.54979705810547\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 49.90728759765625\n",
      "                              Voc = 4.570225238800049\n",
      "                              Jsc = 26.64935874938965\n",
      "                              FF = 64.2106704711914\n",
      "Finished epoch  150\n",
      "On epoch  151\n",
      "Total Epoch Testing MAPE: PCE = 50.511985778808594\n",
      "                              Voc = 4.569659233093262\n",
      "                              Jsc = 26.771196365356445\n",
      "                              FF = 64.84090423583984\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 49.96845626831055\n",
      "                              Voc = 4.717175006866455\n",
      "                              Jsc = 26.664379119873047\n",
      "                              FF = 67.33333587646484\n",
      "Finished epoch  152\n",
      "On epoch  153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.823150634765625\n",
      "                              Voc = 4.889928817749023\n",
      "                              Jsc = 25.598764419555664\n",
      "                              FF = 69.03495025634766\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 48.51397705078125\n",
      "                              Voc = 4.91403341293335\n",
      "                              Jsc = 25.26915740966797\n",
      "                              FF = 70.1015625\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 48.64775848388672\n",
      "                              Voc = 5.082263469696045\n",
      "                              Jsc = 25.242036819458008\n",
      "                              FF = 71.02891540527344\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 47.00233459472656\n",
      "                              Voc = 5.016746997833252\n",
      "                              Jsc = 24.540115356445312\n",
      "                              FF = 70.4996109008789\n",
      "Finished epoch  156\n",
      "On epoch  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 45.89714050292969\n",
      "                              Voc = 4.959229469299316\n",
      "                              Jsc = 24.867515563964844\n",
      "                              FF = 70.10369110107422\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 47.52013397216797\n",
      "                              Voc = 5.080540180206299\n",
      "                              Jsc = 24.863481521606445\n",
      "                              FF = 70.3513412475586\n",
      "Finished epoch  158\n",
      "On epoch  159\n",
      "Total Epoch Testing MAPE: PCE = 46.09324264526367\n",
      "                              Voc = 4.903804302215576\n",
      "                              Jsc = 24.84235191345215\n",
      "                              FF = 70.24617004394531\n",
      "Finished epoch  159\n",
      "On epoch  160\n",
      "Total Epoch Testing MAPE: PCE = 46.84561538696289\n",
      "                              Voc = 4.946922302246094\n",
      "                              Jsc = 24.708141326904297\n",
      "                              FF = 69.62078857421875\n",
      "Finished epoch  160\n",
      "On epoch  161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 47.12160873413086\n",
      "                              Voc = 4.982093334197998\n",
      "                              Jsc = 24.958066940307617\n",
      "                              FF = 69.8775863647461\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 48.89738082885742\n",
      "                              Voc = 4.98874568939209\n",
      "                              Jsc = 24.71613883972168\n",
      "                              FF = 70.52310943603516\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 49.27878189086914\n",
      "                              Voc = 5.0095415115356445\n",
      "                              Jsc = 24.8089599609375\n",
      "                              FF = 71.25379180908203\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 49.46318817138672\n",
      "                              Voc = 4.935860633850098\n",
      "                              Jsc = 24.380586624145508\n",
      "                              FF = 71.13043212890625\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 50.818084716796875\n",
      "                              Voc = 5.0326080322265625\n",
      "                              Jsc = 24.415332794189453\n",
      "                              FF = 70.38208770751953\n",
      "Finished epoch  165\n",
      "On epoch  166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.6702995300293\n",
      "                              Voc = 5.06057596206665\n",
      "                              Jsc = 24.79458236694336\n",
      "                              FF = 71.73167419433594\n",
      "Finished epoch  166\n",
      "On epoch  167\n",
      "Total Epoch Testing MAPE: PCE = 50.646812438964844\n",
      "                              Voc = 5.0498223304748535\n",
      "                              Jsc = 24.933000564575195\n",
      "                              FF = 71.80160522460938\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 50.9383544921875\n",
      "                              Voc = 5.045891761779785\n",
      "                              Jsc = 25.015262603759766\n",
      "                              FF = 71.14895629882812\n",
      "Finished epoch  168\n",
      "On epoch  169\n",
      "Total Epoch Testing MAPE: PCE = 53.6968879699707\n",
      "                              Voc = 5.007772445678711\n",
      "                              Jsc = 24.862531661987305\n",
      "                              FF = 71.0868911743164\n",
      "Finished epoch  169\n",
      "On epoch  170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.472476959228516\n",
      "                              Voc = 4.924449443817139\n",
      "                              Jsc = 24.70453453063965\n",
      "                              FF = 70.92049407958984\n",
      "Finished epoch  170\n",
      "On epoch  171\n",
      "Total Epoch Testing MAPE: PCE = 55.632232666015625\n",
      "                              Voc = 4.934354305267334\n",
      "                              Jsc = 25.0760498046875\n",
      "                              FF = 70.19378662109375\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 58.045650482177734\n",
      "                              Voc = 4.968512058258057\n",
      "                              Jsc = 25.20638084411621\n",
      "                              FF = 69.93853759765625\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 58.22256851196289\n",
      "                              Voc = 4.927880764007568\n",
      "                              Jsc = 25.438934326171875\n",
      "                              FF = 69.27839660644531\n",
      "Finished epoch  173\n",
      "On epoch  174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.55338668823242\n",
      "                              Voc = 4.884965896606445\n",
      "                              Jsc = 25.277814865112305\n",
      "                              FF = 69.3114242553711\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 59.952205657958984\n",
      "                              Voc = 4.843600749969482\n",
      "                              Jsc = 24.815378189086914\n",
      "                              FF = 69.0635986328125\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 60.492557525634766\n",
      "                              Voc = 4.915157318115234\n",
      "                              Jsc = 24.848352432250977\n",
      "                              FF = 68.90409851074219\n",
      "Finished epoch  176\n",
      "On epoch  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 61.80665969848633\n",
      "                              Voc = 4.971065998077393\n",
      "                              Jsc = 24.5916748046875\n",
      "                              FF = 68.1123046875\n",
      "Finished epoch  177\n",
      "On epoch  178\n",
      "Total Epoch Testing MAPE: PCE = 63.2453727722168\n",
      "                              Voc = 5.058913230895996\n",
      "                              Jsc = 24.7049503326416\n",
      "                              FF = 67.43440246582031\n",
      "Finished epoch  178\n",
      "On epoch  179\n",
      "Total Epoch Testing MAPE: PCE = 62.490379333496094\n",
      "                              Voc = 4.992949962615967\n",
      "                              Jsc = 25.695451736450195\n",
      "                              FF = 68.21057891845703\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 63.083065032958984\n",
      "                              Voc = 5.16382360458374\n",
      "                              Jsc = 25.377098083496094\n",
      "                              FF = 68.07361602783203\n",
      "Finished epoch  180\n",
      "On epoch  181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 64.98157501220703\n",
      "                              Voc = 5.160967826843262\n",
      "                              Jsc = 25.47007179260254\n",
      "                              FF = 68.40718078613281\n",
      "Finished epoch  181\n",
      "On epoch  182\n",
      "Total Epoch Testing MAPE: PCE = 65.60095977783203\n",
      "                              Voc = 5.345693588256836\n",
      "                              Jsc = 25.652009963989258\n",
      "                              FF = 68.35249328613281\n",
      "Finished epoch  182\n",
      "On epoch  183\n",
      "Total Epoch Testing MAPE: PCE = 65.38626861572266\n",
      "                              Voc = 4.990161895751953\n",
      "                              Jsc = 25.92202377319336\n",
      "                              FF = 67.77572631835938\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 65.78199005126953\n",
      "                              Voc = 4.986510276794434\n",
      "                              Jsc = 26.211383819580078\n",
      "                              FF = 66.6243667602539\n",
      "Finished epoch  184\n",
      "On epoch  185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 67.01707458496094\n",
      "                              Voc = 4.96791934967041\n",
      "                              Jsc = 26.05814552307129\n",
      "                              FF = 66.71466827392578\n",
      "Finished epoch  185\n",
      "On epoch  186\n",
      "Total Epoch Testing MAPE: PCE = 66.69517517089844\n",
      "                              Voc = 5.099143981933594\n",
      "                              Jsc = 26.22217559814453\n",
      "                              FF = 66.74505615234375\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 67.45169067382812\n",
      "                              Voc = 5.004602432250977\n",
      "                              Jsc = 26.04730987548828\n",
      "                              FF = 66.47137451171875\n",
      "Finished epoch  187\n",
      "On epoch  188\n",
      "Total Epoch Testing MAPE: PCE = 66.74675750732422\n",
      "                              Voc = 4.8730244636535645\n",
      "                              Jsc = 26.061853408813477\n",
      "                              FF = 67.69309997558594\n",
      "Finished epoch  188\n",
      "On epoch  189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 67.23793029785156\n",
      "                              Voc = 4.911980628967285\n",
      "                              Jsc = 26.30229377746582\n",
      "                              FF = 66.89641571044922\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 66.00294494628906\n",
      "                              Voc = 5.0072126388549805\n",
      "                              Jsc = 26.568466186523438\n",
      "                              FF = 68.09473419189453\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 65.93643951416016\n",
      "                              Voc = 4.800834655761719\n",
      "                              Jsc = 26.583576202392578\n",
      "                              FF = 69.34333801269531\n",
      "Finished epoch  191\n",
      "On epoch  192\n",
      "Total Epoch Testing MAPE: PCE = 64.32605743408203\n",
      "                              Voc = 4.746776580810547\n",
      "                              Jsc = 26.856136322021484\n",
      "                              FF = 68.4779052734375\n",
      "Finished epoch  192\n",
      "On epoch  193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 63.6115608215332\n",
      "                              Voc = 4.756947994232178\n",
      "                              Jsc = 26.891347885131836\n",
      "                              FF = 68.04883575439453\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 60.8472900390625\n",
      "                              Voc = 4.750685214996338\n",
      "                              Jsc = 26.914648056030273\n",
      "                              FF = 67.9476547241211\n",
      "Finished epoch  194\n",
      "On epoch  195\n",
      "Total Epoch Testing MAPE: PCE = 60.27645492553711\n",
      "                              Voc = 4.709136962890625\n",
      "                              Jsc = 26.912763595581055\n",
      "                              FF = 67.07868957519531\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 58.595245361328125\n",
      "                              Voc = 4.612947463989258\n",
      "                              Jsc = 26.948389053344727\n",
      "                              FF = 65.91506958007812\n",
      "Finished epoch  196\n",
      "On epoch  197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 61.4727668762207\n",
      "                              Voc = 4.6527581214904785\n",
      "                              Jsc = 26.626136779785156\n",
      "                              FF = 65.23558807373047\n",
      "Finished epoch  197\n",
      "On epoch  198\n",
      "Total Epoch Testing MAPE: PCE = 61.88885498046875\n",
      "                              Voc = 4.765842437744141\n",
      "                              Jsc = 26.60577392578125\n",
      "                              FF = 66.09745788574219\n",
      "Finished epoch  198\n",
      "On epoch  199\n",
      "Total Epoch Testing MAPE: PCE = 60.26032638549805\n",
      "                              Voc = 4.860536575317383\n",
      "                              Jsc = 26.589506149291992\n",
      "                              FF = 66.76236724853516\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 60.655975341796875\n",
      "                              Voc = 4.816682815551758\n",
      "                              Jsc = 26.322895050048828\n",
      "                              FF = 65.5431137084961\n",
      "Finished epoch  200\n",
      "On epoch  201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 61.676998138427734\n",
      "                              Voc = 4.813363552093506\n",
      "                              Jsc = 26.305452346801758\n",
      "                              FF = 64.60014343261719\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 60.646705627441406\n",
      "                              Voc = 4.752956867218018\n",
      "                              Jsc = 26.391902923583984\n",
      "                              FF = 64.64137268066406\n",
      "Finished epoch  202\n",
      "On epoch  203\n",
      "Total Epoch Testing MAPE: PCE = 61.10851287841797\n",
      "                              Voc = 5.145503520965576\n",
      "                              Jsc = 26.761335372924805\n",
      "                              FF = 63.99916458129883\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 63.34292221069336\n",
      "                              Voc = 5.523068428039551\n",
      "                              Jsc = 26.3273868560791\n",
      "                              FF = 64.96340942382812\n",
      "Finished epoch  204\n",
      "On epoch  205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 64.24986267089844\n",
      "                              Voc = 5.800198554992676\n",
      "                              Jsc = 25.786230087280273\n",
      "                              FF = 64.2283706665039\n",
      "Finished epoch  205\n",
      "On epoch  206\n",
      "Total Epoch Testing MAPE: PCE = 63.428871154785156\n",
      "                              Voc = 5.609177112579346\n",
      "                              Jsc = 25.385013580322266\n",
      "                              FF = 64.31208801269531\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 63.348453521728516\n",
      "                              Voc = 5.361005783081055\n",
      "                              Jsc = 25.111818313598633\n",
      "                              FF = 64.09871673583984\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 64.09860229492188\n",
      "                              Voc = 5.338916778564453\n",
      "                              Jsc = 25.33121109008789\n",
      "                              FF = 63.58269500732422\n",
      "Finished epoch  208\n",
      "On epoch  209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 64.6417236328125\n",
      "                              Voc = 5.357780456542969\n",
      "                              Jsc = 24.850160598754883\n",
      "                              FF = 62.5500602722168\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 64.47897338867188\n",
      "                              Voc = 5.457670211791992\n",
      "                              Jsc = 24.772409439086914\n",
      "                              FF = 61.64691925048828\n",
      "Finished epoch  210\n",
      "On epoch  211\n",
      "Total Epoch Testing MAPE: PCE = 64.06025695800781\n",
      "                              Voc = 5.776333332061768\n",
      "                              Jsc = 24.26911735534668\n",
      "                              FF = 61.1504020690918\n",
      "Finished epoch  211\n",
      "On epoch  212\n",
      "Total Epoch Testing MAPE: PCE = 64.8399658203125\n",
      "                              Voc = 5.998913764953613\n",
      "                              Jsc = 23.73458480834961\n",
      "                              FF = 61.454856872558594\n",
      "Finished epoch  212\n",
      "On epoch  213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 63.89573287963867\n",
      "                              Voc = 5.473182201385498\n",
      "                              Jsc = 24.13332748413086\n",
      "                              FF = 62.15584945678711\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 65.070068359375\n",
      "                              Voc = 5.044792175292969\n",
      "                              Jsc = 24.201560974121094\n",
      "                              FF = 61.623348236083984\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 65.32592010498047\n",
      "                              Voc = 5.1598896980285645\n",
      "                              Jsc = 23.951139450073242\n",
      "                              FF = 62.395469665527344\n",
      "Finished epoch  215\n",
      "On epoch  216\n",
      "Total Epoch Testing MAPE: PCE = 65.07888793945312\n",
      "                              Voc = 5.1519012451171875\n",
      "                              Jsc = 24.158119201660156\n",
      "                              FF = 61.955833435058594\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 66.06332397460938\n",
      "                              Voc = 5.162464618682861\n",
      "                              Jsc = 24.321598052978516\n",
      "                              FF = 61.20511245727539\n",
      "Finished epoch  217\n",
      "On epoch  218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 66.97649383544922\n",
      "                              Voc = 5.276578903198242\n",
      "                              Jsc = 24.09973907470703\n",
      "                              FF = 61.3971061706543\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 65.44149017333984\n",
      "                              Voc = 5.234945774078369\n",
      "                              Jsc = 24.130374908447266\n",
      "                              FF = 60.50959777832031\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 67.23792266845703\n",
      "                              Voc = 5.585437774658203\n",
      "                              Jsc = 23.871946334838867\n",
      "                              FF = 60.32512664794922\n",
      "Finished epoch  220\n",
      "On epoch  221\n",
      "Total Epoch Testing MAPE: PCE = 66.58792114257812\n",
      "                              Voc = 5.450900077819824\n",
      "                              Jsc = 24.025636672973633\n",
      "                              FF = 59.91650390625\n",
      "Finished epoch  221\n",
      "On epoch  222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 67.125244140625\n",
      "                              Voc = 5.281499862670898\n",
      "                              Jsc = 23.6829833984375\n",
      "                              FF = 59.59895706176758\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 66.10453796386719\n",
      "                              Voc = 5.4325270652771\n",
      "                              Jsc = 23.583608627319336\n",
      "                              FF = 58.482093811035156\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 67.53113555908203\n",
      "                              Voc = 5.472109317779541\n",
      "                              Jsc = 23.643985748291016\n",
      "                              FF = 58.5207405090332\n",
      "Finished epoch  224\n",
      "On epoch  225\n",
      "Total Epoch Testing MAPE: PCE = 68.13146209716797\n",
      "                              Voc = 5.339034557342529\n",
      "                              Jsc = 23.665813446044922\n",
      "                              FF = 58.563785552978516\n",
      "Finished epoch  225\n",
      "On epoch  226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 68.456787109375\n",
      "                              Voc = 5.548069000244141\n",
      "                              Jsc = 23.353313446044922\n",
      "                              FF = 57.66011428833008\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 67.79011535644531\n",
      "                              Voc = 5.579561710357666\n",
      "                              Jsc = 23.541423797607422\n",
      "                              FF = 57.4380989074707\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 68.050537109375\n",
      "                              Voc = 5.604903221130371\n",
      "                              Jsc = 23.491004943847656\n",
      "                              FF = 57.12894058227539\n",
      "Finished epoch  228\n",
      "On epoch  229\n",
      "Total Epoch Testing MAPE: PCE = 68.48882293701172\n",
      "                              Voc = 5.72491455078125\n",
      "                              Jsc = 23.654823303222656\n",
      "                              FF = 56.53770446777344\n",
      "Finished epoch  229\n",
      "On epoch  230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 66.97151947021484\n",
      "                              Voc = 5.749483108520508\n",
      "                              Jsc = 23.82244110107422\n",
      "                              FF = 55.368751525878906\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 66.3115463256836\n",
      "                              Voc = 5.833040714263916\n",
      "                              Jsc = 24.0513916015625\n",
      "                              FF = 54.86381149291992\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 66.28707885742188\n",
      "                              Voc = 5.477274417877197\n",
      "                              Jsc = 23.970460891723633\n",
      "                              FF = 55.58788299560547\n",
      "Finished epoch  232\n",
      "On epoch  233\n",
      "Total Epoch Testing MAPE: PCE = 65.76956939697266\n",
      "                              Voc = 5.467652797698975\n",
      "                              Jsc = 23.748321533203125\n",
      "                              FF = 55.5195198059082\n",
      "Finished epoch  233\n",
      "On epoch  234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 64.6964340209961\n",
      "                              Voc = 5.542694568634033\n",
      "                              Jsc = 23.63404083251953\n",
      "                              FF = 56.05986022949219\n",
      "Finished epoch  234\n",
      "On epoch  235\n",
      "Total Epoch Testing MAPE: PCE = 65.64920043945312\n",
      "                              Voc = 5.26121187210083\n",
      "                              Jsc = 23.4185791015625\n",
      "                              FF = 56.761192321777344\n",
      "Finished epoch  235\n",
      "On epoch  236\n",
      "Total Epoch Testing MAPE: PCE = 65.1131820678711\n",
      "                              Voc = 5.22723913192749\n",
      "                              Jsc = 23.70241355895996\n",
      "                              FF = 55.54265213012695\n",
      "Finished epoch  236\n",
      "On epoch  237\n",
      "Total Epoch Testing MAPE: PCE = 67.435546875\n",
      "                              Voc = 5.224754810333252\n",
      "                              Jsc = 23.940561294555664\n",
      "                              FF = 56.129432678222656\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 65.46819305419922\n",
      "                              Voc = 5.480727195739746\n",
      "                              Jsc = 24.00623321533203\n",
      "                              FF = 56.142677307128906\n",
      "Finished epoch  238\n",
      "On epoch  239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 65.81595611572266\n",
      "                              Voc = 5.464837551116943\n",
      "                              Jsc = 24.13475799560547\n",
      "                              FF = 56.128482818603516\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 67.2640151977539\n",
      "                              Voc = 5.677484512329102\n",
      "                              Jsc = 24.119657516479492\n",
      "                              FF = 55.642311096191406\n",
      "Finished epoch  240\n",
      "On epoch  241\n",
      "Total Epoch Testing MAPE: PCE = 67.880615234375\n",
      "                              Voc = 5.4455742835998535\n",
      "                              Jsc = 24.122194290161133\n",
      "                              FF = 54.90969467163086\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 66.49396514892578\n",
      "                              Voc = 5.341755390167236\n",
      "                              Jsc = 24.23917579650879\n",
      "                              FF = 56.14289093017578\n",
      "Finished epoch  242\n",
      "On epoch  243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 65.9040298461914\n",
      "                              Voc = 5.594543933868408\n",
      "                              Jsc = 24.21319007873535\n",
      "                              FF = 56.716007232666016\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 65.29519653320312\n",
      "                              Voc = 5.583980083465576\n",
      "                              Jsc = 24.333637237548828\n",
      "                              FF = 55.87990188598633\n",
      "Finished epoch  244\n",
      "On epoch  245\n",
      "Total Epoch Testing MAPE: PCE = 63.99364471435547\n",
      "                              Voc = 5.555047035217285\n",
      "                              Jsc = 24.221553802490234\n",
      "                              FF = 57.31576156616211\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 62.083412170410156\n",
      "                              Voc = 5.539162635803223\n",
      "                              Jsc = 24.048015594482422\n",
      "                              FF = 57.49142837524414\n",
      "Finished epoch  246\n",
      "On epoch  247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 61.60383224487305\n",
      "                              Voc = 5.478935241699219\n",
      "                              Jsc = 23.823720932006836\n",
      "                              FF = 58.14872360229492\n",
      "Finished epoch  247\n",
      "On epoch  248\n",
      "Total Epoch Testing MAPE: PCE = 59.4057731628418\n",
      "                              Voc = 5.451492786407471\n",
      "                              Jsc = 23.499685287475586\n",
      "                              FF = 58.14321517944336\n",
      "Finished epoch  248\n",
      "On epoch  249\n",
      "Total Epoch Testing MAPE: PCE = 57.56353759765625\n",
      "                              Voc = 5.543728351593018\n",
      "                              Jsc = 23.35032844543457\n",
      "                              FF = 57.03392791748047\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 58.36197280883789\n",
      "                              Voc = 5.531601428985596\n",
      "                              Jsc = 23.323198318481445\n",
      "                              FF = 55.02144241333008\n",
      "Finished epoch  250\n",
      "On epoch  251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.335906982421875\n",
      "                              Voc = 5.449708461761475\n",
      "                              Jsc = 22.93263053894043\n",
      "                              FF = 55.50385665893555\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 55.50175476074219\n",
      "                              Voc = 5.629497051239014\n",
      "                              Jsc = 22.853567123413086\n",
      "                              FF = 54.75288772583008\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 57.23105239868164\n",
      "                              Voc = 5.823527812957764\n",
      "                              Jsc = 22.613733291625977\n",
      "                              FF = 53.66080856323242\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 56.43379592895508\n",
      "                              Voc = 5.625324249267578\n",
      "                              Jsc = 22.297563552856445\n",
      "                              FF = 53.5611457824707\n",
      "Finished epoch  254\n",
      "On epoch  255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.28202819824219\n",
      "                              Voc = 5.571223258972168\n",
      "                              Jsc = 22.384262084960938\n",
      "                              FF = 54.07965850830078\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 56.270599365234375\n",
      "                              Voc = 5.486293315887451\n",
      "                              Jsc = 22.423473358154297\n",
      "                              FF = 53.461585998535156\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 54.839237213134766\n",
      "                              Voc = 5.140566349029541\n",
      "                              Jsc = 22.383710861206055\n",
      "                              FF = 54.000885009765625\n",
      "Finished epoch  257\n",
      "On epoch  258\n",
      "Total Epoch Testing MAPE: PCE = 54.115116119384766\n",
      "                              Voc = 5.282759189605713\n",
      "                              Jsc = 22.33847999572754\n",
      "                              FF = 54.17461395263672\n",
      "Finished epoch  258\n",
      "On epoch  259\n",
      "Total Epoch Testing MAPE: PCE = 54.051918029785156\n",
      "                              Voc = 5.314116954803467\n",
      "                              Jsc = 22.290124893188477\n",
      "                              FF = 54.109745025634766\n",
      "Finished epoch  259\n",
      "On epoch  260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.410667419433594\n",
      "                              Voc = 5.335125923156738\n",
      "                              Jsc = 22.21269989013672\n",
      "                              FF = 54.316104888916016\n",
      "Finished epoch  260\n",
      "On epoch  261\n",
      "Total Epoch Testing MAPE: PCE = 54.879005432128906\n",
      "                              Voc = 5.53088903427124\n",
      "                              Jsc = 22.12775230407715\n",
      "                              FF = 54.93877029418945\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 55.69029998779297\n",
      "                              Voc = 5.635890007019043\n",
      "                              Jsc = 22.033315658569336\n",
      "                              FF = 54.40861129760742\n",
      "Finished epoch  262\n",
      "On epoch  263\n",
      "Total Epoch Testing MAPE: PCE = 55.486480712890625\n",
      "                              Voc = 5.460353851318359\n",
      "                              Jsc = 22.152917861938477\n",
      "                              FF = 54.252750396728516\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 54.919795989990234\n",
      "                              Voc = 5.5042338371276855\n",
      "                              Jsc = 22.155303955078125\n",
      "                              FF = 53.61671447753906\n",
      "Finished epoch  264\n",
      "On epoch  265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.73185729980469\n",
      "                              Voc = 5.736794948577881\n",
      "                              Jsc = 22.173507690429688\n",
      "                              FF = 53.4524040222168\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 53.52260971069336\n",
      "                              Voc = 5.769455432891846\n",
      "                              Jsc = 22.274824142456055\n",
      "                              FF = 53.86720275878906\n",
      "Finished epoch  266\n",
      "On epoch  267\n",
      "Total Epoch Testing MAPE: PCE = 54.8396110534668\n",
      "                              Voc = 5.497429370880127\n",
      "                              Jsc = 22.294858932495117\n",
      "                              FF = 53.9747428894043\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 53.32952880859375\n",
      "                              Voc = 5.4315948486328125\n",
      "                              Jsc = 22.298376083374023\n",
      "                              FF = 53.024017333984375\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 52.24199295043945\n",
      "                              Voc = 5.506618022918701\n",
      "                              Jsc = 22.377199172973633\n",
      "                              FF = 53.17414474487305\n",
      "Finished epoch  269\n",
      "On epoch  270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.74509811401367\n",
      "                              Voc = 5.324697494506836\n",
      "                              Jsc = 22.29779624938965\n",
      "                              FF = 51.73588943481445\n",
      "Finished epoch  270\n",
      "On epoch  271\n",
      "Total Epoch Testing MAPE: PCE = 51.37238311767578\n",
      "                              Voc = 5.589134216308594\n",
      "                              Jsc = 22.161861419677734\n",
      "                              FF = 52.47127914428711\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 50.29243087768555\n",
      "                              Voc = 5.5144362449646\n",
      "                              Jsc = 22.157930374145508\n",
      "                              FF = 51.39110565185547\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 50.96996307373047\n",
      "                              Voc = 5.252410888671875\n",
      "                              Jsc = 22.224830627441406\n",
      "                              FF = 50.939361572265625\n",
      "Finished epoch  273\n",
      "On epoch  274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.601810455322266\n",
      "                              Voc = 5.329976558685303\n",
      "                              Jsc = 22.209081649780273\n",
      "                              FF = 50.671142578125\n",
      "Finished epoch  274\n",
      "On epoch  275\n",
      "Total Epoch Testing MAPE: PCE = 51.71979522705078\n",
      "                              Voc = 5.484805107116699\n",
      "                              Jsc = 22.162717819213867\n",
      "                              FF = 51.439369201660156\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 50.416725158691406\n",
      "                              Voc = 5.729251861572266\n",
      "                              Jsc = 22.299800872802734\n",
      "                              FF = 51.19361877441406\n",
      "Finished epoch  276\n",
      "On epoch  277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.68772506713867\n",
      "                              Voc = 5.815251350402832\n",
      "                              Jsc = 22.19819450378418\n",
      "                              FF = 51.43421936035156\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 52.43415832519531\n",
      "                              Voc = 5.68912410736084\n",
      "                              Jsc = 22.1287899017334\n",
      "                              FF = 52.04590606689453\n",
      "Finished epoch  278\n",
      "On epoch  279\n",
      "Total Epoch Testing MAPE: PCE = 51.21857452392578\n",
      "                              Voc = 5.826185703277588\n",
      "                              Jsc = 22.116355895996094\n",
      "                              FF = 52.33454895019531\n",
      "Finished epoch  279\n",
      "On epoch  280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.4793815612793\n",
      "                              Voc = 5.219424724578857\n",
      "                              Jsc = 22.23202896118164\n",
      "                              FF = 52.372371673583984\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 49.15673065185547\n",
      "                              Voc = 5.175054550170898\n",
      "                              Jsc = 22.286287307739258\n",
      "                              FF = 52.417625427246094\n",
      "Finished epoch  281\n",
      "On epoch  282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.340248107910156\n",
      "                              Voc = 5.317506790161133\n",
      "                              Jsc = 22.278213500976562\n",
      "                              FF = 53.03498077392578\n",
      "Finished epoch  282\n",
      "On epoch  283\n",
      "Total Epoch Testing MAPE: PCE = 48.97639083862305\n",
      "                              Voc = 5.503075122833252\n",
      "                              Jsc = 22.336523056030273\n",
      "                              FF = 52.68581008911133\n",
      "Finished epoch  283\n",
      "On epoch  284\n",
      "Total Epoch Testing MAPE: PCE = 48.14361572265625\n",
      "                              Voc = 5.37993860244751\n",
      "                              Jsc = 22.237770080566406\n",
      "                              FF = 53.94429016113281\n",
      "Finished epoch  284\n",
      "On epoch  285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.97203826904297\n",
      "                              Voc = 5.491783142089844\n",
      "                              Jsc = 22.16185760498047\n",
      "                              FF = 53.55648422241211\n",
      "Finished epoch  285\n",
      "On epoch  286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.45269012451172\n",
      "                              Voc = 5.858072757720947\n",
      "                              Jsc = 22.24948501586914\n",
      "                              FF = 53.556800842285156\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 50.563621520996094\n",
      "                              Voc = 5.7681050300598145\n",
      "                              Jsc = 22.095752716064453\n",
      "                              FF = 52.7914924621582\n",
      "Finished epoch  287\n",
      "On epoch  288\n",
      "Total Epoch Testing MAPE: PCE = 50.27495193481445\n",
      "                              Voc = 5.544447422027588\n",
      "                              Jsc = 21.903926849365234\n",
      "                              FF = 53.2142219543457\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 50.664329528808594\n",
      "                              Voc = 5.479538917541504\n",
      "                              Jsc = 21.956470489501953\n",
      "                              FF = 54.0705680847168\n",
      "Finished epoch  289\n",
      "On epoch  290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.69123077392578\n",
      "                              Voc = 5.308081150054932\n",
      "                              Jsc = 21.914287567138672\n",
      "                              FF = 53.835445404052734\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 52.10588455200195\n",
      "                              Voc = 5.5899176597595215\n",
      "                              Jsc = 21.95039939880371\n",
      "                              FF = 53.45464324951172\n",
      "Finished epoch  291\n",
      "On epoch  292\n",
      "Total Epoch Testing MAPE: PCE = 52.5097770690918\n",
      "                              Voc = 5.739997863769531\n",
      "                              Jsc = 22.004274368286133\n",
      "                              FF = 53.022743225097656\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 52.708839416503906\n",
      "                              Voc = 5.558012008666992\n",
      "                              Jsc = 21.990875244140625\n",
      "                              FF = 51.70687484741211\n",
      "Finished epoch  293\n",
      "On epoch  294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.88926315307617\n",
      "                              Voc = 5.501489162445068\n",
      "                              Jsc = 21.93265151977539\n",
      "                              FF = 52.28346633911133\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 51.35487365722656\n",
      "                              Voc = 5.389662265777588\n",
      "                              Jsc = 21.8418025970459\n",
      "                              FF = 53.372467041015625\n",
      "Finished epoch  295\n",
      "On epoch  296\n",
      "Total Epoch Testing MAPE: PCE = 50.07872772216797\n",
      "                              Voc = 5.2025065422058105\n",
      "                              Jsc = 21.762449264526367\n",
      "                              FF = 53.07076644897461\n",
      "Finished epoch  296\n",
      "On epoch  297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.98811721801758\n",
      "                              Voc = 5.050432205200195\n",
      "                              Jsc = 21.754222869873047\n",
      "                              FF = 52.82777404785156\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 49.54740905761719\n",
      "                              Voc = 4.851517677307129\n",
      "                              Jsc = 21.725507736206055\n",
      "                              FF = 53.98539733886719\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 50.71405029296875\n",
      "                              Voc = 4.883408069610596\n",
      "                              Jsc = 21.721019744873047\n",
      "                              FF = 53.58449172973633\n",
      "Finished epoch  299\n",
      "On epoch  300\n",
      "Total Epoch Testing MAPE: PCE = 50.48228454589844\n",
      "                              Voc = 4.817549705505371\n",
      "                              Jsc = 21.6434383392334\n",
      "                              FF = 52.26396179199219\n",
      "Finished epoch  300\n",
      "On epoch  301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.534934997558594\n",
      "                              Voc = 4.965793609619141\n",
      "                              Jsc = 21.519895553588867\n",
      "                              FF = 53.10102081298828\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 52.11781692504883\n",
      "                              Voc = 4.959723949432373\n",
      "                              Jsc = 21.48518180847168\n",
      "                              FF = 53.5531120300293\n",
      "Finished epoch  302\n",
      "On epoch  303\n",
      "Total Epoch Testing MAPE: PCE = 52.904884338378906\n",
      "                              Voc = 4.8510823249816895\n",
      "                              Jsc = 21.495594024658203\n",
      "                              FF = 53.770912170410156\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 53.32489013671875\n",
      "                              Voc = 4.94041109085083\n",
      "                              Jsc = 21.28396987915039\n",
      "                              FF = 52.19976806640625\n",
      "Finished epoch  304\n",
      "On epoch  305\n",
      "Total Epoch Testing MAPE: PCE = 53.2369499206543\n",
      "                              Voc = 4.901289939880371\n",
      "                              Jsc = 21.19261360168457\n",
      "                              FF = 50.96168518066406\n",
      "Finished epoch  305\n",
      "On epoch  306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.03030014038086\n",
      "                              Voc = 4.530864715576172\n",
      "                              Jsc = 21.158344268798828\n",
      "                              FF = 50.99003982543945\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 55.025291442871094\n",
      "                              Voc = 4.486265659332275\n",
      "                              Jsc = 21.162105560302734\n",
      "                              FF = 50.332027435302734\n",
      "Finished epoch  307\n",
      "On epoch  308\n",
      "Total Epoch Testing MAPE: PCE = 57.07928466796875\n",
      "                              Voc = 4.413630962371826\n",
      "                              Jsc = 21.297142028808594\n",
      "                              FF = 50.39390182495117\n",
      "Finished epoch  308\n",
      "On epoch  309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.871212005615234\n",
      "                              Voc = 4.4525580406188965\n",
      "                              Jsc = 21.291654586791992\n",
      "                              FF = 50.74113464355469\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 59.01616287231445\n",
      "                              Voc = 4.438861846923828\n",
      "                              Jsc = 21.20604133605957\n",
      "                              FF = 50.81281661987305\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 58.9826774597168\n",
      "                              Voc = 4.52254056930542\n",
      "                              Jsc = 21.14190101623535\n",
      "                              FF = 50.934452056884766\n",
      "Finished epoch  311\n",
      "On epoch  312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.73889923095703\n",
      "                              Voc = 4.5228681564331055\n",
      "                              Jsc = 21.06886863708496\n",
      "                              FF = 50.83111572265625\n",
      "Finished epoch  312\n",
      "On epoch  313\n",
      "Total Epoch Testing MAPE: PCE = 58.27130126953125\n",
      "                              Voc = 4.567541599273682\n",
      "                              Jsc = 20.964859008789062\n",
      "                              FF = 50.97111892700195\n",
      "Finished epoch  313\n",
      "On epoch  314\n",
      "Total Epoch Testing MAPE: PCE = 56.769012451171875\n",
      "                              Voc = 4.598325252532959\n",
      "                              Jsc = 21.150653839111328\n",
      "                              FF = 51.01715850830078\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 56.53953552246094\n",
      "                              Voc = 4.671305179595947\n",
      "                              Jsc = 21.181535720825195\n",
      "                              FF = 50.37208938598633\n",
      "Finished epoch  315\n",
      "On epoch  316\n",
      "Total Epoch Testing MAPE: PCE = 55.44751739501953\n",
      "                              Voc = 4.686030864715576\n",
      "                              Jsc = 21.124401092529297\n",
      "                              FF = 49.495750427246094\n",
      "Finished epoch  316\n",
      "On epoch  317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.257755279541016\n",
      "                              Voc = 4.673007488250732\n",
      "                              Jsc = 21.053016662597656\n",
      "                              FF = 50.810646057128906\n",
      "Finished epoch  317\n",
      "On epoch  318\n",
      "Total Epoch Testing MAPE: PCE = 55.4727897644043\n",
      "                              Voc = 4.680586814880371\n",
      "                              Jsc = 20.8992919921875\n",
      "                              FF = 50.697391510009766\n",
      "Finished epoch  318\n",
      "On epoch  319\n",
      "Total Epoch Testing MAPE: PCE = 54.66082763671875\n",
      "                              Voc = 4.789710998535156\n",
      "                              Jsc = 20.90213966369629\n",
      "                              FF = 50.761497497558594\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 55.07099151611328\n",
      "                              Voc = 4.996465682983398\n",
      "                              Jsc = 20.960805892944336\n",
      "                              FF = 50.8865966796875\n",
      "Finished epoch  320\n",
      "On epoch  321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.84762954711914\n",
      "                              Voc = 5.056183815002441\n",
      "                              Jsc = 20.87925148010254\n",
      "                              FF = 50.704280853271484\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 54.41083908081055\n",
      "                              Voc = 4.9916276931762695\n",
      "                              Jsc = 20.881771087646484\n",
      "                              FF = 50.4259033203125\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 53.15666961669922\n",
      "                              Voc = 5.025532245635986\n",
      "                              Jsc = 20.93988037109375\n",
      "                              FF = 51.14985656738281\n",
      "Finished epoch  323\n",
      "On epoch  324\n",
      "Total Epoch Testing MAPE: PCE = 51.79517364501953\n",
      "                              Voc = 4.954874515533447\n",
      "                              Jsc = 20.979045867919922\n",
      "                              FF = 51.13557052612305\n",
      "Finished epoch  324\n",
      "On epoch  325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.83435821533203\n",
      "                              Voc = 4.914180278778076\n",
      "                              Jsc = 20.93887710571289\n",
      "                              FF = 50.6314582824707\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 50.553916931152344\n",
      "                              Voc = 4.914316654205322\n",
      "                              Jsc = 20.78989028930664\n",
      "                              FF = 49.29227828979492\n",
      "Finished epoch  326\n",
      "On epoch  327\n",
      "Total Epoch Testing MAPE: PCE = 52.24369812011719\n",
      "                              Voc = 4.902719497680664\n",
      "                              Jsc = 20.858734130859375\n",
      "                              FF = 49.2540283203125\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 50.81024169921875\n",
      "                              Voc = 4.95905876159668\n",
      "                              Jsc = 20.828510284423828\n",
      "                              FF = 49.224491119384766\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 51.22459411621094\n",
      "                              Voc = 5.000382423400879\n",
      "                              Jsc = 20.795753479003906\n",
      "                              FF = 49.002288818359375\n",
      "Finished epoch  329\n",
      "On epoch  330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.90380859375\n",
      "                              Voc = 5.044771194458008\n",
      "                              Jsc = 20.77728843688965\n",
      "                              FF = 49.97099685668945\n",
      "Finished epoch  330\n",
      "On epoch  331\n",
      "Total Epoch Testing MAPE: PCE = 49.46753692626953\n",
      "                              Voc = 5.055813789367676\n",
      "                              Jsc = 20.707317352294922\n",
      "                              FF = 51.12571716308594\n",
      "Finished epoch  331\n",
      "On epoch  332\n",
      "Total Epoch Testing MAPE: PCE = 49.51789093017578\n",
      "                              Voc = 5.044642448425293\n",
      "                              Jsc = 20.71157455444336\n",
      "                              FF = 50.785179138183594\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 50.318443298339844\n",
      "                              Voc = 5.056140422821045\n",
      "                              Jsc = 20.623653411865234\n",
      "                              FF = 49.81546401977539\n",
      "Finished epoch  333\n",
      "On epoch  334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.92092514038086\n",
      "                              Voc = 5.150743007659912\n",
      "                              Jsc = 20.619165420532227\n",
      "                              FF = 48.934085845947266\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 49.106903076171875\n",
      "                              Voc = 5.302420616149902\n",
      "                              Jsc = 20.604366302490234\n",
      "                              FF = 48.01956558227539\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 48.02874755859375\n",
      "                              Voc = 5.241825580596924\n",
      "                              Jsc = 20.58702278137207\n",
      "                              FF = 48.7650032043457\n",
      "Finished epoch  336\n",
      "On epoch  337\n",
      "Total Epoch Testing MAPE: PCE = 47.717952728271484\n",
      "                              Voc = 5.286771774291992\n",
      "                              Jsc = 20.625913619995117\n",
      "                              FF = 49.31782913208008\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 47.71140670776367\n",
      "                              Voc = 5.320734024047852\n",
      "                              Jsc = 20.61688232421875\n",
      "                              FF = 48.43688201904297\n",
      "Finished epoch  338\n",
      "On epoch  339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 46.466190338134766\n",
      "                              Voc = 5.228837013244629\n",
      "                              Jsc = 20.566917419433594\n",
      "                              FF = 48.649269104003906\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 45.49770736694336\n",
      "                              Voc = 5.329686164855957\n",
      "                              Jsc = 20.485523223876953\n",
      "                              FF = 49.87450408935547\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 45.19613265991211\n",
      "                              Voc = 5.368253707885742\n",
      "                              Jsc = 20.395139694213867\n",
      "                              FF = 50.2441291809082\n",
      "Finished epoch  341\n",
      "On epoch  342\n",
      "Total Epoch Testing MAPE: PCE = 45.87710189819336\n",
      "                              Voc = 5.055196762084961\n",
      "                              Jsc = 20.480676651000977\n",
      "                              FF = 50.56598663330078\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 46.62974166870117\n",
      "                              Voc = 5.010464668273926\n",
      "                              Jsc = 20.41857147216797\n",
      "                              FF = 51.1683235168457\n",
      "Finished epoch  343\n",
      "On epoch  344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 47.10136795043945\n",
      "                              Voc = 4.997656345367432\n",
      "                              Jsc = 20.448375701904297\n",
      "                              FF = 50.195587158203125\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 47.57839584350586\n",
      "                              Voc = 5.023204326629639\n",
      "                              Jsc = 20.449867248535156\n",
      "                              FF = 50.81687545776367\n",
      "Finished epoch  345\n",
      "On epoch  346\n",
      "Total Epoch Testing MAPE: PCE = 48.3577880859375\n",
      "                              Voc = 4.972387790679932\n",
      "                              Jsc = 20.48933219909668\n",
      "                              FF = 50.649322509765625\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 48.89642333984375\n",
      "                              Voc = 4.980445384979248\n",
      "                              Jsc = 20.479875564575195\n",
      "                              FF = 50.54875183105469\n",
      "Finished epoch  347\n",
      "On epoch  348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.12946701049805\n",
      "                              Voc = 5.0031609535217285\n",
      "                              Jsc = 20.48802947998047\n",
      "                              FF = 50.05854797363281\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 46.928531646728516\n",
      "                              Voc = 5.038563251495361\n",
      "                              Jsc = 20.36961555480957\n",
      "                              FF = 50.15328598022461\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 47.06632614135742\n",
      "                              Voc = 5.198531627655029\n",
      "                              Jsc = 20.382043838500977\n",
      "                              FF = 51.37810516357422\n",
      "Finished epoch  350\n",
      "On epoch  351\n",
      "Total Epoch Testing MAPE: PCE = 49.17031478881836\n",
      "                              Voc = 5.081099987030029\n",
      "                              Jsc = 20.396053314208984\n",
      "                              FF = 51.7037467956543\n",
      "Finished epoch  351\n",
      "On epoch  352\n",
      "Total Epoch Testing MAPE: PCE = 49.34678649902344\n",
      "                              Voc = 4.973191261291504\n",
      "                              Jsc = 20.42512321472168\n",
      "                              FF = 51.61528778076172\n",
      "Finished epoch  352\n",
      "On epoch  353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.795387268066406\n",
      "                              Voc = 4.946857452392578\n",
      "                              Jsc = 20.394880294799805\n",
      "                              FF = 50.88376998901367\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 48.9886474609375\n",
      "                              Voc = 4.947193145751953\n",
      "                              Jsc = 20.305395126342773\n",
      "                              FF = 51.2892951965332\n",
      "Finished epoch  354\n",
      "On epoch  355\n",
      "Total Epoch Testing MAPE: PCE = 50.333763122558594\n",
      "                              Voc = 4.962508678436279\n",
      "                              Jsc = 20.291194915771484\n",
      "                              FF = 50.7156982421875\n",
      "Finished epoch  355\n",
      "On epoch  356\n",
      "Total Epoch Testing MAPE: PCE = 49.65530776977539\n",
      "                              Voc = 4.943576812744141\n",
      "                              Jsc = 20.318315505981445\n",
      "                              FF = 50.89183044433594\n",
      "Finished epoch  356\n",
      "On epoch  357\n",
      "Total Epoch Testing MAPE: PCE = 49.85310745239258\n",
      "                              Voc = 4.948689937591553\n",
      "                              Jsc = 20.408361434936523\n",
      "                              FF = 50.367591857910156\n",
      "Finished epoch  357\n",
      "On epoch  358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.29594802856445\n",
      "                              Voc = 4.898036956787109\n",
      "                              Jsc = 20.418508529663086\n",
      "                              FF = 50.8814697265625\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 49.359310150146484\n",
      "                              Voc = 4.8712029457092285\n",
      "                              Jsc = 20.516456604003906\n",
      "                              FF = 50.64500427246094\n",
      "Finished epoch  359\n",
      "On epoch  360\n",
      "Total Epoch Testing MAPE: PCE = 49.55718231201172\n",
      "                              Voc = 4.874781608581543\n",
      "                              Jsc = 20.459524154663086\n",
      "                              FF = 51.24718475341797\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 50.27484893798828\n",
      "                              Voc = 4.868020057678223\n",
      "                              Jsc = 20.557024002075195\n",
      "                              FF = 51.741600036621094\n",
      "Finished epoch  361\n",
      "On epoch  362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.40275573730469\n",
      "                              Voc = 4.819045066833496\n",
      "                              Jsc = 20.477487564086914\n",
      "                              FF = 51.671897888183594\n",
      "Finished epoch  362\n",
      "On epoch  363\n",
      "Total Epoch Testing MAPE: PCE = 48.41280746459961\n",
      "                              Voc = 4.7656707763671875\n",
      "                              Jsc = 20.498003005981445\n",
      "                              FF = 51.38916778564453\n",
      "Finished epoch  363\n",
      "On epoch  364\n",
      "Total Epoch Testing MAPE: PCE = 48.341796875\n",
      "                              Voc = 4.873046875\n",
      "                              Jsc = 20.436092376708984\n",
      "                              FF = 51.0495719909668\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 49.83127975463867\n",
      "                              Voc = 4.759115219116211\n",
      "                              Jsc = 20.545398712158203\n",
      "                              FF = 51.263004302978516\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 49.89695739746094\n",
      "                              Voc = 4.723513603210449\n",
      "                              Jsc = 20.49213409423828\n",
      "                              FF = 52.0157470703125\n",
      "Finished epoch  366\n",
      "On epoch  367\n",
      "Total Epoch Testing MAPE: PCE = 50.040863037109375\n",
      "                              Voc = 4.745052814483643\n",
      "                              Jsc = 20.52945327758789\n",
      "                              FF = 51.34758377075195\n",
      "Finished epoch  367\n",
      "On epoch  368\n",
      "Total Epoch Testing MAPE: PCE = 49.7580451965332\n",
      "                              Voc = 4.754557132720947\n",
      "                              Jsc = 20.52869987487793\n",
      "                              FF = 51.630615234375\n",
      "Finished epoch  368\n",
      "On epoch  369\n",
      "Total Epoch Testing MAPE: PCE = 49.623046875\n",
      "                              Voc = 4.810347557067871\n",
      "                              Jsc = 20.400423049926758\n",
      "                              FF = 51.88446044921875\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 49.58611297607422\n",
      "                              Voc = 4.818716049194336\n",
      "                              Jsc = 20.43123435974121\n",
      "                              FF = 52.619361877441406\n",
      "Finished epoch  370\n",
      "On epoch  371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.43414306640625\n",
      "                              Voc = 4.766829490661621\n",
      "                              Jsc = 20.48952865600586\n",
      "                              FF = 52.06732177734375\n",
      "Finished epoch  371\n",
      "On epoch  372\n",
      "Total Epoch Testing MAPE: PCE = 50.34418869018555\n",
      "                              Voc = 4.877173900604248\n",
      "                              Jsc = 20.398456573486328\n",
      "                              FF = 51.88776397705078\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 48.10760498046875\n",
      "                              Voc = 5.101499080657959\n",
      "                              Jsc = 20.427888870239258\n",
      "                              FF = 51.363746643066406\n",
      "Finished epoch  373\n",
      "On epoch  374\n",
      "Total Epoch Testing MAPE: PCE = 48.637577056884766\n",
      "                              Voc = 4.993535995483398\n",
      "                              Jsc = 20.38712501525879\n",
      "                              FF = 52.198177337646484\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 48.68775939941406\n",
      "                              Voc = 5.108303070068359\n",
      "                              Jsc = 20.345151901245117\n",
      "                              FF = 51.91678237915039\n",
      "Finished epoch  375\n",
      "On epoch  376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.33110046386719\n",
      "                              Voc = 5.151706218719482\n",
      "                              Jsc = 20.269329071044922\n",
      "                              FF = 52.4442138671875\n",
      "Finished epoch  376\n",
      "On epoch  377\n",
      "Total Epoch Testing MAPE: PCE = 48.26730728149414\n",
      "                              Voc = 5.02433967590332\n",
      "                              Jsc = 20.25887107849121\n",
      "                              FF = 52.08961868286133\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 47.85091018676758\n",
      "                              Voc = 5.082778453826904\n",
      "                              Jsc = 20.269229888916016\n",
      "                              FF = 51.55724334716797\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 48.10356903076172\n",
      "                              Voc = 5.046098709106445\n",
      "                              Jsc = 20.271814346313477\n",
      "                              FF = 51.874900817871094\n",
      "Finished epoch  379\n",
      "On epoch  380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.82914352416992\n",
      "                              Voc = 4.934180736541748\n",
      "                              Jsc = 20.31409454345703\n",
      "                              FF = 50.78532028198242\n",
      "Finished epoch  380\n",
      "On epoch  381\n",
      "Total Epoch Testing MAPE: PCE = 48.562862396240234\n",
      "                              Voc = 4.968599796295166\n",
      "                              Jsc = 20.32853126525879\n",
      "                              FF = 50.58300018310547\n",
      "Finished epoch  381\n",
      "On epoch  382\n",
      "Total Epoch Testing MAPE: PCE = 47.63143539428711\n",
      "                              Voc = 4.9905219078063965\n",
      "                              Jsc = 20.32065200805664\n",
      "                              FF = 50.742794036865234\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 46.674896240234375\n",
      "                              Voc = 5.019938945770264\n",
      "                              Jsc = 20.306529998779297\n",
      "                              FF = 50.33580017089844\n",
      "Finished epoch  383\n",
      "On epoch  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 45.75218200683594\n",
      "                              Voc = 4.7928080558776855\n",
      "                              Jsc = 20.296621322631836\n",
      "                              FF = 48.67597961425781\n",
      "Finished epoch  384\n",
      "On epoch  385\n",
      "Total Epoch Testing MAPE: PCE = 45.87656784057617\n",
      "                              Voc = 4.691226959228516\n",
      "                              Jsc = 20.255739212036133\n",
      "                              FF = 48.27676010131836\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 47.518863677978516\n",
      "                              Voc = 4.632604122161865\n",
      "                              Jsc = 20.26210594177246\n",
      "                              FF = 48.359901428222656\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 48.7659912109375\n",
      "                              Voc = 4.709810256958008\n",
      "                              Jsc = 20.25799560546875\n",
      "                              FF = 48.16616439819336\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 50.83262252807617\n",
      "                              Voc = 4.819469451904297\n",
      "                              Jsc = 20.235816955566406\n",
      "                              FF = 48.296199798583984\n",
      "Finished epoch  388\n",
      "On epoch  389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.610206604003906\n",
      "                              Voc = 5.162269592285156\n",
      "                              Jsc = 20.28253746032715\n",
      "                              FF = 48.74292755126953\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 50.04061508178711\n",
      "                              Voc = 4.915863990783691\n",
      "                              Jsc = 20.294097900390625\n",
      "                              FF = 48.4752082824707\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 50.16184616088867\n",
      "                              Voc = 4.86206579208374\n",
      "                              Jsc = 20.338457107543945\n",
      "                              FF = 47.627315521240234\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 49.99419403076172\n",
      "                              Voc = 4.853253364562988\n",
      "                              Jsc = 20.319700241088867\n",
      "                              FF = 47.30403518676758\n",
      "Finished epoch  392\n",
      "On epoch  393\n",
      "Total Epoch Testing MAPE: PCE = 51.70167541503906\n",
      "                              Voc = 4.7802581787109375\n",
      "                              Jsc = 20.299598693847656\n",
      "                              FF = 47.60893249511719\n",
      "Finished epoch  393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 53.02125930786133\n",
      "                              Voc = 4.830117702484131\n",
      "                              Jsc = 20.34015464782715\n",
      "                              FF = 47.306068420410156\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 53.045166015625\n",
      "                              Voc = 4.857295513153076\n",
      "                              Jsc = 20.28432846069336\n",
      "                              FF = 47.6227912902832\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 52.61830520629883\n",
      "                              Voc = 4.761458873748779\n",
      "                              Jsc = 20.329984664916992\n",
      "                              FF = 47.870792388916016\n",
      "Finished epoch  396\n",
      "On epoch  397\n",
      "Total Epoch Testing MAPE: PCE = 53.810691833496094\n",
      "                              Voc = 4.957160949707031\n",
      "                              Jsc = 20.318571090698242\n",
      "                              FF = 48.18111801147461\n",
      "Finished epoch  397\n",
      "On epoch  398\n",
      "Total Epoch Testing MAPE: PCE = 53.19438552856445\n",
      "                              Voc = 5.266978740692139\n",
      "                              Jsc = 20.325542449951172\n",
      "                              FF = 47.966739654541016\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 52.22626495361328\n",
      "                              Voc = 5.282008171081543\n",
      "                              Jsc = 20.286632537841797\n",
      "                              FF = 48.774574279785156\n",
      "Finished epoch  399\n",
      "On epoch  400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.97561264038086\n",
      "                              Voc = 5.191401958465576\n",
      "                              Jsc = 20.344881057739258\n",
      "                              FF = 48.525123596191406\n",
      "Finished epoch  400\n",
      "On epoch  401\n",
      "Total Epoch Testing MAPE: PCE = 49.866458892822266\n",
      "                              Voc = 5.017241477966309\n",
      "                              Jsc = 20.290943145751953\n",
      "                              FF = 49.51374435424805\n",
      "Finished epoch  401\n",
      "On epoch  402\n",
      "Total Epoch Testing MAPE: PCE = 50.69635009765625\n",
      "                              Voc = 4.945773601531982\n",
      "                              Jsc = 20.331546783447266\n",
      "                              FF = 48.53397750854492\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 50.9373893737793\n",
      "                              Voc = 4.967690944671631\n",
      "                              Jsc = 20.31179428100586\n",
      "                              FF = 47.62240982055664\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 50.47100830078125\n",
      "                              Voc = 4.79784631729126\n",
      "                              Jsc = 20.324125289916992\n",
      "                              FF = 47.69043731689453\n",
      "Finished epoch  404\n",
      "On epoch  405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.63311004638672\n",
      "                              Voc = 4.9977545738220215\n",
      "                              Jsc = 20.303001403808594\n",
      "                              FF = 48.698822021484375\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 50.52602767944336\n",
      "                              Voc = 4.943404197692871\n",
      "                              Jsc = 20.2761173248291\n",
      "                              FF = 49.37265396118164\n",
      "Finished epoch  406\n",
      "On epoch  407\n",
      "Total Epoch Testing MAPE: PCE = 50.555564880371094\n",
      "                              Voc = 4.999277114868164\n",
      "                              Jsc = 20.269731521606445\n",
      "                              FF = 49.78080749511719\n",
      "Finished epoch  407\n",
      "On epoch  408\n",
      "Total Epoch Testing MAPE: PCE = 51.783782958984375\n",
      "                              Voc = 4.88276481628418\n",
      "                              Jsc = 20.335805892944336\n",
      "                              FF = 49.856422424316406\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 51.50023651123047\n",
      "                              Voc = 4.896159648895264\n",
      "                              Jsc = 20.337474822998047\n",
      "                              FF = 50.08766555786133\n",
      "Finished epoch  409\n",
      "On epoch  410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.75067901611328\n",
      "                              Voc = 4.776266574859619\n",
      "                              Jsc = 20.311080932617188\n",
      "                              FF = 50.11578369140625\n",
      "Finished epoch  410\n",
      "On epoch  411\n",
      "Total Epoch Testing MAPE: PCE = 51.29616928100586\n",
      "                              Voc = 4.583752155303955\n",
      "                              Jsc = 20.274120330810547\n",
      "                              FF = 48.90492630004883\n",
      "Finished epoch  411\n",
      "On epoch  412\n",
      "Total Epoch Testing MAPE: PCE = 50.83125305175781\n",
      "                              Voc = 4.602362155914307\n",
      "                              Jsc = 20.30763053894043\n",
      "                              FF = 49.53612518310547\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 51.34797668457031\n",
      "                              Voc = 4.881335258483887\n",
      "                              Jsc = 20.34453582763672\n",
      "                              FF = 48.847328186035156\n",
      "Finished epoch  413\n",
      "On epoch  414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.244197845458984\n",
      "                              Voc = 4.8259711265563965\n",
      "                              Jsc = 20.354246139526367\n",
      "                              FF = 50.143558502197266\n",
      "Finished epoch  414\n",
      "On epoch  415\n",
      "Total Epoch Testing MAPE: PCE = 51.762847900390625\n",
      "                              Voc = 5.105131149291992\n",
      "                              Jsc = 20.390878677368164\n",
      "                              FF = 50.19719696044922\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 50.6993408203125\n",
      "                              Voc = 5.326685905456543\n",
      "                              Jsc = 20.423120498657227\n",
      "                              FF = 50.15236282348633\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 48.729225158691406\n",
      "                              Voc = 5.667866230010986\n",
      "                              Jsc = 20.45497703552246\n",
      "                              FF = 48.622291564941406\n",
      "Finished epoch  417\n",
      "On epoch  418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 46.96272277832031\n",
      "                              Voc = 5.585352897644043\n",
      "                              Jsc = 20.477197647094727\n",
      "                              FF = 48.81910705566406\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 44.78501510620117\n",
      "                              Voc = 5.556180477142334\n",
      "                              Jsc = 20.426433563232422\n",
      "                              FF = 47.845699310302734\n",
      "Finished epoch  419\n",
      "On epoch  420\n",
      "Total Epoch Testing MAPE: PCE = 42.83516311645508\n",
      "                              Voc = 5.367474555969238\n",
      "                              Jsc = 20.4399471282959\n",
      "                              FF = 47.526390075683594\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 41.838809967041016\n",
      "                              Voc = 5.454254150390625\n",
      "                              Jsc = 20.453197479248047\n",
      "                              FF = 46.71944808959961\n",
      "Finished epoch  421\n",
      "On epoch  422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.6318359375\n",
      "                              Voc = 5.4059648513793945\n",
      "                              Jsc = 20.403804779052734\n",
      "                              FF = 45.96078872680664\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 39.13274383544922\n",
      "                              Voc = 5.560997009277344\n",
      "                              Jsc = 20.350738525390625\n",
      "                              FF = 45.64973068237305\n",
      "Finished epoch  423\n",
      "On epoch  424\n",
      "Total Epoch Testing MAPE: PCE = 39.18865203857422\n",
      "                              Voc = 5.815958023071289\n",
      "                              Jsc = 20.303651809692383\n",
      "                              FF = 46.14386749267578\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 38.025760650634766\n",
      "                              Voc = 5.541935920715332\n",
      "                              Jsc = 20.215614318847656\n",
      "                              FF = 46.347225189208984\n",
      "Finished epoch  425\n",
      "On epoch  426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.990509033203125\n",
      "                              Voc = 5.49758243560791\n",
      "                              Jsc = 20.181650161743164\n",
      "                              FF = 46.39564514160156\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 38.91347885131836\n",
      "                              Voc = 5.814399719238281\n",
      "                              Jsc = 20.227970123291016\n",
      "                              FF = 47.59537124633789\n",
      "Finished epoch  427\n",
      "On epoch  428\n",
      "Total Epoch Testing MAPE: PCE = 39.36707305908203\n",
      "                              Voc = 5.712155818939209\n",
      "                              Jsc = 20.229171752929688\n",
      "                              FF = 47.422306060791016\n",
      "Finished epoch  428\n",
      "On epoch  429\n",
      "Total Epoch Testing MAPE: PCE = 40.12789535522461\n",
      "                              Voc = 5.287280082702637\n",
      "                              Jsc = 20.2696590423584\n",
      "                              FF = 46.97600555419922\n",
      "Finished epoch  429\n",
      "On epoch  430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 40.803043365478516\n",
      "                              Voc = 5.472914695739746\n",
      "                              Jsc = 20.215734481811523\n",
      "                              FF = 48.06504821777344\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 39.16612243652344\n",
      "                              Voc = 5.793616771697998\n",
      "                              Jsc = 20.253387451171875\n",
      "                              FF = 47.65916061401367\n",
      "Finished epoch  431\n",
      "On epoch  432\n",
      "Total Epoch Testing MAPE: PCE = 38.512115478515625\n",
      "                              Voc = 6.022011756896973\n",
      "                              Jsc = 20.363080978393555\n",
      "                              FF = 48.22982406616211\n",
      "Finished epoch  432\n",
      "On epoch  433\n",
      "Total Epoch Testing MAPE: PCE = 38.50273513793945\n",
      "                              Voc = 6.238054275512695\n",
      "                              Jsc = 20.39604949951172\n",
      "                              FF = 47.0744514465332\n",
      "Finished epoch  433\n",
      "On epoch  434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.70185089111328\n",
      "                              Voc = 5.56412935256958\n",
      "                              Jsc = 20.34748649597168\n",
      "                              FF = 46.44342803955078\n",
      "Finished epoch  434\n",
      "On epoch  435\n",
      "Total Epoch Testing MAPE: PCE = 39.892189025878906\n",
      "                              Voc = 5.606926918029785\n",
      "                              Jsc = 20.304250717163086\n",
      "                              FF = 46.811092376708984\n",
      "Finished epoch  435\n",
      "On epoch  436\n",
      "Total Epoch Testing MAPE: PCE = 39.8614387512207\n",
      "                              Voc = 5.756096363067627\n",
      "                              Jsc = 20.344335556030273\n",
      "                              FF = 46.0093994140625\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 39.58436584472656\n",
      "                              Voc = 5.801326274871826\n",
      "                              Jsc = 20.41059684753418\n",
      "                              FF = 45.58908462524414\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 40.33548355102539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Voc = 5.538825035095215\n",
      "                              Jsc = 20.438785552978516\n",
      "                              FF = 45.62691879272461\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 41.04334259033203\n",
      "                              Voc = 5.389513969421387\n",
      "                              Jsc = 20.40209197998047\n",
      "                              FF = 46.004207611083984\n",
      "Finished epoch  439\n",
      "On epoch  440\n",
      "Total Epoch Testing MAPE: PCE = 40.81337356567383\n",
      "                              Voc = 5.581786632537842\n",
      "                              Jsc = 20.392032623291016\n",
      "                              FF = 45.564083099365234\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 40.534523010253906\n",
      "                              Voc = 5.657146453857422\n",
      "                              Jsc = 20.387008666992188\n",
      "                              FF = 44.74396514892578\n",
      "Finished epoch  441\n",
      "On epoch  442\n",
      "Total Epoch Testing MAPE: PCE = 41.21535873413086\n",
      "                              Voc = 5.672547817230225\n",
      "                              Jsc = 20.418563842773438\n",
      "                              FF = 45.001827239990234\n",
      "Finished epoch  442\n",
      "On epoch  443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 41.890560150146484\n",
      "                              Voc = 5.614140033721924\n",
      "                              Jsc = 20.436786651611328\n",
      "                              FF = 45.79017639160156\n",
      "Finished epoch  443\n",
      "On epoch  444\n",
      "Total Epoch Testing MAPE: PCE = 42.4272346496582\n",
      "                              Voc = 5.597677230834961\n",
      "                              Jsc = 20.37668800354004\n",
      "                              FF = 45.792236328125\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 41.900203704833984\n",
      "                              Voc = 5.5924553871154785\n",
      "                              Jsc = 20.3814640045166\n",
      "                              FF = 45.65959548950195\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 41.368682861328125\n",
      "                              Voc = 5.570474147796631\n",
      "                              Jsc = 20.381359100341797\n",
      "                              FF = 45.319915771484375\n",
      "Finished epoch  446\n",
      "On epoch  447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.977970123291016\n",
      "                              Voc = 5.725954532623291\n",
      "                              Jsc = 20.38457489013672\n",
      "                              FF = 45.77619934082031\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 41.467079162597656\n",
      "                              Voc = 5.749056339263916\n",
      "                              Jsc = 20.330875396728516\n",
      "                              FF = 45.43771743774414\n",
      "Finished epoch  448\n",
      "On epoch  449\n",
      "Total Epoch Testing MAPE: PCE = 42.632415771484375\n",
      "                              Voc = 5.7896881103515625\n",
      "                              Jsc = 20.278791427612305\n",
      "                              FF = 45.27960968017578\n",
      "Finished epoch  449\n",
      "On epoch  450\n",
      "Total Epoch Testing MAPE: PCE = 44.1199836730957\n",
      "                              Voc = 5.357265472412109\n",
      "                              Jsc = 20.330984115600586\n",
      "                              FF = 44.68593215942383\n",
      "Finished epoch  450\n",
      "On epoch  451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 44.078277587890625\n",
      "                              Voc = 5.434380054473877\n",
      "                              Jsc = 20.332433700561523\n",
      "                              FF = 44.01118087768555\n",
      "Finished epoch  451\n",
      "On epoch  452\n",
      "Total Epoch Testing MAPE: PCE = 43.84120559692383\n",
      "                              Voc = 5.599615097045898\n",
      "                              Jsc = 20.238021850585938\n",
      "                              FF = 44.529361724853516\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 45.2899169921875\n",
      "                              Voc = 5.800651550292969\n",
      "                              Jsc = 20.26268768310547\n",
      "                              FF = 44.29755401611328\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 45.223514556884766\n",
      "                              Voc = 5.786818504333496\n",
      "                              Jsc = 20.165424346923828\n",
      "                              FF = 44.79572677612305\n",
      "Finished epoch  454\n",
      "On epoch  455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 44.85347366333008\n",
      "                              Voc = 5.875790119171143\n",
      "                              Jsc = 20.182170867919922\n",
      "                              FF = 44.72555923461914\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 44.4618034362793\n",
      "                              Voc = 5.923820495605469\n",
      "                              Jsc = 20.194290161132812\n",
      "                              FF = 43.429229736328125\n",
      "Finished epoch  456\n",
      "On epoch  457\n",
      "Total Epoch Testing MAPE: PCE = 44.70159912109375\n",
      "                              Voc = 5.966732501983643\n",
      "                              Jsc = 20.232881546020508\n",
      "                              FF = 42.761802673339844\n",
      "Finished epoch  457\n",
      "On epoch  458\n",
      "Total Epoch Testing MAPE: PCE = 44.12782287597656\n",
      "                              Voc = 6.07939338684082\n",
      "                              Jsc = 20.235872268676758\n",
      "                              FF = 43.21049880981445\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 44.84254455566406\n",
      "                              Voc = 6.02481746673584\n",
      "                              Jsc = 20.25855255126953\n",
      "                              FF = 43.06299591064453\n",
      "Finished epoch  459\n",
      "On epoch  460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 44.22463607788086\n",
      "                              Voc = 5.613426685333252\n",
      "                              Jsc = 20.292360305786133\n",
      "                              FF = 42.54828643798828\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 44.28249740600586\n",
      "                              Voc = 5.593571186065674\n",
      "                              Jsc = 20.26902961730957\n",
      "                              FF = 42.4890251159668\n",
      "Finished epoch  461\n",
      "On epoch  462\n",
      "Total Epoch Testing MAPE: PCE = 43.55202102661133\n",
      "                              Voc = 5.098732948303223\n",
      "                              Jsc = 20.27210807800293\n",
      "                              FF = 42.51485061645508\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 42.80424880981445\n",
      "                              Voc = 5.240874767303467\n",
      "                              Jsc = 20.2265567779541\n",
      "                              FF = 43.94428253173828\n",
      "Finished epoch  463\n",
      "On epoch  464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 42.28242874145508\n",
      "                              Voc = 5.170574188232422\n",
      "                              Jsc = 20.12693214416504\n",
      "                              FF = 44.21786117553711\n",
      "Finished epoch  464\n",
      "On epoch  465\n",
      "Total Epoch Testing MAPE: PCE = 42.37199783325195\n",
      "                              Voc = 5.740467548370361\n",
      "                              Jsc = 20.147661209106445\n",
      "                              FF = 45.05707550048828\n",
      "Finished epoch  465\n",
      "On epoch  466\n",
      "Total Epoch Testing MAPE: PCE = 41.06136703491211\n",
      "                              Voc = 5.88829231262207\n",
      "                              Jsc = 20.12144660949707\n",
      "                              FF = 44.76338577270508\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 40.61560821533203\n",
      "                              Voc = 5.825067520141602\n",
      "                              Jsc = 20.134397506713867\n",
      "                              FF = 44.603389739990234\n",
      "Finished epoch  467\n",
      "On epoch  468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 40.297176361083984\n",
      "                              Voc = 5.820196151733398\n",
      "                              Jsc = 20.10250473022461\n",
      "                              FF = 43.87596893310547\n",
      "Finished epoch  468\n",
      "On epoch  469\n",
      "Total Epoch Testing MAPE: PCE = 41.382938385009766\n",
      "                              Voc = 5.815539360046387\n",
      "                              Jsc = 20.05605125427246\n",
      "                              FF = 44.341529846191406\n",
      "Finished epoch  469\n",
      "On epoch  470\n",
      "Total Epoch Testing MAPE: PCE = 41.52989196777344\n",
      "                              Voc = 5.6709089279174805\n",
      "                              Jsc = 20.141738891601562\n",
      "                              FF = 45.771236419677734\n",
      "Finished epoch  470\n",
      "On epoch  471\n",
      "Total Epoch Testing MAPE: PCE = 41.91543960571289\n",
      "                              Voc = 5.57262659072876\n",
      "                              Jsc = 20.182397842407227\n",
      "                              FF = 46.361358642578125\n",
      "Finished epoch  471\n",
      "On epoch  472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 41.73515319824219\n",
      "                              Voc = 5.5365824699401855\n",
      "                              Jsc = 20.22420883178711\n",
      "                              FF = 46.3984260559082\n",
      "Finished epoch  472\n",
      "On epoch  473\n",
      "Total Epoch Testing MAPE: PCE = 42.27953338623047\n",
      "                              Voc = 5.522744178771973\n",
      "                              Jsc = 20.134220123291016\n",
      "                              FF = 45.8213996887207\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 43.12407302856445\n",
      "                              Voc = 5.434872150421143\n",
      "                              Jsc = 20.042007446289062\n",
      "                              FF = 45.73790740966797\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 43.065364837646484\n",
      "                              Voc = 5.134850025177002\n",
      "                              Jsc = 20.158668518066406\n",
      "                              FF = 45.43019104003906\n",
      "Finished epoch  475\n",
      "On epoch  476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 41.136226654052734\n",
      "                              Voc = 4.993379592895508\n",
      "                              Jsc = 20.23050880432129\n",
      "                              FF = 45.34043502807617\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 40.80792236328125\n",
      "                              Voc = 4.811839580535889\n",
      "                              Jsc = 20.27128791809082\n",
      "                              FF = 45.524513244628906\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 39.68072509765625\n",
      "                              Voc = 5.099175930023193\n",
      "                              Jsc = 20.24766731262207\n",
      "                              FF = 44.9810791015625\n",
      "Finished epoch  478\n",
      "On epoch  479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.2716064453125\n",
      "                              Voc = 4.931189060211182\n",
      "                              Jsc = 20.292394638061523\n",
      "                              FF = 45.21735382080078\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 41.07355880737305\n",
      "                              Voc = 4.763050556182861\n",
      "                              Jsc = 20.288835525512695\n",
      "                              FF = 45.06690979003906\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 39.63468551635742\n",
      "                              Voc = 4.59840726852417\n",
      "                              Jsc = 20.26076316833496\n",
      "                              FF = 43.84467315673828\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 39.7375602722168\n",
      "                              Voc = 4.422830581665039\n",
      "                              Jsc = 20.246583938598633\n",
      "                              FF = 44.25616455078125\n",
      "Finished epoch  482\n",
      "On epoch  483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 40.12960433959961\n",
      "                              Voc = 4.271913528442383\n",
      "                              Jsc = 20.21249771118164\n",
      "                              FF = 44.59288024902344\n",
      "Finished epoch  483\n",
      "On epoch  484\n",
      "Total Epoch Testing MAPE: PCE = 39.07960510253906\n",
      "                              Voc = 4.2375311851501465\n",
      "                              Jsc = 20.175283432006836\n",
      "                              FF = 44.95307922363281\n",
      "Finished epoch  484\n",
      "On epoch  485\n",
      "Total Epoch Testing MAPE: PCE = 39.740997314453125\n",
      "                              Voc = 4.21270751953125\n",
      "                              Jsc = 20.123037338256836\n",
      "                              FF = 45.45310974121094\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 40.67416763305664\n",
      "                              Voc = 4.187635898590088\n",
      "                              Jsc = 20.214479446411133\n",
      "                              FF = 47.117103576660156\n",
      "Finished epoch  486\n",
      "On epoch  487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 40.89738845825195\n",
      "                              Voc = 4.158349514007568\n",
      "                              Jsc = 20.210962295532227\n",
      "                              FF = 47.522483825683594\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 38.598960876464844\n",
      "                              Voc = 4.116547107696533\n",
      "                              Jsc = 20.19824981689453\n",
      "                              FF = 47.74536895751953\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 39.694366455078125\n",
      "                              Voc = 4.143028736114502\n",
      "                              Jsc = 20.16803550720215\n",
      "                              FF = 47.84891891479492\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 38.64750671386719\n",
      "                              Voc = 4.113759994506836\n",
      "                              Jsc = 20.170536041259766\n",
      "                              FF = 47.9744987487793\n",
      "Finished epoch  490\n",
      "On epoch  491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.47045135498047\n",
      "                              Voc = 4.089664459228516\n",
      "                              Jsc = 20.215600967407227\n",
      "                              FF = 47.33407974243164\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 36.91191101074219\n",
      "                              Voc = 4.086404323577881\n",
      "                              Jsc = 20.230201721191406\n",
      "                              FF = 46.66289520263672\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 37.663978576660156\n",
      "                              Voc = 4.0793657302856445\n",
      "                              Jsc = 20.188581466674805\n",
      "                              FF = 46.41215896606445\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 36.97233200073242\n",
      "                              Voc = 4.07050895690918\n",
      "                              Jsc = 20.204397201538086\n",
      "                              FF = 46.34366989135742\n",
      "Finished epoch  494\n",
      "On epoch  495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 36.52779006958008\n",
      "                              Voc = 4.02482795715332\n",
      "                              Jsc = 20.31241798400879\n",
      "                              FF = 45.51503372192383\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 36.82646179199219\n",
      "                              Voc = 4.025006294250488\n",
      "                              Jsc = 20.350357055664062\n",
      "                              FF = 44.398719787597656\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 37.185157775878906\n",
      "                              Voc = 4.0250420570373535\n",
      "                              Jsc = 20.40607452392578\n",
      "                              FF = 44.62298583984375\n",
      "Finished epoch  497\n",
      "On epoch  498\n",
      "Total Epoch Testing MAPE: PCE = 37.84184265136719\n",
      "                              Voc = 4.0435471534729\n",
      "                              Jsc = 20.40547752380371\n",
      "                              FF = 44.68561935424805\n",
      "Finished epoch  498\n",
      "On epoch  499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.05561447143555\n",
      "                              Voc = 4.037726879119873\n",
      "                              Jsc = 20.348413467407227\n",
      "                              FF = 44.66484832763672\n",
      "Finished epoch  499\n",
      "Fold # 2\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 127.1121597290039\n",
      "                              Voc = 72.3319320678711\n",
      "                              Jsc = 505.74688720703125\n",
      "                              FF = 58.50895690917969\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 102.18635559082031\n",
      "                              Voc = 61.13083267211914\n",
      "                              Jsc = 410.2769470214844\n",
      "                              FF = 51.85251998901367\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 77.94038391113281\n",
      "                              Voc = 47.14910888671875\n",
      "                              Jsc = 334.8154296875\n",
      "                              FF = 46.51647186279297\n",
      "Finished epoch  2\n",
      "On epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.61510467529297\n",
      "                              Voc = 42.889862060546875\n",
      "                              Jsc = 282.6647644042969\n",
      "                              FF = 43.194984436035156\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 44.509071350097656\n",
      "                              Voc = 34.905338287353516\n",
      "                              Jsc = 239.59329223632812\n",
      "                              FF = 38.747833251953125\n",
      "Finished epoch  4\n",
      "On epoch  5\n",
      "Total Epoch Testing MAPE: PCE = 32.88905715942383\n",
      "                              Voc = 27.83539581298828\n",
      "                              Jsc = 208.3497772216797\n",
      "                              FF = 37.00007247924805\n",
      "Finished epoch  5\n",
      "On epoch  6\n",
      "Total Epoch Testing MAPE: PCE = 27.489990234375\n",
      "                              Voc = 21.785245895385742\n",
      "                              Jsc = 182.20921325683594\n",
      "                              FF = 32.745216369628906\n",
      "Finished epoch  6\n",
      "On epoch  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 22.468032836914062\n",
      "                              Voc = 15.93502426147461\n",
      "                              Jsc = 161.4827880859375\n",
      "                              FF = 29.278873443603516\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 20.017608642578125\n",
      "                              Voc = 11.72909164428711\n",
      "                              Jsc = 143.68423461914062\n",
      "                              FF = 24.713228225708008\n",
      "Finished epoch  8\n",
      "On epoch  9\n",
      "Total Epoch Testing MAPE: PCE = 18.381956100463867\n",
      "                              Voc = 9.399761199951172\n",
      "                              Jsc = 128.0264434814453\n",
      "                              FF = 21.721071243286133\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 15.764670372009277\n",
      "                              Voc = 7.643867492675781\n",
      "                              Jsc = 114.09635925292969\n",
      "                              FF = 18.95315933227539\n",
      "Finished epoch  10\n",
      "On epoch  11\n",
      "Total Epoch Testing MAPE: PCE = 15.098682403564453\n",
      "                              Voc = 6.635149002075195\n",
      "                              Jsc = 104.10957336425781\n",
      "                              FF = 17.110984802246094\n",
      "Finished epoch  11\n",
      "On epoch  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.091082572937012\n",
      "                              Voc = 7.009552955627441\n",
      "                              Jsc = 97.21588134765625\n",
      "                              FF = 16.487571716308594\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 11.165849685668945\n",
      "                              Voc = 7.2360405921936035\n",
      "                              Jsc = 91.4532699584961\n",
      "                              FF = 15.752704620361328\n",
      "Finished epoch  13\n",
      "On epoch  14\n",
      "Total Epoch Testing MAPE: PCE = 10.488106727600098\n",
      "                              Voc = 7.6228156089782715\n",
      "                              Jsc = 85.93413543701172\n",
      "                              FF = 15.219369888305664\n",
      "Finished epoch  14\n",
      "On epoch  15\n",
      "Total Epoch Testing MAPE: PCE = 10.032812118530273\n",
      "                              Voc = 8.181378364562988\n",
      "                              Jsc = 81.86115264892578\n",
      "                              FF = 13.860919952392578\n",
      "Finished epoch  15\n",
      "On epoch  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 9.530566215515137\n",
      "                              Voc = 8.375052452087402\n",
      "                              Jsc = 77.38118743896484\n",
      "                              FF = 13.838529586791992\n",
      "Finished epoch  16\n",
      "On epoch  17\n",
      "Total Epoch Testing MAPE: PCE = 10.737363815307617\n",
      "                              Voc = 8.950498580932617\n",
      "                              Jsc = 73.92972564697266\n",
      "                              FF = 13.507548332214355\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 13.170930862426758\n",
      "                              Voc = 8.565731048583984\n",
      "                              Jsc = 70.82585144042969\n",
      "                              FF = 13.230846405029297\n",
      "Finished epoch  18\n",
      "On epoch  19\n",
      "Total Epoch Testing MAPE: PCE = 15.347578048706055\n",
      "                              Voc = 8.345027923583984\n",
      "                              Jsc = 67.41645812988281\n",
      "                              FF = 12.961925506591797\n",
      "Finished epoch  19\n",
      "On epoch  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 17.95792007446289\n",
      "                              Voc = 7.889702320098877\n",
      "                              Jsc = 64.95864868164062\n",
      "                              FF = 12.92225456237793\n",
      "Finished epoch  20\n",
      "On epoch  21\n",
      "Total Epoch Testing MAPE: PCE = 20.498865127563477\n",
      "                              Voc = 7.893653392791748\n",
      "                              Jsc = 63.329620361328125\n",
      "                              FF = 12.81330394744873\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 22.094507217407227\n",
      "                              Voc = 8.027766227722168\n",
      "                              Jsc = 62.46107864379883\n",
      "                              FF = 13.09335994720459\n",
      "Finished epoch  22\n",
      "On epoch  23\n",
      "Total Epoch Testing MAPE: PCE = 23.247617721557617\n",
      "                              Voc = 8.327727317810059\n",
      "                              Jsc = 61.78455352783203\n",
      "                              FF = 13.128763198852539\n",
      "Finished epoch  23\n",
      "On epoch  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 24.907211303710938\n",
      "                              Voc = 8.496217727661133\n",
      "                              Jsc = 60.113704681396484\n",
      "                              FF = 13.679657936096191\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 25.06068992614746\n",
      "                              Voc = 8.820792198181152\n",
      "                              Jsc = 58.79227066040039\n",
      "                              FF = 14.349250793457031\n",
      "Finished epoch  25\n",
      "On epoch  26\n",
      "Total Epoch Testing MAPE: PCE = 25.899919509887695\n",
      "                              Voc = 9.32752513885498\n",
      "                              Jsc = 57.620121002197266\n",
      "                              FF = 15.633766174316406\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 27.22041893005371\n",
      "                              Voc = 10.071736335754395\n",
      "                              Jsc = 55.37632751464844\n",
      "                              FF = 16.30760955810547\n",
      "Finished epoch  27\n",
      "On epoch  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 28.01336097717285\n",
      "                              Voc = 10.657488822937012\n",
      "                              Jsc = 53.987796783447266\n",
      "                              FF = 17.010250091552734\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 28.39691925048828\n",
      "                              Voc = 11.236381530761719\n",
      "                              Jsc = 52.64493179321289\n",
      "                              FF = 17.805835723876953\n",
      "Finished epoch  29\n",
      "On epoch  30\n",
      "Total Epoch Testing MAPE: PCE = 29.66867446899414\n",
      "                              Voc = 12.69101333618164\n",
      "                              Jsc = 50.86522674560547\n",
      "                              FF = 18.680723190307617\n",
      "Finished epoch  30\n",
      "On epoch  31\n",
      "Total Epoch Testing MAPE: PCE = 30.08705711364746\n",
      "                              Voc = 14.46055793762207\n",
      "                              Jsc = 49.62307357788086\n",
      "                              FF = 19.842893600463867\n",
      "Finished epoch  31\n",
      "On epoch  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 30.067583084106445\n",
      "                              Voc = 14.542052268981934\n",
      "                              Jsc = 48.792667388916016\n",
      "                              FF = 20.42243194580078\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 30.47325897216797\n",
      "                              Voc = 15.300719261169434\n",
      "                              Jsc = 48.10169219970703\n",
      "                              FF = 21.037160873413086\n",
      "Finished epoch  33\n",
      "On epoch  34\n",
      "Total Epoch Testing MAPE: PCE = 30.356210708618164\n",
      "                              Voc = 15.636316299438477\n",
      "                              Jsc = 47.749725341796875\n",
      "                              FF = 23.166587829589844\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 29.960281372070312\n",
      "                              Voc = 16.037086486816406\n",
      "                              Jsc = 47.02232360839844\n",
      "                              FF = 23.939775466918945\n",
      "Finished epoch  35\n",
      "On epoch  36\n",
      "Total Epoch Testing MAPE: PCE = 30.806028366088867\n",
      "                              Voc = 16.591968536376953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Jsc = 45.68484115600586\n",
      "                              FF = 24.377017974853516\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 30.59473991394043\n",
      "                              Voc = 17.004680633544922\n",
      "                              Jsc = 44.221134185791016\n",
      "                              FF = 25.658967971801758\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 29.868988037109375\n",
      "                              Voc = 16.719337463378906\n",
      "                              Jsc = 43.385536193847656\n",
      "                              FF = 26.269880294799805\n",
      "Finished epoch  38\n",
      "On epoch  39\n",
      "Total Epoch Testing MAPE: PCE = 29.780799865722656\n",
      "                              Voc = 17.254560470581055\n",
      "                              Jsc = 42.915992736816406\n",
      "                              FF = 27.121376037597656\n",
      "Finished epoch  39\n",
      "On epoch  40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 30.356403350830078\n",
      "                              Voc = 18.833858489990234\n",
      "                              Jsc = 43.31308364868164\n",
      "                              FF = 27.9001407623291\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 30.393505096435547\n",
      "                              Voc = 19.224756240844727\n",
      "                              Jsc = 43.43464279174805\n",
      "                              FF = 29.34223747253418\n",
      "Finished epoch  41\n",
      "On epoch  42\n",
      "Total Epoch Testing MAPE: PCE = 30.19438934326172\n",
      "                              Voc = 20.239652633666992\n",
      "                              Jsc = 43.22203826904297\n",
      "                              FF = 30.76205825805664\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 29.872026443481445\n",
      "                              Voc = 20.4414005279541\n",
      "                              Jsc = 42.97089385986328\n",
      "                              FF = 30.653026580810547\n",
      "Finished epoch  43\n",
      "On epoch  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 30.49496078491211\n",
      "                              Voc = 21.75297737121582\n",
      "                              Jsc = 42.341190338134766\n",
      "                              FF = 30.926258087158203\n",
      "Finished epoch  44\n",
      "On epoch  45\n",
      "Total Epoch Testing MAPE: PCE = 31.117231369018555\n",
      "                              Voc = 22.13075828552246\n",
      "                              Jsc = 42.16551208496094\n",
      "                              FF = 31.778121948242188\n",
      "Finished epoch  45\n",
      "On epoch  46\n",
      "Total Epoch Testing MAPE: PCE = 30.931568145751953\n",
      "                              Voc = 22.763269424438477\n",
      "                              Jsc = 42.75749969482422\n",
      "                              FF = 32.94562530517578\n",
      "Finished epoch  46\n",
      "On epoch  47\n",
      "Total Epoch Testing MAPE: PCE = 30.542922973632812\n",
      "                              Voc = 23.193498611450195\n",
      "                              Jsc = 42.83478927612305\n",
      "                              FF = 34.186153411865234\n",
      "Finished epoch  47\n",
      "On epoch  48\n",
      "Total Epoch Testing MAPE: PCE = 30.528297424316406\n",
      "                              Voc = 23.720924377441406\n",
      "                              Jsc = 42.28388214111328\n",
      "                              FF = 35.05644989013672\n",
      "Finished epoch  48\n",
      "On epoch  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 30.056425094604492\n",
      "                              Voc = 24.185943603515625\n",
      "                              Jsc = 42.054691314697266\n",
      "                              FF = 35.60020065307617\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 29.73317527770996\n",
      "                              Voc = 24.8236083984375\n",
      "                              Jsc = 41.4333381652832\n",
      "                              FF = 37.35206604003906\n",
      "Finished epoch  50\n",
      "On epoch  51\n",
      "Total Epoch Testing MAPE: PCE = 28.529199600219727\n",
      "                              Voc = 24.926799774169922\n",
      "                              Jsc = 41.200927734375\n",
      "                              FF = 38.80796432495117\n",
      "Finished epoch  51\n",
      "On epoch  52\n",
      "Total Epoch Testing MAPE: PCE = 28.08282470703125\n",
      "                              Voc = 25.240690231323242\n",
      "                              Jsc = 39.589195251464844\n",
      "                              FF = 39.81303024291992\n",
      "Finished epoch  52\n",
      "On epoch  53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 27.84653091430664\n",
      "                              Voc = 26.453340530395508\n",
      "                              Jsc = 39.05567932128906\n",
      "                              FF = 39.21010971069336\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 26.78028678894043\n",
      "                              Voc = 26.770517349243164\n",
      "                              Jsc = 37.80793380737305\n",
      "                              FF = 40.856719970703125\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 27.133195877075195\n",
      "                              Voc = 27.15452003479004\n",
      "                              Jsc = 37.582061767578125\n",
      "                              FF = 42.000770568847656\n",
      "Finished epoch  55\n",
      "On epoch  56\n",
      "Total Epoch Testing MAPE: PCE = 26.460002899169922\n",
      "                              Voc = 27.288171768188477\n",
      "                              Jsc = 36.88731002807617\n",
      "                              FF = 42.48843765258789\n",
      "Finished epoch  56\n",
      "On epoch  57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 25.79616355895996\n",
      "                              Voc = 27.882829666137695\n",
      "                              Jsc = 36.8159294128418\n",
      "                              FF = 42.63849639892578\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 26.053836822509766\n",
      "                              Voc = 27.159908294677734\n",
      "                              Jsc = 36.90501022338867\n",
      "                              FF = 42.40070343017578\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 25.336456298828125\n",
      "                              Voc = 28.006237030029297\n",
      "                              Jsc = 36.2039909362793\n",
      "                              FF = 41.75193786621094\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 24.394941329956055\n",
      "                              Voc = 29.276365280151367\n",
      "                              Jsc = 36.7557258605957\n",
      "                              FF = 41.77968978881836\n",
      "Finished epoch  60\n",
      "On epoch  61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 23.670269012451172\n",
      "                              Voc = 30.043092727661133\n",
      "                              Jsc = 37.41233825683594\n",
      "                              FF = 41.016475677490234\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 22.712352752685547\n",
      "                              Voc = 30.424251556396484\n",
      "                              Jsc = 37.78347396850586\n",
      "                              FF = 40.692481994628906\n",
      "Finished epoch  62\n",
      "On epoch  63\n",
      "Total Epoch Testing MAPE: PCE = 21.66353988647461\n",
      "                              Voc = 30.46118927001953\n",
      "                              Jsc = 38.09625244140625\n",
      "                              FF = 41.105133056640625\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 20.439239501953125\n",
      "                              Voc = 31.171688079833984\n",
      "                              Jsc = 38.340789794921875\n",
      "                              FF = 39.7529182434082\n",
      "Finished epoch  64\n",
      "On epoch  65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 20.150747299194336\n",
      "                              Voc = 31.92291831970215\n",
      "                              Jsc = 38.41981887817383\n",
      "                              FF = 39.06898880004883\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 19.532602310180664\n",
      "                              Voc = 31.94422721862793\n",
      "                              Jsc = 39.10953140258789\n",
      "                              FF = 39.12194061279297\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 19.05748176574707\n",
      "                              Voc = 31.743562698364258\n",
      "                              Jsc = 39.43350601196289\n",
      "                              FF = 38.44097137451172\n",
      "Finished epoch  67\n",
      "On epoch  68\n",
      "Total Epoch Testing MAPE: PCE = 18.70610809326172\n",
      "                              Voc = 32.002906799316406\n",
      "                              Jsc = 39.63346862792969\n",
      "                              FF = 36.68712615966797\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 18.683307647705078\n",
      "                              Voc = 32.63188934326172\n",
      "                              Jsc = 40.11004638671875\n",
      "                              FF = 35.947113037109375\n",
      "Finished epoch  69\n",
      "On epoch  70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 18.500272750854492\n",
      "                              Voc = 32.9034538269043\n",
      "                              Jsc = 40.81383514404297\n",
      "                              FF = 36.073123931884766\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 18.996065139770508\n",
      "                              Voc = 32.09564971923828\n",
      "                              Jsc = 41.038814544677734\n",
      "                              FF = 35.62568283081055\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 18.723556518554688\n",
      "                              Voc = 32.062496185302734\n",
      "                              Jsc = 40.26782989501953\n",
      "                              FF = 35.70136260986328\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 18.134212493896484\n",
      "                              Voc = 33.16080856323242\n",
      "                              Jsc = 40.584354400634766\n",
      "                              FF = 34.77836990356445\n",
      "Finished epoch  73\n",
      "On epoch  74\n",
      "Total Epoch Testing MAPE: PCE = 18.355649948120117\n",
      "                              Voc = 32.840084075927734\n",
      "                              Jsc = 38.87944030761719\n",
      "                              FF = 34.89069366455078\n",
      "Finished epoch  74\n",
      "On epoch  75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 18.06761932373047\n",
      "                              Voc = 33.2225341796875\n",
      "                              Jsc = 39.78288650512695\n",
      "                              FF = 34.98467254638672\n",
      "Finished epoch  75\n",
      "On epoch  76\n",
      "Total Epoch Testing MAPE: PCE = 18.537784576416016\n",
      "                              Voc = 32.7690544128418\n",
      "                              Jsc = 39.90036392211914\n",
      "                              FF = 34.675262451171875\n",
      "Finished epoch  76\n",
      "On epoch  77\n",
      "Total Epoch Testing MAPE: PCE = 18.845884323120117\n",
      "                              Voc = 33.20995330810547\n",
      "                              Jsc = 39.92375183105469\n",
      "                              FF = 34.96378707885742\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 18.71400260925293\n",
      "                              Voc = 33.7757682800293\n",
      "                              Jsc = 40.020042419433594\n",
      "                              FF = 34.299766540527344\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 17.892248153686523\n",
      "                              Voc = 34.024906158447266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Jsc = 39.590118408203125\n",
      "                              FF = 34.25260543823242\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 17.948129653930664\n",
      "                              Voc = 34.51815414428711\n",
      "                              Jsc = 39.088661193847656\n",
      "                              FF = 33.503204345703125\n",
      "Finished epoch  80\n",
      "On epoch  81\n",
      "Total Epoch Testing MAPE: PCE = 18.029911041259766\n",
      "                              Voc = 34.8315315246582\n",
      "                              Jsc = 39.12521743774414\n",
      "                              FF = 33.886131286621094\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 17.689857482910156\n",
      "                              Voc = 35.46471405029297\n",
      "                              Jsc = 39.16618347167969\n",
      "                              FF = 33.801727294921875\n",
      "Finished epoch  82\n",
      "On epoch  83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 17.345712661743164\n",
      "                              Voc = 35.40945816040039\n",
      "                              Jsc = 38.56988525390625\n",
      "                              FF = 34.626583099365234\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 17.36809730529785\n",
      "                              Voc = 35.75423812866211\n",
      "                              Jsc = 38.72370910644531\n",
      "                              FF = 34.446067810058594\n",
      "Finished epoch  84\n",
      "On epoch  85\n",
      "Total Epoch Testing MAPE: PCE = 16.84288787841797\n",
      "                              Voc = 35.80799102783203\n",
      "                              Jsc = 38.767486572265625\n",
      "                              FF = 33.308536529541016\n",
      "Finished epoch  85\n",
      "On epoch  86\n",
      "Total Epoch Testing MAPE: PCE = 16.762210845947266\n",
      "                              Voc = 35.453582763671875\n",
      "                              Jsc = 38.39650344848633\n",
      "                              FF = 33.018070220947266\n",
      "Finished epoch  86\n",
      "On epoch  87\n",
      "Total Epoch Testing MAPE: PCE = 16.285629272460938\n",
      "                              Voc = 35.860599517822266\n",
      "                              Jsc = 38.355648040771484\n",
      "                              FF = 33.643287658691406\n",
      "Finished epoch  87\n",
      "On epoch  88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 16.5654239654541\n",
      "                              Voc = 36.15247344970703\n",
      "                              Jsc = 38.20303726196289\n",
      "                              FF = 34.02008819580078\n",
      "Finished epoch  88\n",
      "On epoch  89\n",
      "Total Epoch Testing MAPE: PCE = 16.11185646057129\n",
      "                              Voc = 36.73859786987305\n",
      "                              Jsc = 37.797515869140625\n",
      "                              FF = 34.60843276977539\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 15.61909008026123\n",
      "                              Voc = 36.862056732177734\n",
      "                              Jsc = 37.483219146728516\n",
      "                              FF = 34.921756744384766\n",
      "Finished epoch  90\n",
      "On epoch  91\n",
      "Total Epoch Testing MAPE: PCE = 15.47510051727295\n",
      "                              Voc = 37.44514465332031\n",
      "                              Jsc = 36.773250579833984\n",
      "                              FF = 35.29242706298828\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 15.346256256103516\n",
      "                              Voc = 37.28874588012695\n",
      "                              Jsc = 37.12729263305664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              FF = 35.850521087646484\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 15.162168502807617\n",
      "                              Voc = 37.63282775878906\n",
      "                              Jsc = 36.24764633178711\n",
      "                              FF = 35.25768280029297\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 15.45279598236084\n",
      "                              Voc = 38.85504913330078\n",
      "                              Jsc = 36.071414947509766\n",
      "                              FF = 35.78229904174805\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 15.277987480163574\n",
      "                              Voc = 38.41102981567383\n",
      "                              Jsc = 36.33993911743164\n",
      "                              FF = 36.05931854248047\n",
      "Finished epoch  95\n",
      "On epoch  96\n",
      "Total Epoch Testing MAPE: PCE = 15.084226608276367\n",
      "                              Voc = 37.978965759277344\n",
      "                              Jsc = 35.95522689819336\n",
      "                              FF = 35.70479202270508\n",
      "Finished epoch  96\n",
      "On epoch  97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.866325378417969\n",
      "                              Voc = 38.069175720214844\n",
      "                              Jsc = 36.20120620727539\n",
      "                              FF = 35.775230407714844\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 14.786759376525879\n",
      "                              Voc = 38.84080123901367\n",
      "                              Jsc = 35.6473274230957\n",
      "                              FF = 35.8450927734375\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 14.734437942504883\n",
      "                              Voc = 39.40793991088867\n",
      "                              Jsc = 35.775123596191406\n",
      "                              FF = 35.02826690673828\n",
      "Finished epoch  99\n",
      "On epoch  100\n",
      "Total Epoch Testing MAPE: PCE = 14.615752220153809\n",
      "                              Voc = 39.2597541809082\n",
      "                              Jsc = 35.674007415771484\n",
      "                              FF = 34.34904861450195\n",
      "Finished epoch  100\n",
      "On epoch  101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.95383358001709\n",
      "                              Voc = 39.070701599121094\n",
      "                              Jsc = 35.48725128173828\n",
      "                              FF = 35.62205123901367\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 15.420858383178711\n",
      "                              Voc = 39.12017822265625\n",
      "                              Jsc = 35.345279693603516\n",
      "                              FF = 35.10954666137695\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 15.955984115600586\n",
      "                              Voc = 39.05577087402344\n",
      "                              Jsc = 34.81739807128906\n",
      "                              FF = 34.93193054199219\n",
      "Finished epoch  103\n",
      "On epoch  104\n",
      "Total Epoch Testing MAPE: PCE = 16.177976608276367\n",
      "                              Voc = 39.27176284790039\n",
      "                              Jsc = 34.53577423095703\n",
      "                              FF = 34.455665588378906\n",
      "Finished epoch  104\n",
      "On epoch  105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 16.043529510498047\n",
      "                              Voc = 38.812076568603516\n",
      "                              Jsc = 34.57991409301758\n",
      "                              FF = 34.410552978515625\n",
      "Finished epoch  105\n",
      "On epoch  106\n",
      "Total Epoch Testing MAPE: PCE = 16.09172821044922\n",
      "                              Voc = 38.819801330566406\n",
      "                              Jsc = 34.22615051269531\n",
      "                              FF = 34.49174880981445\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 15.508416175842285\n",
      "                              Voc = 39.06071472167969\n",
      "                              Jsc = 33.84286880493164\n",
      "                              FF = 34.997032165527344\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 15.663619041442871\n",
      "                              Voc = 39.36820983886719\n",
      "                              Jsc = 33.3812370300293\n",
      "                              FF = 35.132606506347656\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 15.720708847045898\n",
      "                              Voc = 39.1804084777832\n",
      "                              Jsc = 33.286312103271484\n",
      "                              FF = 34.094627380371094\n",
      "Finished epoch  109\n",
      "On epoch  110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.712091445922852\n",
      "                              Voc = 38.54286193847656\n",
      "                              Jsc = 33.82663345336914\n",
      "                              FF = 34.163265228271484\n",
      "Finished epoch  110\n",
      "On epoch  111\n",
      "Total Epoch Testing MAPE: PCE = 15.570127487182617\n",
      "                              Voc = 38.73714828491211\n",
      "                              Jsc = 33.084774017333984\n",
      "                              FF = 33.656776428222656\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 15.261533737182617\n",
      "                              Voc = 39.0874137878418\n",
      "                              Jsc = 32.8173942565918\n",
      "                              FF = 34.02250289916992\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 15.5092191696167\n",
      "                              Voc = 39.413780212402344\n",
      "                              Jsc = 32.57743835449219\n",
      "                              FF = 33.62158966064453\n",
      "Finished epoch  113\n",
      "On epoch  114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.474751472473145\n",
      "                              Voc = 39.134796142578125\n",
      "                              Jsc = 32.2368278503418\n",
      "                              FF = 33.0341682434082\n",
      "Finished epoch  114\n",
      "On epoch  115\n",
      "Total Epoch Testing MAPE: PCE = 15.435708999633789\n",
      "                              Voc = 39.400211334228516\n",
      "                              Jsc = 31.269994735717773\n",
      "                              FF = 32.9529914855957\n",
      "Finished epoch  115\n",
      "On epoch  116\n",
      "Total Epoch Testing MAPE: PCE = 15.077011108398438\n",
      "                              Voc = 40.01156997680664\n",
      "                              Jsc = 31.256067276000977\n",
      "                              FF = 32.3817024230957\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 14.523542404174805\n",
      "                              Voc = 40.18853759765625\n",
      "                              Jsc = 31.600666046142578\n",
      "                              FF = 31.978872299194336\n",
      "Finished epoch  117\n",
      "On epoch  118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.333091735839844\n",
      "                              Voc = 40.046634674072266\n",
      "                              Jsc = 31.233062744140625\n",
      "                              FF = 31.885265350341797\n",
      "Finished epoch  118\n",
      "On epoch  119\n",
      "Total Epoch Testing MAPE: PCE = 15.282308578491211\n",
      "                              Voc = 40.033626556396484\n",
      "                              Jsc = 30.85563850402832\n",
      "                              FF = 31.797033309936523\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 15.096796035766602\n",
      "                              Voc = 40.63326644897461\n",
      "                              Jsc = 30.84893798828125\n",
      "                              FF = 32.1286735534668\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 15.563072204589844\n",
      "                              Voc = 41.361106872558594\n",
      "                              Jsc = 30.575817108154297\n",
      "                              FF = 31.89653778076172\n",
      "Finished epoch  121\n",
      "On epoch  122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.810516357421875\n",
      "                              Voc = 42.06226348876953\n",
      "                              Jsc = 31.138532638549805\n",
      "                              FF = 31.466611862182617\n",
      "Finished epoch  122\n",
      "On epoch  123\n",
      "Total Epoch Testing MAPE: PCE = 15.61886978149414\n",
      "                              Voc = 41.8004035949707\n",
      "                              Jsc = 31.398954391479492\n",
      "                              FF = 31.66887092590332\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 15.153135299682617\n",
      "                              Voc = 41.336421966552734\n",
      "                              Jsc = 31.810945510864258\n",
      "                              FF = 30.85845947265625\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 14.786214828491211\n",
      "                              Voc = 40.57814025878906\n",
      "                              Jsc = 31.75412368774414\n",
      "                              FF = 30.95686149597168\n",
      "Finished epoch  125\n",
      "On epoch  126\n",
      "Total Epoch Testing MAPE: PCE = 15.06336784362793\n",
      "                              Voc = 41.21675491333008\n",
      "                              Jsc = 31.823354721069336\n",
      "                              FF = 30.446252822875977\n",
      "Finished epoch  126\n",
      "On epoch  127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.19520092010498\n",
      "                              Voc = 41.0594596862793\n",
      "                              Jsc = 32.34990310668945\n",
      "                              FF = 30.111562728881836\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 15.055088996887207\n",
      "                              Voc = 40.04217529296875\n",
      "                              Jsc = 31.768781661987305\n",
      "                              FF = 29.35637855529785\n",
      "Finished epoch  128\n",
      "On epoch  129\n",
      "Total Epoch Testing MAPE: PCE = 15.08139419555664\n",
      "                              Voc = 39.913150787353516\n",
      "                              Jsc = 30.89400863647461\n",
      "                              FF = 28.505765914916992\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 14.78271484375\n",
      "                              Voc = 38.76933670043945\n",
      "                              Jsc = 30.643722534179688\n",
      "                              FF = 27.76087188720703\n",
      "Finished epoch  130\n",
      "On epoch  131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.885656356811523\n",
      "                              Voc = 38.30695343017578\n",
      "                              Jsc = 30.47240447998047\n",
      "                              FF = 27.48447036743164\n",
      "Finished epoch  131\n",
      "On epoch  132\n",
      "Total Epoch Testing MAPE: PCE = 14.113758087158203\n",
      "                              Voc = 37.43173599243164\n",
      "                              Jsc = 29.991859436035156\n",
      "                              FF = 27.981657028198242\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 14.056070327758789\n",
      "                              Voc = 37.6384391784668\n",
      "                              Jsc = 29.624393463134766\n",
      "                              FF = 28.261167526245117\n",
      "Finished epoch  133\n",
      "On epoch  134\n",
      "Total Epoch Testing MAPE: PCE = 14.10177993774414\n",
      "                              Voc = 37.62062072753906\n",
      "                              Jsc = 29.444746017456055\n",
      "                              FF = 27.97865867614746\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 13.997490882873535\n",
      "                              Voc = 38.57273864746094\n",
      "                              Jsc = 29.798316955566406\n",
      "                              FF = 28.45325469970703\n",
      "Finished epoch  135\n",
      "On epoch  136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.789664268493652\n",
      "                              Voc = 39.39012908935547\n",
      "                              Jsc = 29.518260955810547\n",
      "                              FF = 28.49372673034668\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 13.875062942504883\n",
      "                              Voc = 39.057106018066406\n",
      "                              Jsc = 28.950410842895508\n",
      "                              FF = 28.46766471862793\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 13.811163902282715\n",
      "                              Voc = 38.46809387207031\n",
      "                              Jsc = 29.681665420532227\n",
      "                              FF = 29.440340042114258\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 13.866159439086914\n",
      "                              Voc = 38.67272186279297\n",
      "                              Jsc = 30.08636474609375\n",
      "                              FF = 29.270299911499023\n",
      "Finished epoch  139\n",
      "On epoch  140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.704630851745605\n",
      "                              Voc = 38.353302001953125\n",
      "                              Jsc = 29.529857635498047\n",
      "                              FF = 29.623735427856445\n",
      "Finished epoch  140\n",
      "On epoch  141\n",
      "Total Epoch Testing MAPE: PCE = 13.547100067138672\n",
      "                              Voc = 38.18300247192383\n",
      "                              Jsc = 28.59263038635254\n",
      "                              FF = 29.995161056518555\n",
      "Finished epoch  141\n",
      "On epoch  142\n",
      "Total Epoch Testing MAPE: PCE = 13.724109649658203\n",
      "                              Voc = 38.22616195678711\n",
      "                              Jsc = 28.43413734436035\n",
      "                              FF = 29.675493240356445\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 13.58447265625\n",
      "                              Voc = 37.82398223876953\n",
      "                              Jsc = 28.41838836669922\n",
      "                              FF = 29.56907081604004\n",
      "Finished epoch  143\n",
      "On epoch  144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.25529670715332\n",
      "                              Voc = 38.17529296875\n",
      "                              Jsc = 28.410303115844727\n",
      "                              FF = 29.470476150512695\n",
      "Finished epoch  144\n",
      "On epoch  145\n",
      "Total Epoch Testing MAPE: PCE = 13.185986518859863\n",
      "                              Voc = 39.00508499145508\n",
      "                              Jsc = 28.39293098449707\n",
      "                              FF = 30.059263229370117\n",
      "Finished epoch  145\n",
      "On epoch  146\n",
      "Total Epoch Testing MAPE: PCE = 13.207433700561523\n",
      "                              Voc = 38.04355239868164\n",
      "                              Jsc = 28.29737091064453\n",
      "                              FF = 30.061786651611328\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 13.258931159973145\n",
      "                              Voc = 37.66851806640625\n",
      "                              Jsc = 28.210111618041992\n",
      "                              FF = 30.109560012817383\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 13.557807922363281\n",
      "                              Voc = 38.30617904663086\n",
      "                              Jsc = 28.497072219848633\n",
      "                              FF = 29.61819076538086\n",
      "Finished epoch  148\n",
      "On epoch  149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.474093437194824\n",
      "                              Voc = 38.37022399902344\n",
      "                              Jsc = 28.615528106689453\n",
      "                              FF = 29.678455352783203\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 13.258960723876953\n",
      "                              Voc = 38.2910270690918\n",
      "                              Jsc = 28.941890716552734\n",
      "                              FF = 30.69184112548828\n",
      "Finished epoch  150\n",
      "On epoch  151\n",
      "Total Epoch Testing MAPE: PCE = 13.480429649353027\n",
      "                              Voc = 37.72552490234375\n",
      "                              Jsc = 28.7141170501709\n",
      "                              FF = 30.344575881958008\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 13.391457557678223\n",
      "                              Voc = 38.11965560913086\n",
      "                              Jsc = 28.249042510986328\n",
      "                              FF = 30.726524353027344\n",
      "Finished epoch  152\n",
      "On epoch  153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.518482208251953\n",
      "                              Voc = 37.790748596191406\n",
      "                              Jsc = 27.68310546875\n",
      "                              FF = 30.664548873901367\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 13.881458282470703\n",
      "                              Voc = 38.10584259033203\n",
      "                              Jsc = 27.444988250732422\n",
      "                              FF = 30.927631378173828\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 14.07929515838623\n",
      "                              Voc = 38.36200714111328\n",
      "                              Jsc = 26.877239227294922\n",
      "                              FF = 30.67546272277832\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 14.479530334472656\n",
      "                              Voc = 38.67110824584961\n",
      "                              Jsc = 26.875782012939453\n",
      "                              FF = 30.131351470947266\n",
      "Finished epoch  156\n",
      "On epoch  157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.407003402709961\n",
      "                              Voc = 38.38272476196289\n",
      "                              Jsc = 26.087669372558594\n",
      "                              FF = 29.88155746459961\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 14.812141418457031\n",
      "                              Voc = 38.398475646972656\n",
      "                              Jsc = 26.234071731567383\n",
      "                              FF = 29.660324096679688\n",
      "Finished epoch  158\n",
      "On epoch  159\n",
      "Total Epoch Testing MAPE: PCE = 15.050021171569824\n",
      "                              Voc = 39.382266998291016\n",
      "                              Jsc = 25.671472549438477\n",
      "                              FF = 30.043701171875\n",
      "Finished epoch  159\n",
      "On epoch  160\n",
      "Total Epoch Testing MAPE: PCE = 15.180593490600586\n",
      "                              Voc = 38.713409423828125\n",
      "                              Jsc = 25.87161636352539\n",
      "                              FF = 30.102951049804688\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 15.475809097290039\n",
      "                              Voc = 39.000244140625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Jsc = 25.4219913482666\n",
      "                              FF = 30.53949546813965\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 15.571989059448242\n",
      "                              Voc = 39.71887969970703\n",
      "                              Jsc = 25.38614845275879\n",
      "                              FF = 30.606037139892578\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 15.553081512451172\n",
      "                              Voc = 40.1732063293457\n",
      "                              Jsc = 25.241500854492188\n",
      "                              FF = 30.32575225830078\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 16.1900691986084\n",
      "                              Voc = 40.24528503417969\n",
      "                              Jsc = 24.741371154785156\n",
      "                              FF = 30.75969886779785\n",
      "Finished epoch  164\n",
      "On epoch  165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.692262649536133\n",
      "                              Voc = 39.99980545043945\n",
      "                              Jsc = 24.566730499267578\n",
      "                              FF = 30.961257934570312\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 15.615081787109375\n",
      "                              Voc = 40.23889923095703\n",
      "                              Jsc = 25.381916046142578\n",
      "                              FF = 31.17814064025879\n",
      "Finished epoch  166\n",
      "On epoch  167\n",
      "Total Epoch Testing MAPE: PCE = 15.50763988494873\n",
      "                              Voc = 40.51820755004883\n",
      "                              Jsc = 25.089466094970703\n",
      "                              FF = 30.75798797607422\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 15.578564643859863\n",
      "                              Voc = 40.74033737182617\n",
      "                              Jsc = 24.78792953491211\n",
      "                              FF = 30.735584259033203\n",
      "Finished epoch  168\n",
      "On epoch  169\n",
      "Total Epoch Testing MAPE: PCE = 15.402481079101562\n",
      "                              Voc = 41.10460662841797\n",
      "                              Jsc = 24.125436782836914\n",
      "                              FF = 30.49054718017578\n",
      "Finished epoch  169\n",
      "On epoch  170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.67265510559082\n",
      "                              Voc = 41.58065414428711\n",
      "                              Jsc = 23.98065757751465\n",
      "                              FF = 30.256834030151367\n",
      "Finished epoch  170\n",
      "On epoch  171\n",
      "Total Epoch Testing MAPE: PCE = 15.640239715576172\n",
      "                              Voc = 41.70250701904297\n",
      "                              Jsc = 23.106714248657227\n",
      "                              FF = 31.05608558654785\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 15.624444007873535\n",
      "                              Voc = 42.1154899597168\n",
      "                              Jsc = 23.114452362060547\n",
      "                              FF = 31.102672576904297\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 15.637872695922852\n",
      "                              Voc = 41.711395263671875\n",
      "                              Jsc = 22.765972137451172\n",
      "                              FF = 31.26897430419922\n",
      "Finished epoch  173\n",
      "On epoch  174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.562847137451172\n",
      "                              Voc = 42.241432189941406\n",
      "                              Jsc = 22.010845184326172\n",
      "                              FF = 31.316614151000977\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 15.717487335205078\n",
      "                              Voc = 42.55970001220703\n",
      "                              Jsc = 21.62404441833496\n",
      "                              FF = 31.45233726501465\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 15.49236011505127\n",
      "                              Voc = 42.30496597290039\n",
      "                              Jsc = 21.311038970947266\n",
      "                              FF = 31.079357147216797\n",
      "Finished epoch  176\n",
      "On epoch  177\n",
      "Total Epoch Testing MAPE: PCE = 15.336000442504883\n",
      "                              Voc = 42.56721496582031\n",
      "                              Jsc = 21.484983444213867\n",
      "                              FF = 30.404653549194336\n",
      "Finished epoch  177\n",
      "On epoch  178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.27723217010498\n",
      "                              Voc = 42.0113639831543\n",
      "                              Jsc = 20.785293579101562\n",
      "                              FF = 30.04922866821289\n",
      "Finished epoch  178\n",
      "On epoch  179\n",
      "Total Epoch Testing MAPE: PCE = 15.348349571228027\n",
      "                              Voc = 41.59541702270508\n",
      "                              Jsc = 20.106882095336914\n",
      "                              FF = 29.968408584594727\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 15.366019248962402\n",
      "                              Voc = 40.800811767578125\n",
      "                              Jsc = 19.706981658935547\n",
      "                              FF = 30.031145095825195\n",
      "Finished epoch  180\n",
      "On epoch  181\n",
      "Total Epoch Testing MAPE: PCE = 15.100127220153809\n",
      "                              Voc = 41.50808334350586\n",
      "                              Jsc = 19.963905334472656\n",
      "                              FF = 29.741182327270508\n",
      "Finished epoch  181\n",
      "On epoch  182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.10821533203125\n",
      "                              Voc = 41.66633605957031\n",
      "                              Jsc = 20.08146858215332\n",
      "                              FF = 29.670806884765625\n",
      "Finished epoch  182\n",
      "On epoch  183\n",
      "Total Epoch Testing MAPE: PCE = 14.90896224975586\n",
      "                              Voc = 41.71710968017578\n",
      "                              Jsc = 20.528226852416992\n",
      "                              FF = 29.34688377380371\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 14.668642044067383\n",
      "                              Voc = 41.488956451416016\n",
      "                              Jsc = 20.573196411132812\n",
      "                              FF = 29.393890380859375\n",
      "Finished epoch  184\n",
      "On epoch  185\n",
      "Total Epoch Testing MAPE: PCE = 14.592795372009277\n",
      "                              Voc = 41.67131423950195\n",
      "                              Jsc = 20.76068115234375\n",
      "                              FF = 29.751068115234375\n",
      "Finished epoch  185\n",
      "On epoch  186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.326292037963867\n",
      "                              Voc = 42.18388748168945\n",
      "                              Jsc = 20.26082420349121\n",
      "                              FF = 29.616201400756836\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 14.49554443359375\n",
      "                              Voc = 42.17401885986328\n",
      "                              Jsc = 20.011274337768555\n",
      "                              FF = 29.806856155395508\n",
      "Finished epoch  187\n",
      "On epoch  188\n",
      "Total Epoch Testing MAPE: PCE = 14.075541496276855\n",
      "                              Voc = 42.70076370239258\n",
      "                              Jsc = 20.193450927734375\n",
      "                              FF = 29.761638641357422\n",
      "Finished epoch  188\n",
      "On epoch  189\n",
      "Total Epoch Testing MAPE: PCE = 14.215019226074219\n",
      "                              Voc = 42.327476501464844\n",
      "                              Jsc = 19.682199478149414\n",
      "                              FF = 29.5198974609375\n",
      "Finished epoch  189\n",
      "On epoch  190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.29613208770752\n",
      "                              Voc = 42.952999114990234\n",
      "                              Jsc = 19.582082748413086\n",
      "                              FF = 29.99644660949707\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 14.585373878479004\n",
      "                              Voc = 42.73159408569336\n",
      "                              Jsc = 19.91650390625\n",
      "                              FF = 30.107311248779297\n",
      "Finished epoch  191\n",
      "On epoch  192\n",
      "Total Epoch Testing MAPE: PCE = 14.43609619140625\n",
      "                              Voc = 42.8947868347168\n",
      "                              Jsc = 19.848054885864258\n",
      "                              FF = 29.813823699951172\n",
      "Finished epoch  192\n",
      "On epoch  193\n",
      "Total Epoch Testing MAPE: PCE = 14.616351127624512\n",
      "                              Voc = 43.05436325073242\n",
      "                              Jsc = 20.066585540771484\n",
      "                              FF = 29.266813278198242\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 14.698174476623535\n",
      "                              Voc = 43.456581115722656\n",
      "                              Jsc = 20.066633224487305\n",
      "                              FF = 29.57100486755371\n",
      "Finished epoch  194\n",
      "On epoch  195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.494624137878418\n",
      "                              Voc = 43.36659240722656\n",
      "                              Jsc = 20.21595001220703\n",
      "                              FF = 29.73463249206543\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 15.958333015441895\n",
      "                              Voc = 42.98069381713867\n",
      "                              Jsc = 20.3101749420166\n",
      "                              FF = 30.270755767822266\n",
      "Finished epoch  196\n",
      "On epoch  197\n",
      "Total Epoch Testing MAPE: PCE = 16.234233856201172\n",
      "                              Voc = 42.97935104370117\n",
      "                              Jsc = 20.627614974975586\n",
      "                              FF = 30.125104904174805\n",
      "Finished epoch  197\n",
      "On epoch  198\n",
      "Total Epoch Testing MAPE: PCE = 16.88666343688965\n",
      "                              Voc = 43.74083709716797\n",
      "                              Jsc = 20.00813865661621\n",
      "                              FF = 30.205825805664062\n",
      "Finished epoch  198\n",
      "On epoch  199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 16.726776123046875\n",
      "                              Voc = 43.749481201171875\n",
      "                              Jsc = 20.070262908935547\n",
      "                              FF = 30.431726455688477\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 16.41329002380371\n",
      "                              Voc = 44.10527420043945\n",
      "                              Jsc = 20.230466842651367\n",
      "                              FF = 30.62710189819336\n",
      "Finished epoch  200\n",
      "On epoch  201\n",
      "Total Epoch Testing MAPE: PCE = 16.141267776489258\n",
      "                              Voc = 44.360538482666016\n",
      "                              Jsc = 20.285642623901367\n",
      "                              FF = 30.548078536987305\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 16.084442138671875\n",
      "                              Voc = 44.266422271728516\n",
      "                              Jsc = 20.29753875732422\n",
      "                              FF = 30.598522186279297\n",
      "Finished epoch  202\n",
      "On epoch  203\n",
      "Total Epoch Testing MAPE: PCE = 15.9609956741333\n",
      "                              Voc = 44.25031661987305\n",
      "                              Jsc = 20.8072566986084\n",
      "                              FF = 30.81818199157715\n",
      "Finished epoch  203\n",
      "On epoch  204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.776104927062988\n",
      "                              Voc = 44.11720275878906\n",
      "                              Jsc = 21.229080200195312\n",
      "                              FF = 30.50876808166504\n",
      "Finished epoch  204\n",
      "On epoch  205\n",
      "Total Epoch Testing MAPE: PCE = 15.597688674926758\n",
      "                              Voc = 44.26881408691406\n",
      "                              Jsc = 21.078323364257812\n",
      "                              FF = 30.096370697021484\n",
      "Finished epoch  205\n",
      "On epoch  206\n",
      "Total Epoch Testing MAPE: PCE = 15.27070426940918\n",
      "                              Voc = 43.78805160522461\n",
      "                              Jsc = 20.85641860961914\n",
      "                              FF = 30.120901107788086\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 15.551201820373535\n",
      "                              Voc = 43.976158142089844\n",
      "                              Jsc = 20.85892677307129\n",
      "                              FF = 29.584476470947266\n",
      "Finished epoch  207\n",
      "On epoch  208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.955904006958008\n",
      "                              Voc = 43.567787170410156\n",
      "                              Jsc = 20.800121307373047\n",
      "                              FF = 29.717910766601562\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 15.746909141540527\n",
      "                              Voc = 42.73031234741211\n",
      "                              Jsc = 20.50859260559082\n",
      "                              FF = 30.477863311767578\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 15.730956077575684\n",
      "                              Voc = 42.479740142822266\n",
      "                              Jsc = 20.59000587463379\n",
      "                              FF = 30.813053131103516\n",
      "Finished epoch  210\n",
      "On epoch  211\n",
      "Total Epoch Testing MAPE: PCE = 15.311071395874023\n",
      "                              Voc = 41.984771728515625\n",
      "                              Jsc = 20.527856826782227\n",
      "                              FF = 31.05702781677246\n",
      "Finished epoch  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "On epoch  212\n",
      "Total Epoch Testing MAPE: PCE = 15.547901153564453\n",
      "                              Voc = 42.18694305419922\n",
      "                              Jsc = 19.78342628479004\n",
      "                              FF = 31.28484535217285\n",
      "Finished epoch  212\n",
      "On epoch  213\n",
      "Total Epoch Testing MAPE: PCE = 15.408863067626953\n",
      "                              Voc = 42.15135955810547\n",
      "                              Jsc = 19.67133903503418\n",
      "                              FF = 31.651432037353516\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 15.488828659057617\n",
      "                              Voc = 41.98448944091797\n",
      "                              Jsc = 19.955158233642578\n",
      "                              FF = 31.753829956054688\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 14.89719295501709\n",
      "                              Voc = 41.5788459777832\n",
      "                              Jsc = 19.910438537597656\n",
      "                              FF = 31.406253814697266\n",
      "Finished epoch  215\n",
      "On epoch  216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.777697563171387\n",
      "                              Voc = 41.9707145690918\n",
      "                              Jsc = 19.985321044921875\n",
      "                              FF = 31.188730239868164\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 14.680843353271484\n",
      "                              Voc = 41.93047332763672\n",
      "                              Jsc = 19.965473175048828\n",
      "                              FF = 31.108001708984375\n",
      "Finished epoch  217\n",
      "On epoch  218\n",
      "Total Epoch Testing MAPE: PCE = 14.478127479553223\n",
      "                              Voc = 41.90602111816406\n",
      "                              Jsc = 19.99831199645996\n",
      "                              FF = 31.038272857666016\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 14.60727596282959\n",
      "                              Voc = 41.94289016723633\n",
      "                              Jsc = 20.003963470458984\n",
      "                              FF = 31.48941993713379\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 14.267848014831543\n",
      "                              Voc = 42.44585418701172\n",
      "                              Jsc = 19.72111701965332\n",
      "                              FF = 31.80105972290039\n",
      "Finished epoch  220\n",
      "On epoch  221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.037055015563965\n",
      "                              Voc = 42.78038024902344\n",
      "                              Jsc = 19.321542739868164\n",
      "                              FF = 31.450197219848633\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 14.03516674041748\n",
      "                              Voc = 43.41938018798828\n",
      "                              Jsc = 18.63923454284668\n",
      "                              FF = 31.032487869262695\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 14.185254096984863\n",
      "                              Voc = 43.47372055053711\n",
      "                              Jsc = 17.897790908813477\n",
      "                              FF = 30.416440963745117\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 14.075800895690918\n",
      "                              Voc = 43.08418273925781\n",
      "                              Jsc = 17.721181869506836\n",
      "                              FF = 30.018871307373047\n",
      "Finished epoch  224\n",
      "On epoch  225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.979358673095703\n",
      "                              Voc = 44.02375030517578\n",
      "                              Jsc = 17.29059410095215\n",
      "                              FF = 28.851259231567383\n",
      "Finished epoch  225\n",
      "On epoch  226\n",
      "Total Epoch Testing MAPE: PCE = 13.92521858215332\n",
      "                              Voc = 43.27158737182617\n",
      "                              Jsc = 17.36962127685547\n",
      "                              FF = 29.47917938232422\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 13.936688423156738\n",
      "                              Voc = 42.977447509765625\n",
      "                              Jsc = 17.441373825073242\n",
      "                              FF = 29.258956909179688\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 13.857931137084961\n",
      "                              Voc = 43.252899169921875\n",
      "                              Jsc = 17.354398727416992\n",
      "                              FF = 29.139766693115234\n",
      "Finished epoch  228\n",
      "On epoch  229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.799110412597656\n",
      "                              Voc = 43.040164947509766\n",
      "                              Jsc = 17.024845123291016\n",
      "                              FF = 28.659149169921875\n",
      "Finished epoch  229\n",
      "On epoch  230\n",
      "Total Epoch Testing MAPE: PCE = 13.770708084106445\n",
      "                              Voc = 43.3119010925293\n",
      "                              Jsc = 16.39055824279785\n",
      "                              FF = 28.80840301513672\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 13.852134704589844\n",
      "                              Voc = 43.35648727416992\n",
      "                              Jsc = 16.362628936767578\n",
      "                              FF = 28.414573669433594\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 13.846235275268555\n",
      "                              Voc = 44.1083869934082\n",
      "                              Jsc = 16.308330535888672\n",
      "                              FF = 28.40381622314453\n",
      "Finished epoch  232\n",
      "On epoch  233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.011770248413086\n",
      "                              Voc = 44.011592864990234\n",
      "                              Jsc = 16.526519775390625\n",
      "                              FF = 28.38628578186035\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 14.200173377990723\n",
      "                              Voc = 43.693355560302734\n",
      "                              Jsc = 16.01175308227539\n",
      "                              FF = 28.299468994140625\n",
      "Finished epoch  234\n",
      "On epoch  235\n",
      "Total Epoch Testing MAPE: PCE = 14.342442512512207\n",
      "                              Voc = 43.43341064453125\n",
      "                              Jsc = 15.991847038269043\n",
      "                              FF = 28.290462493896484\n",
      "Finished epoch  235\n",
      "On epoch  236\n",
      "Total Epoch Testing MAPE: PCE = 14.769315719604492\n",
      "                              Voc = 43.40096664428711\n",
      "                              Jsc = 16.145362854003906\n",
      "                              FF = 28.754013061523438\n",
      "Finished epoch  236\n",
      "On epoch  237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.10743522644043\n",
      "                              Voc = 43.19432830810547\n",
      "                              Jsc = 15.989786148071289\n",
      "                              FF = 28.484329223632812\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 15.175925254821777\n",
      "                              Voc = 43.03012466430664\n",
      "                              Jsc = 15.935620307922363\n",
      "                              FF = 28.412050247192383\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 15.11064624786377\n",
      "                              Voc = 42.453147888183594\n",
      "                              Jsc = 16.474708557128906\n",
      "                              FF = 28.022666931152344\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 15.247993469238281\n",
      "                              Voc = 42.575443267822266\n",
      "                              Jsc = 16.712900161743164\n",
      "                              FF = 27.795612335205078\n",
      "Finished epoch  240\n",
      "On epoch  241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.424245834350586\n",
      "                              Voc = 42.42127990722656\n",
      "                              Jsc = 16.392581939697266\n",
      "                              FF = 27.676040649414062\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 15.376472473144531\n",
      "                              Voc = 42.056331634521484\n",
      "                              Jsc = 16.313426971435547\n",
      "                              FF = 28.18977165222168\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 15.151877403259277\n",
      "                              Voc = 41.861419677734375\n",
      "                              Jsc = 16.507362365722656\n",
      "                              FF = 28.46403694152832\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 15.066729545593262\n",
      "                              Voc = 41.782676696777344\n",
      "                              Jsc = 16.64837074279785\n",
      "                              FF = 28.74034881591797\n",
      "Finished epoch  244\n",
      "On epoch  245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.400193214416504\n",
      "                              Voc = 42.27467346191406\n",
      "                              Jsc = 16.813961029052734\n",
      "                              FF = 29.384521484375\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 15.271395683288574\n",
      "                              Voc = 42.51874542236328\n",
      "                              Jsc = 17.017728805541992\n",
      "                              FF = 30.161930084228516\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 15.334580421447754\n",
      "                              Voc = 42.81326675415039\n",
      "                              Jsc = 16.497337341308594\n",
      "                              FF = 30.690326690673828\n",
      "Finished epoch  247\n",
      "On epoch  248\n",
      "Total Epoch Testing MAPE: PCE = 15.096518516540527\n",
      "                              Voc = 43.33487319946289\n",
      "                              Jsc = 16.628507614135742\n",
      "                              FF = 31.23619842529297\n",
      "Finished epoch  248\n",
      "On epoch  249\n",
      "Total Epoch Testing MAPE: PCE = 14.977767944335938\n",
      "                              Voc = 43.769466400146484\n",
      "                              Jsc = 16.460535049438477\n",
      "                              FF = 31.54998779296875\n",
      "Finished epoch  249\n",
      "On epoch  250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.721292495727539\n",
      "                              Voc = 43.521602630615234\n",
      "                              Jsc = 16.296287536621094\n",
      "                              FF = 32.18704605102539\n",
      "Finished epoch  250\n",
      "On epoch  251\n",
      "Total Epoch Testing MAPE: PCE = 14.678878784179688\n",
      "                              Voc = 43.35615158081055\n",
      "                              Jsc = 16.061447143554688\n",
      "                              FF = 32.19752883911133\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 14.497298240661621\n",
      "                              Voc = 43.010765075683594\n",
      "                              Jsc = 15.359038352966309\n",
      "                              FF = 32.37937545776367\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 14.445082664489746\n",
      "                              Voc = 43.346092224121094\n",
      "                              Jsc = 15.372209548950195\n",
      "                              FF = 32.63534164428711\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 14.267927169799805\n",
      "                              Voc = 43.33100128173828\n",
      "                              Jsc = 15.351035118103027\n",
      "                              FF = 32.639503479003906\n",
      "Finished epoch  254\n",
      "On epoch  255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.31995964050293\n",
      "                              Voc = 43.55880355834961\n",
      "                              Jsc = 15.782716751098633\n",
      "                              FF = 32.598533630371094\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 14.390082359313965\n",
      "                              Voc = 43.37482833862305\n",
      "                              Jsc = 15.751899719238281\n",
      "                              FF = 32.8003044128418\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 14.612971305847168\n",
      "                              Voc = 43.85932540893555\n",
      "                              Jsc = 15.722025871276855\n",
      "                              FF = 33.15461349487305\n",
      "Finished epoch  257\n",
      "On epoch  258\n",
      "Total Epoch Testing MAPE: PCE = 14.690637588500977\n",
      "                              Voc = 43.84071350097656\n",
      "                              Jsc = 16.085569381713867\n",
      "                              FF = 33.989444732666016\n",
      "Finished epoch  258\n",
      "On epoch  259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.75536060333252\n",
      "                              Voc = 43.615135192871094\n",
      "                              Jsc = 15.81298542022705\n",
      "                              FF = 33.92714309692383\n",
      "Finished epoch  259\n",
      "On epoch  260\n",
      "Total Epoch Testing MAPE: PCE = 15.277502059936523\n",
      "                              Voc = 43.62125015258789\n",
      "                              Jsc = 15.678451538085938\n",
      "                              FF = 33.83436584472656\n",
      "Finished epoch  260\n",
      "On epoch  261\n",
      "Total Epoch Testing MAPE: PCE = 15.447150230407715\n",
      "                              Voc = 43.62907791137695\n",
      "                              Jsc = 15.836423873901367\n",
      "                              FF = 33.88133239746094\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 15.425094604492188\n",
      "                              Voc = 43.53321075439453\n",
      "                              Jsc = 15.267151832580566\n",
      "                              FF = 33.76273727416992\n",
      "Finished epoch  262\n",
      "On epoch  263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.81299114227295\n",
      "                              Voc = 43.8758659362793\n",
      "                              Jsc = 15.267461776733398\n",
      "                              FF = 33.339210510253906\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 15.866454124450684\n",
      "                              Voc = 43.55706024169922\n",
      "                              Jsc = 15.215188026428223\n",
      "                              FF = 32.64093780517578\n",
      "Finished epoch  264\n",
      "On epoch  265\n",
      "Total Epoch Testing MAPE: PCE = 15.848599433898926\n",
      "                              Voc = 43.20991516113281\n",
      "                              Jsc = 15.322540283203125\n",
      "                              FF = 32.42679977416992\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 15.922194480895996\n",
      "                              Voc = 43.0262336730957\n",
      "                              Jsc = 15.160717010498047\n",
      "                              FF = 31.770559310913086\n",
      "Finished epoch  266\n",
      "On epoch  267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 16.247901916503906\n",
      "                              Voc = 42.61101531982422\n",
      "                              Jsc = 15.301214218139648\n",
      "                              FF = 31.409772872924805\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 16.11659049987793\n",
      "                              Voc = 42.721214294433594\n",
      "                              Jsc = 15.407747268676758\n",
      "                              FF = 31.45899772644043\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 16.692914962768555\n",
      "                              Voc = 41.906105041503906\n",
      "                              Jsc = 14.815308570861816\n",
      "                              FF = 31.090801239013672\n",
      "Finished epoch  269\n",
      "On epoch  270\n",
      "Total Epoch Testing MAPE: PCE = 16.816877365112305\n",
      "                              Voc = 41.40895462036133\n",
      "                              Jsc = 14.293050765991211\n",
      "                              FF = 31.097681045532227\n",
      "Finished epoch  270\n",
      "On epoch  271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 16.684335708618164\n",
      "                              Voc = 41.36763381958008\n",
      "                              Jsc = 14.533263206481934\n",
      "                              FF = 30.80742645263672\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 16.798091888427734\n",
      "                              Voc = 41.28374099731445\n",
      "                              Jsc = 14.282293319702148\n",
      "                              FF = 30.822467803955078\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 16.448644638061523\n",
      "                              Voc = 41.10141372680664\n",
      "                              Jsc = 14.385299682617188\n",
      "                              FF = 30.198373794555664\n",
      "Finished epoch  273\n",
      "On epoch  274\n",
      "Total Epoch Testing MAPE: PCE = 16.284523010253906\n",
      "                              Voc = 41.46704864501953\n",
      "                              Jsc = 13.836060523986816\n",
      "                              FF = 30.218549728393555\n",
      "Finished epoch  274\n",
      "On epoch  275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 16.105430603027344\n",
      "                              Voc = 42.00611877441406\n",
      "                              Jsc = 13.6072998046875\n",
      "                              FF = 30.228588104248047\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 15.832204818725586\n",
      "                              Voc = 42.1032829284668\n",
      "                              Jsc = 13.453727722167969\n",
      "                              FF = 30.18037986755371\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 15.509269714355469\n",
      "                              Voc = 42.122745513916016\n",
      "                              Jsc = 12.82351016998291\n",
      "                              FF = 30.628124237060547\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 15.549098014831543\n",
      "                              Voc = 41.69659423828125\n",
      "                              Jsc = 12.544743537902832\n",
      "                              FF = 30.61927604675293\n",
      "Finished epoch  278\n",
      "On epoch  279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.754429817199707\n",
      "                              Voc = 41.9581184387207\n",
      "                              Jsc = 12.295502662658691\n",
      "                              FF = 30.560550689697266\n",
      "Finished epoch  279\n",
      "On epoch  280\n",
      "Total Epoch Testing MAPE: PCE = 15.752431869506836\n",
      "                              Voc = 41.905086517333984\n",
      "                              Jsc = 12.260772705078125\n",
      "                              FF = 30.447568893432617\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 16.05774688720703\n",
      "                              Voc = 41.887901306152344\n",
      "                              Jsc = 12.079828262329102\n",
      "                              FF = 30.191743850708008\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 15.800990104675293\n",
      "                              Voc = 42.30467987060547\n",
      "                              Jsc = 12.12624454498291\n",
      "                              FF = 29.687246322631836\n",
      "Finished epoch  282\n",
      "On epoch  283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.477866172790527\n",
      "                              Voc = 41.60814666748047\n",
      "                              Jsc = 11.803313255310059\n",
      "                              FF = 29.731487274169922\n",
      "Finished epoch  283\n",
      "On epoch  284\n",
      "Total Epoch Testing MAPE: PCE = 15.003718376159668\n",
      "                              Voc = 41.009159088134766\n",
      "                              Jsc = 11.512075424194336\n",
      "                              FF = 29.678577423095703\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 15.133033752441406\n",
      "                              Voc = 41.72078323364258\n",
      "                              Jsc = 11.111712455749512\n",
      "                              FF = 30.002885818481445\n",
      "Finished epoch  285\n",
      "On epoch  286\n",
      "Total Epoch Testing MAPE: PCE = 14.870579719543457\n",
      "                              Voc = 41.645938873291016\n",
      "                              Jsc = 10.730660438537598\n",
      "                              FF = 30.062496185302734\n",
      "Finished epoch  286\n",
      "On epoch  287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.799861907958984\n",
      "                              Voc = 41.240196228027344\n",
      "                              Jsc = 10.587197303771973\n",
      "                              FF = 29.639488220214844\n",
      "Finished epoch  287\n",
      "On epoch  288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.77657699584961\n",
      "                              Voc = 41.170902252197266\n",
      "                              Jsc = 10.924592971801758\n",
      "                              FF = 29.47617530822754\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 15.01579475402832\n",
      "                              Voc = 41.58985900878906\n",
      "                              Jsc = 10.754161834716797\n",
      "                              FF = 29.450908660888672\n",
      "Finished epoch  289\n",
      "On epoch  290\n",
      "Total Epoch Testing MAPE: PCE = 15.116299629211426\n",
      "                              Voc = 41.513710021972656\n",
      "                              Jsc = 10.667839050292969\n",
      "                              FF = 29.36416244506836\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 14.868614196777344\n",
      "                              Voc = 41.64460754394531\n",
      "                              Jsc = 10.404006004333496\n",
      "                              FF = 29.017057418823242\n",
      "Finished epoch  291\n",
      "On epoch  292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.669597625732422\n",
      "                              Voc = 42.050907135009766\n",
      "                              Jsc = 10.317866325378418\n",
      "                              FF = 29.470544815063477\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 14.673185348510742\n",
      "                              Voc = 41.70405960083008\n",
      "                              Jsc = 10.172734260559082\n",
      "                              FF = 30.135948181152344\n",
      "Finished epoch  293\n",
      "On epoch  294\n",
      "Total Epoch Testing MAPE: PCE = 14.574305534362793\n",
      "                              Voc = 41.31254577636719\n",
      "                              Jsc = 10.15085506439209\n",
      "                              FF = 30.621746063232422\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 14.653593063354492\n",
      "                              Voc = 40.868709564208984\n",
      "                              Jsc = 10.120144844055176\n",
      "                              FF = 30.48253631591797\n",
      "Finished epoch  295\n",
      "On epoch  296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.820796012878418\n",
      "                              Voc = 40.43132019042969\n",
      "                              Jsc = 9.98556137084961\n",
      "                              FF = 30.93108558654785\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 14.831884384155273\n",
      "                              Voc = 40.312713623046875\n",
      "                              Jsc = 9.743896484375\n",
      "                              FF = 30.833179473876953\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 14.77086353302002\n",
      "                              Voc = 39.8681755065918\n",
      "                              Jsc = 9.579318046569824\n",
      "                              FF = 31.665897369384766\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 15.030041694641113\n",
      "                              Voc = 40.08319854736328\n",
      "                              Jsc = 9.609374046325684\n",
      "                              FF = 31.70033073425293\n",
      "Finished epoch  299\n",
      "On epoch  300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.921696662902832\n",
      "                              Voc = 40.00419235229492\n",
      "                              Jsc = 9.95121955871582\n",
      "                              FF = 31.69159507751465\n",
      "Finished epoch  300\n",
      "On epoch  301\n",
      "Total Epoch Testing MAPE: PCE = 14.991447448730469\n",
      "                              Voc = 40.110958099365234\n",
      "                              Jsc = 9.933977127075195\n",
      "                              FF = 30.83333969116211\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 14.987934112548828\n",
      "                              Voc = 39.91920471191406\n",
      "                              Jsc = 9.892412185668945\n",
      "                              FF = 30.659687042236328\n",
      "Finished epoch  302\n",
      "On epoch  303\n",
      "Total Epoch Testing MAPE: PCE = 14.936921119689941\n",
      "                              Voc = 39.52107620239258\n",
      "                              Jsc = 9.720122337341309\n",
      "                              FF = 30.057106018066406\n",
      "Finished epoch  303\n",
      "On epoch  304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.72342586517334\n",
      "                              Voc = 39.49449920654297\n",
      "                              Jsc = 9.985783576965332\n",
      "                              FF = 29.24858283996582\n",
      "Finished epoch  304\n",
      "On epoch  305\n",
      "Total Epoch Testing MAPE: PCE = 14.685994148254395\n",
      "                              Voc = 40.09297561645508\n",
      "                              Jsc = 9.945479393005371\n",
      "                              FF = 29.192102432250977\n",
      "Finished epoch  305\n",
      "On epoch  306\n",
      "Total Epoch Testing MAPE: PCE = 15.178123474121094\n",
      "                              Voc = 40.50236129760742\n",
      "                              Jsc = 9.709492683410645\n",
      "                              FF = 28.493019104003906\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 15.493029594421387\n",
      "                              Voc = 39.91972351074219\n",
      "                              Jsc = 9.687068939208984\n",
      "                              FF = 29.451730728149414\n",
      "Finished epoch  307\n",
      "On epoch  308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.487930297851562\n",
      "                              Voc = 39.32114791870117\n",
      "                              Jsc = 9.487588882446289\n",
      "                              FF = 29.19168472290039\n",
      "Finished epoch  308\n",
      "On epoch  309\n",
      "Total Epoch Testing MAPE: PCE = 15.948101997375488\n",
      "                              Voc = 39.88334655761719\n",
      "                              Jsc = 9.468230247497559\n",
      "                              FF = 29.10120391845703\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 16.019763946533203\n",
      "                              Voc = 39.6123046875\n",
      "                              Jsc = 9.236042022705078\n",
      "                              FF = 29.405641555786133\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 16.271692276000977\n",
      "                              Voc = 39.03108596801758\n",
      "                              Jsc = 9.228121757507324\n",
      "                              FF = 29.417461395263672\n",
      "Finished epoch  311\n",
      "On epoch  312\n",
      "Total Epoch Testing MAPE: PCE = 16.442441940307617\n",
      "                              Voc = 38.94570541381836\n",
      "                              Jsc = 9.328043937683105\n",
      "                              FF = 29.131784439086914\n",
      "Finished epoch  312\n",
      "On epoch  313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 16.23164939880371\n",
      "                              Voc = 38.5474853515625\n",
      "                              Jsc = 9.321975708007812\n",
      "                              FF = 29.282072067260742\n",
      "Finished epoch  313\n",
      "On epoch  314\n",
      "Total Epoch Testing MAPE: PCE = 16.040485382080078\n",
      "                              Voc = 38.45470428466797\n",
      "                              Jsc = 9.334000587463379\n",
      "                              FF = 29.57843589782715\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 16.119972229003906\n",
      "                              Voc = 37.99781799316406\n",
      "                              Jsc = 9.64928913116455\n",
      "                              FF = 29.810041427612305\n",
      "Finished epoch  315\n",
      "On epoch  316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.898811340332031\n",
      "                              Voc = 37.90896224975586\n",
      "                              Jsc = 10.431185722351074\n",
      "                              FF = 29.302635192871094\n",
      "Finished epoch  316\n",
      "On epoch  317\n",
      "Total Epoch Testing MAPE: PCE = 16.548410415649414\n",
      "                              Voc = 37.82021713256836\n",
      "                              Jsc = 10.30885124206543\n",
      "                              FF = 29.467376708984375\n",
      "Finished epoch  317\n",
      "On epoch  318\n",
      "Total Epoch Testing MAPE: PCE = 16.493457794189453\n",
      "                              Voc = 38.198753356933594\n",
      "                              Jsc = 10.30977725982666\n",
      "                              FF = 29.76921844482422\n",
      "Finished epoch  318\n",
      "On epoch  319\n",
      "Total Epoch Testing MAPE: PCE = 16.265602111816406\n",
      "                              Voc = 38.13642120361328\n",
      "                              Jsc = 10.32085132598877\n",
      "                              FF = 29.956993103027344\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 16.185867309570312\n",
      "                              Voc = 37.653770446777344\n",
      "                              Jsc = 10.297487258911133\n",
      "                              FF = 30.138578414916992\n",
      "Finished epoch  320\n",
      "On epoch  321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 16.107725143432617\n",
      "                              Voc = 37.25619888305664\n",
      "                              Jsc = 10.062735557556152\n",
      "                              FF = 30.333948135375977\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 16.139854431152344\n",
      "                              Voc = 37.5385627746582\n",
      "                              Jsc = 10.356071472167969\n",
      "                              FF = 30.341825485229492\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 15.977437973022461\n",
      "                              Voc = 37.14200210571289\n",
      "                              Jsc = 10.340497970581055\n",
      "                              FF = 30.4082088470459\n",
      "Finished epoch  323\n",
      "On epoch  324\n",
      "Total Epoch Testing MAPE: PCE = 15.999448776245117\n",
      "                              Voc = 36.64595031738281\n",
      "                              Jsc = 9.976532936096191\n",
      "                              FF = 30.340805053710938\n",
      "Finished epoch  324\n",
      "On epoch  325\n",
      "Total Epoch Testing MAPE: PCE = 15.750198364257812\n",
      "                              Voc = 36.06522750854492\n",
      "                              Jsc = 9.944066047668457\n",
      "                              FF = 30.44352149963379\n",
      "Finished epoch  325\n",
      "On epoch  326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.470211029052734\n",
      "                              Voc = 36.697059631347656\n",
      "                              Jsc = 9.840835571289062\n",
      "                              FF = 30.792057037353516\n",
      "Finished epoch  326\n",
      "On epoch  327\n",
      "Total Epoch Testing MAPE: PCE = 15.869039535522461\n",
      "                              Voc = 36.47306823730469\n",
      "                              Jsc = 10.18664836883545\n",
      "                              FF = 30.905860900878906\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 15.810614585876465\n",
      "                              Voc = 36.208274841308594\n",
      "                              Jsc = 10.099479675292969\n",
      "                              FF = 30.642980575561523\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 15.568197250366211\n",
      "                              Voc = 35.808414459228516\n",
      "                              Jsc = 9.921841621398926\n",
      "                              FF = 30.12055778503418\n",
      "Finished epoch  329\n",
      "On epoch  330\n",
      "Total Epoch Testing MAPE: PCE = 15.532546043395996\n",
      "                              Voc = 35.419246673583984\n",
      "                              Jsc = 9.98194694519043\n",
      "                              FF = 29.98052215576172\n",
      "Finished epoch  330\n",
      "On epoch  331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.643302917480469\n",
      "                              Voc = 35.061466217041016\n",
      "                              Jsc = 9.999765396118164\n",
      "                              FF = 29.825899124145508\n",
      "Finished epoch  331\n",
      "On epoch  332\n",
      "Total Epoch Testing MAPE: PCE = 15.67223834991455\n",
      "                              Voc = 34.68470764160156\n",
      "                              Jsc = 10.337248802185059\n",
      "                              FF = 29.835105895996094\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 15.492631912231445\n",
      "                              Voc = 34.313201904296875\n",
      "                              Jsc = 10.27553939819336\n",
      "                              FF = 29.52900505065918\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 15.161914825439453\n",
      "                              Voc = 33.88044357299805\n",
      "                              Jsc = 10.602850914001465\n",
      "                              FF = 29.632699966430664\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 15.071521759033203\n",
      "                              Voc = 33.420745849609375\n",
      "                              Jsc = 10.394510269165039\n",
      "                              FF = 29.511016845703125\n",
      "Finished epoch  335\n",
      "On epoch  336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.971782684326172\n",
      "                              Voc = 33.84635925292969\n",
      "                              Jsc = 9.859861373901367\n",
      "                              FF = 29.71567726135254\n",
      "Finished epoch  336\n",
      "On epoch  337\n",
      "Total Epoch Testing MAPE: PCE = 14.669879913330078\n",
      "                              Voc = 33.848148345947266\n",
      "                              Jsc = 9.94394302368164\n",
      "                              FF = 29.659181594848633\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 14.79034423828125\n",
      "                              Voc = 33.26567840576172\n",
      "                              Jsc = 9.96460247039795\n",
      "                              FF = 30.581607818603516\n",
      "Finished epoch  338\n",
      "On epoch  339\n",
      "Total Epoch Testing MAPE: PCE = 14.77768611907959\n",
      "                              Voc = 33.34177017211914\n",
      "                              Jsc = 9.836017608642578\n",
      "                              FF = 30.51066780090332\n",
      "Finished epoch  339\n",
      "On epoch  340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.861066818237305\n",
      "                              Voc = 33.97651290893555\n",
      "                              Jsc = 9.712786674499512\n",
      "                              FF = 30.50263786315918\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 14.663702011108398\n",
      "                              Voc = 32.982913970947266\n",
      "                              Jsc = 9.84588623046875\n",
      "                              FF = 30.780094146728516\n",
      "Finished epoch  341\n",
      "On epoch  342\n",
      "Total Epoch Testing MAPE: PCE = 14.591938018798828\n",
      "                              Voc = 32.98589324951172\n",
      "                              Jsc = 9.771757125854492\n",
      "                              FF = 30.54065704345703\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 14.542616844177246\n",
      "                              Voc = 32.98164367675781\n",
      "                              Jsc = 9.977531433105469\n",
      "                              FF = 30.19074821472168\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 14.48111343383789\n",
      "                              Voc = 32.779537200927734\n",
      "                              Jsc = 9.735788345336914\n",
      "                              FF = 29.907373428344727\n",
      "Finished epoch  344\n",
      "On epoch  345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.361828804016113\n",
      "                              Voc = 32.72785568237305\n",
      "                              Jsc = 10.036087036132812\n",
      "                              FF = 30.341657638549805\n",
      "Finished epoch  345\n",
      "On epoch  346\n",
      "Total Epoch Testing MAPE: PCE = 14.366909980773926\n",
      "                              Voc = 32.44615173339844\n",
      "                              Jsc = 10.030817985534668\n",
      "                              FF = 29.59823989868164\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 14.298160552978516\n",
      "                              Voc = 32.1988639831543\n",
      "                              Jsc = 9.831068992614746\n",
      "                              FF = 29.37125015258789\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 14.190814971923828\n",
      "                              Voc = 32.209136962890625\n",
      "                              Jsc = 9.71871566772461\n",
      "                              FF = 29.007431030273438\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 14.342559814453125\n",
      "                              Voc = 31.838788986206055\n",
      "                              Jsc = 9.556007385253906\n",
      "                              FF = 29.2047176361084\n",
      "Finished epoch  349\n",
      "On epoch  350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.218833923339844\n",
      "                              Voc = 31.599763870239258\n",
      "                              Jsc = 9.81097412109375\n",
      "                              FF = 29.2482967376709\n",
      "Finished epoch  350\n",
      "On epoch  351\n",
      "Total Epoch Testing MAPE: PCE = 14.281441688537598\n",
      "                              Voc = 31.3531494140625\n",
      "                              Jsc = 9.696477890014648\n",
      "                              FF = 29.45992660522461\n",
      "Finished epoch  351\n",
      "On epoch  352\n",
      "Total Epoch Testing MAPE: PCE = 14.31309986114502\n",
      "                              Voc = 31.177425384521484\n",
      "                              Jsc = 9.700602531433105\n",
      "                              FF = 29.4444522857666\n",
      "Finished epoch  352\n",
      "On epoch  353\n",
      "Total Epoch Testing MAPE: PCE = 14.333154678344727\n",
      "                              Voc = 31.06839370727539\n",
      "                              Jsc = 9.831364631652832\n",
      "                              FF = 28.981075286865234\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 14.433831214904785\n",
      "                              Voc = 30.642515182495117\n",
      "                              Jsc = 10.071027755737305\n",
      "                              FF = 29.1079044342041\n",
      "Finished epoch  354\n",
      "On epoch  355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.558638572692871\n",
      "                              Voc = 30.385892868041992\n",
      "                              Jsc = 9.93964672088623\n",
      "                              FF = 29.36074447631836\n",
      "Finished epoch  355\n",
      "On epoch  356\n",
      "Total Epoch Testing MAPE: PCE = 14.576188087463379\n",
      "                              Voc = 30.76247215270996\n",
      "                              Jsc = 9.520236015319824\n",
      "                              FF = 29.603925704956055\n",
      "Finished epoch  356\n",
      "On epoch  357\n",
      "Total Epoch Testing MAPE: PCE = 14.650080680847168\n",
      "                              Voc = 30.81130027770996\n",
      "                              Jsc = 9.432106971740723\n",
      "                              FF = 29.545751571655273\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 14.691573143005371\n",
      "                              Voc = 30.564212799072266\n",
      "                              Jsc = 9.056253433227539\n",
      "                              FF = 28.60521125793457\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 14.821098327636719\n",
      "                              Voc = 29.964614868164062\n",
      "                              Jsc = 8.82264518737793\n",
      "                              FF = 28.478872299194336\n",
      "Finished epoch  359\n",
      "On epoch  360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.751726150512695\n",
      "                              Voc = 29.593944549560547\n",
      "                              Jsc = 8.97148323059082\n",
      "                              FF = 28.39241600036621\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 14.750665664672852\n",
      "                              Voc = 29.498144149780273\n",
      "                              Jsc = 8.889593124389648\n",
      "                              FF = 28.117183685302734\n",
      "Finished epoch  361\n",
      "On epoch  362\n",
      "Total Epoch Testing MAPE: PCE = 14.642707824707031\n",
      "                              Voc = 29.673194885253906\n",
      "                              Jsc = 8.503849983215332\n",
      "                              FF = 28.302091598510742\n",
      "Finished epoch  362\n",
      "On epoch  363\n",
      "Total Epoch Testing MAPE: PCE = 14.735014915466309\n",
      "                              Voc = 29.843341827392578\n",
      "                              Jsc = 8.818052291870117\n",
      "                              FF = 28.00433349609375\n",
      "Finished epoch  363\n",
      "On epoch  364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.632908821105957\n",
      "                              Voc = 29.62118911743164\n",
      "                              Jsc = 9.172532081604004\n",
      "                              FF = 27.14405059814453\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 14.6620512008667\n",
      "                              Voc = 29.247310638427734\n",
      "                              Jsc = 9.108792304992676\n",
      "                              FF = 26.47644805908203\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 14.81785774230957\n",
      "                              Voc = 29.402503967285156\n",
      "                              Jsc = 9.079757690429688\n",
      "                              FF = 26.16109275817871\n",
      "Finished epoch  366\n",
      "On epoch  367\n",
      "Total Epoch Testing MAPE: PCE = 14.878374099731445\n",
      "                              Voc = 28.8699951171875\n",
      "                              Jsc = 9.089715003967285\n",
      "                              FF = 26.138465881347656\n",
      "Finished epoch  367\n",
      "On epoch  368\n",
      "Total Epoch Testing MAPE: PCE = 15.295083999633789\n",
      "                              Voc = 29.2501277923584\n",
      "                              Jsc = 9.018648147583008\n",
      "                              FF = 26.45811653137207\n",
      "Finished epoch  368\n",
      "On epoch  369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.882548332214355\n",
      "                              Voc = 28.9700870513916\n",
      "                              Jsc = 9.02434253692627\n",
      "                              FF = 25.900808334350586\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 14.952251434326172\n",
      "                              Voc = 28.885576248168945\n",
      "                              Jsc = 9.047642707824707\n",
      "                              FF = 25.89908790588379\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 14.890569686889648\n",
      "                              Voc = 29.29762840270996\n",
      "                              Jsc = 9.065868377685547\n",
      "                              FF = 25.94866180419922\n",
      "Finished epoch  371\n",
      "On epoch  372\n",
      "Total Epoch Testing MAPE: PCE = 14.63490104675293\n",
      "                              Voc = 29.58722496032715\n",
      "                              Jsc = 9.176060676574707\n",
      "                              FF = 25.118186950683594\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 14.912017822265625\n",
      "                              Voc = 29.86817741394043\n",
      "                              Jsc = 9.074373245239258\n",
      "                              FF = 25.211578369140625\n",
      "Finished epoch  373\n",
      "On epoch  374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.064075469970703\n",
      "                              Voc = 30.267105102539062\n",
      "                              Jsc = 9.122930526733398\n",
      "                              FF = 25.09857749938965\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 14.822988510131836\n",
      "                              Voc = 30.100622177124023\n",
      "                              Jsc = 9.363450050354004\n",
      "                              FF = 25.225234985351562\n",
      "Finished epoch  375\n",
      "On epoch  376\n",
      "Total Epoch Testing MAPE: PCE = 14.728005409240723\n",
      "                              Voc = 29.702455520629883\n",
      "                              Jsc = 9.61203384399414\n",
      "                              FF = 25.422985076904297\n",
      "Finished epoch  376\n",
      "On epoch  377\n",
      "Total Epoch Testing MAPE: PCE = 14.740406036376953\n",
      "                              Voc = 29.33335304260254\n",
      "                              Jsc = 9.771190643310547\n",
      "                              FF = 25.983015060424805\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 14.753894805908203\n",
      "                              Voc = 29.951446533203125\n",
      "                              Jsc = 9.884035110473633\n",
      "                              FF = 26.550344467163086\n",
      "Finished epoch  378\n",
      "On epoch  379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.713342666625977\n",
      "                              Voc = 29.896894454956055\n",
      "                              Jsc = 10.154814720153809\n",
      "                              FF = 27.139202117919922\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 14.73552131652832\n",
      "                              Voc = 29.84990692138672\n",
      "                              Jsc = 10.549986839294434\n",
      "                              FF = 27.487213134765625\n",
      "Finished epoch  380\n",
      "On epoch  381\n",
      "Total Epoch Testing MAPE: PCE = 14.850886344909668\n",
      "                              Voc = 29.922670364379883\n",
      "                              Jsc = 10.69729995727539\n",
      "                              FF = 28.557613372802734\n",
      "Finished epoch  381\n",
      "On epoch  382\n",
      "Total Epoch Testing MAPE: PCE = 14.727614402770996\n",
      "                              Voc = 29.714519500732422\n",
      "                              Jsc = 11.042668342590332\n",
      "                              FF = 29.85076904296875\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 14.684393882751465\n",
      "                              Voc = 30.033523559570312\n",
      "                              Jsc = 11.241018295288086\n",
      "                              FF = 30.61746597290039\n",
      "Finished epoch  383\n",
      "On epoch  384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.683967590332031\n",
      "                              Voc = 30.409130096435547\n",
      "                              Jsc = 11.137788772583008\n",
      "                              FF = 31.239992141723633\n",
      "Finished epoch  384\n",
      "On epoch  385\n",
      "Total Epoch Testing MAPE: PCE = 14.61952018737793\n",
      "                              Voc = 30.210208892822266\n",
      "                              Jsc = 11.049832344055176\n",
      "                              FF = 31.19378089904785\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 14.710134506225586\n",
      "                              Voc = 30.0212345123291\n",
      "                              Jsc = 10.552544593811035\n",
      "                              FF = 31.28783416748047\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 14.566797256469727\n",
      "                              Voc = 30.233169555664062\n",
      "                              Jsc = 10.91026782989502\n",
      "                              FF = 31.768619537353516\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 14.473106384277344\n",
      "                              Voc = 30.52239990234375\n",
      "                              Jsc = 11.203811645507812\n",
      "                              FF = 31.847116470336914\n",
      "Finished epoch  388\n",
      "On epoch  389\n",
      "Total Epoch Testing MAPE: PCE = 14.344152450561523\n",
      "                              Voc = 30.452547073364258\n",
      "                              Jsc = 11.11741828918457\n",
      "                              FF = 32.22983932495117\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 14.337666511535645\n",
      "                              Voc = 30.684667587280273\n",
      "                              Jsc = 11.136165618896484\n",
      "                              FF = 32.117427825927734\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 14.328728675842285\n",
      "                              Voc = 30.88435173034668\n",
      "                              Jsc = 10.657366752624512\n",
      "                              FF = 31.8447322845459\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 14.341062545776367\n",
      "                              Voc = 31.02789306640625\n",
      "                              Jsc = 10.683830261230469\n",
      "                              FF = 31.265336990356445\n",
      "Finished epoch  392\n",
      "On epoch  393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.310012817382812\n",
      "                              Voc = 31.236461639404297\n",
      "                              Jsc = 10.714932441711426\n",
      "                              FF = 30.682226181030273\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 14.318031311035156\n",
      "                              Voc = 31.240324020385742\n",
      "                              Jsc = 10.275388717651367\n",
      "                              FF = 30.378679275512695\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 14.466227531433105\n",
      "                              Voc = 31.184022903442383\n",
      "                              Jsc = 10.574819564819336\n",
      "                              FF = 29.634056091308594\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 14.296703338623047\n",
      "                              Voc = 30.990747451782227\n",
      "                              Jsc = 10.245712280273438\n",
      "                              FF = 29.47627067565918\n",
      "Finished epoch  396\n",
      "On epoch  397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.383252143859863\n",
      "                              Voc = 30.918874740600586\n",
      "                              Jsc = 10.288246154785156\n",
      "                              FF = 29.117525100708008\n",
      "Finished epoch  397\n",
      "On epoch  398\n",
      "Total Epoch Testing MAPE: PCE = 14.301122665405273\n",
      "                              Voc = 30.966169357299805\n",
      "                              Jsc = 10.278454780578613\n",
      "                              FF = 28.254222869873047\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 14.228741645812988\n",
      "                              Voc = 30.947378158569336\n",
      "                              Jsc = 10.1598482131958\n",
      "                              FF = 28.18531608581543\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 14.170254707336426\n",
      "                              Voc = 31.20363426208496\n",
      "                              Jsc = 10.25033187866211\n",
      "                              FF = 28.056747436523438\n",
      "Finished epoch  400\n",
      "On epoch  401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.942755699157715\n",
      "                              Voc = 30.735857009887695\n",
      "                              Jsc = 9.967912673950195\n",
      "                              FF = 27.24921989440918\n",
      "Finished epoch  401\n",
      "On epoch  402\n",
      "Total Epoch Testing MAPE: PCE = 14.180353164672852\n",
      "                              Voc = 30.495527267456055\n",
      "                              Jsc = 9.800518035888672\n",
      "                              FF = 26.914501190185547\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 14.231649398803711\n",
      "                              Voc = 30.1845645904541\n",
      "                              Jsc = 10.121560096740723\n",
      "                              FF = 26.840627670288086\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 14.348363876342773\n",
      "                              Voc = 30.30754280090332\n",
      "                              Jsc = 10.347075462341309\n",
      "                              FF = 26.00555992126465\n",
      "Finished epoch  404\n",
      "On epoch  405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.35983943939209\n",
      "                              Voc = 30.166744232177734\n",
      "                              Jsc = 10.628876686096191\n",
      "                              FF = 25.396442413330078\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 14.436623573303223\n",
      "                              Voc = 30.085643768310547\n",
      "                              Jsc = 10.665048599243164\n",
      "                              FF = 24.79098892211914\n",
      "Finished epoch  406\n",
      "On epoch  407\n",
      "Total Epoch Testing MAPE: PCE = 14.320405960083008\n",
      "                              Voc = 30.028175354003906\n",
      "                              Jsc = 10.658171653747559\n",
      "                              FF = 24.219295501708984\n",
      "Finished epoch  407\n",
      "On epoch  408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.275492668151855\n",
      "                              Voc = 30.23841094970703\n",
      "                              Jsc = 10.571160316467285\n",
      "                              FF = 23.959857940673828\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 13.971558570861816\n",
      "                              Voc = 29.86248779296875\n",
      "                              Jsc = 10.764042854309082\n",
      "                              FF = 24.18878746032715\n",
      "Finished epoch  409\n",
      "On epoch  410\n",
      "Total Epoch Testing MAPE: PCE = 13.834322929382324\n",
      "                              Voc = 29.409282684326172\n",
      "                              Jsc = 10.75456428527832\n",
      "                              FF = 24.323549270629883\n",
      "Finished epoch  410\n",
      "On epoch  411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.767127990722656\n",
      "                              Voc = 29.34772300720215\n",
      "                              Jsc = 10.567561149597168\n",
      "                              FF = 24.789112091064453\n",
      "Finished epoch  411\n",
      "On epoch  412\n",
      "Total Epoch Testing MAPE: PCE = 13.81408405303955\n",
      "                              Voc = 29.832149505615234\n",
      "                              Jsc = 10.826749801635742\n",
      "                              FF = 25.313335418701172\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 13.676433563232422\n",
      "                              Voc = 29.291719436645508\n",
      "                              Jsc = 10.908259391784668\n",
      "                              FF = 25.812793731689453\n",
      "Finished epoch  413\n",
      "On epoch  414\n",
      "Total Epoch Testing MAPE: PCE = 13.517875671386719\n",
      "                              Voc = 29.05664825439453\n",
      "                              Jsc = 11.013151168823242\n",
      "                              FF = 26.637454986572266\n",
      "Finished epoch  414\n",
      "On epoch  415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.784642219543457\n",
      "                              Voc = 29.106870651245117\n",
      "                              Jsc = 10.894993782043457\n",
      "                              FF = 26.968612670898438\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 13.599605560302734\n",
      "                              Voc = 28.99028968811035\n",
      "                              Jsc = 10.939847946166992\n",
      "                              FF = 26.772462844848633\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 13.559744834899902\n",
      "                              Voc = 29.335100173950195\n",
      "                              Jsc = 10.6460599899292\n",
      "                              FF = 27.48297691345215\n",
      "Finished epoch  417\n",
      "On epoch  418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.70816707611084\n",
      "                              Voc = 29.02066993713379\n",
      "                              Jsc = 10.372762680053711\n",
      "                              FF = 27.78945541381836\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 13.71600341796875\n",
      "                              Voc = 29.414966583251953\n",
      "                              Jsc = 10.08195972442627\n",
      "                              FF = 28.646936416625977\n",
      "Finished epoch  419\n",
      "On epoch  420\n",
      "Total Epoch Testing MAPE: PCE = 13.634696960449219\n",
      "                              Voc = 29.658905029296875\n",
      "                              Jsc = 10.184022903442383\n",
      "                              FF = 29.273948669433594\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 13.528621673583984\n",
      "                              Voc = 29.48160171508789\n",
      "                              Jsc = 10.498072624206543\n",
      "                              FF = 29.729005813598633\n",
      "Finished epoch  421\n",
      "On epoch  422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.346221923828125\n",
      "                              Voc = 29.507680892944336\n",
      "                              Jsc = 10.255260467529297\n",
      "                              FF = 30.286041259765625\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 13.334650993347168\n",
      "                              Voc = 29.24848175048828\n",
      "                              Jsc = 10.271044731140137\n",
      "                              FF = 30.215164184570312\n",
      "Finished epoch  423\n",
      "On epoch  424\n",
      "Total Epoch Testing MAPE: PCE = 13.584250450134277\n",
      "                              Voc = 29.445659637451172\n",
      "                              Jsc = 10.432661056518555\n",
      "                              FF = 30.680747985839844\n",
      "Finished epoch  424\n",
      "On epoch  425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.595756530761719\n",
      "                              Voc = 29.19912338256836\n",
      "                              Jsc = 10.587156295776367\n",
      "                              FF = 31.164968490600586\n",
      "Finished epoch  425\n",
      "On epoch  426\n",
      "Total Epoch Testing MAPE: PCE = 13.649540901184082\n",
      "                              Voc = 29.390562057495117\n",
      "                              Jsc = 10.232695579528809\n",
      "                              FF = 31.285419464111328\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 13.645861625671387\n",
      "                              Voc = 29.870588302612305\n",
      "                              Jsc = 9.927148818969727\n",
      "                              FF = 30.834808349609375\n",
      "Finished epoch  427\n",
      "On epoch  428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.914111137390137\n",
      "                              Voc = 29.982412338256836\n",
      "                              Jsc = 9.732019424438477\n",
      "                              FF = 30.951976776123047\n",
      "Finished epoch  428\n",
      "On epoch  429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.312161445617676\n",
      "                              Voc = 30.474788665771484\n",
      "                              Jsc = 9.6969633102417\n",
      "                              FF = 31.12417221069336\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 14.377617835998535\n",
      "                              Voc = 30.621370315551758\n",
      "                              Jsc = 9.617974281311035\n",
      "                              FF = 30.995187759399414\n",
      "Finished epoch  430\n",
      "On epoch  431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.403984069824219\n",
      "                              Voc = 30.733642578125\n",
      "                              Jsc = 9.847970962524414\n",
      "                              FF = 30.876991271972656\n",
      "Finished epoch  431\n",
      "On epoch  432\n",
      "Total Epoch Testing MAPE: PCE = 14.344996452331543\n",
      "                              Voc = 30.684968948364258\n",
      "                              Jsc = 10.31508731842041\n",
      "                              FF = 31.06985092163086\n",
      "Finished epoch  432\n",
      "On epoch  433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.336729049682617\n",
      "                              Voc = 30.676424026489258\n",
      "                              Jsc = 10.608551979064941\n",
      "                              FF = 30.94651985168457\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 14.434021949768066\n",
      "                              Voc = 30.884605407714844\n",
      "                              Jsc = 10.814719200134277\n",
      "                              FF = 30.133262634277344\n",
      "Finished epoch  434\n",
      "On epoch  435\n",
      "Total Epoch Testing MAPE: PCE = 14.379731178283691\n",
      "                              Voc = 30.944320678710938\n",
      "                              Jsc = 10.817811965942383\n",
      "                              FF = 29.728912353515625\n",
      "Finished epoch  435\n",
      "On epoch  436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.384690284729004\n",
      "                              Voc = 31.821151733398438\n",
      "                              Jsc = 11.122700691223145\n",
      "                              FF = 29.731918334960938\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 14.471733093261719\n",
      "                              Voc = 31.09644317626953\n",
      "                              Jsc = 11.358098983764648\n",
      "                              FF = 29.849464416503906\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 14.209407806396484\n",
      "                              Voc = 30.94162368774414\n",
      "                              Jsc = 11.229364395141602\n",
      "                              FF = 30.136537551879883\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 14.019712448120117\n",
      "                              Voc = 31.00248908996582\n",
      "                              Jsc = 11.181839942932129\n",
      "                              FF = 30.330244064331055\n",
      "Finished epoch  439\n",
      "On epoch  440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.209464073181152\n",
      "                              Voc = 31.17925262451172\n",
      "                              Jsc = 11.165329933166504\n",
      "                              FF = 30.77414321899414\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 14.288267135620117\n",
      "                              Voc = 31.28032112121582\n",
      "                              Jsc = 11.089860916137695\n",
      "                              FF = 30.641075134277344\n",
      "Finished epoch  441\n",
      "On epoch  442\n",
      "Total Epoch Testing MAPE: PCE = 13.985191345214844\n",
      "                              Voc = 31.530132293701172\n",
      "                              Jsc = 10.924538612365723\n",
      "                              FF = 31.167221069335938\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 13.901564598083496\n",
      "                              Voc = 31.513490676879883\n",
      "                              Jsc = 10.864912033081055\n",
      "                              FF = 30.889240264892578\n",
      "Finished epoch  443\n",
      "On epoch  444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.610029220581055\n",
      "                              Voc = 31.388011932373047\n",
      "                              Jsc = 10.57634162902832\n",
      "                              FF = 30.583595275878906\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 13.52823257446289\n",
      "                              Voc = 31.399965286254883\n",
      "                              Jsc = 10.469926834106445\n",
      "                              FF = 30.1568660736084\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 13.511821746826172\n",
      "                              Voc = 31.41585350036621\n",
      "                              Jsc = 10.395905494689941\n",
      "                              FF = 29.85251808166504\n",
      "Finished epoch  446\n",
      "On epoch  447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.661348342895508\n",
      "                              Voc = 31.710445404052734\n",
      "                              Jsc = 10.116425514221191\n",
      "                              FF = 29.36406707763672\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 13.883140563964844\n",
      "                              Voc = 31.746482849121094\n",
      "                              Jsc = 9.862832069396973\n",
      "                              FF = 28.505123138427734\n",
      "Finished epoch  448\n",
      "On epoch  449\n",
      "Total Epoch Testing MAPE: PCE = 13.97716236114502\n",
      "                              Voc = 31.350671768188477\n",
      "                              Jsc = 9.761895179748535\n",
      "                              FF = 28.301311492919922\n",
      "Finished epoch  449\n",
      "On epoch  450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.04880428314209\n",
      "                              Voc = 30.808713912963867\n",
      "                              Jsc = 9.262453079223633\n",
      "                              FF = 27.945444107055664\n",
      "Finished epoch  450\n",
      "On epoch  451\n",
      "Total Epoch Testing MAPE: PCE = 14.011308670043945\n",
      "                              Voc = 30.628061294555664\n",
      "                              Jsc = 9.724854469299316\n",
      "                              FF = 27.510112762451172\n",
      "Finished epoch  451\n",
      "On epoch  452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.808690071105957\n",
      "                              Voc = 30.67694091796875\n",
      "                              Jsc = 9.566540718078613\n",
      "                              FF = 27.009296417236328\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 13.940234184265137\n",
      "                              Voc = 30.29886245727539\n",
      "                              Jsc = 9.7239408493042\n",
      "                              FF = 27.064653396606445\n",
      "Finished epoch  453\n",
      "On epoch  454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 13.801529884338379\n",
      "                              Voc = 30.098575592041016\n",
      "                              Jsc = 9.825553894042969\n",
      "                              FF = 26.94254493713379\n",
      "Finished epoch  454\n",
      "On epoch  455\n",
      "Total Epoch Testing MAPE: PCE = 13.857420921325684\n",
      "                              Voc = 29.93720817565918\n",
      "                              Jsc = 9.732074737548828\n",
      "                              FF = 26.950138092041016\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 14.113533020019531\n",
      "                              Voc = 29.827503204345703\n",
      "                              Jsc = 9.711485862731934\n",
      "                              FF = 26.70234489440918\n",
      "Finished epoch  456\n",
      "On epoch  457\n",
      "Total Epoch Testing MAPE: PCE = 14.36489486694336\n",
      "                              Voc = 30.362070083618164\n",
      "                              Jsc = 9.626561164855957\n",
      "                              FF = 26.556640625\n",
      "Finished epoch  457\n",
      "On epoch  458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.656609535217285\n",
      "                              Voc = 30.6966552734375\n",
      "                              Jsc = 9.895105361938477\n",
      "                              FF = 26.67934226989746\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 14.78915786743164\n",
      "                              Voc = 30.673852920532227\n",
      "                              Jsc = 10.225382804870605\n",
      "                              FF = 27.18602180480957\n",
      "Finished epoch  459\n",
      "On epoch  460\n",
      "Total Epoch Testing MAPE: PCE = 14.912388801574707\n",
      "                              Voc = 30.811277389526367\n",
      "                              Jsc = 10.369582176208496\n",
      "                              FF = 27.670278549194336\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 14.790977478027344\n",
      "                              Voc = 30.6082820892334\n",
      "                              Jsc = 10.338305473327637\n",
      "                              FF = 27.554454803466797\n",
      "Finished epoch  461\n",
      "On epoch  462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.67434310913086\n",
      "                              Voc = 30.782833099365234\n",
      "                              Jsc = 10.290396690368652\n",
      "                              FF = 27.4464168548584\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 15.088123321533203\n",
      "                              Voc = 30.47403335571289\n",
      "                              Jsc = 10.34195327758789\n",
      "                              FF = 28.00545310974121\n",
      "Finished epoch  463\n",
      "On epoch  464\n",
      "Total Epoch Testing MAPE: PCE = 14.936396598815918\n",
      "                              Voc = 30.481287002563477\n",
      "                              Jsc = 10.213237762451172\n",
      "                              FF = 28.1041202545166\n",
      "Finished epoch  464\n",
      "On epoch  465\n",
      "Total Epoch Testing MAPE: PCE = 14.813261985778809\n",
      "                              Voc = 30.346101760864258\n",
      "                              Jsc = 10.05710220336914\n",
      "                              FF = 27.688100814819336\n",
      "Finished epoch  465\n",
      "On epoch  466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.627710342407227\n",
      "                              Voc = 29.801544189453125\n",
      "                              Jsc = 9.972811698913574\n",
      "                              FF = 27.586591720581055\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 14.719822883605957\n",
      "                              Voc = 29.70863914489746\n",
      "                              Jsc = 9.844103813171387\n",
      "                              FF = 27.336421966552734\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 14.70467758178711\n",
      "                              Voc = 29.647790908813477\n",
      "                              Jsc = 9.974627494812012\n",
      "                              FF = 27.347341537475586\n",
      "Finished epoch  468\n",
      "On epoch  469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.76315689086914\n",
      "                              Voc = 29.620880126953125\n",
      "                              Jsc = 10.16908073425293\n",
      "                              FF = 27.471214294433594\n",
      "Finished epoch  469\n",
      "On epoch  470\n",
      "Total Epoch Testing MAPE: PCE = 14.763458251953125\n",
      "                              Voc = 29.33424949645996\n",
      "                              Jsc = 9.917222023010254\n",
      "                              FF = 27.831247329711914\n",
      "Finished epoch  470\n",
      "On epoch  471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.441893577575684\n",
      "                              Voc = 29.39041519165039\n",
      "                              Jsc = 10.01206111907959\n",
      "                              FF = 28.077667236328125\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 14.312073707580566\n",
      "                              Voc = 29.524898529052734\n",
      "                              Jsc = 9.863255500793457\n",
      "                              FF = 28.052881240844727\n",
      "Finished epoch  472\n",
      "On epoch  473\n",
      "Total Epoch Testing MAPE: PCE = 14.415346145629883\n",
      "                              Voc = 29.486339569091797\n",
      "                              Jsc = 10.027029037475586\n",
      "                              FF = 27.81831932067871\n",
      "Finished epoch  473\n",
      "On epoch  474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.297582626342773\n",
      "                              Voc = 29.388566970825195\n",
      "                              Jsc = 10.070008277893066\n",
      "                              FF = 27.79887580871582\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 14.235183715820312\n",
      "                              Voc = 29.009172439575195\n",
      "                              Jsc = 10.428650856018066\n",
      "                              FF = 27.28988265991211\n",
      "Finished epoch  475\n",
      "On epoch  476\n",
      "Total Epoch Testing MAPE: PCE = 14.211013793945312\n",
      "                              Voc = 28.9544734954834\n",
      "                              Jsc = 10.377004623413086\n",
      "                              FF = 27.523958206176758\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 14.24836540222168\n",
      "                              Voc = 28.368087768554688\n",
      "                              Jsc = 10.542496681213379\n",
      "                              FF = 26.97635841369629\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 14.216012001037598\n",
      "                              Voc = 28.120800018310547\n",
      "                              Jsc = 10.72420597076416\n",
      "                              FF = 27.072492599487305\n",
      "Finished epoch  478\n",
      "On epoch  479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.564238548278809\n",
      "                              Voc = 27.938302993774414\n",
      "                              Jsc = 10.513399124145508\n",
      "                              FF = 26.615610122680664\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 14.410013198852539\n",
      "                              Voc = 27.990949630737305\n",
      "                              Jsc = 10.42612361907959\n",
      "                              FF = 26.754432678222656\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 14.414776802062988\n",
      "                              Voc = 27.996793746948242\n",
      "                              Jsc = 10.1034574508667\n",
      "                              FF = 26.726457595825195\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 14.547734260559082\n",
      "                              Voc = 27.97265625\n",
      "                              Jsc = 9.895217895507812\n",
      "                              FF = 26.48581886291504\n",
      "Finished epoch  482\n",
      "On epoch  483\n",
      "Total Epoch Testing MAPE: PCE = 14.875923156738281\n",
      "                              Voc = 27.762121200561523\n",
      "                              Jsc = 9.891304016113281\n",
      "                              FF = 26.767284393310547\n",
      "Finished epoch  483\n",
      "On epoch  484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.708605766296387\n",
      "                              Voc = 27.561914443969727\n",
      "                              Jsc = 9.953400611877441\n",
      "                              FF = 26.7154598236084\n",
      "Finished epoch  484\n",
      "On epoch  485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.568130493164062\n",
      "                              Voc = 27.35249137878418\n",
      "                              Jsc = 9.890850067138672\n",
      "                              FF = 26.923423767089844\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 14.632536888122559\n",
      "                              Voc = 27.306047439575195\n",
      "                              Jsc = 9.989447593688965\n",
      "                              FF = 27.157411575317383\n",
      "Finished epoch  486\n",
      "On epoch  487\n",
      "Total Epoch Testing MAPE: PCE = 14.655271530151367\n",
      "                              Voc = 27.458545684814453\n",
      "                              Jsc = 9.985442161560059\n",
      "                              FF = 27.166494369506836\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 14.689144134521484\n",
      "                              Voc = 27.412912368774414\n",
      "                              Jsc = 9.822877883911133\n",
      "                              FF = 27.518770217895508\n",
      "Finished epoch  488\n",
      "On epoch  489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.45798110961914\n",
      "                              Voc = 26.534563064575195\n",
      "                              Jsc = 9.889886856079102\n",
      "                              FF = 27.862545013427734\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 14.270586967468262\n",
      "                              Voc = 26.6534366607666\n",
      "                              Jsc = 9.767264366149902\n",
      "                              FF = 28.19087791442871\n",
      "Finished epoch  490\n",
      "On epoch  491\n",
      "Total Epoch Testing MAPE: PCE = 14.173182487487793\n",
      "                              Voc = 27.025297164916992\n",
      "                              Jsc = 9.824414253234863\n",
      "                              FF = 28.512351989746094\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 14.394536972045898\n",
      "                              Voc = 26.983060836791992\n",
      "                              Jsc = 9.907157897949219\n",
      "                              FF = 27.983123779296875\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 14.235885620117188\n",
      "                              Voc = 26.89553451538086\n",
      "                              Jsc = 9.411905288696289\n",
      "                              FF = 28.153980255126953\n",
      "Finished epoch  493\n",
      "On epoch  494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 14.626993179321289\n",
      "                              Voc = 27.199352264404297\n",
      "                              Jsc = 9.469188690185547\n",
      "                              FF = 28.7413387298584\n",
      "Finished epoch  494\n",
      "On epoch  495\n",
      "Total Epoch Testing MAPE: PCE = 14.782645225524902\n",
      "                              Voc = 27.317567825317383\n",
      "                              Jsc = 9.488484382629395\n",
      "                              FF = 29.293474197387695\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 14.732909202575684\n",
      "                              Voc = 27.66234588623047\n",
      "                              Jsc = 9.4268159866333\n",
      "                              FF = 29.406373977661133\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 14.97374439239502\n",
      "                              Voc = 27.584705352783203\n",
      "                              Jsc = 9.444616317749023\n",
      "                              FF = 29.454391479492188\n",
      "Finished epoch  497\n",
      "On epoch  498\n",
      "Total Epoch Testing MAPE: PCE = 15.210805892944336\n",
      "                              Voc = 27.73223876953125\n",
      "                              Jsc = 9.759723663330078\n",
      "                              FF = 29.675966262817383\n",
      "Finished epoch  498\n",
      "On epoch  499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 15.431051254272461\n",
      "                              Voc = 27.808502197265625\n",
      "                              Jsc = 9.973952293395996\n",
      "                              FF = 29.952302932739258\n",
      "Finished epoch  499\n",
      "Fold # 3\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 475.586181640625\n",
      "                              Voc = 128.43975830078125\n",
      "                              Jsc = 52.48196029663086\n",
      "                              FF = 99.54564666748047\n",
      "Finished epoch  0\n",
      "On epoch  1\n",
      "Total Epoch Testing MAPE: PCE = 351.2867431640625\n",
      "                              Voc = 73.25273895263672\n",
      "                              Jsc = 47.771183013916016\n",
      "                              FF = 99.80731964111328\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 266.8139953613281\n",
      "                              Voc = 45.194217681884766\n",
      "                              Jsc = 47.35020065307617\n",
      "                              FF = 99.58806610107422\n",
      "Finished epoch  2\n",
      "On epoch  3\n",
      "Total Epoch Testing MAPE: PCE = 204.10520935058594\n",
      "                              Voc = 24.30377960205078\n",
      "                              Jsc = 46.312828063964844\n",
      "                              FF = 99.27027130126953\n",
      "Finished epoch  3\n",
      "On epoch  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 159.5399932861328\n",
      "                              Voc = 21.49258041381836\n",
      "                              Jsc = 44.24689483642578\n",
      "                              FF = 97.1864013671875\n",
      "Finished epoch  4\n",
      "On epoch  5\n",
      "Total Epoch Testing MAPE: PCE = 122.23693084716797\n",
      "                              Voc = 22.392620086669922\n",
      "                              Jsc = 43.46235275268555\n",
      "                              FF = 95.11009979248047\n",
      "Finished epoch  5\n",
      "On epoch  6\n",
      "Total Epoch Testing MAPE: PCE = 87.96698760986328\n",
      "                              Voc = 24.088748931884766\n",
      "                              Jsc = 42.37324142456055\n",
      "                              FF = 93.20613861083984\n",
      "Finished epoch  6\n",
      "On epoch  7\n",
      "Total Epoch Testing MAPE: PCE = 62.960594177246094\n",
      "                              Voc = 24.991107940673828\n",
      "                              Jsc = 41.21859359741211\n",
      "                              FF = 92.05908966064453\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 46.54521560668945\n",
      "                              Voc = 29.414865493774414\n",
      "                              Jsc = 41.54716873168945\n",
      "                              FF = 90.6398696899414\n",
      "Finished epoch  8\n",
      "On epoch  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.51815414428711\n",
      "                              Voc = 35.05177307128906\n",
      "                              Jsc = 40.937015533447266\n",
      "                              FF = 89.58158111572266\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 31.2613582611084\n",
      "                              Voc = 38.94892883300781\n",
      "                              Jsc = 40.4156379699707\n",
      "                              FF = 88.88027954101562\n",
      "Finished epoch  10\n",
      "On epoch  11\n",
      "Total Epoch Testing MAPE: PCE = 30.17304801940918\n",
      "                              Voc = 40.66779708862305\n",
      "                              Jsc = 42.46810531616211\n",
      "                              FF = 88.06674194335938\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 32.317840576171875\n",
      "                              Voc = 46.01969909667969\n",
      "                              Jsc = 45.34489059448242\n",
      "                              FF = 87.3360366821289\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 35.686161041259766\n",
      "                              Voc = 47.18941879272461\n",
      "                              Jsc = 48.768516540527344\n",
      "                              FF = 87.42821502685547\n",
      "Finished epoch  13\n",
      "On epoch  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.54208755493164\n",
      "                              Voc = 48.787086486816406\n",
      "                              Jsc = 52.23859405517578\n",
      "                              FF = 87.61488342285156\n",
      "Finished epoch  14\n",
      "On epoch  15\n",
      "Total Epoch Testing MAPE: PCE = 41.09762191772461\n",
      "                              Voc = 50.182334899902344\n",
      "                              Jsc = 54.86815643310547\n",
      "                              FF = 87.93956756591797\n",
      "Finished epoch  15\n",
      "On epoch  16\n",
      "Total Epoch Testing MAPE: PCE = 43.99374771118164\n",
      "                              Voc = 49.42253875732422\n",
      "                              Jsc = 57.34017562866211\n",
      "                              FF = 87.98511505126953\n",
      "Finished epoch  16\n",
      "On epoch  17\n",
      "Total Epoch Testing MAPE: PCE = 46.45891571044922\n",
      "                              Voc = 48.69483184814453\n",
      "                              Jsc = 60.01112365722656\n",
      "                              FF = 87.93014526367188\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 48.55097961425781\n",
      "                              Voc = 48.215579986572266\n",
      "                              Jsc = 62.556827545166016\n",
      "                              FF = 87.83238983154297\n",
      "Finished epoch  18\n",
      "On epoch  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.44801330566406\n",
      "                              Voc = 47.6401252746582\n",
      "                              Jsc = 64.00924682617188\n",
      "                              FF = 88.0386734008789\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 51.09847640991211\n",
      "                              Voc = 46.369327545166016\n",
      "                              Jsc = 65.580078125\n",
      "                              FF = 88.6844482421875\n",
      "Finished epoch  20\n",
      "On epoch  21\n",
      "Total Epoch Testing MAPE: PCE = 52.2796630859375\n",
      "                              Voc = 45.44565963745117\n",
      "                              Jsc = 67.85640716552734\n",
      "                              FF = 89.22518157958984\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 52.3881721496582\n",
      "                              Voc = 41.50719451904297\n",
      "                              Jsc = 69.2263412475586\n",
      "                              FF = 89.2774429321289\n",
      "Finished epoch  22\n",
      "On epoch  23\n",
      "Total Epoch Testing MAPE: PCE = 52.87383270263672\n",
      "                              Voc = 40.088871002197266\n",
      "                              Jsc = 71.18939208984375\n",
      "                              FF = 89.53373718261719\n",
      "Finished epoch  23\n",
      "On epoch  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.80411148071289\n",
      "                              Voc = 39.060054779052734\n",
      "                              Jsc = 72.16181945800781\n",
      "                              FF = 89.80370330810547\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 51.9244270324707\n",
      "                              Voc = 34.58012771606445\n",
      "                              Jsc = 73.2955551147461\n",
      "                              FF = 89.91629791259766\n",
      "Finished epoch  25\n",
      "On epoch  26\n",
      "Total Epoch Testing MAPE: PCE = 51.35032653808594\n",
      "                              Voc = 33.159400939941406\n",
      "                              Jsc = 74.04849243164062\n",
      "                              FF = 90.75746154785156\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 50.37300109863281\n",
      "                              Voc = 30.73064422607422\n",
      "                              Jsc = 74.67593383789062\n",
      "                              FF = 90.95328521728516\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 50.22937774658203\n",
      "                              Voc = 27.81707000732422\n",
      "                              Jsc = 75.17769622802734\n",
      "                              FF = 91.12716674804688\n",
      "Finished epoch  28\n",
      "On epoch  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.126136779785156\n",
      "                              Voc = 26.94131088256836\n",
      "                              Jsc = 75.41126251220703\n",
      "                              FF = 91.24569702148438\n",
      "Finished epoch  29\n",
      "On epoch  30\n",
      "Total Epoch Testing MAPE: PCE = 49.61858367919922\n",
      "                              Voc = 25.602561950683594\n",
      "                              Jsc = 75.54705810546875\n",
      "                              FF = 91.48294067382812\n",
      "Finished epoch  30\n",
      "On epoch  31\n",
      "Total Epoch Testing MAPE: PCE = 50.11232376098633\n",
      "                              Voc = 25.81390953063965\n",
      "                              Jsc = 75.7828140258789\n",
      "                              FF = 91.38582611083984\n",
      "Finished epoch  31\n",
      "On epoch  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.4554557800293\n",
      "                              Voc = 25.41594886779785\n",
      "                              Jsc = 75.17321014404297\n",
      "                              FF = 91.2648696899414\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 49.323707580566406\n",
      "                              Voc = 24.609222412109375\n",
      "                              Jsc = 75.1328125\n",
      "                              FF = 91.82830810546875\n",
      "Finished epoch  33\n",
      "On epoch  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.70090103149414\n",
      "                              Voc = 23.558151245117188\n",
      "                              Jsc = 75.32010650634766\n",
      "                              FF = 92.60718536376953\n",
      "Finished epoch  34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 49.81319046020508\n",
      "                              Voc = 22.146888732910156\n",
      "                              Jsc = 75.11421203613281\n",
      "                              FF = 92.67168426513672\n",
      "Finished epoch  35\n",
      "On epoch  36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 50.503421783447266\n",
      "                              Voc = 21.554597854614258\n",
      "                              Jsc = 75.32948303222656\n",
      "                              FF = 92.9089584350586\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 52.1038932800293\n",
      "                              Voc = 22.040721893310547\n",
      "                              Jsc = 75.65740966796875\n",
      "                              FF = 92.78140258789062\n",
      "Finished epoch  37\n",
      "On epoch  38\n",
      "Total Epoch Testing MAPE: PCE = 53.08354568481445\n",
      "                              Voc = 22.218904495239258\n",
      "                              Jsc = 75.37754821777344\n",
      "                              FF = 92.7830810546875\n",
      "Finished epoch  38\n",
      "On epoch  39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.32627868652344\n",
      "                              Voc = 22.008089065551758\n",
      "                              Jsc = 75.50491333007812\n",
      "                              FF = 93.08181762695312\n",
      "Finished epoch  39\n",
      "On epoch  40\n",
      "Total Epoch Testing MAPE: PCE = 54.177921295166016\n",
      "                              Voc = 21.673675537109375\n",
      "                              Jsc = 75.32379913330078\n",
      "                              FF = 93.25940704345703\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 54.040008544921875\n",
      "                              Voc = 22.408597946166992\n",
      "                              Jsc = 75.19227600097656\n",
      "                              FF = 93.38394165039062\n",
      "Finished epoch  41\n",
      "On epoch  42\n",
      "Total Epoch Testing MAPE: PCE = 53.8386116027832\n",
      "                              Voc = 23.092071533203125\n",
      "                              Jsc = 75.28358459472656\n",
      "                              FF = 93.4498291015625\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 54.23043441772461\n",
      "                              Voc = 24.44234848022461\n",
      "                              Jsc = 75.20893859863281\n",
      "                              FF = 93.81925964355469\n",
      "Finished epoch  43\n",
      "On epoch  44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.145538330078125\n",
      "                              Voc = 24.891550064086914\n",
      "                              Jsc = 75.06774139404297\n",
      "                              FF = 93.92157745361328\n",
      "Finished epoch  44\n",
      "On epoch  45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.93757629394531\n",
      "                              Voc = 25.353374481201172\n",
      "                              Jsc = 75.11282348632812\n",
      "                              FF = 94.37741088867188\n",
      "Finished epoch  45\n",
      "On epoch  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.458290100097656\n",
      "                              Voc = 24.69330596923828\n",
      "                              Jsc = 74.98318481445312\n",
      "                              FF = 95.04690551757812\n",
      "Finished epoch  46\n",
      "On epoch  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.157997131347656\n",
      "                              Voc = 24.934555053710938\n",
      "                              Jsc = 74.7802505493164\n",
      "                              FF = 95.120849609375\n",
      "Finished epoch  47\n",
      "On epoch  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.9740104675293\n",
      "                              Voc = 25.67215347290039\n",
      "                              Jsc = 74.75505828857422\n",
      "                              FF = 95.1622314453125\n",
      "Finished epoch  48\n",
      "On epoch  49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.1361198425293\n",
      "                              Voc = 26.80499839782715\n",
      "                              Jsc = 74.82061004638672\n",
      "                              FF = 95.01944732666016\n",
      "Finished epoch  49\n",
      "On epoch  50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.60459899902344\n",
      "                              Voc = 28.063100814819336\n",
      "                              Jsc = 74.89313507080078\n",
      "                              FF = 95.41767120361328\n",
      "Finished epoch  50\n",
      "On epoch  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 47.63492202758789\n",
      "                              Voc = 29.731645584106445\n",
      "                              Jsc = 75.2062759399414\n",
      "                              FF = 95.41539001464844\n",
      "Finished epoch  51\n",
      "On epoch  52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 46.34634017944336\n",
      "                              Voc = 30.47701072692871\n",
      "                              Jsc = 75.68453216552734\n",
      "                              FF = 95.6960678100586\n",
      "Finished epoch  52\n",
      "On epoch  53\n",
      "Total Epoch Testing MAPE: PCE = 45.189632415771484\n",
      "                              Voc = 32.870731353759766\n",
      "                              Jsc = 75.81180572509766\n",
      "                              FF = 95.99504852294922\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 43.52416229248047\n",
      "                              Voc = 33.595733642578125\n",
      "                              Jsc = 75.71043395996094\n",
      "                              FF = 96.17706298828125\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 41.264041900634766\n",
      "                              Voc = 36.010616302490234\n",
      "                              Jsc = 76.28319549560547\n",
      "                              FF = 96.26468658447266\n",
      "Finished epoch  55\n",
      "On epoch  56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.3255500793457\n",
      "                              Voc = 37.68833541870117\n",
      "                              Jsc = 76.2306900024414\n",
      "                              FF = 96.26454162597656\n",
      "Finished epoch  56\n",
      "On epoch  57\n",
      "Total Epoch Testing MAPE: PCE = 37.95186996459961\n",
      "                              Voc = 39.09743881225586\n",
      "                              Jsc = 76.31501007080078\n",
      "                              FF = 96.4532470703125\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 36.883724212646484\n",
      "                              Voc = 38.847198486328125\n",
      "                              Jsc = 76.1250228881836\n",
      "                              FF = 95.89603424072266\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 35.96023941040039\n",
      "                              Voc = 38.4704704284668\n",
      "                              Jsc = 76.18577575683594\n",
      "                              FF = 95.79207611083984\n",
      "Finished epoch  59\n",
      "On epoch  60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.564170837402344\n",
      "                              Voc = 38.68430709838867\n",
      "                              Jsc = 76.46005249023438\n",
      "                              FF = 95.67054748535156\n",
      "Finished epoch  60\n",
      "On epoch  61\n",
      "Total Epoch Testing MAPE: PCE = 34.895294189453125\n",
      "                              Voc = 37.89564895629883\n",
      "                              Jsc = 76.60363006591797\n",
      "                              FF = 95.56204986572266\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 34.701473236083984\n",
      "                              Voc = 38.38846206665039\n",
      "                              Jsc = 76.40681457519531\n",
      "                              FF = 95.83514404296875\n",
      "Finished epoch  62\n",
      "On epoch  63\n",
      "Total Epoch Testing MAPE: PCE = 34.274410247802734\n",
      "                              Voc = 38.54154968261719\n",
      "                              Jsc = 75.77251434326172\n",
      "                              FF = 95.76737976074219\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 33.8322639465332\n",
      "                              Voc = 37.868324279785156\n",
      "                              Jsc = 74.88782501220703\n",
      "                              FF = 95.7782974243164\n",
      "Finished epoch  64\n",
      "On epoch  65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.767093658447266\n",
      "                              Voc = 36.952781677246094\n",
      "                              Jsc = 74.33045959472656\n",
      "                              FF = 95.69857025146484\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 34.10018539428711\n",
      "                              Voc = 36.162109375\n",
      "                              Jsc = 73.99053955078125\n",
      "                              FF = 95.61945343017578\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 34.642234802246094\n",
      "                              Voc = 35.590248107910156\n",
      "                              Jsc = 73.47132873535156\n",
      "                              FF = 95.28260803222656\n",
      "Finished epoch  67\n",
      "On epoch  68\n",
      "Total Epoch Testing MAPE: PCE = 34.52887725830078\n",
      "                              Voc = 34.92792510986328\n",
      "                              Jsc = 72.42613220214844\n",
      "                              FF = 95.2068099975586\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 34.841670989990234\n",
      "                              Voc = 35.38380432128906\n",
      "                              Jsc = 69.68934631347656\n",
      "                              FF = 95.27472686767578\n",
      "Finished epoch  69\n",
      "On epoch  70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.66707992553711\n",
      "                              Voc = 35.70766830444336\n",
      "                              Jsc = 66.80496215820312\n",
      "                              FF = 95.35749816894531\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 34.599815368652344\n",
      "                              Voc = 35.45978927612305\n",
      "                              Jsc = 64.88511657714844\n",
      "                              FF = 95.90911865234375\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 33.59541320800781\n",
      "                              Voc = 36.444862365722656\n",
      "                              Jsc = 62.281333923339844\n",
      "                              FF = 95.75291442871094\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 33.76783752441406\n",
      "                              Voc = 37.05884552001953\n",
      "                              Jsc = 60.336761474609375\n",
      "                              FF = 95.79229736328125\n",
      "Finished epoch  73\n",
      "On epoch  74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.507633209228516\n",
      "                              Voc = 37.90480041503906\n",
      "                              Jsc = 58.71099090576172\n",
      "                              FF = 95.61406707763672\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 33.43784713745117\n",
      "                              Voc = 39.00474166870117\n",
      "                              Jsc = 56.434852600097656\n",
      "                              FF = 95.5566635131836\n",
      "Finished epoch  75\n",
      "On epoch  76\n",
      "Total Epoch Testing MAPE: PCE = 32.712860107421875\n",
      "                              Voc = 38.809391021728516\n",
      "                              Jsc = 53.572750091552734\n",
      "                              FF = 95.96376037597656\n",
      "Finished epoch  76\n",
      "On epoch  77\n",
      "Total Epoch Testing MAPE: PCE = 32.85487365722656\n",
      "                              Voc = 38.49248123168945\n",
      "                              Jsc = 51.75839614868164\n",
      "                              FF = 96.24396514892578\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 32.416744232177734\n",
      "                              Voc = 39.49026870727539\n",
      "                              Jsc = 49.87503433227539\n",
      "                              FF = 96.18230438232422\n",
      "Finished epoch  78\n",
      "On epoch  79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.488094329833984\n",
      "                              Voc = 40.25774002075195\n",
      "                              Jsc = 47.8336067199707\n",
      "                              FF = 96.0937728881836\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 32.06878662109375\n",
      "                              Voc = 41.65001678466797\n",
      "                              Jsc = 45.85783386230469\n",
      "                              FF = 95.82655334472656\n",
      "Finished epoch  80\n",
      "On epoch  81\n",
      "Total Epoch Testing MAPE: PCE = 31.588756561279297\n",
      "                              Voc = 42.79652786254883\n",
      "                              Jsc = 43.73602294921875\n",
      "                              FF = 95.43026733398438\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 31.373607635498047\n",
      "                              Voc = 42.501373291015625\n",
      "                              Jsc = 41.50367736816406\n",
      "                              FF = 95.84002685546875\n",
      "Finished epoch  82\n",
      "On epoch  83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.346189498901367\n",
      "                              Voc = 43.98320770263672\n",
      "                              Jsc = 39.804840087890625\n",
      "                              FF = 95.8923110961914\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 31.492801666259766\n",
      "                              Voc = 44.15682601928711\n",
      "                              Jsc = 39.07603073120117\n",
      "                              FF = 95.91981506347656\n",
      "Finished epoch  84\n",
      "On epoch  85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.777301788330078\n",
      "                              Voc = 45.51420974731445\n",
      "                              Jsc = 38.23844528198242\n",
      "                              FF = 95.87627410888672\n",
      "Finished epoch  85\n",
      "On epoch  86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.925573348999023\n",
      "                              Voc = 45.15873336791992\n",
      "                              Jsc = 36.678096771240234\n",
      "                              FF = 95.90138244628906\n",
      "Finished epoch  86\n",
      "On epoch  87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.1329460144043\n",
      "                              Voc = 46.38788986206055\n",
      "                              Jsc = 35.914039611816406\n",
      "                              FF = 95.9041748046875\n",
      "Finished epoch  87\n",
      "On epoch  88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.099239349365234\n",
      "                              Voc = 46.063682556152344\n",
      "                              Jsc = 36.71149444580078\n",
      "                              FF = 96.16001892089844\n",
      "Finished epoch  88\n",
      "On epoch  89\n",
      "Total Epoch Testing MAPE: PCE = 32.538394927978516\n",
      "                              Voc = 45.84392166137695\n",
      "                              Jsc = 36.39830017089844\n",
      "                              FF = 96.33899688720703\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 33.073760986328125\n",
      "                              Voc = 44.53738784790039\n",
      "                              Jsc = 36.65260696411133\n",
      "                              FF = 96.13542175292969\n",
      "Finished epoch  90\n",
      "On epoch  91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.597389221191406\n",
      "                              Voc = 43.978729248046875\n",
      "                              Jsc = 36.49932861328125\n",
      "                              FF = 96.08685302734375\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 34.043548583984375\n",
      "                              Voc = 42.79879379272461\n",
      "                              Jsc = 37.79052734375\n",
      "                              FF = 96.19305419921875\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 34.15013885498047\n",
      "                              Voc = 41.15990447998047\n",
      "                              Jsc = 37.87388610839844\n",
      "                              FF = 96.08100128173828\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 34.50170135498047\n",
      "                              Voc = 40.23413848876953\n",
      "                              Jsc = 38.07239532470703\n",
      "                              FF = 96.143798828125\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 34.502010345458984\n",
      "                              Voc = 38.00052261352539\n",
      "                              Jsc = 37.451210021972656\n",
      "                              FF = 96.12662506103516\n",
      "Finished epoch  95\n",
      "On epoch  96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.47974395751953\n",
      "                              Voc = 36.98655700683594\n",
      "                              Jsc = 37.05207824707031\n",
      "                              FF = 96.14086151123047\n",
      "Finished epoch  96\n",
      "On epoch  97\n",
      "Total Epoch Testing MAPE: PCE = 34.58675765991211\n",
      "                              Voc = 35.79876708984375\n",
      "                              Jsc = 37.694339752197266\n",
      "                              FF = 96.11479187011719\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 34.89326095581055\n",
      "                              Voc = 33.905757904052734\n",
      "                              Jsc = 37.410919189453125\n",
      "                              FF = 96.2209701538086\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 34.99248504638672\n",
      "                              Voc = 32.086788177490234\n",
      "                              Jsc = 37.27849197387695\n",
      "                              FF = 96.22681427001953\n",
      "Finished epoch  99\n",
      "On epoch  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.99435043334961\n",
      "                              Voc = 30.83709716796875\n",
      "                              Jsc = 37.46493148803711\n",
      "                              FF = 96.37967681884766\n",
      "Finished epoch  100\n",
      "On epoch  101\n",
      "Total Epoch Testing MAPE: PCE = 34.98917770385742\n",
      "                              Voc = 31.49167251586914\n",
      "                              Jsc = 37.90300369262695\n",
      "                              FF = 96.29615783691406\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 35.048946380615234\n",
      "                              Voc = 31.02017593383789\n",
      "                              Jsc = 39.37758255004883\n",
      "                              FF = 96.2115478515625\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 34.73161315917969\n",
      "                              Voc = 31.930065155029297\n",
      "                              Jsc = 39.54399490356445\n",
      "                              FF = 95.97348022460938\n",
      "Finished epoch  103\n",
      "On epoch  104\n",
      "Total Epoch Testing MAPE: PCE = 34.67244338989258\n",
      "                              Voc = 32.07875442504883\n",
      "                              Jsc = 40.09198760986328\n",
      "                              FF = 95.81961822509766\n",
      "Finished epoch  104\n",
      "On epoch  105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.61756896972656\n",
      "                              Voc = 32.36318588256836\n",
      "                              Jsc = 40.15388870239258\n",
      "                              FF = 95.67271423339844\n",
      "Finished epoch  105\n",
      "On epoch  106\n",
      "Total Epoch Testing MAPE: PCE = 34.771080017089844\n",
      "                              Voc = 31.826383590698242\n",
      "                              Jsc = 40.422969818115234\n",
      "                              FF = 95.6088638305664\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 34.925933837890625\n",
      "                              Voc = 31.868282318115234\n",
      "                              Jsc = 40.94546890258789\n",
      "                              FF = 95.6104736328125\n",
      "Finished epoch  107\n",
      "On epoch  108\n",
      "Total Epoch Testing MAPE: PCE = 34.5623779296875\n",
      "                              Voc = 31.361848831176758\n",
      "                              Jsc = 40.69724655151367\n",
      "                              FF = 95.55450439453125\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 34.358882904052734\n",
      "                              Voc = 31.0028018951416\n",
      "                              Jsc = 39.4467658996582\n",
      "                              FF = 95.76043701171875\n",
      "Finished epoch  109\n",
      "On epoch  110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.09355926513672\n",
      "                              Voc = 31.9055233001709\n",
      "                              Jsc = 39.03847122192383\n",
      "                              FF = 95.79402923583984\n",
      "Finished epoch  110\n",
      "On epoch  111\n",
      "Total Epoch Testing MAPE: PCE = 34.08517837524414\n",
      "                              Voc = 33.728816986083984\n",
      "                              Jsc = 38.78282165527344\n",
      "                              FF = 95.77034759521484\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 34.21199417114258\n",
      "                              Voc = 34.66782760620117\n",
      "                              Jsc = 39.0405387878418\n",
      "                              FF = 95.70915985107422\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 33.75712203979492\n",
      "                              Voc = 35.42607116699219\n",
      "                              Jsc = 37.792236328125\n",
      "                              FF = 95.73123931884766\n",
      "Finished epoch  113\n",
      "On epoch  114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.43763732910156\n",
      "                              Voc = 36.877323150634766\n",
      "                              Jsc = 37.93326950073242\n",
      "                              FF = 95.75476837158203\n",
      "Finished epoch  114\n",
      "On epoch  115\n",
      "Total Epoch Testing MAPE: PCE = 33.798744201660156\n",
      "                              Voc = 37.7819709777832\n",
      "                              Jsc = 37.9167366027832\n",
      "                              FF = 95.92816925048828\n",
      "Finished epoch  115\n",
      "On epoch  116\n",
      "Total Epoch Testing MAPE: PCE = 33.73188781738281\n",
      "                              Voc = 38.80625915527344\n",
      "                              Jsc = 38.11044692993164\n",
      "                              FF = 96.1032485961914\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 33.841983795166016\n",
      "                              Voc = 38.46171569824219\n",
      "                              Jsc = 38.210113525390625\n",
      "                              FF = 96.14539337158203\n",
      "Finished epoch  117\n",
      "On epoch  118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.551170349121094\n",
      "                              Voc = 39.04499435424805\n",
      "                              Jsc = 38.08396530151367\n",
      "                              FF = 96.13687133789062\n",
      "Finished epoch  118\n",
      "On epoch  119\n",
      "Total Epoch Testing MAPE: PCE = 33.44770812988281\n",
      "                              Voc = 39.70053482055664\n",
      "                              Jsc = 36.6762580871582\n",
      "                              FF = 95.98170471191406\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 33.6811408996582\n",
      "                              Voc = 40.72943878173828\n",
      "                              Jsc = 37.40596389770508\n",
      "                              FF = 95.83865356445312\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 33.55072021484375\n",
      "                              Voc = 41.60372543334961\n",
      "                              Jsc = 37.068260192871094\n",
      "                              FF = 95.76820373535156\n",
      "Finished epoch  121\n",
      "On epoch  122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.58442306518555\n",
      "                              Voc = 42.30210876464844\n",
      "                              Jsc = 37.24563217163086\n",
      "                              FF = 95.81090545654297\n",
      "Finished epoch  122\n",
      "On epoch  123\n",
      "Total Epoch Testing MAPE: PCE = 33.689876556396484\n",
      "                              Voc = 43.04963302612305\n",
      "                              Jsc = 37.03194046020508\n",
      "                              FF = 95.82743072509766\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 33.971275329589844\n",
      "                              Voc = 43.46549987792969\n",
      "                              Jsc = 35.524810791015625\n",
      "                              FF = 95.96224212646484\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 33.996971130371094\n",
      "                              Voc = 43.830806732177734\n",
      "                              Jsc = 34.833473205566406\n",
      "                              FF = 96.04956817626953\n",
      "Finished epoch  125\n",
      "On epoch  126\n",
      "Total Epoch Testing MAPE: PCE = 34.5711784362793\n",
      "                              Voc = 44.154232025146484\n",
      "                              Jsc = 33.519412994384766\n",
      "                              FF = 96.0623550415039\n",
      "Finished epoch  126\n",
      "On epoch  127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.11671447753906\n",
      "                              Voc = 44.24134826660156\n",
      "                              Jsc = 33.727455139160156\n",
      "                              FF = 96.10216522216797\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 35.84891891479492\n",
      "                              Voc = 44.69722366333008\n",
      "                              Jsc = 34.29072952270508\n",
      "                              FF = 96.1787109375\n",
      "Finished epoch  128\n",
      "On epoch  129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 36.64946365356445\n",
      "                              Voc = 43.99758529663086\n",
      "                              Jsc = 33.81303405761719\n",
      "                              FF = 96.29875946044922\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 37.271934509277344\n",
      "                              Voc = 43.11336135864258\n",
      "                              Jsc = 33.60787582397461\n",
      "                              FF = 96.33206176757812\n",
      "Finished epoch  130\n",
      "On epoch  131\n",
      "Total Epoch Testing MAPE: PCE = 37.718563079833984\n",
      "                              Voc = 42.815582275390625\n",
      "                              Jsc = 34.179229736328125\n",
      "                              FF = 96.2243881225586\n",
      "Finished epoch  131\n",
      "On epoch  132\n",
      "Total Epoch Testing MAPE: PCE = 38.24455261230469\n",
      "                              Voc = 43.07721710205078\n",
      "                              Jsc = 34.305904388427734\n",
      "                              FF = 96.29335021972656\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 38.58921432495117\n",
      "                              Voc = 42.45314025878906\n",
      "                              Jsc = 33.79262161254883\n",
      "                              FF = 96.28103637695312\n",
      "Finished epoch  133\n",
      "On epoch  134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.94056701660156\n",
      "                              Voc = 41.55573272705078\n",
      "                              Jsc = 34.37892532348633\n",
      "                              FF = 96.39404296875\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 39.01061248779297\n",
      "                              Voc = 40.29832458496094\n",
      "                              Jsc = 34.476505279541016\n",
      "                              FF = 96.34884643554688\n",
      "Finished epoch  135\n",
      "On epoch  136\n",
      "Total Epoch Testing MAPE: PCE = 39.291351318359375\n",
      "                              Voc = 38.350711822509766\n",
      "                              Jsc = 34.93117141723633\n",
      "                              FF = 96.47282409667969\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 39.69538497924805\n",
      "                              Voc = 37.783302307128906\n",
      "                              Jsc = 35.327430725097656\n",
      "                              FF = 96.36297607421875\n",
      "Finished epoch  137\n",
      "On epoch  138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.91676712036133\n",
      "                              Voc = 36.87953567504883\n",
      "                              Jsc = 35.38978958129883\n",
      "                              FF = 96.36724853515625\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 40.565731048583984\n",
      "                              Voc = 35.976524353027344\n",
      "                              Jsc = 36.016361236572266\n",
      "                              FF = 96.35563659667969\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 40.38380813598633\n",
      "                              Voc = 34.90069580078125\n",
      "                              Jsc = 35.625667572021484\n",
      "                              FF = 96.33232116699219\n",
      "Finished epoch  140\n",
      "On epoch  141\n",
      "Total Epoch Testing MAPE: PCE = 40.130958557128906\n",
      "                              Voc = 34.55442810058594\n",
      "                              Jsc = 36.23970031738281\n",
      "                              FF = 96.33473205566406\n",
      "Finished epoch  141\n",
      "On epoch  142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 40.031288146972656\n",
      "                              Voc = 34.15296173095703\n",
      "                              Jsc = 36.84685516357422\n",
      "                              FF = 96.39204406738281\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 39.45589065551758\n",
      "                              Voc = 33.92390823364258\n",
      "                              Jsc = 36.76122283935547\n",
      "                              FF = 96.28450775146484\n",
      "Finished epoch  143\n",
      "On epoch  144\n",
      "Total Epoch Testing MAPE: PCE = 39.60214614868164\n",
      "                              Voc = 34.171409606933594\n",
      "                              Jsc = 36.48273468017578\n",
      "                              FF = 96.28722381591797\n",
      "Finished epoch  144\n",
      "On epoch  145\n",
      "Total Epoch Testing MAPE: PCE = 39.31720733642578\n",
      "                              Voc = 33.33626937866211\n",
      "                              Jsc = 36.977882385253906\n",
      "                              FF = 96.2850570678711\n",
      "Finished epoch  145\n",
      "On epoch  146\n",
      "Total Epoch Testing MAPE: PCE = 39.026947021484375\n",
      "                              Voc = 33.284671783447266\n",
      "                              Jsc = 35.81380844116211\n",
      "                              FF = 96.23609924316406\n",
      "Finished epoch  146\n",
      "On epoch  147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.788856506347656\n",
      "                              Voc = 32.551795959472656\n",
      "                              Jsc = 35.430397033691406\n",
      "                              FF = 96.27922821044922\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 38.36058807373047\n",
      "                              Voc = 32.32924270629883\n",
      "                              Jsc = 35.34281539916992\n",
      "                              FF = 96.3018798828125\n",
      "Finished epoch  148\n",
      "On epoch  149\n",
      "Total Epoch Testing MAPE: PCE = 37.7412223815918\n",
      "                              Voc = 31.90055274963379\n",
      "                              Jsc = 35.0980224609375\n",
      "                              FF = 96.4363784790039\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 37.64906692504883\n",
      "                              Voc = 31.517702102661133\n",
      "                              Jsc = 35.530845642089844\n",
      "                              FF = 96.50397491455078\n",
      "Finished epoch  150\n",
      "On epoch  151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.65964126586914\n",
      "                              Voc = 30.945709228515625\n",
      "                              Jsc = 35.869361877441406\n",
      "                              FF = 96.45553588867188\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 37.555320739746094\n",
      "                              Voc = 32.417659759521484\n",
      "                              Jsc = 35.4869384765625\n",
      "                              FF = 96.4078598022461\n",
      "Finished epoch  152\n",
      "On epoch  153\n",
      "Total Epoch Testing MAPE: PCE = 37.53934097290039\n",
      "                              Voc = 33.614234924316406\n",
      "                              Jsc = 35.37364196777344\n",
      "                              FF = 96.3924789428711\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 37.66407012939453\n",
      "                              Voc = 34.78748321533203\n",
      "                              Jsc = 34.60285186767578\n",
      "                              FF = 96.13978576660156\n",
      "Finished epoch  154\n",
      "On epoch  155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.69917297363281\n",
      "                              Voc = 36.260860443115234\n",
      "                              Jsc = 34.10974884033203\n",
      "                              FF = 95.87467193603516\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 37.841270446777344\n",
      "                              Voc = 37.201202392578125\n",
      "                              Jsc = 34.33742141723633\n",
      "                              FF = 95.90814971923828\n",
      "Finished epoch  156\n",
      "On epoch  157\n",
      "Total Epoch Testing MAPE: PCE = 37.853126525878906\n",
      "                              Voc = 38.5793571472168\n",
      "                              Jsc = 34.069149017333984\n",
      "                              FF = 95.86825561523438\n",
      "Finished epoch  157\n",
      "On epoch  158\n",
      "Total Epoch Testing MAPE: PCE = 37.926429748535156\n",
      "                              Voc = 39.05789566040039\n",
      "                              Jsc = 33.368629455566406\n",
      "                              FF = 95.94611358642578\n",
      "Finished epoch  158\n",
      "On epoch  159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.50294494628906\n",
      "                              Voc = 38.94853210449219\n",
      "                              Jsc = 33.95246887207031\n",
      "                              FF = 96.084228515625\n",
      "Finished epoch  159\n",
      "On epoch  160\n",
      "Total Epoch Testing MAPE: PCE = 38.107975006103516\n",
      "                              Voc = 40.31999969482422\n",
      "                              Jsc = 34.52621841430664\n",
      "                              FF = 95.99617767333984\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 38.300819396972656\n",
      "                              Voc = 40.38412094116211\n",
      "                              Jsc = 34.85693359375\n",
      "                              FF = 95.89741516113281\n",
      "Finished epoch  161\n",
      "On epoch  162\n",
      "Total Epoch Testing MAPE: PCE = 38.6688117980957\n",
      "                              Voc = 41.15711212158203\n",
      "                              Jsc = 34.34016799926758\n",
      "                              FF = 95.83355712890625\n",
      "Finished epoch  162\n",
      "On epoch  163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.6932258605957\n",
      "                              Voc = 41.321075439453125\n",
      "                              Jsc = 34.97804641723633\n",
      "                              FF = 95.81253814697266\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 39.07160186767578\n",
      "                              Voc = 41.93117141723633\n",
      "                              Jsc = 35.2725944519043\n",
      "                              FF = 95.81281280517578\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 39.469058990478516\n",
      "                              Voc = 41.823333740234375\n",
      "                              Jsc = 35.418296813964844\n",
      "                              FF = 95.86395263671875\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 39.38758850097656\n",
      "                              Voc = 41.85116195678711\n",
      "                              Jsc = 35.34824752807617\n",
      "                              FF = 95.75342559814453\n",
      "Finished epoch  166\n",
      "On epoch  167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.670631408691406\n",
      "                              Voc = 42.524070739746094\n",
      "                              Jsc = 35.0850715637207\n",
      "                              FF = 95.83502960205078\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 39.47999954223633\n",
      "                              Voc = 42.27598571777344\n",
      "                              Jsc = 34.14608383178711\n",
      "                              FF = 95.82546997070312\n",
      "Finished epoch  168\n",
      "On epoch  169\n",
      "Total Epoch Testing MAPE: PCE = 39.65171432495117\n",
      "                              Voc = 42.27880859375\n",
      "                              Jsc = 32.724037170410156\n",
      "                              FF = 95.88473510742188\n",
      "Finished epoch  169\n",
      "On epoch  170\n",
      "Total Epoch Testing MAPE: PCE = 39.39585494995117\n",
      "                              Voc = 42.04873275756836\n",
      "                              Jsc = 32.36841583251953\n",
      "                              FF = 95.95716094970703\n",
      "Finished epoch  170\n",
      "On epoch  171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.93766403198242\n",
      "                              Voc = 41.33833694458008\n",
      "                              Jsc = 31.77582359313965\n",
      "                              FF = 96.0515365600586\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 38.901092529296875\n",
      "                              Voc = 40.606204986572266\n",
      "                              Jsc = 31.855356216430664\n",
      "                              FF = 95.90706634521484\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 38.80359649658203\n",
      "                              Voc = 40.48882293701172\n",
      "                              Jsc = 30.936885833740234\n",
      "                              FF = 96.01467895507812\n",
      "Finished epoch  173\n",
      "On epoch  174\n",
      "Total Epoch Testing MAPE: PCE = 38.84346008300781\n",
      "                              Voc = 40.063018798828125\n",
      "                              Jsc = 30.6951961517334\n",
      "                              FF = 95.8436279296875\n",
      "Finished epoch  174\n",
      "On epoch  175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 39.125328063964844\n",
      "                              Voc = 39.166385650634766\n",
      "                              Jsc = 31.20635414123535\n",
      "                              FF = 95.76141357421875\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 39.01295471191406\n",
      "                              Voc = 37.92551803588867\n",
      "                              Jsc = 30.40376853942871\n",
      "                              FF = 95.9139175415039\n",
      "Finished epoch  176\n",
      "On epoch  177\n",
      "Total Epoch Testing MAPE: PCE = 38.631744384765625\n",
      "                              Voc = 37.14625549316406\n",
      "                              Jsc = 31.51906394958496\n",
      "                              FF = 95.9275131225586\n",
      "Finished epoch  177\n",
      "On epoch  178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.17415237426758\n",
      "                              Voc = 36.64454650878906\n",
      "                              Jsc = 31.1894474029541\n",
      "                              FF = 95.88868713378906\n",
      "Finished epoch  178\n",
      "On epoch  179\n",
      "Total Epoch Testing MAPE: PCE = 37.85941696166992\n",
      "                              Voc = 35.11947250366211\n",
      "                              Jsc = 31.423446655273438\n",
      "                              FF = 95.79700469970703\n",
      "Finished epoch  179\n",
      "On epoch  180\n",
      "Total Epoch Testing MAPE: PCE = 37.953861236572266\n",
      "                              Voc = 35.38325881958008\n",
      "                              Jsc = 31.132816314697266\n",
      "                              FF = 95.59052276611328\n",
      "Finished epoch  180\n",
      "On epoch  181\n",
      "Total Epoch Testing MAPE: PCE = 37.79475402832031\n",
      "                              Voc = 35.44440841674805\n",
      "                              Jsc = 30.908754348754883\n",
      "                              FF = 95.61011505126953\n",
      "Finished epoch  181\n",
      "On epoch  182\n",
      "Total Epoch Testing MAPE: PCE = 37.51773452758789\n",
      "                              Voc = 35.62249755859375\n",
      "                              Jsc = 30.834880828857422\n",
      "                              FF = 95.60975646972656\n",
      "Finished epoch  182\n",
      "On epoch  183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.45946502685547\n",
      "                              Voc = 35.00411605834961\n",
      "                              Jsc = 31.86115264892578\n",
      "                              FF = 95.57594299316406\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 37.87026596069336\n",
      "                              Voc = 35.26445770263672\n",
      "                              Jsc = 31.478830337524414\n",
      "                              FF = 95.65621948242188\n",
      "Finished epoch  184\n",
      "On epoch  185\n",
      "Total Epoch Testing MAPE: PCE = 37.775665283203125\n",
      "                              Voc = 34.96503448486328\n",
      "                              Jsc = 32.055301666259766\n",
      "                              FF = 95.7413101196289\n",
      "Finished epoch  185\n",
      "On epoch  186\n",
      "Total Epoch Testing MAPE: PCE = 37.71672821044922\n",
      "                              Voc = 34.774879455566406\n",
      "                              Jsc = 32.122344970703125\n",
      "                              FF = 95.63717651367188\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 37.803436279296875\n",
      "                              Voc = 33.5766487121582\n",
      "                              Jsc = 32.628971099853516\n",
      "                              FF = 95.56438446044922\n",
      "Finished epoch  187\n",
      "On epoch  188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.69377517700195\n",
      "                              Voc = 34.83692169189453\n",
      "                              Jsc = 33.053531646728516\n",
      "                              FF = 95.51213836669922\n",
      "Finished epoch  188\n",
      "On epoch  189\n",
      "Total Epoch Testing MAPE: PCE = 37.90055465698242\n",
      "                              Voc = 33.69126510620117\n",
      "                              Jsc = 32.96259689331055\n",
      "                              FF = 95.52778625488281\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 37.85282516479492\n",
      "                              Voc = 33.85979080200195\n",
      "                              Jsc = 32.85814666748047\n",
      "                              FF = 95.5257797241211\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 37.552520751953125\n",
      "                              Voc = 34.34880447387695\n",
      "                              Jsc = 33.5308837890625\n",
      "                              FF = 95.47183990478516\n",
      "Finished epoch  191\n",
      "On epoch  192\n",
      "Total Epoch Testing MAPE: PCE = 37.3028678894043\n",
      "                              Voc = 34.214969635009766\n",
      "                              Jsc = 34.28054428100586\n",
      "                              FF = 95.53104400634766\n",
      "Finished epoch  192\n",
      "On epoch  193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.411354064941406\n",
      "                              Voc = 34.092281341552734\n",
      "                              Jsc = 34.584781646728516\n",
      "                              FF = 95.41224670410156\n",
      "Finished epoch  193\n",
      "On epoch  194\n",
      "Total Epoch Testing MAPE: PCE = 37.42646408081055\n",
      "                              Voc = 34.97128677368164\n",
      "                              Jsc = 34.95454025268555\n",
      "                              FF = 95.33000946044922\n",
      "Finished epoch  194\n",
      "On epoch  195\n",
      "Total Epoch Testing MAPE: PCE = 37.22417449951172\n",
      "                              Voc = 35.56959533691406\n",
      "                              Jsc = 34.61541748046875\n",
      "                              FF = 95.30467224121094\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 37.07545852661133\n",
      "                              Voc = 34.789283752441406\n",
      "                              Jsc = 34.040279388427734\n",
      "                              FF = 95.33207702636719\n",
      "Finished epoch  196\n",
      "On epoch  197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.13630294799805\n",
      "                              Voc = 34.4796257019043\n",
      "                              Jsc = 34.20936584472656\n",
      "                              FF = 95.31243133544922\n",
      "Finished epoch  197\n",
      "On epoch  198\n",
      "Total Epoch Testing MAPE: PCE = 37.33495330810547\n",
      "                              Voc = 33.83518981933594\n",
      "                              Jsc = 34.065635681152344\n",
      "                              FF = 95.4110336303711\n",
      "Finished epoch  198\n",
      "On epoch  199\n",
      "Total Epoch Testing MAPE: PCE = 37.53837966918945\n",
      "                              Voc = 34.37900161743164\n",
      "                              Jsc = 33.810752868652344\n",
      "                              FF = 95.2935791015625\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 37.5384407043457\n",
      "                              Voc = 34.071624755859375\n",
      "                              Jsc = 33.148902893066406\n",
      "                              FF = 95.37042999267578\n",
      "Finished epoch  200\n",
      "On epoch  201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.68581008911133\n",
      "                              Voc = 34.20218276977539\n",
      "                              Jsc = 33.26484298706055\n",
      "                              FF = 95.38461303710938\n",
      "Finished epoch  201\n",
      "On epoch  202\n",
      "Total Epoch Testing MAPE: PCE = 38.02627182006836\n",
      "                              Voc = 34.00992965698242\n",
      "                              Jsc = 32.854454040527344\n",
      "                              FF = 95.47285461425781\n",
      "Finished epoch  202\n",
      "On epoch  203\n",
      "Total Epoch Testing MAPE: PCE = 38.04779815673828\n",
      "                              Voc = 33.40583801269531\n",
      "                              Jsc = 32.69581604003906\n",
      "                              FF = 95.3426513671875\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 38.07978057861328\n",
      "                              Voc = 32.3320198059082\n",
      "                              Jsc = 31.816287994384766\n",
      "                              FF = 95.28639221191406\n",
      "Finished epoch  204\n",
      "On epoch  205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.195404052734375\n",
      "                              Voc = 32.908260345458984\n",
      "                              Jsc = 30.679325103759766\n",
      "                              FF = 95.39876556396484\n",
      "Finished epoch  205\n",
      "On epoch  206\n",
      "Total Epoch Testing MAPE: PCE = 38.47208786010742\n",
      "                              Voc = 32.73948287963867\n",
      "                              Jsc = 31.352237701416016\n",
      "                              FF = 95.48594665527344\n",
      "Finished epoch  206\n",
      "On epoch  207\n",
      "Total Epoch Testing MAPE: PCE = 38.50251770019531\n",
      "                              Voc = 33.237335205078125\n",
      "                              Jsc = 30.99342918395996\n",
      "                              FF = 95.46937561035156\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 38.42620086669922\n",
      "                              Voc = 33.29468536376953\n",
      "                              Jsc = 30.83793830871582\n",
      "                              FF = 95.33365631103516\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 38.34648513793945\n",
      "                              Voc = 32.94829559326172\n",
      "                              Jsc = 30.726850509643555\n",
      "                              FF = 95.32625579833984\n",
      "Finished epoch  209\n",
      "On epoch  210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.15046691894531\n",
      "                              Voc = 33.103485107421875\n",
      "                              Jsc = 29.827362060546875\n",
      "                              FF = 95.38763427734375\n",
      "Finished epoch  210\n",
      "On epoch  211\n",
      "Total Epoch Testing MAPE: PCE = 38.040958404541016\n",
      "                              Voc = 32.90105438232422\n",
      "                              Jsc = 29.95499610900879\n",
      "                              FF = 95.3365707397461\n",
      "Finished epoch  211\n",
      "On epoch  212\n",
      "Total Epoch Testing MAPE: PCE = 37.883480072021484\n",
      "                              Voc = 32.62342071533203\n",
      "                              Jsc = 28.91600799560547\n",
      "                              FF = 95.36087799072266\n",
      "Finished epoch  212\n",
      "On epoch  213\n",
      "Total Epoch Testing MAPE: PCE = 37.892669677734375\n",
      "                              Voc = 31.523704528808594\n",
      "                              Jsc = 28.276935577392578\n",
      "                              FF = 95.24115753173828\n",
      "Finished epoch  213\n",
      "On epoch  214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.75535202026367\n",
      "                              Voc = 31.82788848876953\n",
      "                              Jsc = 27.99896812438965\n",
      "                              FF = 95.3818588256836\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 37.60926818847656\n",
      "                              Voc = 32.35832214355469\n",
      "                              Jsc = 27.926549911499023\n",
      "                              FF = 95.12724304199219\n",
      "Finished epoch  215\n",
      "On epoch  216\n",
      "Total Epoch Testing MAPE: PCE = 37.20489501953125\n",
      "                              Voc = 33.06536102294922\n",
      "                              Jsc = 27.36605453491211\n",
      "                              FF = 94.97856140136719\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 37.44554901123047\n",
      "                              Voc = 32.624290466308594\n",
      "                              Jsc = 27.970096588134766\n",
      "                              FF = 94.87764739990234\n",
      "Finished epoch  217\n",
      "On epoch  218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.34913635253906\n",
      "                              Voc = 32.029571533203125\n",
      "                              Jsc = 29.38542938232422\n",
      "                              FF = 94.9808578491211\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 37.17404556274414\n",
      "                              Voc = 31.62052345275879\n",
      "                              Jsc = 29.208904266357422\n",
      "                              FF = 94.79486083984375\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 37.091087341308594\n",
      "                              Voc = 31.420469284057617\n",
      "                              Jsc = 29.532161712646484\n",
      "                              FF = 94.79976654052734\n",
      "Finished epoch  220\n",
      "On epoch  221\n",
      "Total Epoch Testing MAPE: PCE = 37.19017028808594\n",
      "                              Voc = 31.732580184936523\n",
      "                              Jsc = 30.66926383972168\n",
      "                              FF = 95.13190460205078\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 37.510963439941406\n",
      "                              Voc = 31.64116096496582\n",
      "                              Jsc = 31.047883987426758\n",
      "                              FF = 95.16535949707031\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 37.759559631347656\n",
      "                              Voc = 31.553173065185547\n",
      "                              Jsc = 31.340349197387695\n",
      "                              FF = 95.35929870605469\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 37.816009521484375\n",
      "                              Voc = 31.66912841796875\n",
      "                              Jsc = 31.254493713378906\n",
      "                              FF = 95.3365249633789\n",
      "Finished epoch  224\n",
      "On epoch  225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.793216705322266\n",
      "                              Voc = 32.349037170410156\n",
      "                              Jsc = 31.01387596130371\n",
      "                              FF = 95.32167053222656\n",
      "Finished epoch  225\n",
      "On epoch  226\n",
      "Total Epoch Testing MAPE: PCE = 37.97922897338867\n",
      "                              Voc = 31.330507278442383\n",
      "                              Jsc = 31.2841739654541\n",
      "                              FF = 94.9942626953125\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 38.03842544555664\n",
      "                              Voc = 32.251121520996094\n",
      "                              Jsc = 32.328277587890625\n",
      "                              FF = 94.8083724975586\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 38.23994827270508\n",
      "                              Voc = 33.73652648925781\n",
      "                              Jsc = 32.754764556884766\n",
      "                              FF = 94.65072631835938\n",
      "Finished epoch  228\n",
      "On epoch  229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.54228210449219\n",
      "                              Voc = 33.87649917602539\n",
      "                              Jsc = 33.349830627441406\n",
      "                              FF = 94.6402359008789\n",
      "Finished epoch  229\n",
      "On epoch  230\n",
      "Total Epoch Testing MAPE: PCE = 38.17601013183594\n",
      "                              Voc = 34.180137634277344\n",
      "                              Jsc = 34.290679931640625\n",
      "                              FF = 94.57260131835938\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 37.66245651245117\n",
      "                              Voc = 33.72661590576172\n",
      "                              Jsc = 35.174495697021484\n",
      "                              FF = 94.48918151855469\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 37.46614456176758\n",
      "                              Voc = 32.835201263427734\n",
      "                              Jsc = 35.608455657958984\n",
      "                              FF = 94.43191528320312\n",
      "Finished epoch  232\n",
      "On epoch  233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.29677200317383\n",
      "                              Voc = 31.969865798950195\n",
      "                              Jsc = 34.842735290527344\n",
      "                              FF = 94.25856018066406\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 36.95402908325195\n",
      "                              Voc = 31.322830200195312\n",
      "                              Jsc = 34.61922836303711\n",
      "                              FF = 94.66146087646484\n",
      "Finished epoch  234\n",
      "On epoch  235\n",
      "Total Epoch Testing MAPE: PCE = 36.86558532714844\n",
      "                              Voc = 30.79255485534668\n",
      "                              Jsc = 34.115867614746094\n",
      "                              FF = 94.88497924804688\n",
      "Finished epoch  235\n",
      "On epoch  236\n",
      "Total Epoch Testing MAPE: PCE = 36.26471710205078\n",
      "                              Voc = 30.34827995300293\n",
      "                              Jsc = 33.38417053222656\n",
      "                              FF = 94.73970794677734\n",
      "Finished epoch  236\n",
      "On epoch  237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 36.34911346435547\n",
      "                              Voc = 29.329002380371094\n",
      "                              Jsc = 33.74173355102539\n",
      "                              FF = 94.76895141601562\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 36.19063949584961\n",
      "                              Voc = 27.662458419799805\n",
      "                              Jsc = 33.21017074584961\n",
      "                              FF = 94.82709503173828\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 36.302791595458984\n",
      "                              Voc = 26.630428314208984\n",
      "                              Jsc = 32.16646194458008\n",
      "                              FF = 94.83025360107422\n",
      "Finished epoch  239\n",
      "On epoch  240\n",
      "Total Epoch Testing MAPE: PCE = 36.60621643066406\n",
      "                              Voc = 26.034822463989258\n",
      "                              Jsc = 31.35574722290039\n",
      "                              FF = 94.92091369628906\n",
      "Finished epoch  240\n",
      "On epoch  241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.01128387451172\n",
      "                              Voc = 24.275489807128906\n",
      "                              Jsc = 31.30486297607422\n",
      "                              FF = 95.12519073486328\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 37.451717376708984\n",
      "                              Voc = 23.0344295501709\n",
      "                              Jsc = 30.93336296081543\n",
      "                              FF = 95.10619354248047\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 37.54471206665039\n",
      "                              Voc = 23.56377410888672\n",
      "                              Jsc = 30.444162368774414\n",
      "                              FF = 95.06981658935547\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 37.53078079223633\n",
      "                              Voc = 23.464536666870117\n",
      "                              Jsc = 30.352384567260742\n",
      "                              FF = 95.19861602783203\n",
      "Finished epoch  244\n",
      "On epoch  245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.472835540771484\n",
      "                              Voc = 23.03240203857422\n",
      "                              Jsc = 31.36860466003418\n",
      "                              FF = 95.17243957519531\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 37.31647872924805\n",
      "                              Voc = 22.92003059387207\n",
      "                              Jsc = 32.508583068847656\n",
      "                              FF = 95.21770477294922\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 37.13118362426758\n",
      "                              Voc = 23.105770111083984\n",
      "                              Jsc = 33.715675354003906\n",
      "                              FF = 95.00774383544922\n",
      "Finished epoch  247\n",
      "On epoch  248\n",
      "Total Epoch Testing MAPE: PCE = 37.34286880493164\n",
      "                              Voc = 22.846435546875\n",
      "                              Jsc = 33.68930435180664\n",
      "                              FF = 95.04349517822266\n",
      "Finished epoch  248\n",
      "On epoch  249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.33124542236328\n",
      "                              Voc = 23.538612365722656\n",
      "                              Jsc = 34.23947525024414\n",
      "                              FF = 94.75202178955078\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 37.18095397949219\n",
      "                              Voc = 24.431533813476562\n",
      "                              Jsc = 34.04743576049805\n",
      "                              FF = 94.62886810302734\n",
      "Finished epoch  250\n",
      "On epoch  251\n",
      "Total Epoch Testing MAPE: PCE = 37.221500396728516\n",
      "                              Voc = 25.60690689086914\n",
      "                              Jsc = 33.63014602661133\n",
      "                              FF = 94.56221008300781\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 37.4110107421875\n",
      "                              Voc = 27.070594787597656\n",
      "                              Jsc = 33.67497253417969\n",
      "                              FF = 94.52647399902344\n",
      "Finished epoch  252\n",
      "On epoch  253\n",
      "Total Epoch Testing MAPE: PCE = 37.37636947631836\n",
      "                              Voc = 28.473018646240234\n",
      "                              Jsc = 33.6303825378418\n",
      "                              FF = 94.42318725585938\n",
      "Finished epoch  253\n",
      "On epoch  254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.47682571411133\n",
      "                              Voc = 29.089344024658203\n",
      "                              Jsc = 33.70842361450195\n",
      "                              FF = 94.583740234375\n",
      "Finished epoch  254\n",
      "On epoch  255\n",
      "Total Epoch Testing MAPE: PCE = 37.327423095703125\n",
      "                              Voc = 29.6697998046875\n",
      "                              Jsc = 32.493858337402344\n",
      "                              FF = 94.50640869140625\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 37.122169494628906\n",
      "                              Voc = 29.82856559753418\n",
      "                              Jsc = 32.127586364746094\n",
      "                              FF = 94.36630249023438\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 36.74082565307617\n",
      "                              Voc = 29.532182693481445\n",
      "                              Jsc = 31.455167770385742\n",
      "                              FF = 94.18865966796875\n",
      "Finished epoch  257\n",
      "On epoch  258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 36.7317008972168\n",
      "                              Voc = 29.719961166381836\n",
      "                              Jsc = 30.673837661743164\n",
      "                              FF = 94.44224548339844\n",
      "Finished epoch  258\n",
      "On epoch  259\n",
      "Total Epoch Testing MAPE: PCE = 36.92060852050781\n",
      "                              Voc = 29.666698455810547\n",
      "                              Jsc = 30.149003982543945\n",
      "                              FF = 94.75592041015625\n",
      "Finished epoch  259\n",
      "On epoch  260\n",
      "Total Epoch Testing MAPE: PCE = 37.235496520996094\n",
      "                              Voc = 29.52400779724121\n",
      "                              Jsc = 29.79978370666504\n",
      "                              FF = 94.58043670654297\n",
      "Finished epoch  260\n",
      "On epoch  261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.328521728515625\n",
      "                              Voc = 30.130443572998047\n",
      "                              Jsc = 29.244665145874023\n",
      "                              FF = 94.62681579589844\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 37.24723815917969\n",
      "                              Voc = 30.704286575317383\n",
      "                              Jsc = 29.318639755249023\n",
      "                              FF = 94.44667053222656\n",
      "Finished epoch  262\n",
      "On epoch  263\n",
      "Total Epoch Testing MAPE: PCE = 37.24911117553711\n",
      "                              Voc = 30.832862854003906\n",
      "                              Jsc = 28.90506935119629\n",
      "                              FF = 94.24365997314453\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 37.4279899597168\n",
      "                              Voc = 31.488901138305664\n",
      "                              Jsc = 29.77338409423828\n",
      "                              FF = 94.11902618408203\n",
      "Finished epoch  264\n",
      "On epoch  265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.06640625\n",
      "                              Voc = 31.658100128173828\n",
      "                              Jsc = 30.664512634277344\n",
      "                              FF = 94.43338775634766\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 37.0539665222168\n",
      "                              Voc = 32.7136116027832\n",
      "                              Jsc = 31.257152557373047\n",
      "                              FF = 94.37535858154297\n",
      "Finished epoch  266\n",
      "On epoch  267\n",
      "Total Epoch Testing MAPE: PCE = 37.3935546875\n",
      "                              Voc = 34.04692077636719\n",
      "                              Jsc = 32.23783493041992\n",
      "                              FF = 94.40441131591797\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 37.504940032958984\n",
      "                              Voc = 33.91580581665039\n",
      "                              Jsc = 33.01905822753906\n",
      "                              FF = 94.6109390258789\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 37.44484329223633\n",
      "                              Voc = 34.05992889404297\n",
      "                              Jsc = 33.7962760925293\n",
      "                              FF = 94.776611328125\n",
      "Finished epoch  269\n",
      "On epoch  270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.0899658203125\n",
      "                              Voc = 34.64939498901367\n",
      "                              Jsc = 34.27186965942383\n",
      "                              FF = 94.74263763427734\n",
      "Finished epoch  270\n",
      "On epoch  271\n",
      "Total Epoch Testing MAPE: PCE = 36.904178619384766\n",
      "                              Voc = 34.59414291381836\n",
      "                              Jsc = 34.83896255493164\n",
      "                              FF = 94.73262786865234\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 36.499900817871094\n",
      "                              Voc = 34.41184616088867\n",
      "                              Jsc = 36.40102005004883\n",
      "                              FF = 94.46574401855469\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 35.984352111816406\n",
      "                              Voc = 34.94050216674805\n",
      "                              Jsc = 36.45980453491211\n",
      "                              FF = 94.54559326171875\n",
      "Finished epoch  273\n",
      "On epoch  274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 36.03544235229492\n",
      "                              Voc = 34.577247619628906\n",
      "                              Jsc = 36.77340316772461\n",
      "                              FF = 94.44015502929688\n",
      "Finished epoch  274\n",
      "On epoch  275\n",
      "Total Epoch Testing MAPE: PCE = 35.94264221191406\n",
      "                              Voc = 34.067081451416016\n",
      "                              Jsc = 36.64543151855469\n",
      "                              FF = 94.11995697021484\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 36.22435760498047\n",
      "                              Voc = 32.9487190246582\n",
      "                              Jsc = 36.090728759765625\n",
      "                              FF = 94.15218353271484\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 36.44157791137695\n",
      "                              Voc = 32.549251556396484\n",
      "                              Jsc = 35.758235931396484\n",
      "                              FF = 94.22051239013672\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 36.63741683959961\n",
      "                              Voc = 32.92987060546875\n",
      "                              Jsc = 36.03690719604492\n",
      "                              FF = 94.1604232788086\n",
      "Finished epoch  278\n",
      "On epoch  279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 36.7083625793457\n",
      "                              Voc = 32.95929718017578\n",
      "                              Jsc = 36.301055908203125\n",
      "                              FF = 94.05352783203125\n",
      "Finished epoch  279\n",
      "On epoch  280\n",
      "Total Epoch Testing MAPE: PCE = 36.61773681640625\n",
      "                              Voc = 31.978702545166016\n",
      "                              Jsc = 36.01802444458008\n",
      "                              FF = 94.03175354003906\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 36.670169830322266\n",
      "                              Voc = 30.762300491333008\n",
      "                              Jsc = 36.042232513427734\n",
      "                              FF = 94.11152648925781\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 36.70228576660156\n",
      "                              Voc = 30.694448471069336\n",
      "                              Jsc = 35.72369384765625\n",
      "                              FF = 94.1065444946289\n",
      "Finished epoch  282\n",
      "On epoch  283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 37.05868911743164\n",
      "                              Voc = 29.997392654418945\n",
      "                              Jsc = 34.95011901855469\n",
      "                              FF = 93.92976379394531\n",
      "Finished epoch  283\n",
      "On epoch  284\n",
      "Total Epoch Testing MAPE: PCE = 37.34553909301758\n",
      "                              Voc = 29.289615631103516\n",
      "                              Jsc = 34.88626480102539\n",
      "                              FF = 93.84963989257812\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 37.04666519165039\n",
      "                              Voc = 30.009052276611328\n",
      "                              Jsc = 34.90681457519531\n",
      "                              FF = 93.83688354492188\n",
      "Finished epoch  285\n",
      "On epoch  286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 36.689918518066406\n",
      "                              Voc = 29.967100143432617\n",
      "                              Jsc = 35.12450408935547\n",
      "                              FF = 93.8433609008789\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 36.45344543457031\n",
      "                              Voc = 29.939794540405273\n",
      "                              Jsc = 35.0203971862793\n",
      "                              FF = 93.8376235961914\n",
      "Finished epoch  287\n",
      "On epoch  288\n",
      "Total Epoch Testing MAPE: PCE = 36.19719696044922\n",
      "                              Voc = 29.685157775878906\n",
      "                              Jsc = 35.323760986328125\n",
      "                              FF = 93.64900207519531\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 36.09696960449219\n",
      "                              Voc = 30.29121208190918\n",
      "                              Jsc = 35.27617263793945\n",
      "                              FF = 93.86969757080078\n",
      "Finished epoch  289\n",
      "On epoch  290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.883453369140625\n",
      "                              Voc = 31.34041404724121\n",
      "                              Jsc = 34.77214431762695\n",
      "                              FF = 93.65802764892578\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 35.63088607788086\n",
      "                              Voc = 31.962934494018555\n",
      "                              Jsc = 35.15424346923828\n",
      "                              FF = 93.71527099609375\n",
      "Finished epoch  291\n",
      "On epoch  292\n",
      "Total Epoch Testing MAPE: PCE = 35.493988037109375\n",
      "                              Voc = 33.64546585083008\n",
      "                              Jsc = 34.34465789794922\n",
      "                              FF = 93.61587524414062\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 35.43577194213867\n",
      "                              Voc = 34.37611389160156\n",
      "                              Jsc = 34.60391616821289\n",
      "                              FF = 93.55131530761719\n",
      "Finished epoch  293\n",
      "On epoch  294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.272796630859375\n",
      "                              Voc = 34.690494537353516\n",
      "                              Jsc = 34.52885437011719\n",
      "                              FF = 93.42655181884766\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 35.220054626464844\n",
      "                              Voc = 34.06620788574219\n",
      "                              Jsc = 34.3815803527832\n",
      "                              FF = 93.39214324951172\n",
      "Finished epoch  295\n",
      "On epoch  296\n",
      "Total Epoch Testing MAPE: PCE = 35.362361907958984\n",
      "                              Voc = 33.66499328613281\n",
      "                              Jsc = 34.065433502197266\n",
      "                              FF = 93.31625366210938\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 35.6455192565918\n",
      "                              Voc = 32.49627685546875\n",
      "                              Jsc = 33.87491226196289\n",
      "                              FF = 93.47681427001953\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 35.70602035522461\n",
      "                              Voc = 31.84501838684082\n",
      "                              Jsc = 33.867801666259766\n",
      "                              FF = 93.4627685546875\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 35.691864013671875\n",
      "                              Voc = 31.983253479003906\n",
      "                              Jsc = 33.8387565612793\n",
      "                              FF = 93.43535614013672\n",
      "Finished epoch  299\n",
      "On epoch  300\n",
      "Total Epoch Testing MAPE: PCE = 35.501853942871094\n",
      "                              Voc = 31.72671890258789\n",
      "                              Jsc = 33.08293914794922\n",
      "                              FF = 93.43529510498047\n",
      "Finished epoch  300\n",
      "On epoch  301\n",
      "Total Epoch Testing MAPE: PCE = 35.35901641845703\n",
      "                              Voc = 31.546628952026367\n",
      "                              Jsc = 33.571659088134766\n",
      "                              FF = 93.51301574707031\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 35.14727020263672\n",
      "                              Voc = 30.59023666381836\n",
      "                              Jsc = 34.25550079345703\n",
      "                              FF = 93.56360626220703\n",
      "Finished epoch  302\n",
      "On epoch  303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.89006423950195\n",
      "                              Voc = 30.398706436157227\n",
      "                              Jsc = 34.580665588378906\n",
      "                              FF = 93.52001953125\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 34.715850830078125\n",
      "                              Voc = 30.96002960205078\n",
      "                              Jsc = 35.217002868652344\n",
      "                              FF = 93.51050567626953\n",
      "Finished epoch  304\n",
      "On epoch  305\n",
      "Total Epoch Testing MAPE: PCE = 34.53188705444336\n",
      "                              Voc = 31.43020248413086\n",
      "                              Jsc = 35.240577697753906\n",
      "                              FF = 93.50105285644531\n",
      "Finished epoch  305\n",
      "On epoch  306\n",
      "Total Epoch Testing MAPE: PCE = 34.64264678955078\n",
      "                              Voc = 31.733903884887695\n",
      "                              Jsc = 35.646766662597656\n",
      "                              FF = 93.61372375488281\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 35.007686614990234\n",
      "                              Voc = 32.59532165527344\n",
      "                              Jsc = 35.59991455078125\n",
      "                              FF = 93.47040557861328\n",
      "Finished epoch  307\n",
      "On epoch  308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.41813659667969\n",
      "                              Voc = 32.06180191040039\n",
      "                              Jsc = 34.78197479248047\n",
      "                              FF = 93.46167755126953\n",
      "Finished epoch  308\n",
      "On epoch  309\n",
      "Total Epoch Testing MAPE: PCE = 35.70909881591797\n",
      "                              Voc = 31.86945915222168\n",
      "                              Jsc = 34.391780853271484\n",
      "                              FF = 93.18534088134766\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 35.82459259033203\n",
      "                              Voc = 31.332735061645508\n",
      "                              Jsc = 34.44910430908203\n",
      "                              FF = 93.1797103881836\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 36.28528594970703\n",
      "                              Voc = 30.57245635986328\n",
      "                              Jsc = 34.78388595581055\n",
      "                              FF = 92.8949203491211\n",
      "Finished epoch  311\n",
      "On epoch  312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 36.408321380615234\n",
      "                              Voc = 30.57069206237793\n",
      "                              Jsc = 34.81864929199219\n",
      "                              FF = 93.17237091064453\n",
      "Finished epoch  312\n",
      "On epoch  313\n",
      "Total Epoch Testing MAPE: PCE = 36.14583969116211\n",
      "                              Voc = 30.625877380371094\n",
      "                              Jsc = 35.30769729614258\n",
      "                              FF = 93.10943603515625\n",
      "Finished epoch  313\n",
      "On epoch  314\n",
      "Total Epoch Testing MAPE: PCE = 35.9451789855957\n",
      "                              Voc = 31.125349044799805\n",
      "                              Jsc = 35.06608581542969\n",
      "                              FF = 93.23401641845703\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 35.22520065307617\n",
      "                              Voc = 32.23585510253906\n",
      "                              Jsc = 35.75730895996094\n",
      "                              FF = 93.14834594726562\n",
      "Finished epoch  315\n",
      "On epoch  316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.8955078125\n",
      "                              Voc = 31.73583221435547\n",
      "                              Jsc = 35.84627151489258\n",
      "                              FF = 93.07762908935547\n",
      "Finished epoch  316\n",
      "On epoch  317\n",
      "Total Epoch Testing MAPE: PCE = 34.248111724853516\n",
      "                              Voc = 31.815284729003906\n",
      "                              Jsc = 36.19071960449219\n",
      "                              FF = 93.11656951904297\n",
      "Finished epoch  317\n",
      "On epoch  318\n",
      "Total Epoch Testing MAPE: PCE = 33.968116760253906\n",
      "                              Voc = 31.82952117919922\n",
      "                              Jsc = 35.923160552978516\n",
      "                              FF = 92.90652465820312\n",
      "Finished epoch  318\n",
      "On epoch  319\n",
      "Total Epoch Testing MAPE: PCE = 33.578582763671875\n",
      "                              Voc = 31.882164001464844\n",
      "                              Jsc = 36.121097564697266\n",
      "                              FF = 93.0286865234375\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 33.237701416015625\n",
      "                              Voc = 31.466415405273438\n",
      "                              Jsc = 35.44136428833008\n",
      "                              FF = 93.0850830078125\n",
      "Finished epoch  320\n",
      "On epoch  321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.836360931396484\n",
      "                              Voc = 30.744033813476562\n",
      "                              Jsc = 34.85137939453125\n",
      "                              FF = 93.28062438964844\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 32.87946701049805\n",
      "                              Voc = 29.890178680419922\n",
      "                              Jsc = 35.05116271972656\n",
      "                              FF = 93.460693359375\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 32.81740188598633\n",
      "                              Voc = 29.48185157775879\n",
      "                              Jsc = 35.452110290527344\n",
      "                              FF = 93.42479705810547\n",
      "Finished epoch  323\n",
      "On epoch  324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.067996978759766\n",
      "                              Voc = 29.281166076660156\n",
      "                              Jsc = 35.279815673828125\n",
      "                              FF = 93.53741455078125\n",
      "Finished epoch  324\n",
      "On epoch  325\n",
      "Total Epoch Testing MAPE: PCE = 33.09202194213867\n",
      "                              Voc = 29.526391983032227\n",
      "                              Jsc = 34.9586296081543\n",
      "                              FF = 93.47239685058594\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 33.662261962890625\n",
      "                              Voc = 30.49131965637207\n",
      "                              Jsc = 34.52150344848633\n",
      "                              FF = 93.58000183105469\n",
      "Finished epoch  326\n",
      "On epoch  327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.347206115722656\n",
      "                              Voc = 29.16518211364746\n",
      "                              Jsc = 33.86510467529297\n",
      "                              FF = 93.77113342285156\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 35.15745544433594\n",
      "                              Voc = 28.329627990722656\n",
      "                              Jsc = 33.244903564453125\n",
      "                              FF = 94.0262222290039\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 35.61171340942383\n",
      "                              Voc = 27.652498245239258\n",
      "                              Jsc = 33.457420349121094\n",
      "                              FF = 93.99758911132812\n",
      "Finished epoch  329\n",
      "On epoch  330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.85430145263672\n",
      "                              Voc = 27.070554733276367\n",
      "                              Jsc = 33.452056884765625\n",
      "                              FF = 93.92406463623047\n",
      "Finished epoch  330\n",
      "On epoch  331\n",
      "Total Epoch Testing MAPE: PCE = 36.09899139404297\n",
      "                              Voc = 25.994565963745117\n",
      "                              Jsc = 33.35890197753906\n",
      "                              FF = 93.79405975341797\n",
      "Finished epoch  331\n",
      "On epoch  332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.91007995605469\n",
      "                              Voc = 26.086055755615234\n",
      "                              Jsc = 34.15729904174805\n",
      "                              FF = 93.95538330078125\n",
      "Finished epoch  332\n",
      "On epoch  333\n",
      "Total Epoch Testing MAPE: PCE = 36.030738830566406\n",
      "                              Voc = 26.698928833007812\n",
      "                              Jsc = 33.88390350341797\n",
      "                              FF = 94.0511245727539\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 35.93662643432617\n",
      "                              Voc = 26.752927780151367\n",
      "                              Jsc = 34.066951751708984\n",
      "                              FF = 93.96701049804688\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 35.497657775878906\n",
      "                              Voc = 27.355743408203125\n",
      "                              Jsc = 33.32785415649414\n",
      "                              FF = 94.04721069335938\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 35.24101638793945\n",
      "                              Voc = 27.989912033081055\n",
      "                              Jsc = 33.21847915649414\n",
      "                              FF = 93.72369384765625\n",
      "Finished epoch  336\n",
      "On epoch  337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.237056732177734\n",
      "                              Voc = 28.522762298583984\n",
      "                              Jsc = 33.472572326660156\n",
      "                              FF = 93.48938751220703\n",
      "Finished epoch  337\n",
      "On epoch  338\n",
      "Total Epoch Testing MAPE: PCE = 35.2270393371582\n",
      "                              Voc = 28.166383743286133\n",
      "                              Jsc = 33.34564971923828\n",
      "                              FF = 93.45899963378906\n",
      "Finished epoch  338\n",
      "On epoch  339\n",
      "Total Epoch Testing MAPE: PCE = 35.268226623535156\n",
      "                              Voc = 28.09603500366211\n",
      "                              Jsc = 33.27906799316406\n",
      "                              FF = 93.52131652832031\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 35.457576751708984\n",
      "                              Voc = 27.392274856567383\n",
      "                              Jsc = 32.995079040527344\n",
      "                              FF = 93.32289123535156\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 35.57294845581055\n",
      "                              Voc = 27.38880157470703\n",
      "                              Jsc = 33.522682189941406\n",
      "                              FF = 93.06742095947266\n",
      "Finished epoch  341\n",
      "On epoch  342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.79753494262695\n",
      "                              Voc = 27.637895584106445\n",
      "                              Jsc = 33.32041931152344\n",
      "                              FF = 93.0644760131836\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 35.944793701171875\n",
      "                              Voc = 27.093448638916016\n",
      "                              Jsc = 33.317298889160156\n",
      "                              FF = 92.8470458984375\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 35.95820236206055\n",
      "                              Voc = 27.350940704345703\n",
      "                              Jsc = 33.61658477783203\n",
      "                              FF = 92.94976806640625\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 35.6717643737793\n",
      "                              Voc = 26.642349243164062\n",
      "                              Jsc = 33.3389892578125\n",
      "                              FF = 92.73662567138672\n",
      "Finished epoch  345\n",
      "On epoch  346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.53274917602539\n",
      "                              Voc = 25.446584701538086\n",
      "                              Jsc = 33.127647399902344\n",
      "                              FF = 93.00846099853516\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 35.38336181640625\n",
      "                              Voc = 26.116809844970703\n",
      "                              Jsc = 32.51523208618164\n",
      "                              FF = 92.96099853515625\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 35.260868072509766\n",
      "                              Voc = 25.898239135742188\n",
      "                              Jsc = 32.45429229736328\n",
      "                              FF = 92.96868133544922\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 35.41887283325195\n",
      "                              Voc = 25.251861572265625\n",
      "                              Jsc = 32.490028381347656\n",
      "                              FF = 92.94611358642578\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 35.46617889404297\n",
      "                              Voc = 24.730003356933594\n",
      "                              Jsc = 32.30718231201172\n",
      "                              FF = 92.71865844726562\n",
      "Finished epoch  350\n",
      "On epoch  351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.622352600097656\n",
      "                              Voc = 24.55768585205078\n",
      "                              Jsc = 32.56829071044922\n",
      "                              FF = 92.51424407958984\n",
      "Finished epoch  351\n",
      "On epoch  352\n",
      "Total Epoch Testing MAPE: PCE = 35.5375862121582\n",
      "                              Voc = 25.033573150634766\n",
      "                              Jsc = 32.86308288574219\n",
      "                              FF = 92.42473602294922\n",
      "Finished epoch  352\n",
      "On epoch  353\n",
      "Total Epoch Testing MAPE: PCE = 35.50493240356445\n",
      "                              Voc = 25.64236831665039\n",
      "                              Jsc = 32.94617462158203\n",
      "                              FF = 92.24412536621094\n",
      "Finished epoch  353\n",
      "On epoch  354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.462100982666016\n",
      "                              Voc = 26.314470291137695\n",
      "                              Jsc = 33.658851623535156\n",
      "                              FF = 92.07551574707031\n",
      "Finished epoch  354\n",
      "On epoch  355\n",
      "Total Epoch Testing MAPE: PCE = 35.34413146972656\n",
      "                              Voc = 27.13485336303711\n",
      "                              Jsc = 33.318729400634766\n",
      "                              FF = 92.03081512451172\n",
      "Finished epoch  355\n",
      "On epoch  356\n",
      "Total Epoch Testing MAPE: PCE = 35.41387176513672\n",
      "                              Voc = 27.645687103271484\n",
      "                              Jsc = 33.85542678833008\n",
      "                              FF = 91.77490234375\n",
      "Finished epoch  356\n",
      "On epoch  357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.2468376159668\n",
      "                              Voc = 27.980127334594727\n",
      "                              Jsc = 33.45645523071289\n",
      "                              FF = 91.67228698730469\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 35.01299285888672\n",
      "                              Voc = 28.592926025390625\n",
      "                              Jsc = 33.16259002685547\n",
      "                              FF = 91.62811279296875\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 34.180213928222656\n",
      "                              Voc = 29.29350471496582\n",
      "                              Jsc = 32.883846282958984\n",
      "                              FF = 91.63684844970703\n",
      "Finished epoch  359\n",
      "On epoch  360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.82225799560547\n",
      "                              Voc = 29.904142379760742\n",
      "                              Jsc = 33.41989517211914\n",
      "                              FF = 91.4114761352539\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 33.721221923828125\n",
      "                              Voc = 30.295791625976562\n",
      "                              Jsc = 33.53452682495117\n",
      "                              FF = 91.38641357421875\n",
      "Finished epoch  361\n",
      "On epoch  362\n",
      "Total Epoch Testing MAPE: PCE = 33.64334487915039\n",
      "                              Voc = 30.411602020263672\n",
      "                              Jsc = 33.248756408691406\n",
      "                              FF = 91.6339111328125\n",
      "Finished epoch  362\n",
      "On epoch  363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.435638427734375\n",
      "                              Voc = 30.87680435180664\n",
      "                              Jsc = 33.06560516357422\n",
      "                              FF = 91.67543029785156\n",
      "Finished epoch  363\n",
      "On epoch  364\n",
      "Total Epoch Testing MAPE: PCE = 33.276405334472656\n",
      "                              Voc = 30.774837493896484\n",
      "                              Jsc = 31.930330276489258\n",
      "                              FF = 91.59081268310547\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 33.324562072753906\n",
      "                              Voc = 31.145719528198242\n",
      "                              Jsc = 32.38362503051758\n",
      "                              FF = 91.51307678222656\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 33.46050262451172\n",
      "                              Voc = 30.601152420043945\n",
      "                              Jsc = 31.499174118041992\n",
      "                              FF = 91.55367279052734\n",
      "Finished epoch  366\n",
      "On epoch  367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.58420944213867\n",
      "                              Voc = 30.758445739746094\n",
      "                              Jsc = 31.750032424926758\n",
      "                              FF = 91.50392150878906\n",
      "Finished epoch  367\n",
      "On epoch  368\n",
      "Total Epoch Testing MAPE: PCE = 33.65155029296875\n",
      "                              Voc = 30.206315994262695\n",
      "                              Jsc = 31.75140953063965\n",
      "                              FF = 91.28736877441406\n",
      "Finished epoch  368\n",
      "On epoch  369\n",
      "Total Epoch Testing MAPE: PCE = 33.698062896728516\n",
      "                              Voc = 29.591279983520508\n",
      "                              Jsc = 31.20463752746582\n",
      "                              FF = 91.43827819824219\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 33.97270965576172\n",
      "                              Voc = 28.124156951904297\n",
      "                              Jsc = 31.944719314575195\n",
      "                              FF = 91.53667449951172\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 34.171302795410156\n",
      "                              Voc = 26.596054077148438\n",
      "                              Jsc = 32.28944778442383\n",
      "                              FF = 91.56230926513672\n",
      "Finished epoch  371\n",
      "On epoch  372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.26972198486328\n",
      "                              Voc = 27.091577529907227\n",
      "                              Jsc = 32.30366897583008\n",
      "                              FF = 91.20622253417969\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 34.363590240478516\n",
      "                              Voc = 28.074338912963867\n",
      "                              Jsc = 32.06694030761719\n",
      "                              FF = 91.15409088134766\n",
      "Finished epoch  373\n",
      "On epoch  374\n",
      "Total Epoch Testing MAPE: PCE = 34.41908264160156\n",
      "                              Voc = 28.439044952392578\n",
      "                              Jsc = 32.013282775878906\n",
      "                              FF = 91.23086547851562\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 34.663330078125\n",
      "                              Voc = 27.954504013061523\n",
      "                              Jsc = 32.08594512939453\n",
      "                              FF = 91.29444122314453\n",
      "Finished epoch  375\n",
      "On epoch  376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.74847412109375\n",
      "                              Voc = 27.629756927490234\n",
      "                              Jsc = 31.998720169067383\n",
      "                              FF = 91.1672592163086\n",
      "Finished epoch  376\n",
      "On epoch  377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.02916717529297\n",
      "                              Voc = 26.476043701171875\n",
      "                              Jsc = 32.700687408447266\n",
      "                              FF = 90.92678833007812\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 35.323341369628906\n",
      "                              Voc = 26.38473129272461\n",
      "                              Jsc = 32.19001007080078\n",
      "                              FF = 90.96470642089844\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 35.34807586669922\n",
      "                              Voc = 26.274755477905273\n",
      "                              Jsc = 33.01483917236328\n",
      "                              FF = 90.95313262939453\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 35.29502868652344\n",
      "                              Voc = 25.988174438476562\n",
      "                              Jsc = 34.365196228027344\n",
      "                              FF = 90.7538833618164\n",
      "Finished epoch  380\n",
      "On epoch  381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 35.32538604736328\n",
      "                              Voc = 25.71730613708496\n",
      "                              Jsc = 34.96879196166992\n",
      "                              FF = 90.59595489501953\n",
      "Finished epoch  381\n",
      "On epoch  382\n",
      "Total Epoch Testing MAPE: PCE = 35.12388229370117\n",
      "                              Voc = 25.17351531982422\n",
      "                              Jsc = 35.63413619995117\n",
      "                              FF = 90.89089965820312\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 34.9252815246582\n",
      "                              Voc = 24.742347717285156\n",
      "                              Jsc = 35.65335464477539\n",
      "                              FF = 90.81233978271484\n",
      "Finished epoch  383\n",
      "On epoch  384\n",
      "Total Epoch Testing MAPE: PCE = 34.876644134521484\n",
      "                              Voc = 23.634300231933594\n",
      "                              Jsc = 36.08884048461914\n",
      "                              FF = 90.81914520263672\n",
      "Finished epoch  384\n",
      "On epoch  385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.47616958618164\n",
      "                              Voc = 22.38083839416504\n",
      "                              Jsc = 36.61893081665039\n",
      "                              FF = 91.1138916015625\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 34.1065788269043\n",
      "                              Voc = 21.775196075439453\n",
      "                              Jsc = 37.00521469116211\n",
      "                              FF = 91.14010620117188\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 34.11956024169922\n",
      "                              Voc = 22.403032302856445\n",
      "                              Jsc = 36.214962005615234\n",
      "                              FF = 91.13784790039062\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 33.97703170776367\n",
      "                              Voc = 23.08366584777832\n",
      "                              Jsc = 36.20148849487305\n",
      "                              FF = 91.20536804199219\n",
      "Finished epoch  388\n",
      "On epoch  389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.11933135986328\n",
      "                              Voc = 23.361591339111328\n",
      "                              Jsc = 35.823402404785156\n",
      "                              FF = 91.30401611328125\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 34.150970458984375\n",
      "                              Voc = 23.808025360107422\n",
      "                              Jsc = 35.11106872558594\n",
      "                              FF = 91.33560943603516\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 34.17878341674805\n",
      "                              Voc = 24.064876556396484\n",
      "                              Jsc = 34.75753402709961\n",
      "                              FF = 91.44341278076172\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 33.84041976928711\n",
      "                              Voc = 24.70793342590332\n",
      "                              Jsc = 34.955078125\n",
      "                              FF = 91.40921020507812\n",
      "Finished epoch  392\n",
      "On epoch  393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.063934326171875\n",
      "                              Voc = 24.78705406188965\n",
      "                              Jsc = 35.06393814086914\n",
      "                              FF = 91.36582946777344\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 33.913246154785156\n",
      "                              Voc = 24.819412231445312\n",
      "                              Jsc = 35.120506286621094\n",
      "                              FF = 91.3061294555664\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 33.94428634643555\n",
      "                              Voc = 24.772926330566406\n",
      "                              Jsc = 35.460750579833984\n",
      "                              FF = 91.21829223632812\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 33.768619537353516\n",
      "                              Voc = 25.658065795898438\n",
      "                              Jsc = 35.585506439208984\n",
      "                              FF = 91.22828674316406\n",
      "Finished epoch  396\n",
      "On epoch  397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.88389205932617\n",
      "                              Voc = 25.29684066772461\n",
      "                              Jsc = 35.34516525268555\n",
      "                              FF = 91.18975067138672\n",
      "Finished epoch  397\n",
      "On epoch  398\n",
      "Total Epoch Testing MAPE: PCE = 33.86070251464844\n",
      "                              Voc = 24.858638763427734\n",
      "                              Jsc = 35.70528030395508\n",
      "                              FF = 91.3138427734375\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 33.68558883666992\n",
      "                              Voc = 24.644479751586914\n",
      "                              Jsc = 36.23550033569336\n",
      "                              FF = 91.29106140136719\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 33.65373229980469\n",
      "                              Voc = 23.855070114135742\n",
      "                              Jsc = 36.94645309448242\n",
      "                              FF = 91.08007049560547\n",
      "Finished epoch  400\n",
      "On epoch  401\n",
      "Total Epoch Testing MAPE: PCE = 33.249290466308594\n",
      "                              Voc = 22.826826095581055\n",
      "                              Jsc = 36.97283935546875\n",
      "                              FF = 91.29754638671875\n",
      "Finished epoch  401\n",
      "On epoch  402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.54803466796875\n",
      "                              Voc = 22.653593063354492\n",
      "                              Jsc = 36.70280456542969\n",
      "                              FF = 91.34738159179688\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 33.194766998291016\n",
      "                              Voc = 23.77975845336914\n",
      "                              Jsc = 35.65985107421875\n",
      "                              FF = 91.34918212890625\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 33.36255645751953\n",
      "                              Voc = 24.466201782226562\n",
      "                              Jsc = 34.958892822265625\n",
      "                              FF = 91.48731231689453\n",
      "Finished epoch  404\n",
      "On epoch  405\n",
      "Total Epoch Testing MAPE: PCE = 33.550445556640625\n",
      "                              Voc = 25.15595245361328\n",
      "                              Jsc = 34.83451461791992\n",
      "                              FF = 91.27909088134766\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 33.87423324584961\n",
      "                              Voc = 26.74065589904785\n",
      "                              Jsc = 34.46876907348633\n",
      "                              FF = 91.18035125732422\n",
      "Finished epoch  406\n",
      "On epoch  407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.72483825683594\n",
      "                              Voc = 28.924461364746094\n",
      "                              Jsc = 32.9067497253418\n",
      "                              FF = 91.03206634521484\n",
      "Finished epoch  407\n",
      "On epoch  408\n",
      "Total Epoch Testing MAPE: PCE = 33.43244552612305\n",
      "                              Voc = 28.738075256347656\n",
      "                              Jsc = 32.39420700073242\n",
      "                              FF = 90.81082153320312\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 33.30900192260742\n",
      "                              Voc = 29.94213104248047\n",
      "                              Jsc = 32.42369842529297\n",
      "                              FF = 90.91766357421875\n",
      "Finished epoch  409\n",
      "On epoch  410\n",
      "Total Epoch Testing MAPE: PCE = 33.1517219543457\n",
      "                              Voc = 29.616943359375\n",
      "                              Jsc = 32.25703811645508\n",
      "                              FF = 91.04244995117188\n",
      "Finished epoch  410\n",
      "On epoch  411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.17154312133789\n",
      "                              Voc = 30.27349090576172\n",
      "                              Jsc = 32.71985626220703\n",
      "                              FF = 90.9962387084961\n",
      "Finished epoch  411\n",
      "On epoch  412\n",
      "Total Epoch Testing MAPE: PCE = 33.41668701171875\n",
      "                              Voc = 30.657255172729492\n",
      "                              Jsc = 32.56007766723633\n",
      "                              FF = 90.98247528076172\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 33.556427001953125\n",
      "                              Voc = 30.699026107788086\n",
      "                              Jsc = 32.670963287353516\n",
      "                              FF = 90.93704223632812\n",
      "Finished epoch  413\n",
      "On epoch  414\n",
      "Total Epoch Testing MAPE: PCE = 33.41915512084961\n",
      "                              Voc = 30.854711532592773\n",
      "                              Jsc = 31.757673263549805\n",
      "                              FF = 90.7217025756836\n",
      "Finished epoch  414\n",
      "On epoch  415\n",
      "Total Epoch Testing MAPE: PCE = 33.69744110107422\n",
      "                              Voc = 30.694263458251953\n",
      "                              Jsc = 31.70659828186035\n",
      "                              FF = 90.81906127929688\n",
      "Finished epoch  415\n",
      "On epoch  416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.68965148925781\n",
      "                              Voc = 30.273479461669922\n",
      "                              Jsc = 31.293970108032227\n",
      "                              FF = 90.7623291015625\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 33.622650146484375\n",
      "                              Voc = 29.78574562072754\n",
      "                              Jsc = 31.347366333007812\n",
      "                              FF = 90.96493530273438\n",
      "Finished epoch  417\n",
      "On epoch  418\n",
      "Total Epoch Testing MAPE: PCE = 33.767642974853516\n",
      "                              Voc = 29.079692840576172\n",
      "                              Jsc = 30.553382873535156\n",
      "                              FF = 91.0908432006836\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 33.86711883544922\n",
      "                              Voc = 29.227861404418945\n",
      "                              Jsc = 30.194293975830078\n",
      "                              FF = 91.06220245361328\n",
      "Finished epoch  419\n",
      "On epoch  420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.649234771728516\n",
      "                              Voc = 29.117849349975586\n",
      "                              Jsc = 29.88502311706543\n",
      "                              FF = 91.19819641113281\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 33.69751739501953\n",
      "                              Voc = 28.881376266479492\n",
      "                              Jsc = 29.668123245239258\n",
      "                              FF = 91.02983856201172\n",
      "Finished epoch  421\n",
      "On epoch  422\n",
      "Total Epoch Testing MAPE: PCE = 33.674190521240234\n",
      "                              Voc = 29.04456901550293\n",
      "                              Jsc = 29.15554428100586\n",
      "                              FF = 91.11707305908203\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 33.50211715698242\n",
      "                              Voc = 28.54828453063965\n",
      "                              Jsc = 29.20087242126465\n",
      "                              FF = 91.13533782958984\n",
      "Finished epoch  423\n",
      "On epoch  424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.59739685058594\n",
      "                              Voc = 27.66492462158203\n",
      "                              Jsc = 29.531639099121094\n",
      "                              FF = 91.19850158691406\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 33.60772705078125\n",
      "                              Voc = 27.81536293029785\n",
      "                              Jsc = 30.033985137939453\n",
      "                              FF = 91.23986053466797\n",
      "Finished epoch  425\n",
      "On epoch  426\n",
      "Total Epoch Testing MAPE: PCE = 33.542232513427734\n",
      "                              Voc = 26.649703979492188\n",
      "                              Jsc = 30.283370971679688\n",
      "                              FF = 91.27108764648438\n",
      "Finished epoch  426\n",
      "On epoch  427\n",
      "Total Epoch Testing MAPE: PCE = 33.14545822143555\n",
      "                              Voc = 26.626375198364258\n",
      "                              Jsc = 31.07309913635254\n",
      "                              FF = 91.08473205566406\n",
      "Finished epoch  427\n",
      "On epoch  428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.033329010009766\n",
      "                              Voc = 26.322608947753906\n",
      "                              Jsc = 31.05699348449707\n",
      "                              FF = 91.22577667236328\n",
      "Finished epoch  428\n",
      "On epoch  429\n",
      "Total Epoch Testing MAPE: PCE = 33.01896286010742\n",
      "                              Voc = 25.79543685913086\n",
      "                              Jsc = 31.958234786987305\n",
      "                              FF = 91.26824951171875\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 32.772274017333984\n",
      "                              Voc = 23.971874237060547\n",
      "                              Jsc = 32.501121520996094\n",
      "                              FF = 91.150634765625\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 33.03380584716797\n",
      "                              Voc = 23.87460708618164\n",
      "                              Jsc = 31.416793823242188\n",
      "                              FF = 91.03636169433594\n",
      "Finished epoch  431\n",
      "On epoch  432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.319644927978516\n",
      "                              Voc = 22.800376892089844\n",
      "                              Jsc = 31.65871810913086\n",
      "                              FF = 91.02438354492188\n",
      "Finished epoch  432\n",
      "On epoch  433\n",
      "Total Epoch Testing MAPE: PCE = 33.307533264160156\n",
      "                              Voc = 21.56732177734375\n",
      "                              Jsc = 31.543251037597656\n",
      "                              FF = 90.87715911865234\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 33.05165100097656\n",
      "                              Voc = 21.679759979248047\n",
      "                              Jsc = 32.79741668701172\n",
      "                              FF = 90.9066162109375\n",
      "Finished epoch  434\n",
      "On epoch  435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.80504608154297\n",
      "                              Voc = 22.3067684173584\n",
      "                              Jsc = 33.06484603881836\n",
      "                              FF = 90.9171371459961\n",
      "Finished epoch  435\n",
      "On epoch  436\n",
      "Total Epoch Testing MAPE: PCE = 32.09722137451172\n",
      "                              Voc = 22.860939025878906\n",
      "                              Jsc = 32.91889953613281\n",
      "                              FF = 90.92853546142578\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 31.934648513793945\n",
      "                              Voc = 22.555620193481445\n",
      "                              Jsc = 33.328792572021484\n",
      "                              FF = 90.87122344970703\n",
      "Finished epoch  437\n",
      "On epoch  438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.7929630279541\n",
      "                              Voc = 23.059782028198242\n",
      "                              Jsc = 33.883506774902344\n",
      "                              FF = 91.03031158447266\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 31.791778564453125\n",
      "                              Voc = 24.7582950592041\n",
      "                              Jsc = 34.32888412475586\n",
      "                              FF = 91.00968933105469\n",
      "Finished epoch  439\n",
      "On epoch  440\n",
      "Total Epoch Testing MAPE: PCE = 31.846826553344727\n",
      "                              Voc = 25.72238540649414\n",
      "                              Jsc = 35.32136535644531\n",
      "                              FF = 90.98149108886719\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 31.71025848388672\n",
      "                              Voc = 26.70225715637207\n",
      "                              Jsc = 36.317081451416016\n",
      "                              FF = 90.76839447021484\n",
      "Finished epoch  441\n",
      "On epoch  442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.664859771728516\n",
      "                              Voc = 27.48648452758789\n",
      "                              Jsc = 37.03841781616211\n",
      "                              FF = 90.8375244140625\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 31.707181930541992\n",
      "                              Voc = 27.99980926513672\n",
      "                              Jsc = 37.40242385864258\n",
      "                              FF = 90.77079010009766\n",
      "Finished epoch  443\n",
      "On epoch  444\n",
      "Total Epoch Testing MAPE: PCE = 31.780323028564453\n",
      "                              Voc = 28.716054916381836\n",
      "                              Jsc = 37.39154815673828\n",
      "                              FF = 90.7845230102539\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 31.908512115478516\n",
      "                              Voc = 28.98271942138672\n",
      "                              Jsc = 37.552242279052734\n",
      "                              FF = 90.86798858642578\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 32.436458587646484\n",
      "                              Voc = 29.11890411376953\n",
      "                              Jsc = 37.474857330322266\n",
      "                              FF = 90.76236724853516\n",
      "Finished epoch  446\n",
      "On epoch  447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.475791931152344\n",
      "                              Voc = 29.675308227539062\n",
      "                              Jsc = 36.9727668762207\n",
      "                              FF = 90.69160461425781\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 32.71907424926758\n",
      "                              Voc = 30.116668701171875\n",
      "                              Jsc = 36.96343994140625\n",
      "                              FF = 90.48080444335938\n",
      "Finished epoch  448\n",
      "On epoch  449\n",
      "Total Epoch Testing MAPE: PCE = 33.03331756591797\n",
      "                              Voc = 29.761119842529297\n",
      "                              Jsc = 36.27355194091797\n",
      "                              FF = 90.348388671875\n",
      "Finished epoch  449\n",
      "On epoch  450\n",
      "Total Epoch Testing MAPE: PCE = 33.09345245361328\n",
      "                              Voc = 29.75510025024414\n",
      "                              Jsc = 35.590232849121094\n",
      "                              FF = 90.34031677246094\n",
      "Finished epoch  450\n",
      "On epoch  451\n",
      "Total Epoch Testing MAPE: PCE = 33.36841583251953\n",
      "                              Voc = 29.87586212158203\n",
      "                              Jsc = 34.75642395019531\n",
      "                              FF = 90.43962860107422\n",
      "Finished epoch  451\n",
      "On epoch  452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.17133331298828\n",
      "                              Voc = 28.98917579650879\n",
      "                              Jsc = 34.700740814208984\n",
      "                              FF = 90.30059814453125\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 33.059043884277344\n",
      "                              Voc = 28.856971740722656\n",
      "                              Jsc = 32.9416618347168\n",
      "                              FF = 90.18832397460938\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 33.07757568359375\n",
      "                              Voc = 28.123062133789062\n",
      "                              Jsc = 31.28061866760254\n",
      "                              FF = 90.29708862304688\n",
      "Finished epoch  454\n",
      "On epoch  455\n",
      "Total Epoch Testing MAPE: PCE = 32.9932746887207\n",
      "                              Voc = 27.48359489440918\n",
      "                              Jsc = 30.787626266479492\n",
      "                              FF = 90.36430358886719\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 33.19218063354492\n",
      "                              Voc = 27.80953598022461\n",
      "                              Jsc = 31.181198120117188\n",
      "                              FF = 90.23456573486328\n",
      "Finished epoch  456\n",
      "On epoch  457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.08769607543945\n",
      "                              Voc = 27.862844467163086\n",
      "                              Jsc = 30.213329315185547\n",
      "                              FF = 90.06089782714844\n",
      "Finished epoch  457\n",
      "On epoch  458\n",
      "Total Epoch Testing MAPE: PCE = 32.59003829956055\n",
      "                              Voc = 28.30563735961914\n",
      "                              Jsc = 29.49395751953125\n",
      "                              FF = 90.13436126708984\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 32.51186752319336\n",
      "                              Voc = 28.177330017089844\n",
      "                              Jsc = 29.65923309326172\n",
      "                              FF = 90.30643463134766\n",
      "Finished epoch  459\n",
      "On epoch  460\n",
      "Total Epoch Testing MAPE: PCE = 32.186302185058594\n",
      "                              Voc = 28.968503952026367\n",
      "                              Jsc = 30.751949310302734\n",
      "                              FF = 90.2303237915039\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 31.979965209960938\n",
      "                              Voc = 29.11541748046875\n",
      "                              Jsc = 30.409774780273438\n",
      "                              FF = 90.24266052246094\n",
      "Finished epoch  461\n",
      "On epoch  462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.25226974487305\n",
      "                              Voc = 29.081226348876953\n",
      "                              Jsc = 30.689350128173828\n",
      "                              FF = 90.25397491455078\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 32.156288146972656\n",
      "                              Voc = 29.608110427856445\n",
      "                              Jsc = 30.866518020629883\n",
      "                              FF = 90.24665069580078\n",
      "Finished epoch  463\n",
      "On epoch  464\n",
      "Total Epoch Testing MAPE: PCE = 32.296348571777344\n",
      "                              Voc = 30.112098693847656\n",
      "                              Jsc = 31.512563705444336\n",
      "                              FF = 90.12864685058594\n",
      "Finished epoch  464\n",
      "On epoch  465\n",
      "Total Epoch Testing MAPE: PCE = 32.00958251953125\n",
      "                              Voc = 30.010587692260742\n",
      "                              Jsc = 32.75339889526367\n",
      "                              FF = 90.24675750732422\n",
      "Finished epoch  465\n",
      "On epoch  466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.829980850219727\n",
      "                              Voc = 29.919645309448242\n",
      "                              Jsc = 33.514892578125\n",
      "                              FF = 90.49513244628906\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 31.662309646606445\n",
      "                              Voc = 29.901012420654297\n",
      "                              Jsc = 34.19181823730469\n",
      "                              FF = 90.50894165039062\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 31.549787521362305\n",
      "                              Voc = 29.49049186706543\n",
      "                              Jsc = 34.29460906982422\n",
      "                              FF = 90.4852294921875\n",
      "Finished epoch  468\n",
      "On epoch  469\n",
      "Total Epoch Testing MAPE: PCE = 31.232982635498047\n",
      "                              Voc = 28.973726272583008\n",
      "                              Jsc = 35.33027648925781\n",
      "                              FF = 90.66574096679688\n",
      "Finished epoch  469\n",
      "On epoch  470\n",
      "Total Epoch Testing MAPE: PCE = 31.221759796142578\n",
      "                              Voc = 28.629762649536133\n",
      "                              Jsc = 35.42515182495117\n",
      "                              FF = 90.82550048828125\n",
      "Finished epoch  470\n",
      "On epoch  471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.228805541992188\n",
      "                              Voc = 28.415945053100586\n",
      "                              Jsc = 35.45827102661133\n",
      "                              FF = 90.72593688964844\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 31.337600708007812\n",
      "                              Voc = 27.526336669921875\n",
      "                              Jsc = 35.936031341552734\n",
      "                              FF = 90.74391174316406\n",
      "Finished epoch  472\n",
      "On epoch  473\n",
      "Total Epoch Testing MAPE: PCE = 31.273042678833008\n",
      "                              Voc = 27.10054588317871\n",
      "                              Jsc = 35.42824935913086\n",
      "                              FF = 90.73670959472656\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 31.22836685180664\n",
      "                              Voc = 26.003820419311523\n",
      "                              Jsc = 35.01456832885742\n",
      "                              FF = 90.80923461914062\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 31.220230102539062\n",
      "                              Voc = 25.972042083740234\n",
      "                              Jsc = 34.78303146362305\n",
      "                              FF = 90.7490463256836\n",
      "Finished epoch  475\n",
      "On epoch  476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.336837768554688\n",
      "                              Voc = 25.7989501953125\n",
      "                              Jsc = 34.64404296875\n",
      "                              FF = 90.82652282714844\n",
      "Finished epoch  476\n",
      "On epoch  477\n",
      "Total Epoch Testing MAPE: PCE = 31.300565719604492\n",
      "                              Voc = 25.24786949157715\n",
      "                              Jsc = 34.1594123840332\n",
      "                              FF = 90.82427978515625\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 31.299087524414062\n",
      "                              Voc = 24.515527725219727\n",
      "                              Jsc = 33.675743103027344\n",
      "                              FF = 90.82624816894531\n",
      "Finished epoch  478\n",
      "On epoch  479\n",
      "Total Epoch Testing MAPE: PCE = 31.269332885742188\n",
      "                              Voc = 23.916826248168945\n",
      "                              Jsc = 32.93899154663086\n",
      "                              FF = 90.84896850585938\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 31.224653244018555\n",
      "                              Voc = 23.99127960205078\n",
      "                              Jsc = 32.619686126708984\n",
      "                              FF = 90.87179565429688\n",
      "Finished epoch  480\n",
      "On epoch  481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.153362274169922\n",
      "                              Voc = 24.177780151367188\n",
      "                              Jsc = 32.46509552001953\n",
      "                              FF = 90.70050048828125\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 31.029159545898438\n",
      "                              Voc = 24.3704891204834\n",
      "                              Jsc = 31.966014862060547\n",
      "                              FF = 90.56118774414062\n",
      "Finished epoch  482\n",
      "On epoch  483\n",
      "Total Epoch Testing MAPE: PCE = 30.869314193725586\n",
      "                              Voc = 25.438100814819336\n",
      "                              Jsc = 31.760961532592773\n",
      "                              FF = 90.3519515991211\n",
      "Finished epoch  483\n",
      "On epoch  484\n",
      "Total Epoch Testing MAPE: PCE = 30.82120704650879\n",
      "                              Voc = 26.382919311523438\n",
      "                              Jsc = 31.652271270751953\n",
      "                              FF = 90.40739440917969\n",
      "Finished epoch  484\n",
      "On epoch  485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 30.898574829101562\n",
      "                              Voc = 27.398658752441406\n",
      "                              Jsc = 30.738603591918945\n",
      "                              FF = 90.51380920410156\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 31.075740814208984\n",
      "                              Voc = 27.060779571533203\n",
      "                              Jsc = 30.654720306396484\n",
      "                              FF = 90.4876937866211\n",
      "Finished epoch  486\n",
      "On epoch  487\n",
      "Total Epoch Testing MAPE: PCE = 31.1678409576416\n",
      "                              Voc = 27.455446243286133\n",
      "                              Jsc = 30.2178897857666\n",
      "                              FF = 90.44847869873047\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 31.597713470458984\n",
      "                              Voc = 28.1748046875\n",
      "                              Jsc = 29.939434051513672\n",
      "                              FF = 90.426025390625\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 31.962818145751953\n",
      "                              Voc = 28.154558181762695\n",
      "                              Jsc = 29.52428436279297\n",
      "                              FF = 90.36641693115234\n",
      "Finished epoch  489\n",
      "On epoch  490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.904855728149414\n",
      "                              Voc = 28.672334671020508\n",
      "                              Jsc = 30.026241302490234\n",
      "                              FF = 90.27576446533203\n",
      "Finished epoch  490\n",
      "On epoch  491\n",
      "Total Epoch Testing MAPE: PCE = 31.844324111938477\n",
      "                              Voc = 29.523670196533203\n",
      "                              Jsc = 30.17401695251465\n",
      "                              FF = 90.18766021728516\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 31.855825424194336\n",
      "                              Voc = 29.74162483215332\n",
      "                              Jsc = 30.484527587890625\n",
      "                              FF = 90.44373321533203\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 31.466516494750977\n",
      "                              Voc = 29.43098258972168\n",
      "                              Jsc = 31.24207305908203\n",
      "                              FF = 90.2681655883789\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 31.44443702697754\n",
      "                              Voc = 28.945484161376953\n",
      "                              Jsc = 30.964914321899414\n",
      "                              FF = 90.30814361572266\n",
      "Finished epoch  494\n",
      "On epoch  495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.347972869873047\n",
      "                              Voc = 28.077178955078125\n",
      "                              Jsc = 31.163331985473633\n",
      "                              FF = 90.28054809570312\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 31.382179260253906\n",
      "                              Voc = 27.55032730102539\n",
      "                              Jsc = 31.804901123046875\n",
      "                              FF = 90.43126678466797\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 31.43400001525879\n",
      "                              Voc = 26.415916442871094\n",
      "                              Jsc = 31.76053237915039\n",
      "                              FF = 90.35398864746094\n",
      "Finished epoch  497\n",
      "On epoch  498\n",
      "Total Epoch Testing MAPE: PCE = 31.63033103942871\n",
      "                              Voc = 25.96595001220703\n",
      "                              Jsc = 32.201499938964844\n",
      "                              FF = 90.25232696533203\n",
      "Finished epoch  498\n",
      "On epoch  499\n",
      "Total Epoch Testing MAPE: PCE = 31.919475555419922\n",
      "                              Voc = 25.28863525390625\n",
      "                              Jsc = 31.079191207885742\n",
      "                              FF = 90.1371078491211\n",
      "Finished epoch  499\n",
      "Fold # 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "On epoch  0\n",
      "Total Epoch Testing MAPE: PCE = 96.16820526123047\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  0\n",
      "On epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 63.353843688964844\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  1\n",
      "On epoch  2\n",
      "Total Epoch Testing MAPE: PCE = 49.40534973144531\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  2\n",
      "On epoch  3\n",
      "Total Epoch Testing MAPE: PCE = 41.306297302246094\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  3\n",
      "On epoch  4\n",
      "Total Epoch Testing MAPE: PCE = 35.92051696777344\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  4\n",
      "On epoch  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.70142364501953\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  5\n",
      "On epoch  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 31.877899169921875\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  6\n",
      "On epoch  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 30.668617248535156\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  7\n",
      "On epoch  8\n",
      "Total Epoch Testing MAPE: PCE = 30.520889282226562\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  8\n",
      "On epoch  9\n",
      "Total Epoch Testing MAPE: PCE = 30.775117874145508\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  9\n",
      "On epoch  10\n",
      "Total Epoch Testing MAPE: PCE = 31.2220516204834\n",
      "                              Voc = 100.0\n",
      "                              Jsc = 100.0\n",
      "                              FF = 100.0\n",
      "Finished epoch  10\n",
      "On epoch  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.73834228515625\n",
      "                              Voc = 98.43453216552734\n",
      "                              Jsc = 100.0\n",
      "                              FF = 98.06041717529297\n",
      "Finished epoch  11\n",
      "On epoch  12\n",
      "Total Epoch Testing MAPE: PCE = 33.822601318359375\n",
      "                              Voc = 94.48393249511719\n",
      "                              Jsc = 100.0\n",
      "                              FF = 95.70448303222656\n",
      "Finished epoch  12\n",
      "On epoch  13\n",
      "Total Epoch Testing MAPE: PCE = 33.574867248535156\n",
      "                              Voc = 86.92156219482422\n",
      "                              Jsc = 100.0\n",
      "                              FF = 92.00090026855469\n",
      "Finished epoch  13\n",
      "On epoch  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.29489517211914\n",
      "                              Voc = 79.96056365966797\n",
      "                              Jsc = 100.0\n",
      "                              FF = 88.49378967285156\n",
      "Finished epoch  14\n",
      "On epoch  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 34.24051284790039\n",
      "                              Voc = 73.85997772216797\n",
      "                              Jsc = 100.0\n",
      "                              FF = 86.15835571289062\n",
      "Finished epoch  15\n",
      "On epoch  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.4411735534668\n",
      "                              Voc = 68.62723541259766\n",
      "                              Jsc = 100.0\n",
      "                              FF = 82.31246185302734\n",
      "Finished epoch  16\n",
      "On epoch  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 32.56203079223633\n",
      "                              Voc = 64.49164581298828\n",
      "                              Jsc = 100.0\n",
      "                              FF = 76.63011169433594\n",
      "Finished epoch  17\n",
      "On epoch  18\n",
      "Total Epoch Testing MAPE: PCE = 32.15657424926758\n",
      "                              Voc = 62.7711067199707\n",
      "                              Jsc = 100.0\n",
      "                              FF = 70.87420654296875\n",
      "Finished epoch  18\n",
      "On epoch  19\n",
      "Total Epoch Testing MAPE: PCE = 32.04287338256836\n",
      "                              Voc = 58.996986389160156\n",
      "                              Jsc = 100.0\n",
      "                              FF = 65.77777862548828\n",
      "Finished epoch  19\n",
      "On epoch  20\n",
      "Total Epoch Testing MAPE: PCE = 32.143211364746094\n",
      "                              Voc = 55.68741989135742\n",
      "                              Jsc = 99.96806335449219\n",
      "                              FF = 61.27177429199219\n",
      "Finished epoch  20\n",
      "On epoch  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 33.01851272583008\n",
      "                              Voc = 53.64596939086914\n",
      "                              Jsc = 99.6351089477539\n",
      "                              FF = 58.363075256347656\n",
      "Finished epoch  21\n",
      "On epoch  22\n",
      "Total Epoch Testing MAPE: PCE = 32.52276611328125\n",
      "                              Voc = 51.387977600097656\n",
      "                              Jsc = 99.34842681884766\n",
      "                              FF = 55.282371520996094\n",
      "Finished epoch  22\n",
      "On epoch  23\n",
      "Total Epoch Testing MAPE: PCE = 33.38872528076172\n",
      "                              Voc = 50.29549026489258\n",
      "                              Jsc = 99.07998657226562\n",
      "                              FF = 52.307308197021484\n",
      "Finished epoch  23\n",
      "On epoch  24\n",
      "Total Epoch Testing MAPE: PCE = 34.84027862548828\n",
      "                              Voc = 47.86323547363281\n",
      "                              Jsc = 98.91159057617188\n",
      "                              FF = 50.20934295654297\n",
      "Finished epoch  24\n",
      "On epoch  25\n",
      "Total Epoch Testing MAPE: PCE = 36.44868087768555\n",
      "                              Voc = 46.030513763427734\n",
      "                              Jsc = 98.6655502319336\n",
      "                              FF = 48.72279357910156\n",
      "Finished epoch  25\n",
      "On epoch  26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 38.710262298583984\n",
      "                              Voc = 44.13842010498047\n",
      "                              Jsc = 98.36914825439453\n",
      "                              FF = 47.4415283203125\n",
      "Finished epoch  26\n",
      "On epoch  27\n",
      "Total Epoch Testing MAPE: PCE = 39.95820617675781\n",
      "                              Voc = 41.7239990234375\n",
      "                              Jsc = 98.35647583007812\n",
      "                              FF = 46.29926681518555\n",
      "Finished epoch  27\n",
      "On epoch  28\n",
      "Total Epoch Testing MAPE: PCE = 42.059661865234375\n",
      "                              Voc = 38.878421783447266\n",
      "                              Jsc = 98.34104919433594\n",
      "                              FF = 45.39224624633789\n",
      "Finished epoch  28\n",
      "On epoch  29\n",
      "Total Epoch Testing MAPE: PCE = 43.54572677612305\n",
      "                              Voc = 36.66971206665039\n",
      "                              Jsc = 98.3056640625\n",
      "                              FF = 44.50886917114258\n",
      "Finished epoch  29\n",
      "On epoch  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 45.04463195800781\n",
      "                              Voc = 33.86470031738281\n",
      "                              Jsc = 98.1939468383789\n",
      "                              FF = 44.07293701171875\n",
      "Finished epoch  30\n",
      "On epoch  31\n",
      "Total Epoch Testing MAPE: PCE = 47.20803451538086\n",
      "                              Voc = 29.84267234802246\n",
      "                              Jsc = 98.15200805664062\n",
      "                              FF = 43.94704818725586\n",
      "Finished epoch  31\n",
      "On epoch  32\n",
      "Total Epoch Testing MAPE: PCE = 48.566341400146484\n",
      "                              Voc = 28.63212013244629\n",
      "                              Jsc = 98.05819702148438\n",
      "                              FF = 43.38369369506836\n",
      "Finished epoch  32\n",
      "On epoch  33\n",
      "Total Epoch Testing MAPE: PCE = 50.82541275024414\n",
      "                              Voc = 30.056814193725586\n",
      "                              Jsc = 97.9577865600586\n",
      "                              FF = 43.725826263427734\n",
      "Finished epoch  33\n",
      "On epoch  34\n",
      "Total Epoch Testing MAPE: PCE = 51.788063049316406\n",
      "                              Voc = 29.2202205657959\n",
      "                              Jsc = 98.00048828125\n",
      "                              FF = 43.2966423034668\n",
      "Finished epoch  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "On epoch  35\n",
      "Total Epoch Testing MAPE: PCE = 54.46547317504883\n",
      "                              Voc = 29.142526626586914\n",
      "                              Jsc = 97.80372619628906\n",
      "                              FF = 42.418853759765625\n",
      "Finished epoch  35\n",
      "On epoch  36\n",
      "Total Epoch Testing MAPE: PCE = 54.38509750366211\n",
      "                              Voc = 29.51403045654297\n",
      "                              Jsc = 97.46047973632812\n",
      "                              FF = 42.32838821411133\n",
      "Finished epoch  36\n",
      "On epoch  37\n",
      "Total Epoch Testing MAPE: PCE = 54.976444244384766\n",
      "                              Voc = 31.562179565429688\n",
      "                              Jsc = 97.25979614257812\n",
      "                              FF = 42.17473220825195\n",
      "Finished epoch  37\n",
      "On epoch  38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.312835693359375\n",
      "                              Voc = 33.806495666503906\n",
      "                              Jsc = 97.03540802001953\n",
      "                              FF = 42.04133605957031\n",
      "Finished epoch  38\n",
      "On epoch  39\n",
      "Total Epoch Testing MAPE: PCE = 57.460975646972656\n",
      "                              Voc = 34.80427932739258\n",
      "                              Jsc = 96.98596954345703\n",
      "                              FF = 42.09760284423828\n",
      "Finished epoch  39\n",
      "On epoch  40\n",
      "Total Epoch Testing MAPE: PCE = 58.08794021606445\n",
      "                              Voc = 36.162288665771484\n",
      "                              Jsc = 96.98926544189453\n",
      "                              FF = 42.0594596862793\n",
      "Finished epoch  40\n",
      "On epoch  41\n",
      "Total Epoch Testing MAPE: PCE = 59.140628814697266\n",
      "                              Voc = 36.2498893737793\n",
      "                              Jsc = 96.84778594970703\n",
      "                              FF = 42.18715286254883\n",
      "Finished epoch  41\n",
      "On epoch  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.514076232910156\n",
      "                              Voc = 36.41117477416992\n",
      "                              Jsc = 96.8215103149414\n",
      "                              FF = 42.15726089477539\n",
      "Finished epoch  42\n",
      "On epoch  43\n",
      "Total Epoch Testing MAPE: PCE = 61.63191223144531\n",
      "                              Voc = 36.849029541015625\n",
      "                              Jsc = 96.57996368408203\n",
      "                              FF = 41.70744323730469\n",
      "Finished epoch  43\n",
      "On epoch  44\n",
      "Total Epoch Testing MAPE: PCE = 61.917049407958984\n",
      "                              Voc = 37.82048034667969\n",
      "                              Jsc = 96.55085754394531\n",
      "                              FF = 41.59019470214844\n",
      "Finished epoch  44\n",
      "On epoch  45\n",
      "Total Epoch Testing MAPE: PCE = 63.23569869995117\n",
      "                              Voc = 37.66904830932617\n",
      "                              Jsc = 96.51180267333984\n",
      "                              FF = 41.6055908203125\n",
      "Finished epoch  45\n",
      "On epoch  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 63.038230895996094\n",
      "                              Voc = 39.493072509765625\n",
      "                              Jsc = 96.22267150878906\n",
      "                              FF = 41.11144256591797\n",
      "Finished epoch  46\n",
      "On epoch  47\n",
      "Total Epoch Testing MAPE: PCE = 62.54568099975586\n",
      "                              Voc = 38.265438079833984\n",
      "                              Jsc = 95.69275665283203\n",
      "                              FF = 41.075653076171875\n",
      "Finished epoch  47\n",
      "On epoch  48\n",
      "Total Epoch Testing MAPE: PCE = 63.013668060302734\n",
      "                              Voc = 37.09611129760742\n",
      "                              Jsc = 95.19901275634766\n",
      "                              FF = 40.771949768066406\n",
      "Finished epoch  48\n",
      "On epoch  49\n",
      "Total Epoch Testing MAPE: PCE = 62.66645812988281\n",
      "                              Voc = 36.53352355957031\n",
      "                              Jsc = 95.25970458984375\n",
      "                              FF = 39.6897087097168\n",
      "Finished epoch  49\n",
      "On epoch  50\n",
      "Total Epoch Testing MAPE: PCE = 62.8742561340332\n",
      "                              Voc = 36.30604934692383\n",
      "                              Jsc = 94.70099639892578\n",
      "                              FF = 38.69597625732422\n",
      "Finished epoch  50\n",
      "On epoch  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 62.63752746582031\n",
      "                              Voc = 35.265777587890625\n",
      "                              Jsc = 94.74626159667969\n",
      "                              FF = 38.386077880859375\n",
      "Finished epoch  51\n",
      "On epoch  52\n",
      "Total Epoch Testing MAPE: PCE = 62.1978874206543\n",
      "                              Voc = 33.27253723144531\n",
      "                              Jsc = 94.31864166259766\n",
      "                              FF = 38.630001068115234\n",
      "Finished epoch  52\n",
      "On epoch  53\n",
      "Total Epoch Testing MAPE: PCE = 61.82688522338867\n",
      "                              Voc = 30.320194244384766\n",
      "                              Jsc = 94.38408660888672\n",
      "                              FF = 39.498443603515625\n",
      "Finished epoch  53\n",
      "On epoch  54\n",
      "Total Epoch Testing MAPE: PCE = 62.399269104003906\n",
      "                              Voc = 28.819557189941406\n",
      "                              Jsc = 94.23736572265625\n",
      "                              FF = 39.72384262084961\n",
      "Finished epoch  54\n",
      "On epoch  55\n",
      "Total Epoch Testing MAPE: PCE = 62.50141906738281\n",
      "                              Voc = 26.94131088256836\n",
      "                              Jsc = 94.03528594970703\n",
      "                              FF = 40.32230758666992\n",
      "Finished epoch  55\n",
      "On epoch  56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 61.93133544921875\n",
      "                              Voc = 26.176189422607422\n",
      "                              Jsc = 93.56401062011719\n",
      "                              FF = 40.83369445800781\n",
      "Finished epoch  56\n",
      "On epoch  57\n",
      "Total Epoch Testing MAPE: PCE = 61.65492630004883\n",
      "                              Voc = 25.610807418823242\n",
      "                              Jsc = 94.354736328125\n",
      "                              FF = 41.83852005004883\n",
      "Finished epoch  57\n",
      "On epoch  58\n",
      "Total Epoch Testing MAPE: PCE = 61.332427978515625\n",
      "                              Voc = 25.638399124145508\n",
      "                              Jsc = 94.64990234375\n",
      "                              FF = 42.37855529785156\n",
      "Finished epoch  58\n",
      "On epoch  59\n",
      "Total Epoch Testing MAPE: PCE = 61.3553466796875\n",
      "                              Voc = 24.60416030883789\n",
      "                              Jsc = 95.27848815917969\n",
      "                              FF = 42.807884216308594\n",
      "Finished epoch  59\n",
      "On epoch  60\n",
      "Total Epoch Testing MAPE: PCE = 60.30369567871094\n",
      "                              Voc = 23.37457847595215\n",
      "                              Jsc = 95.65331268310547\n",
      "                              FF = 43.67273712158203\n",
      "Finished epoch  60\n",
      "On epoch  61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 59.68226623535156\n",
      "                              Voc = 22.724397659301758\n",
      "                              Jsc = 95.54776000976562\n",
      "                              FF = 44.39383316040039\n",
      "Finished epoch  61\n",
      "On epoch  62\n",
      "Total Epoch Testing MAPE: PCE = 59.39402770996094\n",
      "                              Voc = 20.739765167236328\n",
      "                              Jsc = 94.8758773803711\n",
      "                              FF = 44.645748138427734\n",
      "Finished epoch  62\n",
      "On epoch  63\n",
      "Total Epoch Testing MAPE: PCE = 59.621055603027344\n",
      "                              Voc = 20.08562469482422\n",
      "                              Jsc = 94.49178314208984\n",
      "                              FF = 45.02808380126953\n",
      "Finished epoch  63\n",
      "On epoch  64\n",
      "Total Epoch Testing MAPE: PCE = 59.45307159423828\n",
      "                              Voc = 19.579429626464844\n",
      "                              Jsc = 94.17301177978516\n",
      "                              FF = 46.14043045043945\n",
      "Finished epoch  64\n",
      "On epoch  65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 59.09769058227539\n",
      "                              Voc = 20.52505111694336\n",
      "                              Jsc = 93.9643325805664\n",
      "                              FF = 46.77889633178711\n",
      "Finished epoch  65\n",
      "On epoch  66\n",
      "Total Epoch Testing MAPE: PCE = 58.18186950683594\n",
      "                              Voc = 21.578495025634766\n",
      "                              Jsc = 94.51983642578125\n",
      "                              FF = 47.493247985839844\n",
      "Finished epoch  66\n",
      "On epoch  67\n",
      "Total Epoch Testing MAPE: PCE = 57.96012878417969\n",
      "                              Voc = 23.15729331970215\n",
      "                              Jsc = 95.18003845214844\n",
      "                              FF = 48.22407531738281\n",
      "Finished epoch  67\n",
      "On epoch  68\n",
      "Total Epoch Testing MAPE: PCE = 58.85817337036133\n",
      "                              Voc = 23.80238151550293\n",
      "                              Jsc = 95.10457611083984\n",
      "                              FF = 48.939640045166016\n",
      "Finished epoch  68\n",
      "On epoch  69\n",
      "Total Epoch Testing MAPE: PCE = 58.95404052734375\n",
      "                              Voc = 24.28665542602539\n",
      "                              Jsc = 94.71331787109375\n",
      "                              FF = 48.888118743896484\n",
      "Finished epoch  69\n",
      "On epoch  70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.251548767089844\n",
      "                              Voc = 24.79336929321289\n",
      "                              Jsc = 94.46572875976562\n",
      "                              FF = 48.755332946777344\n",
      "Finished epoch  70\n",
      "On epoch  71\n",
      "Total Epoch Testing MAPE: PCE = 58.26774215698242\n",
      "                              Voc = 25.09431266784668\n",
      "                              Jsc = 94.63630676269531\n",
      "                              FF = 48.413169860839844\n",
      "Finished epoch  71\n",
      "On epoch  72\n",
      "Total Epoch Testing MAPE: PCE = 56.98089599609375\n",
      "                              Voc = 24.844453811645508\n",
      "                              Jsc = 94.34751892089844\n",
      "                              FF = 48.71712875366211\n",
      "Finished epoch  72\n",
      "On epoch  73\n",
      "Total Epoch Testing MAPE: PCE = 57.169891357421875\n",
      "                              Voc = 25.060007095336914\n",
      "                              Jsc = 94.63760375976562\n",
      "                              FF = 48.537452697753906\n",
      "Finished epoch  73\n",
      "On epoch  74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.7079963684082\n",
      "                              Voc = 26.078426361083984\n",
      "                              Jsc = 94.96334838867188\n",
      "                              FF = 48.53062057495117\n",
      "Finished epoch  74\n",
      "On epoch  75\n",
      "Total Epoch Testing MAPE: PCE = 56.33992004394531\n",
      "                              Voc = 26.614151000976562\n",
      "                              Jsc = 94.75421905517578\n",
      "                              FF = 48.462371826171875\n",
      "Finished epoch  75\n",
      "On epoch  76\n",
      "Total Epoch Testing MAPE: PCE = 55.228172302246094\n",
      "                              Voc = 26.930072784423828\n",
      "                              Jsc = 94.51359558105469\n",
      "                              FF = 48.93306350708008\n",
      "Finished epoch  76\n",
      "On epoch  77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.9305419921875\n",
      "                              Voc = 27.90990447998047\n",
      "                              Jsc = 94.52076721191406\n",
      "                              FF = 49.14978790283203\n",
      "Finished epoch  77\n",
      "On epoch  78\n",
      "Total Epoch Testing MAPE: PCE = 54.663352966308594\n",
      "                              Voc = 29.366609573364258\n",
      "                              Jsc = 94.46676635742188\n",
      "                              FF = 49.95900344848633\n",
      "Finished epoch  78\n",
      "On epoch  79\n",
      "Total Epoch Testing MAPE: PCE = 53.52793884277344\n",
      "                              Voc = 30.43906021118164\n",
      "                              Jsc = 94.3172607421875\n",
      "                              FF = 49.53891372680664\n",
      "Finished epoch  79\n",
      "On epoch  80\n",
      "Total Epoch Testing MAPE: PCE = 53.37411117553711\n",
      "                              Voc = 30.805936813354492\n",
      "                              Jsc = 94.28872680664062\n",
      "                              FF = 49.98920822143555\n",
      "Finished epoch  80\n",
      "On epoch  81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.664886474609375\n",
      "                              Voc = 31.809925079345703\n",
      "                              Jsc = 93.64668273925781\n",
      "                              FF = 50.121437072753906\n",
      "Finished epoch  81\n",
      "On epoch  82\n",
      "Total Epoch Testing MAPE: PCE = 53.846717834472656\n",
      "                              Voc = 32.110877990722656\n",
      "                              Jsc = 93.13053131103516\n",
      "                              FF = 50.673954010009766\n",
      "Finished epoch  82\n",
      "On epoch  83\n",
      "Total Epoch Testing MAPE: PCE = 53.468971252441406\n",
      "                              Voc = 32.440067291259766\n",
      "                              Jsc = 92.85176849365234\n",
      "                              FF = 50.747276306152344\n",
      "Finished epoch  83\n",
      "On epoch  84\n",
      "Total Epoch Testing MAPE: PCE = 53.595184326171875\n",
      "                              Voc = 33.49493408203125\n",
      "                              Jsc = 92.93659973144531\n",
      "                              FF = 50.32845687866211\n",
      "Finished epoch  84\n",
      "On epoch  85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.64549255371094\n",
      "                              Voc = 34.071468353271484\n",
      "                              Jsc = 93.0771255493164\n",
      "                              FF = 49.6539306640625\n",
      "Finished epoch  85\n",
      "On epoch  86\n",
      "Total Epoch Testing MAPE: PCE = 55.34575653076172\n",
      "                              Voc = 34.52791976928711\n",
      "                              Jsc = 92.70751953125\n",
      "                              FF = 49.23488235473633\n",
      "Finished epoch  86\n",
      "On epoch  87\n",
      "Total Epoch Testing MAPE: PCE = 56.056602478027344\n",
      "                              Voc = 34.37385940551758\n",
      "                              Jsc = 92.7207260131836\n",
      "                              FF = 48.474586486816406\n",
      "Finished epoch  87\n",
      "On epoch  88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.96818923950195\n",
      "                              Voc = 35.14149856567383\n",
      "                              Jsc = 92.25164031982422\n",
      "                              FF = 47.56564712524414\n",
      "Finished epoch  88\n",
      "On epoch  89\n",
      "Total Epoch Testing MAPE: PCE = 56.5983772277832\n",
      "                              Voc = 35.0731315612793\n",
      "                              Jsc = 92.17161560058594\n",
      "                              FF = 47.923160552978516\n",
      "Finished epoch  89\n",
      "On epoch  90\n",
      "Total Epoch Testing MAPE: PCE = 57.712345123291016\n",
      "                              Voc = 35.099395751953125\n",
      "                              Jsc = 91.9256362915039\n",
      "                              FF = 47.2992057800293\n",
      "Finished epoch  90\n",
      "On epoch  91\n",
      "Total Epoch Testing MAPE: PCE = 57.41492462158203\n",
      "                              Voc = 35.092918395996094\n",
      "                              Jsc = 91.54740905761719\n",
      "                              FF = 47.3982048034668\n",
      "Finished epoch  91\n",
      "On epoch  92\n",
      "Total Epoch Testing MAPE: PCE = 56.79093551635742\n",
      "                              Voc = 35.46902084350586\n",
      "                              Jsc = 91.2297592163086\n",
      "                              FF = 47.43640899658203\n",
      "Finished epoch  92\n",
      "On epoch  93\n",
      "Total Epoch Testing MAPE: PCE = 55.50086975097656\n",
      "                              Voc = 35.99974060058594\n",
      "                              Jsc = 90.81893920898438\n",
      "                              FF = 47.15580368041992\n",
      "Finished epoch  93\n",
      "On epoch  94\n",
      "Total Epoch Testing MAPE: PCE = 55.05944061279297\n",
      "                              Voc = 36.46820068359375\n",
      "                              Jsc = 91.08612823486328\n",
      "                              FF = 47.26708984375\n",
      "Finished epoch  94\n",
      "On epoch  95\n",
      "Total Epoch Testing MAPE: PCE = 55.75586700439453\n",
      "                              Voc = 36.83417892456055\n",
      "                              Jsc = 91.01432037353516\n",
      "                              FF = 47.64203643798828\n",
      "Finished epoch  95\n",
      "On epoch  96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.04169464111328\n",
      "                              Voc = 36.98735046386719\n",
      "                              Jsc = 91.11077117919922\n",
      "                              FF = 47.14666748046875\n",
      "Finished epoch  96\n",
      "On epoch  97\n",
      "Total Epoch Testing MAPE: PCE = 55.69525909423828\n",
      "                              Voc = 36.98942947387695\n",
      "                              Jsc = 91.30547332763672\n",
      "                              FF = 46.6285514831543\n",
      "Finished epoch  97\n",
      "On epoch  98\n",
      "Total Epoch Testing MAPE: PCE = 56.167964935302734\n",
      "                              Voc = 37.01781463623047\n",
      "                              Jsc = 91.04527282714844\n",
      "                              FF = 45.993919372558594\n",
      "Finished epoch  98\n",
      "On epoch  99\n",
      "Total Epoch Testing MAPE: PCE = 55.377262115478516\n",
      "                              Voc = 37.4444465637207\n",
      "                              Jsc = 91.04285430908203\n",
      "                              FF = 46.42766571044922\n",
      "Finished epoch  99\n",
      "On epoch  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.63570785522461\n",
      "                              Voc = 36.1522331237793\n",
      "                              Jsc = 90.75100708007812\n",
      "                              FF = 45.8729362487793\n",
      "Finished epoch  100\n",
      "On epoch  101\n",
      "Total Epoch Testing MAPE: PCE = 54.31731414794922\n",
      "                              Voc = 34.74647521972656\n",
      "                              Jsc = 90.59717559814453\n",
      "                              FF = 45.46479415893555\n",
      "Finished epoch  101\n",
      "On epoch  102\n",
      "Total Epoch Testing MAPE: PCE = 54.41978454589844\n",
      "                              Voc = 34.423187255859375\n",
      "                              Jsc = 90.52509307861328\n",
      "                              FF = 45.51546859741211\n",
      "Finished epoch  102\n",
      "On epoch  103\n",
      "Total Epoch Testing MAPE: PCE = 54.6130256652832\n",
      "                              Voc = 33.26737976074219\n",
      "                              Jsc = 90.18013763427734\n",
      "                              FF = 44.7773551940918\n",
      "Finished epoch  103\n",
      "On epoch  104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.123023986816406\n",
      "                              Voc = 33.188743591308594\n",
      "                              Jsc = 90.1941909790039\n",
      "                              FF = 45.09978485107422\n",
      "Finished epoch  104\n",
      "On epoch  105\n",
      "Total Epoch Testing MAPE: PCE = 54.1015625\n",
      "                              Voc = 32.46577835083008\n",
      "                              Jsc = 90.04795837402344\n",
      "                              FF = 45.453121185302734\n",
      "Finished epoch  105\n",
      "On epoch  106\n",
      "Total Epoch Testing MAPE: PCE = 53.31526565551758\n",
      "                              Voc = 30.700891494750977\n",
      "                              Jsc = 89.96630096435547\n",
      "                              FF = 45.348731994628906\n",
      "Finished epoch  106\n",
      "On epoch  107\n",
      "Total Epoch Testing MAPE: PCE = 53.150760650634766\n",
      "                              Voc = 29.14891242980957\n",
      "                              Jsc = 90.18235778808594\n",
      "                              FF = 45.15868377685547\n",
      "Finished epoch  107\n",
      "On epoch  108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.7977409362793\n",
      "                              Voc = 27.962675094604492\n",
      "                              Jsc = 90.62651062011719\n",
      "                              FF = 45.6405029296875\n",
      "Finished epoch  108\n",
      "On epoch  109\n",
      "Total Epoch Testing MAPE: PCE = 52.579654693603516\n",
      "                              Voc = 26.221168518066406\n",
      "                              Jsc = 90.47256469726562\n",
      "                              FF = 46.07965087890625\n",
      "Finished epoch  109\n",
      "On epoch  110\n",
      "Total Epoch Testing MAPE: PCE = 52.50648880004883\n",
      "                              Voc = 25.517892837524414\n",
      "                              Jsc = 90.5281982421875\n",
      "                              FF = 46.6732177734375\n",
      "Finished epoch  110\n",
      "On epoch  111\n",
      "Total Epoch Testing MAPE: PCE = 52.09895324707031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Voc = 25.366178512573242\n",
      "                              Jsc = 91.18743896484375\n",
      "                              FF = 46.568450927734375\n",
      "Finished epoch  111\n",
      "On epoch  112\n",
      "Total Epoch Testing MAPE: PCE = 51.35176086425781\n",
      "                              Voc = 24.83225440979004\n",
      "                              Jsc = 91.51177215576172\n",
      "                              FF = 46.85877990722656\n",
      "Finished epoch  112\n",
      "On epoch  113\n",
      "Total Epoch Testing MAPE: PCE = 50.54613494873047\n",
      "                              Voc = 23.60915184020996\n",
      "                              Jsc = 91.70133972167969\n",
      "                              FF = 47.13322067260742\n",
      "Finished epoch  113\n",
      "On epoch  114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 49.85311508178711\n",
      "                              Voc = 23.457263946533203\n",
      "                              Jsc = 91.69013214111328\n",
      "                              FF = 47.37335968017578\n",
      "Finished epoch  114\n",
      "On epoch  115\n",
      "Total Epoch Testing MAPE: PCE = 49.344268798828125\n",
      "                              Voc = 23.342443466186523\n",
      "                              Jsc = 91.72489929199219\n",
      "                              FF = 47.94511795043945\n",
      "Finished epoch  115\n",
      "On epoch  116\n",
      "Total Epoch Testing MAPE: PCE = 48.90159606933594\n",
      "                              Voc = 23.46426010131836\n",
      "                              Jsc = 91.45989227294922\n",
      "                              FF = 48.60627746582031\n",
      "Finished epoch  116\n",
      "On epoch  117\n",
      "Total Epoch Testing MAPE: PCE = 48.52928161621094\n",
      "                              Voc = 24.01238441467285\n",
      "                              Jsc = 91.70947265625\n",
      "                              FF = 49.73950958251953\n",
      "Finished epoch  117\n",
      "On epoch  118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 48.74269104003906\n",
      "                              Voc = 24.577938079833984\n",
      "                              Jsc = 91.55876922607422\n",
      "                              FF = 50.21189498901367\n",
      "Finished epoch  118\n",
      "On epoch  119\n",
      "Total Epoch Testing MAPE: PCE = 48.81714630126953\n",
      "                              Voc = 24.912372589111328\n",
      "                              Jsc = 91.76606750488281\n",
      "                              FF = 50.40200424194336\n",
      "Finished epoch  119\n",
      "On epoch  120\n",
      "Total Epoch Testing MAPE: PCE = 49.58158874511719\n",
      "                              Voc = 25.394893646240234\n",
      "                              Jsc = 92.41024780273438\n",
      "                              FF = 49.71434783935547\n",
      "Finished epoch  120\n",
      "On epoch  121\n",
      "Total Epoch Testing MAPE: PCE = 50.53466033935547\n",
      "                              Voc = 25.710308074951172\n",
      "                              Jsc = 92.64482116699219\n",
      "                              FF = 49.63795852661133\n",
      "Finished epoch  121\n",
      "On epoch  122\n",
      "Total Epoch Testing MAPE: PCE = 50.72142028808594\n",
      "                              Voc = 25.707561492919922\n",
      "                              Jsc = 92.79244232177734\n",
      "                              FF = 49.81636047363281\n",
      "Finished epoch  122\n",
      "On epoch  123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 51.197410583496094\n",
      "                              Voc = 25.715538024902344\n",
      "                              Jsc = 92.91773986816406\n",
      "                              FF = 49.26533508300781\n",
      "Finished epoch  123\n",
      "On epoch  124\n",
      "Total Epoch Testing MAPE: PCE = 51.377315521240234\n",
      "                              Voc = 26.395097732543945\n",
      "                              Jsc = 92.60491943359375\n",
      "                              FF = 48.947750091552734\n",
      "Finished epoch  124\n",
      "On epoch  125\n",
      "Total Epoch Testing MAPE: PCE = 51.55708312988281\n",
      "                              Voc = 26.81940460205078\n",
      "                              Jsc = 92.61051177978516\n",
      "                              FF = 48.4073486328125\n",
      "Finished epoch  125\n",
      "On epoch  126\n",
      "Total Epoch Testing MAPE: PCE = 51.80271911621094\n",
      "                              Voc = 26.628440856933594\n",
      "                              Jsc = 92.21952056884766\n",
      "                              FF = 48.423641204833984\n",
      "Finished epoch  126\n",
      "On epoch  127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.415828704833984\n",
      "                              Voc = 26.692508697509766\n",
      "                              Jsc = 91.9672622680664\n",
      "                              FF = 48.966556549072266\n",
      "Finished epoch  127\n",
      "On epoch  128\n",
      "Total Epoch Testing MAPE: PCE = 54.40756607055664\n",
      "                              Voc = 26.99521255493164\n",
      "                              Jsc = 91.4283447265625\n",
      "                              FF = 48.95226287841797\n",
      "Finished epoch  128\n",
      "On epoch  129\n",
      "Total Epoch Testing MAPE: PCE = 54.889591217041016\n",
      "                              Voc = 27.40138816833496\n",
      "                              Jsc = 91.31394958496094\n",
      "                              FF = 49.34067916870117\n",
      "Finished epoch  129\n",
      "On epoch  130\n",
      "Total Epoch Testing MAPE: PCE = 55.48405075073242\n",
      "                              Voc = 27.11137580871582\n",
      "                              Jsc = 91.02604675292969\n",
      "                              FF = 49.89494323730469\n",
      "Finished epoch  130\n",
      "On epoch  131\n",
      "Total Epoch Testing MAPE: PCE = 54.981971740722656\n",
      "                              Voc = 26.525745391845703\n",
      "                              Jsc = 90.99629974365234\n",
      "                              FF = 51.1927604675293\n",
      "Finished epoch  131\n",
      "On epoch  132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.7972297668457\n",
      "                              Voc = 26.274818420410156\n",
      "                              Jsc = 90.62168884277344\n",
      "                              FF = 51.21794891357422\n",
      "Finished epoch  132\n",
      "On epoch  133\n",
      "Total Epoch Testing MAPE: PCE = 55.81697082519531\n",
      "                              Voc = 25.090923309326172\n",
      "                              Jsc = 90.49401092529297\n",
      "                              FF = 51.223899841308594\n",
      "Finished epoch  133\n",
      "On epoch  134\n",
      "Total Epoch Testing MAPE: PCE = 54.923583984375\n",
      "                              Voc = 25.044301986694336\n",
      "                              Jsc = 90.7999038696289\n",
      "                              FF = 51.18821716308594\n",
      "Finished epoch  134\n",
      "On epoch  135\n",
      "Total Epoch Testing MAPE: PCE = 54.37174987792969\n",
      "                              Voc = 24.8844051361084\n",
      "                              Jsc = 90.79501342773438\n",
      "                              FF = 51.47693634033203\n",
      "Finished epoch  135\n",
      "On epoch  136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.178165435791016\n",
      "                              Voc = 24.336456298828125\n",
      "                              Jsc = 90.78506469726562\n",
      "                              FF = 51.90428924560547\n",
      "Finished epoch  136\n",
      "On epoch  137\n",
      "Total Epoch Testing MAPE: PCE = 54.004520416259766\n",
      "                              Voc = 23.65872573852539\n",
      "                              Jsc = 90.55150604248047\n",
      "                              FF = 52.083988189697266\n",
      "Finished epoch  137\n",
      "On epoch  138\n",
      "Total Epoch Testing MAPE: PCE = 54.13454818725586\n",
      "                              Voc = 23.10771942138672\n",
      "                              Jsc = 90.37820434570312\n",
      "                              FF = 52.23270034790039\n",
      "Finished epoch  138\n",
      "On epoch  139\n",
      "Total Epoch Testing MAPE: PCE = 53.24553680419922\n",
      "                              Voc = 23.509239196777344\n",
      "                              Jsc = 90.1183853149414\n",
      "                              FF = 52.28444290161133\n",
      "Finished epoch  139\n",
      "On epoch  140\n",
      "Total Epoch Testing MAPE: PCE = 53.17610549926758\n",
      "                              Voc = 24.678922653198242\n",
      "                              Jsc = 90.33688354492188\n",
      "                              FF = 51.717525482177734\n",
      "Finished epoch  140\n",
      "On epoch  141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.12604904174805\n",
      "                              Voc = 24.86921501159668\n",
      "                              Jsc = 89.96758270263672\n",
      "                              FF = 51.79526901245117\n",
      "Finished epoch  141\n",
      "On epoch  142\n",
      "Total Epoch Testing MAPE: PCE = 52.72354507446289\n",
      "                              Voc = 26.16657829284668\n",
      "                              Jsc = 89.90238952636719\n",
      "                              FF = 51.568145751953125\n",
      "Finished epoch  142\n",
      "On epoch  143\n",
      "Total Epoch Testing MAPE: PCE = 52.478729248046875\n",
      "                              Voc = 27.32521629333496\n",
      "                              Jsc = 89.94265747070312\n",
      "                              FF = 51.84459686279297\n",
      "Finished epoch  143\n",
      "On epoch  144\n",
      "Total Epoch Testing MAPE: PCE = 52.02656555175781\n",
      "                              Voc = 27.811771392822266\n",
      "                              Jsc = 89.91692352294922\n",
      "                              FF = 51.291622161865234\n",
      "Finished epoch  144\n",
      "On epoch  145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.54431915283203\n",
      "                              Voc = 27.60489845275879\n",
      "                              Jsc = 89.8814468383789\n",
      "                              FF = 51.949588775634766\n",
      "Finished epoch  145\n",
      "On epoch  146\n",
      "Total Epoch Testing MAPE: PCE = 52.45702362060547\n",
      "                              Voc = 28.421836853027344\n",
      "                              Jsc = 89.78616333007812\n",
      "                              FF = 51.29273986816406\n",
      "Finished epoch  146\n",
      "On epoch  147\n",
      "Total Epoch Testing MAPE: PCE = 52.44427490234375\n",
      "                              Voc = 29.557449340820312\n",
      "                              Jsc = 89.90558624267578\n",
      "                              FF = 51.07953643798828\n",
      "Finished epoch  147\n",
      "On epoch  148\n",
      "Total Epoch Testing MAPE: PCE = 52.709285736083984\n",
      "                              Voc = 29.49675941467285\n",
      "                              Jsc = 89.9164047241211\n",
      "                              FF = 51.06827163696289\n",
      "Finished epoch  148\n",
      "On epoch  149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.28573226928711\n",
      "                              Voc = 30.721023559570312\n",
      "                              Jsc = 89.81098175048828\n",
      "                              FF = 51.199275970458984\n",
      "Finished epoch  149\n",
      "On epoch  150\n",
      "Total Epoch Testing MAPE: PCE = 52.055442810058594\n",
      "                              Voc = 31.118684768676758\n",
      "                              Jsc = 89.70338439941406\n",
      "                              FF = 51.37961959838867\n",
      "Finished epoch  150\n",
      "On epoch  151\n",
      "Total Epoch Testing MAPE: PCE = 52.47626876831055\n",
      "                              Voc = 31.134870529174805\n",
      "                              Jsc = 89.23426818847656\n",
      "                              FF = 51.15068054199219\n",
      "Finished epoch  151\n",
      "On epoch  152\n",
      "Total Epoch Testing MAPE: PCE = 53.011627197265625\n",
      "                              Voc = 30.87855339050293\n",
      "                              Jsc = 89.15376281738281\n",
      "                              FF = 51.20423889160156\n",
      "Finished epoch  152\n",
      "On epoch  153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.75905227661133\n",
      "                              Voc = 29.813095092773438\n",
      "                              Jsc = 89.23886108398438\n",
      "                              FF = 51.157474517822266\n",
      "Finished epoch  153\n",
      "On epoch  154\n",
      "Total Epoch Testing MAPE: PCE = 53.22821044921875\n",
      "                              Voc = 30.03903579711914\n",
      "                              Jsc = 88.5162353515625\n",
      "                              FF = 50.666786193847656\n",
      "Finished epoch  154\n",
      "On epoch  155\n",
      "Total Epoch Testing MAPE: PCE = 52.66169357299805\n",
      "                              Voc = 30.043813705444336\n",
      "                              Jsc = 88.53191375732422\n",
      "                              FF = 50.601261138916016\n",
      "Finished epoch  155\n",
      "On epoch  156\n",
      "Total Epoch Testing MAPE: PCE = 52.53871536254883\n",
      "                              Voc = 30.562171936035156\n",
      "                              Jsc = 88.1283187866211\n",
      "                              FF = 50.50343322753906\n",
      "Finished epoch  156\n",
      "On epoch  157\n",
      "Total Epoch Testing MAPE: PCE = 52.32391357421875\n",
      "                              Voc = 30.943330764770508\n",
      "                              Jsc = 87.65715026855469\n",
      "                              FF = 50.39756393432617\n",
      "Finished epoch  157\n",
      "On epoch  158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.054603576660156\n",
      "                              Voc = 31.244409561157227\n",
      "                              Jsc = 87.29088592529297\n",
      "                              FF = 50.38604736328125\n",
      "Finished epoch  158\n",
      "On epoch  159\n",
      "Total Epoch Testing MAPE: PCE = 52.105194091796875\n",
      "                              Voc = 31.791568756103516\n",
      "                              Jsc = 87.27425384521484\n",
      "                              FF = 50.07776641845703\n",
      "Finished epoch  159\n",
      "On epoch  160\n",
      "Total Epoch Testing MAPE: PCE = 51.99229049682617\n",
      "                              Voc = 31.6010684967041\n",
      "                              Jsc = 86.97785186767578\n",
      "                              FF = 49.76059341430664\n",
      "Finished epoch  160\n",
      "On epoch  161\n",
      "Total Epoch Testing MAPE: PCE = 52.773834228515625\n",
      "                              Voc = 31.604463577270508\n",
      "                              Jsc = 86.35069274902344\n",
      "                              FF = 49.19957733154297\n",
      "Finished epoch  161\n",
      "On epoch  162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 52.112857818603516\n",
      "                              Voc = 31.22318458557129\n",
      "                              Jsc = 85.60608673095703\n",
      "                              FF = 49.58831787109375\n",
      "Finished epoch  162\n",
      "On epoch  163\n",
      "Total Epoch Testing MAPE: PCE = 52.66501998901367\n",
      "                              Voc = 30.623546600341797\n",
      "                              Jsc = 85.45585632324219\n",
      "                              FF = 49.789161682128906\n",
      "Finished epoch  163\n",
      "On epoch  164\n",
      "Total Epoch Testing MAPE: PCE = 53.30954360961914\n",
      "                              Voc = 31.351226806640625\n",
      "                              Jsc = 85.61819458007812\n",
      "                              FF = 49.66796875\n",
      "Finished epoch  164\n",
      "On epoch  165\n",
      "Total Epoch Testing MAPE: PCE = 53.545188903808594\n",
      "                              Voc = 30.93383026123047\n",
      "                              Jsc = 85.55868530273438\n",
      "                              FF = 49.99113845825195\n",
      "Finished epoch  165\n",
      "On epoch  166\n",
      "Total Epoch Testing MAPE: PCE = 53.8538703918457\n",
      "                              Voc = 30.56916618347168\n",
      "                              Jsc = 85.46867370605469\n",
      "                              FF = 50.03346252441406\n",
      "Finished epoch  166\n",
      "On epoch  167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.26654052734375\n",
      "                              Voc = 30.52961540222168\n",
      "                              Jsc = 85.27762603759766\n",
      "                              FF = 50.52494430541992\n",
      "Finished epoch  167\n",
      "On epoch  168\n",
      "Total Epoch Testing MAPE: PCE = 54.23805618286133\n",
      "                              Voc = 30.423721313476562\n",
      "                              Jsc = 85.4106216430664\n",
      "                              FF = 50.991905212402344\n",
      "Finished epoch  168\n",
      "On epoch  169\n",
      "Total Epoch Testing MAPE: PCE = 54.678131103515625\n",
      "                              Voc = 30.53350830078125\n",
      "                              Jsc = 85.3359603881836\n",
      "                              FF = 50.93988037109375\n",
      "Finished epoch  169\n",
      "On epoch  170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.32567596435547\n",
      "                              Voc = 31.312763214111328\n",
      "                              Jsc = 84.99705505371094\n",
      "                              FF = 51.371036529541016\n",
      "Finished epoch  170\n",
      "On epoch  171\n",
      "Total Epoch Testing MAPE: PCE = 55.2889518737793\n",
      "                              Voc = 31.179841995239258\n",
      "                              Jsc = 84.6299057006836\n",
      "                              FF = 51.7099494934082\n",
      "Finished epoch  171\n",
      "On epoch  172\n",
      "Total Epoch Testing MAPE: PCE = 55.533973693847656\n",
      "                              Voc = 31.522777557373047\n",
      "                              Jsc = 84.28216552734375\n",
      "                              FF = 52.43515396118164\n",
      "Finished epoch  172\n",
      "On epoch  173\n",
      "Total Epoch Testing MAPE: PCE = 56.656551361083984\n",
      "                              Voc = 32.44779586791992\n",
      "                              Jsc = 83.93329620361328\n",
      "                              FF = 52.345001220703125\n",
      "Finished epoch  173\n",
      "On epoch  174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.36155700683594\n",
      "                              Voc = 33.0198860168457\n",
      "                              Jsc = 83.71798706054688\n",
      "                              FF = 52.7394905090332\n",
      "Finished epoch  174\n",
      "On epoch  175\n",
      "Total Epoch Testing MAPE: PCE = 56.77776336669922\n",
      "                              Voc = 33.123680114746094\n",
      "                              Jsc = 83.84037017822266\n",
      "                              FF = 52.468692779541016\n",
      "Finished epoch  175\n",
      "On epoch  176\n",
      "Total Epoch Testing MAPE: PCE = 57.28143310546875\n",
      "                              Voc = 33.25973892211914\n",
      "                              Jsc = 83.97935485839844\n",
      "                              FF = 52.06362533569336\n",
      "Finished epoch  176\n",
      "On epoch  177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.479679107666016\n",
      "                              Voc = 32.498897552490234\n",
      "                              Jsc = 84.22669982910156\n",
      "                              FF = 51.470924377441406\n",
      "Finished epoch  177\n",
      "On epoch  178\n",
      "Total Epoch Testing MAPE: PCE = 58.092044830322266\n",
      "                              Voc = 32.649600982666016\n",
      "                              Jsc = 84.07189178466797\n",
      "                              FF = 51.818443298339844\n",
      "Finished epoch  178\n",
      "On epoch  179\n",
      "Total Epoch Testing MAPE: PCE = 57.74989700317383\n",
      "                              Voc = 32.46815490722656\n",
      "                              Jsc = 83.9864730834961\n",
      "                              FF = 51.50385665893555\n",
      "Finished epoch  179\n",
      "On epoch  180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.70906066894531\n",
      "                              Voc = 31.78863525390625\n",
      "                              Jsc = 83.58477020263672\n",
      "                              FF = 51.633811950683594\n",
      "Finished epoch  180\n",
      "On epoch  181\n",
      "Total Epoch Testing MAPE: PCE = 58.130409240722656\n",
      "                              Voc = 32.37470626831055\n",
      "                              Jsc = 83.82310485839844\n",
      "                              FF = 50.905372619628906\n",
      "Finished epoch  181\n",
      "On epoch  182\n",
      "Total Epoch Testing MAPE: PCE = 57.92509841918945\n",
      "                              Voc = 31.433122634887695\n",
      "                              Jsc = 83.91554260253906\n",
      "                              FF = 51.03529739379883\n",
      "Finished epoch  182\n",
      "On epoch  183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.192466735839844\n",
      "                              Voc = 31.290191650390625\n",
      "                              Jsc = 83.74085235595703\n",
      "                              FF = 50.92202377319336\n",
      "Finished epoch  183\n",
      "On epoch  184\n",
      "Total Epoch Testing MAPE: PCE = 58.776973724365234\n",
      "                              Voc = 31.25334930419922\n",
      "                              Jsc = 83.66857147216797\n",
      "                              FF = 50.57284927368164\n",
      "Finished epoch  184\n",
      "On epoch  185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.26047134399414\n",
      "                              Voc = 29.926210403442383\n",
      "                              Jsc = 83.58607482910156\n",
      "                              FF = 50.38834762573242\n",
      "Finished epoch  185\n",
      "On epoch  186\n",
      "Total Epoch Testing MAPE: PCE = 58.827537536621094\n",
      "                              Voc = 29.806114196777344\n",
      "                              Jsc = 83.71881866455078\n",
      "                              FF = 49.908164978027344\n",
      "Finished epoch  186\n",
      "On epoch  187\n",
      "Total Epoch Testing MAPE: PCE = 58.20469284057617\n",
      "                              Voc = 30.851612091064453\n",
      "                              Jsc = 83.66081237792969\n",
      "                              FF = 49.54315185546875\n",
      "Finished epoch  187\n",
      "On epoch  188\n",
      "Total Epoch Testing MAPE: PCE = 58.63562774658203\n",
      "                              Voc = 30.61600112915039\n",
      "                              Jsc = 83.60627746582031\n",
      "                              FF = 49.71173858642578\n",
      "Finished epoch  188\n",
      "On epoch  189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.779273986816406\n",
      "                              Voc = 31.73232650756836\n",
      "                              Jsc = 83.85980224609375\n",
      "                              FF = 49.88451385498047\n",
      "Finished epoch  189\n",
      "On epoch  190\n",
      "Total Epoch Testing MAPE: PCE = 58.69424819946289\n",
      "                              Voc = 31.766185760498047\n",
      "                              Jsc = 84.01025390625\n",
      "                              FF = 50.176631927490234\n",
      "Finished epoch  190\n",
      "On epoch  191\n",
      "Total Epoch Testing MAPE: PCE = 58.49825668334961\n",
      "                              Voc = 31.701696395874023\n",
      "                              Jsc = 84.2348403930664\n",
      "                              FF = 50.58015060424805\n",
      "Finished epoch  191\n",
      "On epoch  192\n",
      "Total Epoch Testing MAPE: PCE = 57.934303283691406\n",
      "                              Voc = 32.39447784423828\n",
      "                              Jsc = 83.95191955566406\n",
      "                              FF = 50.14717483520508\n",
      "Finished epoch  192\n",
      "On epoch  193\n",
      "Total Epoch Testing MAPE: PCE = 57.38722610473633\n",
      "                              Voc = 32.75954818725586\n",
      "                              Jsc = 83.74543762207031\n",
      "                              FF = 50.035160064697266\n",
      "Finished epoch  193\n",
      "On epoch  194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.19078826904297\n",
      "                              Voc = 33.35111999511719\n",
      "                              Jsc = 83.9140625\n",
      "                              FF = 49.72245788574219\n",
      "Finished epoch  194\n",
      "On epoch  195\n",
      "Total Epoch Testing MAPE: PCE = 57.48277282714844\n",
      "                              Voc = 33.83885955810547\n",
      "                              Jsc = 83.9896240234375\n",
      "                              FF = 49.395511627197266\n",
      "Finished epoch  195\n",
      "On epoch  196\n",
      "Total Epoch Testing MAPE: PCE = 56.68510818481445\n",
      "                              Voc = 33.81631851196289\n",
      "                              Jsc = 84.32091522216797\n",
      "                              FF = 49.42831802368164\n",
      "Finished epoch  196\n",
      "On epoch  197\n",
      "Total Epoch Testing MAPE: PCE = 56.370269775390625\n",
      "                              Voc = 34.54579162597656\n",
      "                              Jsc = 84.28850555419922\n",
      "                              FF = 49.643619537353516\n",
      "Finished epoch  197\n",
      "On epoch  198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.738555908203125\n",
      "                              Voc = 35.233489990234375\n",
      "                              Jsc = 83.70732116699219\n",
      "                              FF = 50.25505447387695\n",
      "Finished epoch  198\n",
      "On epoch  199\n",
      "Total Epoch Testing MAPE: PCE = 55.28914260864258\n",
      "                              Voc = 35.57065200805664\n",
      "                              Jsc = 83.56263732910156\n",
      "                              FF = 50.814884185791016\n",
      "Finished epoch  199\n",
      "On epoch  200\n",
      "Total Epoch Testing MAPE: PCE = 54.972076416015625\n",
      "                              Voc = 35.99610900878906\n",
      "                              Jsc = 83.18744659423828\n",
      "                              FF = 51.35380554199219\n",
      "Finished epoch  200\n",
      "On epoch  201\n",
      "Total Epoch Testing MAPE: PCE = 55.16402053833008\n",
      "                              Voc = 36.52485275268555\n",
      "                              Jsc = 82.85935974121094\n",
      "                              FF = 50.914852142333984\n",
      "Finished epoch  201\n",
      "On epoch  202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.516483306884766\n",
      "                              Voc = 36.64771270751953\n",
      "                              Jsc = 82.94602966308594\n",
      "                              FF = 50.30549240112305\n",
      "Finished epoch  202\n",
      "On epoch  203\n",
      "Total Epoch Testing MAPE: PCE = 53.981422424316406\n",
      "                              Voc = 36.66225814819336\n",
      "                              Jsc = 82.66292572021484\n",
      "                              FF = 50.49188232421875\n",
      "Finished epoch  203\n",
      "On epoch  204\n",
      "Total Epoch Testing MAPE: PCE = 53.92226028442383\n",
      "                              Voc = 37.12466812133789\n",
      "                              Jsc = 82.48466491699219\n",
      "                              FF = 50.75534439086914\n",
      "Finished epoch  204\n",
      "On epoch  205\n",
      "Total Epoch Testing MAPE: PCE = 53.29765701293945\n",
      "                              Voc = 37.86365509033203\n",
      "                              Jsc = 82.42340087890625\n",
      "                              FF = 50.34335708618164\n",
      "Finished epoch  205\n",
      "On epoch  206\n",
      "Total Epoch Testing MAPE: PCE = 53.59069061279297\n",
      "                              Voc = 38.21943283081055\n",
      "                              Jsc = 82.30673217773438\n",
      "                              FF = 50.772708892822266\n",
      "Finished epoch  206\n",
      "On epoch  207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.83644104003906\n",
      "                              Voc = 38.118919372558594\n",
      "                              Jsc = 82.15267944335938\n",
      "                              FF = 50.7960205078125\n",
      "Finished epoch  207\n",
      "On epoch  208\n",
      "Total Epoch Testing MAPE: PCE = 54.291046142578125\n",
      "                              Voc = 38.464290618896484\n",
      "                              Jsc = 82.18474578857422\n",
      "                              FF = 51.184356689453125\n",
      "Finished epoch  208\n",
      "On epoch  209\n",
      "Total Epoch Testing MAPE: PCE = 54.62812805175781\n",
      "                              Voc = 38.179988861083984\n",
      "                              Jsc = 82.25837707519531\n",
      "                              FF = 51.54573440551758\n",
      "Finished epoch  209\n",
      "On epoch  210\n",
      "Total Epoch Testing MAPE: PCE = 54.592124938964844\n",
      "                              Voc = 38.93819808959961\n",
      "                              Jsc = 82.03608703613281\n",
      "                              FF = 51.30371856689453\n",
      "Finished epoch  210\n",
      "On epoch  211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.30732345581055\n",
      "                              Voc = 38.82353210449219\n",
      "                              Jsc = 82.13646697998047\n",
      "                              FF = 51.82628631591797\n",
      "Finished epoch  211\n",
      "On epoch  212\n",
      "Total Epoch Testing MAPE: PCE = 55.56596374511719\n",
      "                              Voc = 39.29447555541992\n",
      "                              Jsc = 82.13044738769531\n",
      "                              FF = 52.09126281738281\n",
      "Finished epoch  212\n",
      "On epoch  213\n",
      "Total Epoch Testing MAPE: PCE = 55.20756149291992\n",
      "                              Voc = 39.10265350341797\n",
      "                              Jsc = 82.22701263427734\n",
      "                              FF = 52.870643615722656\n",
      "Finished epoch  213\n",
      "On epoch  214\n",
      "Total Epoch Testing MAPE: PCE = 54.7103157043457\n",
      "                              Voc = 38.44953155517578\n",
      "                              Jsc = 81.8202896118164\n",
      "                              FF = 52.94377136230469\n",
      "Finished epoch  214\n",
      "On epoch  215\n",
      "Total Epoch Testing MAPE: PCE = 54.2597770690918\n",
      "                              Voc = 38.361236572265625\n",
      "                              Jsc = 82.21388244628906\n",
      "                              FF = 52.99871063232422\n",
      "Finished epoch  215\n",
      "On epoch  216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.021488189697266\n",
      "                              Voc = 37.911163330078125\n",
      "                              Jsc = 81.72215270996094\n",
      "                              FF = 53.23923110961914\n",
      "Finished epoch  216\n",
      "On epoch  217\n",
      "Total Epoch Testing MAPE: PCE = 54.136104583740234\n",
      "                              Voc = 37.257991790771484\n",
      "                              Jsc = 81.96121978759766\n",
      "                              FF = 52.71059799194336\n",
      "Finished epoch  217\n",
      "On epoch  218\n",
      "Total Epoch Testing MAPE: PCE = 54.25441360473633\n",
      "                              Voc = 36.24551010131836\n",
      "                              Jsc = 81.71510314941406\n",
      "                              FF = 52.84956741333008\n",
      "Finished epoch  218\n",
      "On epoch  219\n",
      "Total Epoch Testing MAPE: PCE = 54.75401306152344\n",
      "                              Voc = 35.59750747680664\n",
      "                              Jsc = 81.52904510498047\n",
      "                              FF = 52.01829528808594\n",
      "Finished epoch  219\n",
      "On epoch  220\n",
      "Total Epoch Testing MAPE: PCE = 55.48688888549805\n",
      "                              Voc = 35.378055572509766\n",
      "                              Jsc = 81.6534423828125\n",
      "                              FF = 51.82144546508789\n",
      "Finished epoch  220\n",
      "On epoch  221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.59012985229492\n",
      "                              Voc = 35.79414749145508\n",
      "                              Jsc = 81.48200225830078\n",
      "                              FF = 51.66134262084961\n",
      "Finished epoch  221\n",
      "On epoch  222\n",
      "Total Epoch Testing MAPE: PCE = 55.77662658691406\n",
      "                              Voc = 36.30217361450195\n",
      "                              Jsc = 81.51410675048828\n",
      "                              FF = 51.27544403076172\n",
      "Finished epoch  222\n",
      "On epoch  223\n",
      "Total Epoch Testing MAPE: PCE = 56.04425811767578\n",
      "                              Voc = 35.88270568847656\n",
      "                              Jsc = 81.75713348388672\n",
      "                              FF = 51.37245178222656\n",
      "Finished epoch  223\n",
      "On epoch  224\n",
      "Total Epoch Testing MAPE: PCE = 56.10480880737305\n",
      "                              Voc = 35.339229583740234\n",
      "                              Jsc = 81.69017028808594\n",
      "                              FF = 51.25450897216797\n",
      "Finished epoch  224\n",
      "On epoch  225\n",
      "Total Epoch Testing MAPE: PCE = 56.26226806640625\n",
      "                              Voc = 34.93057632446289\n",
      "                              Jsc = 81.78707122802734\n",
      "                              FF = 51.74837875366211\n",
      "Finished epoch  225\n",
      "On epoch  226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.08982467651367\n",
      "                              Voc = 34.31089401245117\n",
      "                              Jsc = 81.77383422851562\n",
      "                              FF = 51.978458404541016\n",
      "Finished epoch  226\n",
      "On epoch  227\n",
      "Total Epoch Testing MAPE: PCE = 55.91302490234375\n",
      "                              Voc = 33.07691955566406\n",
      "                              Jsc = 81.868408203125\n",
      "                              FF = 51.863807678222656\n",
      "Finished epoch  227\n",
      "On epoch  228\n",
      "Total Epoch Testing MAPE: PCE = 55.89629364013672\n",
      "                              Voc = 32.749332427978516\n",
      "                              Jsc = 81.93789672851562\n",
      "                              FF = 51.70642852783203\n",
      "Finished epoch  228\n",
      "On epoch  229\n",
      "Total Epoch Testing MAPE: PCE = 56.197547912597656\n",
      "                              Voc = 32.44413757324219\n",
      "                              Jsc = 81.8309326171875\n",
      "                              FF = 52.0491943359375\n",
      "Finished epoch  229\n",
      "On epoch  230\n",
      "Total Epoch Testing MAPE: PCE = 56.368614196777344\n",
      "                              Voc = 31.927114486694336\n",
      "                              Jsc = 81.72881317138672\n",
      "                              FF = 51.878570556640625\n",
      "Finished epoch  230\n",
      "On epoch  231\n",
      "Total Epoch Testing MAPE: PCE = 56.3213996887207\n",
      "                              Voc = 32.83866882324219\n",
      "                              Jsc = 81.71387481689453\n",
      "                              FF = 52.41843032836914\n",
      "Finished epoch  231\n",
      "On epoch  232\n",
      "Total Epoch Testing MAPE: PCE = 56.18704605102539\n",
      "                              Voc = 32.67206573486328\n",
      "                              Jsc = 81.7032241821289\n",
      "                              FF = 52.73979949951172\n",
      "Finished epoch  232\n",
      "On epoch  233\n",
      "Total Epoch Testing MAPE: PCE = 56.22038269042969\n",
      "                              Voc = 32.36515808105469\n",
      "                              Jsc = 81.76878356933594\n",
      "                              FF = 52.82760238647461\n",
      "Finished epoch  233\n",
      "On epoch  234\n",
      "Total Epoch Testing MAPE: PCE = 55.30753707885742\n",
      "                              Voc = 32.37519836425781\n",
      "                              Jsc = 81.67733764648438\n",
      "                              FF = 52.46074295043945\n",
      "Finished epoch  234\n",
      "On epoch  235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.33167266845703\n",
      "                              Voc = 31.974050521850586\n",
      "                              Jsc = 82.03510284423828\n",
      "                              FF = 52.239280700683594\n",
      "Finished epoch  235\n",
      "On epoch  236\n",
      "Total Epoch Testing MAPE: PCE = 55.578678131103516\n",
      "                              Voc = 32.2380256652832\n",
      "                              Jsc = 81.73475646972656\n",
      "                              FF = 52.107852935791016\n",
      "Finished epoch  236\n",
      "On epoch  237\n",
      "Total Epoch Testing MAPE: PCE = 55.66702651977539\n",
      "                              Voc = 32.88246536254883\n",
      "                              Jsc = 81.65097045898438\n",
      "                              FF = 51.87239074707031\n",
      "Finished epoch  237\n",
      "On epoch  238\n",
      "Total Epoch Testing MAPE: PCE = 55.99680709838867\n",
      "                              Voc = 33.51166534423828\n",
      "                              Jsc = 81.32823944091797\n",
      "                              FF = 51.845829010009766\n",
      "Finished epoch  238\n",
      "On epoch  239\n",
      "Total Epoch Testing MAPE: PCE = 56.75146484375\n",
      "                              Voc = 33.233642578125\n",
      "                              Jsc = 81.20092010498047\n",
      "                              FF = 51.549339294433594\n",
      "Finished epoch  239\n",
      "On epoch  240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.99756622314453\n",
      "                              Voc = 33.02104568481445\n",
      "                              Jsc = 81.23204803466797\n",
      "                              FF = 51.95647048950195\n",
      "Finished epoch  240\n",
      "On epoch  241\n",
      "Total Epoch Testing MAPE: PCE = 57.52482986450195\n",
      "                              Voc = 33.688411712646484\n",
      "                              Jsc = 81.54254913330078\n",
      "                              FF = 52.464454650878906\n",
      "Finished epoch  241\n",
      "On epoch  242\n",
      "Total Epoch Testing MAPE: PCE = 58.17509841918945\n",
      "                              Voc = 33.98683166503906\n",
      "                              Jsc = 81.65235900878906\n",
      "                              FF = 52.659271240234375\n",
      "Finished epoch  242\n",
      "On epoch  243\n",
      "Total Epoch Testing MAPE: PCE = 58.77666473388672\n",
      "                              Voc = 34.4745979309082\n",
      "                              Jsc = 81.513427734375\n",
      "                              FF = 52.573673248291016\n",
      "Finished epoch  243\n",
      "On epoch  244\n",
      "Total Epoch Testing MAPE: PCE = 58.92816925048828\n",
      "                              Voc = 34.56275939941406\n",
      "                              Jsc = 81.52188873291016\n",
      "                              FF = 52.68574905395508\n",
      "Finished epoch  244\n",
      "On epoch  245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 59.61254119873047\n",
      "                              Voc = 34.62395095825195\n",
      "                              Jsc = 81.6355209350586\n",
      "                              FF = 52.61153030395508\n",
      "Finished epoch  245\n",
      "On epoch  246\n",
      "Total Epoch Testing MAPE: PCE = 59.502349853515625\n",
      "                              Voc = 34.98963928222656\n",
      "                              Jsc = 81.75038146972656\n",
      "                              FF = 52.59552764892578\n",
      "Finished epoch  246\n",
      "On epoch  247\n",
      "Total Epoch Testing MAPE: PCE = 59.342002868652344\n",
      "                              Voc = 35.286407470703125\n",
      "                              Jsc = 82.28861236572266\n",
      "                              FF = 52.8818473815918\n",
      "Finished epoch  247\n",
      "On epoch  248\n",
      "Total Epoch Testing MAPE: PCE = 58.83514404296875\n",
      "                              Voc = 35.633487701416016\n",
      "                              Jsc = 82.14447021484375\n",
      "                              FF = 52.814674377441406\n",
      "Finished epoch  248\n",
      "On epoch  249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.76153564453125\n",
      "                              Voc = 36.01226043701172\n",
      "                              Jsc = 81.85464477539062\n",
      "                              FF = 52.39716339111328\n",
      "Finished epoch  249\n",
      "On epoch  250\n",
      "Total Epoch Testing MAPE: PCE = 58.36285400390625\n",
      "                              Voc = 36.23381042480469\n",
      "                              Jsc = 81.93728637695312\n",
      "                              FF = 52.28857421875\n",
      "Finished epoch  250\n",
      "On epoch  251\n",
      "Total Epoch Testing MAPE: PCE = 57.885887145996094\n",
      "                              Voc = 37.108219146728516\n",
      "                              Jsc = 81.80819702148438\n",
      "                              FF = 53.29555892944336\n",
      "Finished epoch  251\n",
      "On epoch  252\n",
      "Total Epoch Testing MAPE: PCE = 57.198429107666016\n",
      "                              Voc = 37.70751953125\n",
      "                              Jsc = 81.67696380615234\n",
      "                              FF = 52.95702362060547\n",
      "Finished epoch  252\n",
      "On epoch  253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.832271575927734\n",
      "                              Voc = 38.141597747802734\n",
      "                              Jsc = 81.86876678466797\n",
      "                              FF = 52.99175262451172\n",
      "Finished epoch  253\n",
      "On epoch  254\n",
      "Total Epoch Testing MAPE: PCE = 56.94919967651367\n",
      "                              Voc = 38.402565002441406\n",
      "                              Jsc = 82.0372314453125\n",
      "                              FF = 52.83770751953125\n",
      "Finished epoch  254\n",
      "On epoch  255\n",
      "Total Epoch Testing MAPE: PCE = 56.561649322509766\n",
      "                              Voc = 38.52020263671875\n",
      "                              Jsc = 81.9820556640625\n",
      "                              FF = 52.7470817565918\n",
      "Finished epoch  255\n",
      "On epoch  256\n",
      "Total Epoch Testing MAPE: PCE = 56.28034973144531\n",
      "                              Voc = 39.655330657958984\n",
      "                              Jsc = 81.96150970458984\n",
      "                              FF = 52.95655059814453\n",
      "Finished epoch  256\n",
      "On epoch  257\n",
      "Total Epoch Testing MAPE: PCE = 55.921573638916016\n",
      "                              Voc = 39.76592254638672\n",
      "                              Jsc = 81.64402770996094\n",
      "                              FF = 52.9360466003418\n",
      "Finished epoch  257\n",
      "On epoch  258\n",
      "Total Epoch Testing MAPE: PCE = 55.65518569946289\n",
      "                              Voc = 39.59067916870117\n",
      "                              Jsc = 82.13160705566406\n",
      "                              FF = 52.804630279541016\n",
      "Finished epoch  258\n",
      "On epoch  259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.40266036987305\n",
      "                              Voc = 39.974029541015625\n",
      "                              Jsc = 81.90924835205078\n",
      "                              FF = 52.61023712158203\n",
      "Finished epoch  259\n",
      "On epoch  260\n",
      "Total Epoch Testing MAPE: PCE = 55.060340881347656\n",
      "                              Voc = 40.353843688964844\n",
      "                              Jsc = 82.00153350830078\n",
      "                              FF = 52.955810546875\n",
      "Finished epoch  260\n",
      "On epoch  261\n",
      "Total Epoch Testing MAPE: PCE = 54.57094192504883\n",
      "                              Voc = 40.003639221191406\n",
      "                              Jsc = 81.5317611694336\n",
      "                              FF = 52.96123504638672\n",
      "Finished epoch  261\n",
      "On epoch  262\n",
      "Total Epoch Testing MAPE: PCE = 54.74139404296875\n",
      "                              Voc = 39.6899299621582\n",
      "                              Jsc = 81.33740997314453\n",
      "                              FF = 52.766597747802734\n",
      "Finished epoch  262\n",
      "On epoch  263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.74179458618164\n",
      "                              Voc = 39.36473846435547\n",
      "                              Jsc = 81.20626831054688\n",
      "                              FF = 52.79107666015625\n",
      "Finished epoch  263\n",
      "On epoch  264\n",
      "Total Epoch Testing MAPE: PCE = 54.588722229003906\n",
      "                              Voc = 39.12006378173828\n",
      "                              Jsc = 81.08159637451172\n",
      "                              FF = 52.607242584228516\n",
      "Finished epoch  264\n",
      "On epoch  265\n",
      "Total Epoch Testing MAPE: PCE = 54.492515563964844\n",
      "                              Voc = 39.22019577026367\n",
      "                              Jsc = 80.98674011230469\n",
      "                              FF = 53.06432342529297\n",
      "Finished epoch  265\n",
      "On epoch  266\n",
      "Total Epoch Testing MAPE: PCE = 54.66033935546875\n",
      "                              Voc = 39.72772979736328\n",
      "                              Jsc = 80.81172943115234\n",
      "                              FF = 53.2620964050293\n",
      "Finished epoch  266\n",
      "On epoch  267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.25789260864258\n",
      "                              Voc = 39.44934844970703\n",
      "                              Jsc = 80.54186248779297\n",
      "                              FF = 53.34450149536133\n",
      "Finished epoch  267\n",
      "On epoch  268\n",
      "Total Epoch Testing MAPE: PCE = 54.249935150146484\n",
      "                              Voc = 39.949424743652344\n",
      "                              Jsc = 80.98965454101562\n",
      "                              FF = 53.08213806152344\n",
      "Finished epoch  268\n",
      "On epoch  269\n",
      "Total Epoch Testing MAPE: PCE = 53.95686721801758\n",
      "                              Voc = 40.042579650878906\n",
      "                              Jsc = 80.92374420166016\n",
      "                              FF = 53.08676528930664\n",
      "Finished epoch  269\n",
      "On epoch  270\n",
      "Total Epoch Testing MAPE: PCE = 54.03706359863281\n",
      "                              Voc = 39.32787322998047\n",
      "                              Jsc = 81.01213073730469\n",
      "                              FF = 52.88724136352539\n",
      "Finished epoch  270\n",
      "On epoch  271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.636505126953125\n",
      "                              Voc = 39.64498519897461\n",
      "                              Jsc = 81.20687103271484\n",
      "                              FF = 52.57748794555664\n",
      "Finished epoch  271\n",
      "On epoch  272\n",
      "Total Epoch Testing MAPE: PCE = 53.50539016723633\n",
      "                              Voc = 40.084651947021484\n",
      "                              Jsc = 81.19815826416016\n",
      "                              FF = 52.399784088134766\n",
      "Finished epoch  272\n",
      "On epoch  273\n",
      "Total Epoch Testing MAPE: PCE = 53.292579650878906\n",
      "                              Voc = 40.33707809448242\n",
      "                              Jsc = 81.06974029541016\n",
      "                              FF = 52.137935638427734\n",
      "Finished epoch  273\n",
      "On epoch  274\n",
      "Total Epoch Testing MAPE: PCE = 53.22492599487305\n",
      "                              Voc = 40.71617126464844\n",
      "                              Jsc = 81.39934539794922\n",
      "                              FF = 52.337486267089844\n",
      "Finished epoch  274\n",
      "On epoch  275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 53.82817459106445\n",
      "                              Voc = 40.866817474365234\n",
      "                              Jsc = 81.50837707519531\n",
      "                              FF = 52.233306884765625\n",
      "Finished epoch  275\n",
      "On epoch  276\n",
      "Total Epoch Testing MAPE: PCE = 54.34115982055664\n",
      "                              Voc = 40.572200775146484\n",
      "                              Jsc = 81.29861450195312\n",
      "                              FF = 52.351497650146484\n",
      "Finished epoch  276\n",
      "On epoch  277\n",
      "Total Epoch Testing MAPE: PCE = 55.17262268066406\n",
      "                              Voc = 41.09349822998047\n",
      "                              Jsc = 81.35199737548828\n",
      "                              FF = 53.033626556396484\n",
      "Finished epoch  277\n",
      "On epoch  278\n",
      "Total Epoch Testing MAPE: PCE = 55.42934036254883\n",
      "                              Voc = 41.94042205810547\n",
      "                              Jsc = 81.42428588867188\n",
      "                              FF = 53.316593170166016\n",
      "Finished epoch  278\n",
      "On epoch  279\n",
      "Total Epoch Testing MAPE: PCE = 55.73299026489258\n",
      "                              Voc = 41.5550422668457\n",
      "                              Jsc = 81.4217300415039\n",
      "                              FF = 53.4745979309082\n",
      "Finished epoch  279\n",
      "On epoch  280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.43720626831055\n",
      "                              Voc = 41.21002960205078\n",
      "                              Jsc = 81.27299499511719\n",
      "                              FF = 53.73312759399414\n",
      "Finished epoch  280\n",
      "On epoch  281\n",
      "Total Epoch Testing MAPE: PCE = 56.62118911743164\n",
      "                              Voc = 40.83160400390625\n",
      "                              Jsc = 81.55364990234375\n",
      "                              FF = 54.26095199584961\n",
      "Finished epoch  281\n",
      "On epoch  282\n",
      "Total Epoch Testing MAPE: PCE = 56.462162017822266\n",
      "                              Voc = 40.166465759277344\n",
      "                              Jsc = 81.60513305664062\n",
      "                              FF = 53.812896728515625\n",
      "Finished epoch  282\n",
      "On epoch  283\n",
      "Total Epoch Testing MAPE: PCE = 56.30812072753906\n",
      "                              Voc = 38.9566764831543\n",
      "                              Jsc = 81.46788787841797\n",
      "                              FF = 53.902523040771484\n",
      "Finished epoch  283\n",
      "On epoch  284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.15498352050781\n",
      "                              Voc = 38.63568878173828\n",
      "                              Jsc = 81.45355224609375\n",
      "                              FF = 53.62995910644531\n",
      "Finished epoch  284\n",
      "On epoch  285\n",
      "Total Epoch Testing MAPE: PCE = 55.773563385009766\n",
      "                              Voc = 38.352298736572266\n",
      "                              Jsc = 81.31531524658203\n",
      "                              FF = 53.8402214050293\n",
      "Finished epoch  285\n",
      "On epoch  286\n",
      "Total Epoch Testing MAPE: PCE = 55.56114959716797\n",
      "                              Voc = 38.27140426635742\n",
      "                              Jsc = 81.26380157470703\n",
      "                              FF = 53.57819747924805\n",
      "Finished epoch  286\n",
      "On epoch  287\n",
      "Total Epoch Testing MAPE: PCE = 55.620201110839844\n",
      "                              Voc = 37.39616775512695\n",
      "                              Jsc = 81.1374740600586\n",
      "                              FF = 53.5158576965332\n",
      "Finished epoch  287\n",
      "On epoch  288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.297916412353516\n",
      "                              Voc = 36.9669303894043\n",
      "                              Jsc = 81.18019104003906\n",
      "                              FF = 53.37590408325195\n",
      "Finished epoch  288\n",
      "On epoch  289\n",
      "Total Epoch Testing MAPE: PCE = 55.076045989990234\n",
      "                              Voc = 37.00128936767578\n",
      "                              Jsc = 81.0515365600586\n",
      "                              FF = 53.648311614990234\n",
      "Finished epoch  289\n",
      "On epoch  290\n",
      "Total Epoch Testing MAPE: PCE = 55.190826416015625\n",
      "                              Voc = 36.323707580566406\n",
      "                              Jsc = 81.08053588867188\n",
      "                              FF = 53.354454040527344\n",
      "Finished epoch  290\n",
      "On epoch  291\n",
      "Total Epoch Testing MAPE: PCE = 55.215728759765625\n",
      "                              Voc = 36.23396301269531\n",
      "                              Jsc = 81.28129577636719\n",
      "                              FF = 52.653987884521484\n",
      "Finished epoch  291\n",
      "On epoch  292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.53187561035156\n",
      "                              Voc = 36.55966567993164\n",
      "                              Jsc = 81.61865234375\n",
      "                              FF = 52.403587341308594\n",
      "Finished epoch  292\n",
      "On epoch  293\n",
      "Total Epoch Testing MAPE: PCE = 56.020748138427734\n",
      "                              Voc = 35.48012161254883\n",
      "                              Jsc = 81.38761901855469\n",
      "                              FF = 51.67415237426758\n",
      "Finished epoch  293\n",
      "On epoch  294\n",
      "Total Epoch Testing MAPE: PCE = 56.05605697631836\n",
      "                              Voc = 35.066650390625\n",
      "                              Jsc = 81.16242980957031\n",
      "                              FF = 51.708961486816406\n",
      "Finished epoch  294\n",
      "On epoch  295\n",
      "Total Epoch Testing MAPE: PCE = 56.70682907104492\n",
      "                              Voc = 35.15178680419922\n",
      "                              Jsc = 81.27481079101562\n",
      "                              FF = 51.748130798339844\n",
      "Finished epoch  295\n",
      "On epoch  296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.8638801574707\n",
      "                              Voc = 36.21663284301758\n",
      "                              Jsc = 81.29603576660156\n",
      "                              FF = 51.393775939941406\n",
      "Finished epoch  296\n",
      "On epoch  297\n",
      "Total Epoch Testing MAPE: PCE = 57.969696044921875\n",
      "                              Voc = 36.82590866088867\n",
      "                              Jsc = 81.14506530761719\n",
      "                              FF = 51.66476821899414\n",
      "Finished epoch  297\n",
      "On epoch  298\n",
      "Total Epoch Testing MAPE: PCE = 58.16807174682617\n",
      "                              Voc = 37.610416412353516\n",
      "                              Jsc = 81.09764862060547\n",
      "                              FF = 51.906497955322266\n",
      "Finished epoch  298\n",
      "On epoch  299\n",
      "Total Epoch Testing MAPE: PCE = 59.043800354003906\n",
      "                              Voc = 37.973968505859375\n",
      "                              Jsc = 81.11265563964844\n",
      "                              FF = 52.055572509765625\n",
      "Finished epoch  299\n",
      "On epoch  300\n",
      "Total Epoch Testing MAPE: PCE = 59.3625602722168\n",
      "                              Voc = 37.891273498535156\n",
      "                              Jsc = 81.40498352050781\n",
      "                              FF = 52.2939453125\n",
      "Finished epoch  300\n",
      "On epoch  301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 59.40610885620117\n",
      "                              Voc = 38.93896484375\n",
      "                              Jsc = 81.5570297241211\n",
      "                              FF = 52.28947448730469\n",
      "Finished epoch  301\n",
      "On epoch  302\n",
      "Total Epoch Testing MAPE: PCE = 59.73838424682617\n",
      "                              Voc = 38.93948745727539\n",
      "                              Jsc = 81.62617492675781\n",
      "                              FF = 52.11605453491211\n",
      "Finished epoch  302\n",
      "On epoch  303\n",
      "Total Epoch Testing MAPE: PCE = 59.86285400390625\n",
      "                              Voc = 39.7208251953125\n",
      "                              Jsc = 81.70358276367188\n",
      "                              FF = 52.12398910522461\n",
      "Finished epoch  303\n",
      "On epoch  304\n",
      "Total Epoch Testing MAPE: PCE = 60.10348892211914\n",
      "                              Voc = 39.740848541259766\n",
      "                              Jsc = 81.53626251220703\n",
      "                              FF = 52.4385986328125\n",
      "Finished epoch  304\n",
      "On epoch  305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.301979064941406\n",
      "                              Voc = 39.60960388183594\n",
      "                              Jsc = 81.60645294189453\n",
      "                              FF = 52.335201263427734\n",
      "Finished epoch  305\n",
      "On epoch  306\n",
      "Total Epoch Testing MAPE: PCE = 60.82516860961914\n",
      "                              Voc = 38.88978576660156\n",
      "                              Jsc = 81.3216323852539\n",
      "                              FF = 52.46906280517578\n",
      "Finished epoch  306\n",
      "On epoch  307\n",
      "Total Epoch Testing MAPE: PCE = 60.65372085571289\n",
      "                              Voc = 39.30500030517578\n",
      "                              Jsc = 81.60104370117188\n",
      "                              FF = 52.135498046875\n",
      "Finished epoch  307\n",
      "On epoch  308\n",
      "Total Epoch Testing MAPE: PCE = 60.48329544067383\n",
      "                              Voc = 38.65348815917969\n",
      "                              Jsc = 81.6604232788086\n",
      "                              FF = 52.13667678833008\n",
      "Finished epoch  308\n",
      "On epoch  309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.498382568359375\n",
      "                              Voc = 38.23040008544922\n",
      "                              Jsc = 81.5352554321289\n",
      "                              FF = 52.07612991333008\n",
      "Finished epoch  309\n",
      "On epoch  310\n",
      "Total Epoch Testing MAPE: PCE = 60.27965545654297\n",
      "                              Voc = 38.37333297729492\n",
      "                              Jsc = 81.74553680419922\n",
      "                              FF = 51.98423767089844\n",
      "Finished epoch  310\n",
      "On epoch  311\n",
      "Total Epoch Testing MAPE: PCE = 59.779876708984375\n",
      "                              Voc = 37.35219192504883\n",
      "                              Jsc = 81.55830383300781\n",
      "                              FF = 51.96780014038086\n",
      "Finished epoch  311\n",
      "On epoch  312\n",
      "Total Epoch Testing MAPE: PCE = 59.27241516113281\n",
      "                              Voc = 37.26352310180664\n",
      "                              Jsc = 81.65288543701172\n",
      "                              FF = 51.40774917602539\n",
      "Finished epoch  312\n",
      "On epoch  313\n",
      "Total Epoch Testing MAPE: PCE = 59.24819564819336\n",
      "                              Voc = 37.29602813720703\n",
      "                              Jsc = 81.5266342163086\n",
      "                              FF = 51.44746780395508\n",
      "Finished epoch  313\n",
      "On epoch  314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 59.20463180541992\n",
      "                              Voc = 37.35877227783203\n",
      "                              Jsc = 81.47880554199219\n",
      "                              FF = 51.49344253540039\n",
      "Finished epoch  314\n",
      "On epoch  315\n",
      "Total Epoch Testing MAPE: PCE = 59.05292510986328\n",
      "                              Voc = 38.14973449707031\n",
      "                              Jsc = 81.49614715576172\n",
      "                              FF = 51.06984329223633\n",
      "Finished epoch  315\n",
      "On epoch  316\n",
      "Total Epoch Testing MAPE: PCE = 59.202239990234375\n",
      "                              Voc = 38.001644134521484\n",
      "                              Jsc = 81.14726257324219\n",
      "                              FF = 50.79419708251953\n",
      "Finished epoch  316\n",
      "On epoch  317\n",
      "Total Epoch Testing MAPE: PCE = 59.02473831176758\n",
      "                              Voc = 37.80107498168945\n",
      "                              Jsc = 80.9534683227539\n",
      "                              FF = 50.66950225830078\n",
      "Finished epoch  317\n",
      "On epoch  318\n",
      "Total Epoch Testing MAPE: PCE = 58.767974853515625\n",
      "                              Voc = 38.13045883178711\n",
      "                              Jsc = 80.59864044189453\n",
      "                              FF = 51.14968490600586\n",
      "Finished epoch  318\n",
      "On epoch  319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.91366195678711\n",
      "                              Voc = 37.674705505371094\n",
      "                              Jsc = 80.54368591308594\n",
      "                              FF = 51.70269012451172\n",
      "Finished epoch  319\n",
      "On epoch  320\n",
      "Total Epoch Testing MAPE: PCE = 57.76691436767578\n",
      "                              Voc = 38.14252471923828\n",
      "                              Jsc = 80.12582397460938\n",
      "                              FF = 51.76184844970703\n",
      "Finished epoch  320\n",
      "On epoch  321\n",
      "Total Epoch Testing MAPE: PCE = 57.82353210449219\n",
      "                              Voc = 38.39653015136719\n",
      "                              Jsc = 80.00655364990234\n",
      "                              FF = 51.9879264831543\n",
      "Finished epoch  321\n",
      "On epoch  322\n",
      "Total Epoch Testing MAPE: PCE = 57.55179977416992\n",
      "                              Voc = 38.88628005981445\n",
      "                              Jsc = 80.27129364013672\n",
      "                              FF = 51.82044982910156\n",
      "Finished epoch  322\n",
      "On epoch  323\n",
      "Total Epoch Testing MAPE: PCE = 57.12250518798828\n",
      "                              Voc = 39.410057067871094\n",
      "                              Jsc = 80.27278900146484\n",
      "                              FF = 51.829463958740234\n",
      "Finished epoch  323\n",
      "On epoch  324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.627166748046875\n",
      "                              Voc = 39.0191535949707\n",
      "                              Jsc = 79.9587631225586\n",
      "                              FF = 51.83911895751953\n",
      "Finished epoch  324\n",
      "On epoch  325\n",
      "Total Epoch Testing MAPE: PCE = 56.569854736328125\n",
      "                              Voc = 39.29642868041992\n",
      "                              Jsc = 79.80351257324219\n",
      "                              FF = 51.88249969482422\n",
      "Finished epoch  325\n",
      "On epoch  326\n",
      "Total Epoch Testing MAPE: PCE = 56.29918670654297\n",
      "                              Voc = 39.65753173828125\n",
      "                              Jsc = 79.98853302001953\n",
      "                              FF = 51.76548767089844\n",
      "Finished epoch  326\n",
      "On epoch  327\n",
      "Total Epoch Testing MAPE: PCE = 56.22136688232422\n",
      "                              Voc = 39.882774353027344\n",
      "                              Jsc = 80.14773559570312\n",
      "                              FF = 51.582698822021484\n",
      "Finished epoch  327\n",
      "On epoch  328\n",
      "Total Epoch Testing MAPE: PCE = 55.6088981628418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                              Voc = 39.76667785644531\n",
      "                              Jsc = 80.07963562011719\n",
      "                              FF = 51.152793884277344\n",
      "Finished epoch  328\n",
      "On epoch  329\n",
      "Total Epoch Testing MAPE: PCE = 55.369842529296875\n",
      "                              Voc = 39.54941177368164\n",
      "                              Jsc = 80.43982696533203\n",
      "                              FF = 51.17117691040039\n",
      "Finished epoch  329\n",
      "On epoch  330\n",
      "Total Epoch Testing MAPE: PCE = 55.17293930053711\n",
      "                              Voc = 39.881778717041016\n",
      "                              Jsc = 80.48497772216797\n",
      "                              FF = 51.13416290283203\n",
      "Finished epoch  330\n",
      "On epoch  331\n",
      "Total Epoch Testing MAPE: PCE = 54.91427230834961\n",
      "                              Voc = 39.37069320678711\n",
      "                              Jsc = 80.2677001953125\n",
      "                              FF = 51.25638198852539\n",
      "Finished epoch  331\n",
      "On epoch  332\n",
      "Total Epoch Testing MAPE: PCE = 54.804508209228516\n",
      "                              Voc = 38.8599853515625\n",
      "                              Jsc = 80.56141662597656\n",
      "                              FF = 51.52028274536133\n",
      "Finished epoch  332\n",
      "On epoch  333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.24800109863281\n",
      "                              Voc = 38.563594818115234\n",
      "                              Jsc = 80.64958190917969\n",
      "                              FF = 51.22526931762695\n",
      "Finished epoch  333\n",
      "On epoch  334\n",
      "Total Epoch Testing MAPE: PCE = 54.463623046875\n",
      "                              Voc = 37.954124450683594\n",
      "                              Jsc = 80.68075561523438\n",
      "                              FF = 51.51214599609375\n",
      "Finished epoch  334\n",
      "On epoch  335\n",
      "Total Epoch Testing MAPE: PCE = 54.862815856933594\n",
      "                              Voc = 37.2753791809082\n",
      "                              Jsc = 80.70164489746094\n",
      "                              FF = 51.00788497924805\n",
      "Finished epoch  335\n",
      "On epoch  336\n",
      "Total Epoch Testing MAPE: PCE = 55.47770309448242\n",
      "                              Voc = 37.57638931274414\n",
      "                              Jsc = 80.85332489013672\n",
      "                              FF = 51.03303146362305\n",
      "Finished epoch  336\n",
      "On epoch  337\n",
      "Total Epoch Testing MAPE: PCE = 55.368568420410156\n",
      "                              Voc = 37.14410400390625\n",
      "                              Jsc = 80.5141830444336\n",
      "                              FF = 50.925167083740234\n",
      "Finished epoch  337\n",
      "On epoch  338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.33748245239258\n",
      "                              Voc = 37.62932586669922\n",
      "                              Jsc = 80.07125854492188\n",
      "                              FF = 51.103275299072266\n",
      "Finished epoch  338\n",
      "On epoch  339\n",
      "Total Epoch Testing MAPE: PCE = 55.419986724853516\n",
      "                              Voc = 37.731689453125\n",
      "                              Jsc = 80.0728759765625\n",
      "                              FF = 51.56195068359375\n",
      "Finished epoch  339\n",
      "On epoch  340\n",
      "Total Epoch Testing MAPE: PCE = 55.64162826538086\n",
      "                              Voc = 37.548423767089844\n",
      "                              Jsc = 79.89766693115234\n",
      "                              FF = 51.378936767578125\n",
      "Finished epoch  340\n",
      "On epoch  341\n",
      "Total Epoch Testing MAPE: PCE = 55.86301803588867\n",
      "                              Voc = 37.6775016784668\n",
      "                              Jsc = 79.42298889160156\n",
      "                              FF = 51.748130798339844\n",
      "Finished epoch  341\n",
      "On epoch  342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.184120178222656\n",
      "                              Voc = 37.23098373413086\n",
      "                              Jsc = 79.05560302734375\n",
      "                              FF = 51.89484786987305\n",
      "Finished epoch  342\n",
      "On epoch  343\n",
      "Total Epoch Testing MAPE: PCE = 56.843536376953125\n",
      "                              Voc = 36.505096435546875\n",
      "                              Jsc = 78.69434356689453\n",
      "                              FF = 52.32826614379883\n",
      "Finished epoch  343\n",
      "On epoch  344\n",
      "Total Epoch Testing MAPE: PCE = 57.120323181152344\n",
      "                              Voc = 36.05202865600586\n",
      "                              Jsc = 78.64911651611328\n",
      "                              FF = 52.06383514404297\n",
      "Finished epoch  344\n",
      "On epoch  345\n",
      "Total Epoch Testing MAPE: PCE = 57.180763244628906\n",
      "                              Voc = 36.74479293823242\n",
      "                              Jsc = 78.84739685058594\n",
      "                              FF = 52.115684509277344\n",
      "Finished epoch  345\n",
      "On epoch  346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.827266693115234\n",
      "                              Voc = 36.07559585571289\n",
      "                              Jsc = 78.68289184570312\n",
      "                              FF = 52.05564498901367\n",
      "Finished epoch  346\n",
      "On epoch  347\n",
      "Total Epoch Testing MAPE: PCE = 56.952720642089844\n",
      "                              Voc = 36.33335494995117\n",
      "                              Jsc = 78.8753890991211\n",
      "                              FF = 52.29940414428711\n",
      "Finished epoch  347\n",
      "On epoch  348\n",
      "Total Epoch Testing MAPE: PCE = 57.03665542602539\n",
      "                              Voc = 36.665565490722656\n",
      "                              Jsc = 78.6746826171875\n",
      "                              FF = 52.13096237182617\n",
      "Finished epoch  348\n",
      "On epoch  349\n",
      "Total Epoch Testing MAPE: PCE = 57.266075134277344\n",
      "                              Voc = 36.395545959472656\n",
      "                              Jsc = 78.72224426269531\n",
      "                              FF = 52.5655403137207\n",
      "Finished epoch  349\n",
      "On epoch  350\n",
      "Total Epoch Testing MAPE: PCE = 57.530887603759766\n",
      "                              Voc = 35.97092056274414\n",
      "                              Jsc = 78.91343688964844\n",
      "                              FF = 52.74791717529297\n",
      "Finished epoch  350\n",
      "On epoch  351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.096527099609375\n",
      "                              Voc = 35.60114288330078\n",
      "                              Jsc = 79.33655548095703\n",
      "                              FF = 52.78575897216797\n",
      "Finished epoch  351\n",
      "On epoch  352\n",
      "Total Epoch Testing MAPE: PCE = 58.38747024536133\n",
      "                              Voc = 35.65523910522461\n",
      "                              Jsc = 79.51380920410156\n",
      "                              FF = 52.81377410888672\n",
      "Finished epoch  352\n",
      "On epoch  353\n",
      "Total Epoch Testing MAPE: PCE = 58.81926345825195\n",
      "                              Voc = 34.966033935546875\n",
      "                              Jsc = 79.40873718261719\n",
      "                              FF = 52.541011810302734\n",
      "Finished epoch  353\n",
      "On epoch  354\n",
      "Total Epoch Testing MAPE: PCE = 59.38965606689453\n",
      "                              Voc = 35.30622100830078\n",
      "                              Jsc = 79.5676040649414\n",
      "                              FF = 52.0296630859375\n",
      "Finished epoch  354\n",
      "On epoch  355\n",
      "Total Epoch Testing MAPE: PCE = 60.00847244262695\n",
      "                              Voc = 35.613224029541016\n",
      "                              Jsc = 79.60075378417969\n",
      "                              FF = 51.891361236572266\n",
      "Finished epoch  355\n",
      "On epoch  356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.307945251464844\n",
      "                              Voc = 36.59806823730469\n",
      "                              Jsc = 79.43727111816406\n",
      "                              FF = 52.33297348022461\n",
      "Finished epoch  356\n",
      "On epoch  357\n",
      "Total Epoch Testing MAPE: PCE = 60.18120574951172\n",
      "                              Voc = 36.37116241455078\n",
      "                              Jsc = 79.36416625976562\n",
      "                              FF = 51.984256744384766\n",
      "Finished epoch  357\n",
      "On epoch  358\n",
      "Total Epoch Testing MAPE: PCE = 60.26100540161133\n",
      "                              Voc = 36.5567626953125\n",
      "                              Jsc = 79.20459747314453\n",
      "                              FF = 51.628692626953125\n",
      "Finished epoch  358\n",
      "On epoch  359\n",
      "Total Epoch Testing MAPE: PCE = 60.27833938598633\n",
      "                              Voc = 36.43152618408203\n",
      "                              Jsc = 79.23971557617188\n",
      "                              FF = 51.52780532836914\n",
      "Finished epoch  359\n",
      "On epoch  360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 60.335391998291016\n",
      "                              Voc = 36.75737762451172\n",
      "                              Jsc = 79.27184295654297\n",
      "                              FF = 50.83001708984375\n",
      "Finished epoch  360\n",
      "On epoch  361\n",
      "Total Epoch Testing MAPE: PCE = 60.35796356201172\n",
      "                              Voc = 37.68532943725586\n",
      "                              Jsc = 79.05084228515625\n",
      "                              FF = 50.49237060546875\n",
      "Finished epoch  361\n",
      "On epoch  362\n",
      "Total Epoch Testing MAPE: PCE = 60.275386810302734\n",
      "                              Voc = 37.54643249511719\n",
      "                              Jsc = 78.78286743164062\n",
      "                              FF = 50.33631896972656\n",
      "Finished epoch  362\n",
      "On epoch  363\n",
      "Total Epoch Testing MAPE: PCE = 60.19679260253906\n",
      "                              Voc = 37.49175262451172\n",
      "                              Jsc = 78.92654418945312\n",
      "                              FF = 50.307960510253906\n",
      "Finished epoch  363\n",
      "On epoch  364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 59.8594856262207\n",
      "                              Voc = 37.94148635864258\n",
      "                              Jsc = 79.06623840332031\n",
      "                              FF = 50.330631256103516\n",
      "Finished epoch  364\n",
      "On epoch  365\n",
      "Total Epoch Testing MAPE: PCE = 59.38673782348633\n",
      "                              Voc = 38.4321174621582\n",
      "                              Jsc = 79.37548828125\n",
      "                              FF = 50.128875732421875\n",
      "Finished epoch  365\n",
      "On epoch  366\n",
      "Total Epoch Testing MAPE: PCE = 58.583961486816406\n",
      "                              Voc = 38.60008239746094\n",
      "                              Jsc = 79.28251647949219\n",
      "                              FF = 49.33904266357422\n",
      "Finished epoch  366\n",
      "On epoch  367\n",
      "Total Epoch Testing MAPE: PCE = 58.07933807373047\n",
      "                              Voc = 39.595458984375\n",
      "                              Jsc = 78.99405670166016\n",
      "                              FF = 48.94925308227539\n",
      "Finished epoch  367\n",
      "On epoch  368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.451725006103516\n",
      "                              Voc = 39.31684494018555\n",
      "                              Jsc = 78.92138671875\n",
      "                              FF = 49.087982177734375\n",
      "Finished epoch  368\n",
      "On epoch  369\n",
      "Total Epoch Testing MAPE: PCE = 56.965965270996094\n",
      "                              Voc = 39.873008728027344\n",
      "                              Jsc = 79.22533416748047\n",
      "                              FF = 48.98530197143555\n",
      "Finished epoch  369\n",
      "On epoch  370\n",
      "Total Epoch Testing MAPE: PCE = 57.190792083740234\n",
      "                              Voc = 40.98212432861328\n",
      "                              Jsc = 79.46361541748047\n",
      "                              FF = 49.015594482421875\n",
      "Finished epoch  370\n",
      "On epoch  371\n",
      "Total Epoch Testing MAPE: PCE = 56.760440826416016\n",
      "                              Voc = 41.136085510253906\n",
      "                              Jsc = 79.28363037109375\n",
      "                              FF = 49.08491134643555\n",
      "Finished epoch  371\n",
      "On epoch  372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.89933776855469\n",
      "                              Voc = 41.52934265136719\n",
      "                              Jsc = 79.54146575927734\n",
      "                              FF = 48.681243896484375\n",
      "Finished epoch  372\n",
      "On epoch  373\n",
      "Total Epoch Testing MAPE: PCE = 56.71712112426758\n",
      "                              Voc = 41.56425476074219\n",
      "                              Jsc = 79.74749755859375\n",
      "                              FF = 48.69883346557617\n",
      "Finished epoch  373\n",
      "On epoch  374\n",
      "Total Epoch Testing MAPE: PCE = 56.59825134277344\n",
      "                              Voc = 42.05356979370117\n",
      "                              Jsc = 79.48151397705078\n",
      "                              FF = 48.8264274597168\n",
      "Finished epoch  374\n",
      "On epoch  375\n",
      "Total Epoch Testing MAPE: PCE = 56.32917785644531\n",
      "                              Voc = 42.156795501708984\n",
      "                              Jsc = 79.30461883544922\n",
      "                              FF = 49.07492446899414\n",
      "Finished epoch  375\n",
      "On epoch  376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.9849853515625\n",
      "                              Voc = 42.32105255126953\n",
      "                              Jsc = 79.25189208984375\n",
      "                              FF = 49.373374938964844\n",
      "Finished epoch  376\n",
      "On epoch  377\n",
      "Total Epoch Testing MAPE: PCE = 55.52204895019531\n",
      "                              Voc = 42.858028411865234\n",
      "                              Jsc = 79.35758209228516\n",
      "                              FF = 49.810020446777344\n",
      "Finished epoch  377\n",
      "On epoch  378\n",
      "Total Epoch Testing MAPE: PCE = 54.76966094970703\n",
      "                              Voc = 42.507118225097656\n",
      "                              Jsc = 79.45388793945312\n",
      "                              FF = 50.21070861816406\n",
      "Finished epoch  378\n",
      "On epoch  379\n",
      "Total Epoch Testing MAPE: PCE = 54.590850830078125\n",
      "                              Voc = 42.589351654052734\n",
      "                              Jsc = 79.57193756103516\n",
      "                              FF = 50.633121490478516\n",
      "Finished epoch  379\n",
      "On epoch  380\n",
      "Total Epoch Testing MAPE: PCE = 54.76186752319336\n",
      "                              Voc = 41.47344970703125\n",
      "                              Jsc = 79.42570495605469\n",
      "                              FF = 50.60680389404297\n",
      "Finished epoch  380\n",
      "On epoch  381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 54.84726333618164\n",
      "                              Voc = 41.38641357421875\n",
      "                              Jsc = 79.44966888427734\n",
      "                              FF = 51.42778778076172\n",
      "Finished epoch  381\n",
      "On epoch  382\n",
      "Total Epoch Testing MAPE: PCE = 54.88878631591797\n",
      "                              Voc = 41.61017990112305\n",
      "                              Jsc = 79.56254577636719\n",
      "                              FF = 52.00799560546875\n",
      "Finished epoch  382\n",
      "On epoch  383\n",
      "Total Epoch Testing MAPE: PCE = 54.677547454833984\n",
      "                              Voc = 41.35343551635742\n",
      "                              Jsc = 79.96273040771484\n",
      "                              FF = 52.388755798339844\n",
      "Finished epoch  383\n",
      "On epoch  384\n",
      "Total Epoch Testing MAPE: PCE = 55.16736602783203\n",
      "                              Voc = 40.58407974243164\n",
      "                              Jsc = 80.24761199951172\n",
      "                              FF = 52.234519958496094\n",
      "Finished epoch  384\n",
      "On epoch  385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.057125091552734\n",
      "                              Voc = 40.24038314819336\n",
      "                              Jsc = 80.35250091552734\n",
      "                              FF = 52.0982551574707\n",
      "Finished epoch  385\n",
      "On epoch  386\n",
      "Total Epoch Testing MAPE: PCE = 54.88618469238281\n",
      "                              Voc = 40.003639221191406\n",
      "                              Jsc = 80.48413848876953\n",
      "                              FF = 51.59343338012695\n",
      "Finished epoch  386\n",
      "On epoch  387\n",
      "Total Epoch Testing MAPE: PCE = 54.996131896972656\n",
      "                              Voc = 40.006649017333984\n",
      "                              Jsc = 80.5227279663086\n",
      "                              FF = 51.07761001586914\n",
      "Finished epoch  387\n",
      "On epoch  388\n",
      "Total Epoch Testing MAPE: PCE = 55.11368942260742\n",
      "                              Voc = 40.53692626953125\n",
      "                              Jsc = 80.69218444824219\n",
      "                              FF = 51.03179168701172\n",
      "Finished epoch  388\n",
      "On epoch  389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.36919403076172\n",
      "                              Voc = 40.16035461425781\n",
      "                              Jsc = 80.89521026611328\n",
      "                              FF = 51.208030700683594\n",
      "Finished epoch  389\n",
      "On epoch  390\n",
      "Total Epoch Testing MAPE: PCE = 55.52840805053711\n",
      "                              Voc = 39.07648849487305\n",
      "                              Jsc = 81.19080352783203\n",
      "                              FF = 51.3956298828125\n",
      "Finished epoch  390\n",
      "On epoch  391\n",
      "Total Epoch Testing MAPE: PCE = 55.514503479003906\n",
      "                              Voc = 39.1370849609375\n",
      "                              Jsc = 81.04154205322266\n",
      "                              FF = 51.5115966796875\n",
      "Finished epoch  391\n",
      "On epoch  392\n",
      "Total Epoch Testing MAPE: PCE = 55.07241439819336\n",
      "                              Voc = 38.97813415527344\n",
      "                              Jsc = 80.89299011230469\n",
      "                              FF = 52.03915786743164\n",
      "Finished epoch  392\n",
      "On epoch  393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.259639739990234\n",
      "                              Voc = 38.1419563293457\n",
      "                              Jsc = 81.03783416748047\n",
      "                              FF = 52.32243347167969\n",
      "Finished epoch  393\n",
      "On epoch  394\n",
      "Total Epoch Testing MAPE: PCE = 55.44914627075195\n",
      "                              Voc = 37.823612213134766\n",
      "                              Jsc = 81.12885284423828\n",
      "                              FF = 52.794898986816406\n",
      "Finished epoch  394\n",
      "On epoch  395\n",
      "Total Epoch Testing MAPE: PCE = 55.709171295166016\n",
      "                              Voc = 36.898170471191406\n",
      "                              Jsc = 81.07752990722656\n",
      "                              FF = 53.575557708740234\n",
      "Finished epoch  395\n",
      "On epoch  396\n",
      "Total Epoch Testing MAPE: PCE = 55.795623779296875\n",
      "                              Voc = 36.726192474365234\n",
      "                              Jsc = 81.2576675415039\n",
      "                              FF = 53.70021057128906\n",
      "Finished epoch  396\n",
      "On epoch  397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.135196685791016\n",
      "                              Voc = 37.030296325683594\n",
      "                              Jsc = 81.16132354736328\n",
      "                              FF = 53.332374572753906\n",
      "Finished epoch  397\n",
      "On epoch  398\n",
      "Total Epoch Testing MAPE: PCE = 56.44574737548828\n",
      "                              Voc = 36.850303649902344\n",
      "                              Jsc = 81.28202819824219\n",
      "                              FF = 53.42023468017578\n",
      "Finished epoch  398\n",
      "On epoch  399\n",
      "Total Epoch Testing MAPE: PCE = 56.40729522705078\n",
      "                              Voc = 37.09860610961914\n",
      "                              Jsc = 81.64309692382812\n",
      "                              FF = 53.343421936035156\n",
      "Finished epoch  399\n",
      "On epoch  400\n",
      "Total Epoch Testing MAPE: PCE = 56.580543518066406\n",
      "                              Voc = 36.65803146362305\n",
      "                              Jsc = 81.84632110595703\n",
      "                              FF = 52.89134979248047\n",
      "Finished epoch  400\n",
      "On epoch  401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.186485290527344\n",
      "                              Voc = 37.27566146850586\n",
      "                              Jsc = 81.7480239868164\n",
      "                              FF = 53.423065185546875\n",
      "Finished epoch  401\n",
      "On epoch  402\n",
      "Total Epoch Testing MAPE: PCE = 57.5613899230957\n",
      "                              Voc = 36.66148376464844\n",
      "                              Jsc = 81.56855773925781\n",
      "                              FF = 53.44855880737305\n",
      "Finished epoch  402\n",
      "On epoch  403\n",
      "Total Epoch Testing MAPE: PCE = 58.202789306640625\n",
      "                              Voc = 35.87867736816406\n",
      "                              Jsc = 81.70225524902344\n",
      "                              FF = 53.960968017578125\n",
      "Finished epoch  403\n",
      "On epoch  404\n",
      "Total Epoch Testing MAPE: PCE = 58.60521697998047\n",
      "                              Voc = 35.6425666809082\n",
      "                              Jsc = 82.16622161865234\n",
      "                              FF = 53.836334228515625\n",
      "Finished epoch  404\n",
      "On epoch  405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.74765396118164\n",
      "                              Voc = 35.14612579345703\n",
      "                              Jsc = 82.43962860107422\n",
      "                              FF = 54.619598388671875\n",
      "Finished epoch  405\n",
      "On epoch  406\n",
      "Total Epoch Testing MAPE: PCE = 58.53076171875\n",
      "                              Voc = 35.06690979003906\n",
      "                              Jsc = 82.66715240478516\n",
      "                              FF = 54.98371887207031\n",
      "Finished epoch  406\n",
      "On epoch  407\n",
      "Total Epoch Testing MAPE: PCE = 58.507232666015625\n",
      "                              Voc = 34.698272705078125\n",
      "                              Jsc = 82.64124298095703\n",
      "                              FF = 55.43062210083008\n",
      "Finished epoch  407\n",
      "On epoch  408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.567787170410156\n",
      "                              Voc = 33.837772369384766\n",
      "                              Jsc = 82.55213165283203\n",
      "                              FF = 55.31269836425781\n",
      "Finished epoch  408\n",
      "On epoch  409\n",
      "Total Epoch Testing MAPE: PCE = 58.28122329711914\n",
      "                              Voc = 33.91446304321289\n",
      "                              Jsc = 82.63362884521484\n",
      "                              FF = 55.511962890625\n",
      "Finished epoch  409\n",
      "On epoch  410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.92978286743164\n",
      "                              Voc = 34.23153305053711\n",
      "                              Jsc = 82.7425537109375\n",
      "                              FF = 55.19231033325195\n",
      "Finished epoch  410\n",
      "On epoch  411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.17487335205078\n",
      "                              Voc = 33.9935188293457\n",
      "                              Jsc = 83.17053985595703\n",
      "                              FF = 55.064666748046875\n",
      "Finished epoch  411\n",
      "On epoch  412\n",
      "Total Epoch Testing MAPE: PCE = 58.162498474121094\n",
      "                              Voc = 34.40807342529297\n",
      "                              Jsc = 83.10730743408203\n",
      "                              FF = 55.29757308959961\n",
      "Finished epoch  412\n",
      "On epoch  413\n",
      "Total Epoch Testing MAPE: PCE = 58.22631072998047\n",
      "                              Voc = 35.32048416137695\n",
      "                              Jsc = 83.3067626953125\n",
      "                              FF = 54.787052154541016\n",
      "Finished epoch  413\n",
      "On epoch  414\n",
      "Total Epoch Testing MAPE: PCE = 58.2852783203125\n",
      "                              Voc = 35.88512420654297\n",
      "                              Jsc = 83.23654174804688\n",
      "                              FF = 55.04267883300781\n",
      "Finished epoch  414\n",
      "On epoch  415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.28725814819336\n",
      "                              Voc = 36.06258010864258\n",
      "                              Jsc = 83.25807189941406\n",
      "                              FF = 54.91648483276367\n",
      "Finished epoch  415\n",
      "On epoch  416\n",
      "Total Epoch Testing MAPE: PCE = 58.49488830566406\n",
      "                              Voc = 36.439056396484375\n",
      "                              Jsc = 83.24989318847656\n",
      "                              FF = 54.57925796508789\n",
      "Finished epoch  416\n",
      "On epoch  417\n",
      "Total Epoch Testing MAPE: PCE = 58.38784408569336\n",
      "                              Voc = 36.909854888916016\n",
      "                              Jsc = 83.23253631591797\n",
      "                              FF = 53.987396240234375\n",
      "Finished epoch  417\n",
      "On epoch  418\n",
      "Total Epoch Testing MAPE: PCE = 58.42219543457031\n",
      "                              Voc = 37.4223747253418\n",
      "                              Jsc = 83.02687072753906\n",
      "                              FF = 53.69536209106445\n",
      "Finished epoch  418\n",
      "On epoch  419\n",
      "Total Epoch Testing MAPE: PCE = 58.368980407714844\n",
      "                              Voc = 38.146060943603516\n",
      "                              Jsc = 82.6556396484375\n",
      "                              FF = 53.76905822753906\n",
      "Finished epoch  419\n",
      "On epoch  420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.308021545410156\n",
      "                              Voc = 38.67058563232422\n",
      "                              Jsc = 82.67413330078125\n",
      "                              FF = 53.09864807128906\n",
      "Finished epoch  420\n",
      "On epoch  421\n",
      "Total Epoch Testing MAPE: PCE = 58.584224700927734\n",
      "                              Voc = 39.42886734008789\n",
      "                              Jsc = 82.76721954345703\n",
      "                              FF = 52.93254852294922\n",
      "Finished epoch  421\n",
      "On epoch  422\n",
      "Total Epoch Testing MAPE: PCE = 58.13945007324219\n",
      "                              Voc = 39.89445114135742\n",
      "                              Jsc = 82.5980453491211\n",
      "                              FF = 52.99198532104492\n",
      "Finished epoch  422\n",
      "On epoch  423\n",
      "Total Epoch Testing MAPE: PCE = 57.875091552734375\n",
      "                              Voc = 40.08476257324219\n",
      "                              Jsc = 82.42572784423828\n",
      "                              FF = 52.69017791748047\n",
      "Finished epoch  423\n",
      "On epoch  424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.61640167236328\n",
      "                              Voc = 40.3697509765625\n",
      "                              Jsc = 82.54007720947266\n",
      "                              FF = 51.91728973388672\n",
      "Finished epoch  424\n",
      "On epoch  425\n",
      "Total Epoch Testing MAPE: PCE = 57.228492736816406\n",
      "                              Voc = 40.776248931884766\n",
      "                              Jsc = 82.54279327392578\n",
      "                              FF = 51.93550491333008\n",
      "Finished epoch  425\n",
      "On epoch  426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.335750579833984\n",
      "                              Voc = 40.98529815673828\n",
      "                              Jsc = 82.59767150878906\n",
      "                              FF = 52.07896423339844\n",
      "Finished epoch  426\n",
      "On epoch  427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.69752883911133\n",
      "                              Voc = 41.33283615112305\n",
      "                              Jsc = 82.92052459716797\n",
      "                              FF = 52.78591537475586\n",
      "Finished epoch  427\n",
      "On epoch  428\n",
      "Total Epoch Testing MAPE: PCE = 58.22861099243164\n",
      "                              Voc = 41.62137222290039\n",
      "                              Jsc = 83.09468841552734\n",
      "                              FF = 52.807003021240234\n",
      "Finished epoch  428\n",
      "On epoch  429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.41606140136719\n",
      "                              Voc = 41.29059982299805\n",
      "                              Jsc = 83.2053451538086\n",
      "                              FF = 52.071224212646484\n",
      "Finished epoch  429\n",
      "On epoch  430\n",
      "Total Epoch Testing MAPE: PCE = 58.6566276550293\n",
      "                              Voc = 41.44790267944336\n",
      "                              Jsc = 83.0794677734375\n",
      "                              FF = 51.99204635620117\n",
      "Finished epoch  430\n",
      "On epoch  431\n",
      "Total Epoch Testing MAPE: PCE = 58.78299331665039\n",
      "                              Voc = 41.46176528930664\n",
      "                              Jsc = 82.95407104492188\n",
      "                              FF = 51.63688659667969\n",
      "Finished epoch  431\n",
      "On epoch  432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.89421081542969\n",
      "                              Voc = 41.98154830932617\n",
      "                              Jsc = 82.81971740722656\n",
      "                              FF = 51.16114044189453\n",
      "Finished epoch  432\n",
      "On epoch  433\n",
      "Total Epoch Testing MAPE: PCE = 59.38994216918945\n",
      "                              Voc = 41.9758186340332\n",
      "                              Jsc = 82.66116333007812\n",
      "                              FF = 51.46735382080078\n",
      "Finished epoch  433\n",
      "On epoch  434\n",
      "Total Epoch Testing MAPE: PCE = 59.324012756347656\n",
      "                              Voc = 41.74652862548828\n",
      "                              Jsc = 82.65434265136719\n",
      "                              FF = 51.46412658691406\n",
      "Finished epoch  434\n",
      "On epoch  435\n",
      "Total Epoch Testing MAPE: PCE = 58.895748138427734\n",
      "                              Voc = 41.331329345703125\n",
      "                              Jsc = 82.71540069580078\n",
      "                              FF = 51.48938751220703\n",
      "Finished epoch  435\n",
      "On epoch  436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 59.16678237915039\n",
      "                              Voc = 40.181766510009766\n",
      "                              Jsc = 82.79399871826172\n",
      "                              FF = 51.76469039916992\n",
      "Finished epoch  436\n",
      "On epoch  437\n",
      "Total Epoch Testing MAPE: PCE = 58.64458084106445\n",
      "                              Voc = 39.8362922668457\n",
      "                              Jsc = 82.7432632446289\n",
      "                              FF = 51.499420166015625\n",
      "Finished epoch  437\n",
      "On epoch  438\n",
      "Total Epoch Testing MAPE: PCE = 57.99860382080078\n",
      "                              Voc = 39.74002456665039\n",
      "                              Jsc = 82.64835357666016\n",
      "                              FF = 51.629722595214844\n",
      "Finished epoch  438\n",
      "On epoch  439\n",
      "Total Epoch Testing MAPE: PCE = 58.12387466430664\n",
      "                              Voc = 38.75828552246094\n",
      "                              Jsc = 82.52743530273438\n",
      "                              FF = 51.365482330322266\n",
      "Finished epoch  439\n",
      "On epoch  440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.32042694091797\n",
      "                              Voc = 38.08954620361328\n",
      "                              Jsc = 82.54662322998047\n",
      "                              FF = 51.41197967529297\n",
      "Finished epoch  440\n",
      "On epoch  441\n",
      "Total Epoch Testing MAPE: PCE = 56.94860076904297\n",
      "                              Voc = 38.00157928466797\n",
      "                              Jsc = 82.73194122314453\n",
      "                              FF = 51.369049072265625\n",
      "Finished epoch  441\n",
      "On epoch  442\n",
      "Total Epoch Testing MAPE: PCE = 56.81937789916992\n",
      "                              Voc = 37.48480224609375\n",
      "                              Jsc = 82.54539489746094\n",
      "                              FF = 52.19178009033203\n",
      "Finished epoch  442\n",
      "On epoch  443\n",
      "Total Epoch Testing MAPE: PCE = 56.1877326965332\n",
      "                              Voc = 37.07844924926758\n",
      "                              Jsc = 82.39635467529297\n",
      "                              FF = 52.63513946533203\n",
      "Finished epoch  443\n",
      "On epoch  444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.089019775390625\n",
      "                              Voc = 36.96762466430664\n",
      "                              Jsc = 82.5521011352539\n",
      "                              FF = 53.15564727783203\n",
      "Finished epoch  444\n",
      "On epoch  445\n",
      "Total Epoch Testing MAPE: PCE = 55.63511657714844\n",
      "                              Voc = 36.453453063964844\n",
      "                              Jsc = 82.58264923095703\n",
      "                              FF = 53.503204345703125\n",
      "Finished epoch  445\n",
      "On epoch  446\n",
      "Total Epoch Testing MAPE: PCE = 55.53466033935547\n",
      "                              Voc = 35.66306686401367\n",
      "                              Jsc = 82.4139404296875\n",
      "                              FF = 54.11983871459961\n",
      "Finished epoch  446\n",
      "On epoch  447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.17041778564453\n",
      "                              Voc = 35.14286422729492\n",
      "                              Jsc = 82.1983642578125\n",
      "                              FF = 54.05260467529297\n",
      "Finished epoch  447\n",
      "On epoch  448\n",
      "Total Epoch Testing MAPE: PCE = 54.77483367919922\n",
      "                              Voc = 34.37443542480469\n",
      "                              Jsc = 82.05152130126953\n",
      "                              FF = 53.82560348510742\n",
      "Finished epoch  448\n",
      "On epoch  449\n",
      "Total Epoch Testing MAPE: PCE = 54.92082214355469\n",
      "                              Voc = 33.311981201171875\n",
      "                              Jsc = 81.95172119140625\n",
      "                              FF = 53.99454116821289\n",
      "Finished epoch  449\n",
      "On epoch  450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.165035247802734\n",
      "                              Voc = 32.47105026245117\n",
      "                              Jsc = 81.918701171875\n",
      "                              FF = 54.09885787963867\n",
      "Finished epoch  450\n",
      "On epoch  451\n",
      "Total Epoch Testing MAPE: PCE = 55.289058685302734\n",
      "                              Voc = 32.17745590209961\n",
      "                              Jsc = 81.93357849121094\n",
      "                              FF = 53.8980598449707\n",
      "Finished epoch  451\n",
      "On epoch  452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 55.483192443847656\n",
      "                              Voc = 31.799617767333984\n",
      "                              Jsc = 81.93464660644531\n",
      "                              FF = 53.647361755371094\n",
      "Finished epoch  452\n",
      "On epoch  453\n",
      "Total Epoch Testing MAPE: PCE = 55.897552490234375\n",
      "                              Voc = 31.335847854614258\n",
      "                              Jsc = 81.98150634765625\n",
      "                              FF = 53.45416259765625\n",
      "Finished epoch  453\n",
      "On epoch  454\n",
      "Total Epoch Testing MAPE: PCE = 56.32225799560547\n",
      "                              Voc = 31.333934783935547\n",
      "                              Jsc = 81.772216796875\n",
      "                              FF = 53.2121467590332\n",
      "Finished epoch  454\n",
      "On epoch  455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.27943420410156\n",
      "                              Voc = 30.932723999023438\n",
      "                              Jsc = 82.06300354003906\n",
      "                              FF = 53.020328521728516\n",
      "Finished epoch  455\n",
      "On epoch  456\n",
      "Total Epoch Testing MAPE: PCE = 56.684391021728516\n",
      "                              Voc = 31.206438064575195\n",
      "                              Jsc = 82.07560729980469\n",
      "                              FF = 52.96424102783203\n",
      "Finished epoch  456\n",
      "On epoch  457\n",
      "Total Epoch Testing MAPE: PCE = 57.176185607910156\n",
      "                              Voc = 31.9761962890625\n",
      "                              Jsc = 82.31999969482422\n",
      "                              FF = 52.975345611572266\n",
      "Finished epoch  457\n",
      "On epoch  458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.54808044433594\n",
      "                              Voc = 32.00666046142578\n",
      "                              Jsc = 82.42227935791016\n",
      "                              FF = 52.498985290527344\n",
      "Finished epoch  458\n",
      "On epoch  459\n",
      "Total Epoch Testing MAPE: PCE = 58.044700622558594\n",
      "                              Voc = 32.11906051635742\n",
      "                              Jsc = 82.37150573730469\n",
      "                              FF = 52.593624114990234\n",
      "Finished epoch  459\n",
      "On epoch  460\n",
      "Total Epoch Testing MAPE: PCE = 58.1068115234375\n",
      "                              Voc = 32.511634826660156\n",
      "                              Jsc = 82.4302749633789\n",
      "                              FF = 52.332252502441406\n",
      "Finished epoch  460\n",
      "On epoch  461\n",
      "Total Epoch Testing MAPE: PCE = 58.09183883666992\n",
      "                              Voc = 32.26332473754883\n",
      "                              Jsc = 82.56272888183594\n",
      "                              FF = 52.520965576171875\n",
      "Finished epoch  461\n",
      "On epoch  462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.05732727050781\n",
      "                              Voc = 31.956708908081055\n",
      "                              Jsc = 82.68311309814453\n",
      "                              FF = 52.45830535888672\n",
      "Finished epoch  462\n",
      "On epoch  463\n",
      "Total Epoch Testing MAPE: PCE = 58.247711181640625\n",
      "                              Voc = 32.21819305419922\n",
      "                              Jsc = 82.71259307861328\n",
      "                              FF = 52.64464569091797\n",
      "Finished epoch  463\n",
      "On epoch  464\n",
      "Total Epoch Testing MAPE: PCE = 58.538021087646484\n",
      "                              Voc = 31.95581817626953\n",
      "                              Jsc = 82.8368911743164\n",
      "                              FF = 52.370487213134766\n",
      "Finished epoch  464\n",
      "On epoch  465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.5195198059082\n",
      "                              Voc = 32.29988098144531\n",
      "                              Jsc = 82.97674560546875\n",
      "                              FF = 52.51094055175781\n",
      "Finished epoch  465\n",
      "On epoch  466\n",
      "Total Epoch Testing MAPE: PCE = 58.62423324584961\n",
      "                              Voc = 32.090415954589844\n",
      "                              Jsc = 83.09646606445312\n",
      "                              FF = 52.73657989501953\n",
      "Finished epoch  466\n",
      "On epoch  467\n",
      "Total Epoch Testing MAPE: PCE = 58.51317596435547\n",
      "                              Voc = 32.090110778808594\n",
      "                              Jsc = 82.9745864868164\n",
      "                              FF = 52.81037521362305\n",
      "Finished epoch  467\n",
      "On epoch  468\n",
      "Total Epoch Testing MAPE: PCE = 58.646888732910156\n",
      "                              Voc = 32.40397644042969\n",
      "                              Jsc = 82.96441650390625\n",
      "                              FF = 52.90563201904297\n",
      "Finished epoch  468\n",
      "On epoch  469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.55265808105469\n",
      "                              Voc = 32.69977951049805\n",
      "                              Jsc = 82.86613464355469\n",
      "                              FF = 52.637325286865234\n",
      "Finished epoch  469\n",
      "On epoch  470\n",
      "Total Epoch Testing MAPE: PCE = 58.29994201660156\n",
      "                              Voc = 33.071678161621094\n",
      "                              Jsc = 82.96631622314453\n",
      "                              FF = 52.61239242553711\n",
      "Finished epoch  470\n",
      "On epoch  471\n",
      "Total Epoch Testing MAPE: PCE = 58.215003967285156\n",
      "                              Voc = 33.620208740234375\n",
      "                              Jsc = 83.03234100341797\n",
      "                              FF = 52.54214859008789\n",
      "Finished epoch  471\n",
      "On epoch  472\n",
      "Total Epoch Testing MAPE: PCE = 58.1873779296875\n",
      "                              Voc = 34.63043975830078\n",
      "                              Jsc = 82.99464416503906\n",
      "                              FF = 52.178043365478516\n",
      "Finished epoch  472\n",
      "On epoch  473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.491973876953125\n",
      "                              Voc = 34.91046905517578\n",
      "                              Jsc = 82.83943939208984\n",
      "                              FF = 52.16594314575195\n",
      "Finished epoch  473\n",
      "On epoch  474\n",
      "Total Epoch Testing MAPE: PCE = 58.44402313232422\n",
      "                              Voc = 35.2669792175293\n",
      "                              Jsc = 82.9720687866211\n",
      "                              FF = 51.977264404296875\n",
      "Finished epoch  474\n",
      "On epoch  475\n",
      "Total Epoch Testing MAPE: PCE = 58.661014556884766\n",
      "                              Voc = 36.22743606567383\n",
      "                              Jsc = 82.90291595458984\n",
      "                              FF = 51.936363220214844\n",
      "Finished epoch  475\n",
      "On epoch  476\n",
      "Total Epoch Testing MAPE: PCE = 58.65669250488281\n",
      "                              Voc = 36.57398986816406\n",
      "                              Jsc = 82.82115173339844\n",
      "                              FF = 51.69367218017578\n",
      "Finished epoch  476\n",
      "On epoch  477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.77817916870117\n",
      "                              Voc = 36.76164627075195\n",
      "                              Jsc = 82.71002197265625\n",
      "                              FF = 52.208984375\n",
      "Finished epoch  477\n",
      "On epoch  478\n",
      "Total Epoch Testing MAPE: PCE = 58.68566131591797\n",
      "                              Voc = 37.17931365966797\n",
      "                              Jsc = 82.43128967285156\n",
      "                              FF = 52.53527069091797\n",
      "Finished epoch  478\n",
      "On epoch  479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.01229476928711\n",
      "                              Voc = 37.4654426574707\n",
      "                              Jsc = 82.30906677246094\n",
      "                              FF = 52.34512710571289\n",
      "Finished epoch  479\n",
      "On epoch  480\n",
      "Total Epoch Testing MAPE: PCE = 57.900333404541016\n",
      "                              Voc = 37.66521072387695\n",
      "                              Jsc = 82.35665130615234\n",
      "                              FF = 51.904361724853516\n",
      "Finished epoch  480\n",
      "On epoch  481\n",
      "Total Epoch Testing MAPE: PCE = 57.811378479003906\n",
      "                              Voc = 38.023712158203125\n",
      "                              Jsc = 82.51821899414062\n",
      "                              FF = 51.988075256347656\n",
      "Finished epoch  481\n",
      "On epoch  482\n",
      "Total Epoch Testing MAPE: PCE = 57.791202545166016\n",
      "                              Voc = 38.49086380004883\n",
      "                              Jsc = 82.4407730102539\n",
      "                              FF = 52.16352844238281\n",
      "Finished epoch  482\n",
      "On epoch  483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.55939483642578\n",
      "                              Voc = 38.51604080200195\n",
      "                              Jsc = 82.60941314697266\n",
      "                              FF = 52.01740264892578\n",
      "Finished epoch  483\n",
      "On epoch  484\n",
      "Total Epoch Testing MAPE: PCE = 57.73500061035156\n",
      "                              Voc = 38.75235366821289\n",
      "                              Jsc = 82.75826263427734\n",
      "                              FF = 51.70395278930664\n",
      "Finished epoch  484\n",
      "On epoch  485\n",
      "Total Epoch Testing MAPE: PCE = 58.04351043701172\n",
      "                              Voc = 38.66999435424805\n",
      "                              Jsc = 83.05616760253906\n",
      "                              FF = 51.69709396362305\n",
      "Finished epoch  485\n",
      "On epoch  486\n",
      "Total Epoch Testing MAPE: PCE = 58.028846740722656\n",
      "                              Voc = 38.60192108154297\n",
      "                              Jsc = 83.1705093383789\n",
      "                              FF = 51.74140930175781\n",
      "Finished epoch  486\n",
      "On epoch  487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 58.20222473144531\n",
      "                              Voc = 37.59041976928711\n",
      "                              Jsc = 83.26881408691406\n",
      "                              FF = 51.905906677246094\n",
      "Finished epoch  487\n",
      "On epoch  488\n",
      "Total Epoch Testing MAPE: PCE = 58.227149963378906\n",
      "                              Voc = 38.027523040771484\n",
      "                              Jsc = 83.27534484863281\n",
      "                              FF = 51.79927444458008\n",
      "Finished epoch  488\n",
      "On epoch  489\n",
      "Total Epoch Testing MAPE: PCE = 58.23712921142578\n",
      "                              Voc = 37.74809265136719\n",
      "                              Jsc = 83.36532592773438\n",
      "                              FF = 51.8587532043457\n",
      "Finished epoch  489\n",
      "On epoch  490\n",
      "Total Epoch Testing MAPE: PCE = 57.577144622802734\n",
      "                              Voc = 37.445255279541016\n",
      "                              Jsc = 83.47112274169922\n",
      "                              FF = 51.7972526550293\n",
      "Finished epoch  490\n",
      "On epoch  491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 57.439273834228516\n",
      "                              Voc = 37.138851165771484\n",
      "                              Jsc = 83.66741943359375\n",
      "                              FF = 51.741554260253906\n",
      "Finished epoch  491\n",
      "On epoch  492\n",
      "Total Epoch Testing MAPE: PCE = 57.01968765258789\n",
      "                              Voc = 37.729209899902344\n",
      "                              Jsc = 83.65123748779297\n",
      "                              FF = 51.47010040283203\n",
      "Finished epoch  492\n",
      "On epoch  493\n",
      "Total Epoch Testing MAPE: PCE = 56.85626220703125\n",
      "                              Voc = 37.89506149291992\n",
      "                              Jsc = 83.66692352294922\n",
      "                              FF = 51.084129333496094\n",
      "Finished epoch  493\n",
      "On epoch  494\n",
      "Total Epoch Testing MAPE: PCE = 56.67857360839844\n",
      "                              Voc = 37.804012298583984\n",
      "                              Jsc = 83.88089752197266\n",
      "                              FF = 50.51457977294922\n",
      "Finished epoch  494\n",
      "On epoch  495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch Testing MAPE: PCE = 56.47166442871094\n",
      "                              Voc = 37.813385009765625\n",
      "                              Jsc = 83.6730728149414\n",
      "                              FF = 50.062782287597656\n",
      "Finished epoch  495\n",
      "On epoch  496\n",
      "Total Epoch Testing MAPE: PCE = 56.13101577758789\n",
      "                              Voc = 37.647193908691406\n",
      "                              Jsc = 83.81138610839844\n",
      "                              FF = 49.69458770751953\n",
      "Finished epoch  496\n",
      "On epoch  497\n",
      "Total Epoch Testing MAPE: PCE = 55.492313385009766\n",
      "                              Voc = 37.8872184753418\n",
      "                              Jsc = 83.79589080810547\n",
      "                              FF = 49.5035400390625\n",
      "Finished epoch  497\n",
      "On epoch  498\n",
      "Total Epoch Testing MAPE: PCE = 55.684513092041016\n",
      "                              Voc = 37.609352111816406\n",
      "                              Jsc = 84.28260040283203\n",
      "                              FF = 49.38221740722656\n",
      "Finished epoch  498\n",
      "On epoch  499\n",
      "Total Epoch Testing MAPE: PCE = 55.81504821777344\n",
      "                              Voc = 37.560062408447266\n",
      "                              Jsc = 84.33861541748047\n",
      "                              FF = 49.258872985839844\n",
      "Finished epoch  499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/wesleytatum/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:432: UserWarning: Using a target size (torch.Size([293])) that is different to the input size (torch.Size([293, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "cv_fits = {}\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n",
    "    print(f'Fold # {fold}')\n",
    "    print('-----------------------------')\n",
    "    \n",
    "    train_loader, test_loader = nuts.get_fold_dataloaders_df(x_train, y_train, train_index, test_index)\n",
    "        \n",
    "    model = net.OPV_df_NN(in_dims = in_dims, out_dims = out_dims).to(device)\n",
    "    model.apply(nuts.init_weights)\n",
    "    \n",
    "    cv_fits[fold] = nuts.CV_OPV_fit(model, train_loader, test_loader, lr = learning_rate, epochs = num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'lr': 1e-05,\n",
       "  'best_loss_epoch': 18,\n",
       "  'best_acc_epoch': 107,\n",
       "  'best_r2_epoch': 23,\n",
       "  'pce_loss': [0.11115662008523941,\n",
       "   0.1099947839975357,\n",
       "   0.11049965023994446,\n",
       "   0.10817433893680573,\n",
       "   0.11088453978300095,\n",
       "   0.11402972787618637,\n",
       "   0.11226285248994827,\n",
       "   0.11159917712211609,\n",
       "   0.10914776474237442,\n",
       "   0.10995162278413773,\n",
       "   0.11201531440019608,\n",
       "   0.10523857921361923,\n",
       "   0.0927177369594574,\n",
       "   0.08440355956554413,\n",
       "   0.07622473686933517,\n",
       "   0.07227897644042969,\n",
       "   0.07016313076019287,\n",
       "   0.06972901523113251,\n",
       "   0.0691228061914444,\n",
       "   0.06715001910924911,\n",
       "   0.06806261092424393,\n",
       "   0.0690355896949768,\n",
       "   0.07091391831636429,\n",
       "   0.07369124889373779,\n",
       "   0.07161710411310196,\n",
       "   0.07637665420770645,\n",
       "   0.07504823803901672,\n",
       "   0.0751730278134346,\n",
       "   0.07961652427911758,\n",
       "   0.0807248055934906,\n",
       "   0.0810188427567482,\n",
       "   0.07866876572370529,\n",
       "   0.07841984927654266,\n",
       "   0.07774960249662399,\n",
       "   0.07643424719572067,\n",
       "   0.07223093509674072,\n",
       "   0.0710441842675209,\n",
       "   0.07360990345478058,\n",
       "   0.075975202023983,\n",
       "   0.07816693931818008,\n",
       "   0.08580995351076126,\n",
       "   0.08876032382249832,\n",
       "   0.08629705011844635,\n",
       "   0.08656613528728485,\n",
       "   0.08534304797649384,\n",
       "   0.08616205304861069,\n",
       "   0.08742517977952957,\n",
       "   0.09145566821098328,\n",
       "   0.08688968420028687,\n",
       "   0.0866323709487915,\n",
       "   0.08488952368497849,\n",
       "   0.08914768695831299,\n",
       "   0.0884765014052391,\n",
       "   0.08368009328842163,\n",
       "   0.08426441252231598,\n",
       "   0.08258481323719025,\n",
       "   0.07879072427749634,\n",
       "   0.08216240257024765,\n",
       "   0.07958338409662247,\n",
       "   0.0758611410856247,\n",
       "   0.07885049283504486,\n",
       "   0.07441200315952301,\n",
       "   0.07624763995409012,\n",
       "   0.07216090708971024,\n",
       "   0.06733587384223938,\n",
       "   0.06370709836483002,\n",
       "   0.06013093888759613,\n",
       "   0.060414351522922516,\n",
       "   0.05907099321484566,\n",
       "   0.061045531183481216,\n",
       "   0.058010540902614594,\n",
       "   0.05356670916080475,\n",
       "   0.05230778083205223,\n",
       "   0.05084735155105591,\n",
       "   0.053637586534023285,\n",
       "   0.053738102316856384,\n",
       "   0.050984859466552734,\n",
       "   0.05114639922976494,\n",
       "   0.04651178419589996,\n",
       "   0.047307368367910385,\n",
       "   0.047768089920282364,\n",
       "   0.04983876273036003,\n",
       "   0.04928147420287132,\n",
       "   0.04874972254037857,\n",
       "   0.04852966219186783,\n",
       "   0.04663056880235672,\n",
       "   0.04922686517238617,\n",
       "   0.04757858067750931,\n",
       "   0.044410791248083115,\n",
       "   0.04376418888568878,\n",
       "   0.04362257570028305,\n",
       "   0.044277578592300415,\n",
       "   0.04306406527757645,\n",
       "   0.04256901517510414,\n",
       "   0.04065390303730965,\n",
       "   0.04048670455813408,\n",
       "   0.0405440554022789,\n",
       "   0.042543914169073105,\n",
       "   0.04372471943497658,\n",
       "   0.043710943311452866,\n",
       "   0.04320043325424194,\n",
       "   0.04197796434164047,\n",
       "   0.039963144809007645,\n",
       "   0.04052184149622917,\n",
       "   0.03968578949570656,\n",
       "   0.041076261550188065,\n",
       "   0.03952311724424362,\n",
       "   0.03815256059169769,\n",
       "   0.0409797802567482,\n",
       "   0.040307559072971344,\n",
       "   0.03973325341939926,\n",
       "   0.04030255600810051,\n",
       "   0.04241583123803139,\n",
       "   0.04263099282979965,\n",
       "   0.044333554804325104,\n",
       "   0.04454904794692993,\n",
       "   0.04581994190812111,\n",
       "   0.04562004655599594,\n",
       "   0.04399300739169121,\n",
       "   0.045376431196928024,\n",
       "   0.04586326330900192,\n",
       "   0.044979047030210495,\n",
       "   0.04373195394873619,\n",
       "   0.04538722336292267,\n",
       "   0.04334106296300888,\n",
       "   0.04148190841078758,\n",
       "   0.03780893608927727,\n",
       "   0.03640642389655113,\n",
       "   0.034494347870349884,\n",
       "   0.03093765489757061,\n",
       "   0.03392258659005165,\n",
       "   0.0351109616458416,\n",
       "   0.03586725518107414,\n",
       "   0.03661322221159935,\n",
       "   0.03672695532441139,\n",
       "   0.03512280434370041,\n",
       "   0.03291543573141098,\n",
       "   0.033064987510442734,\n",
       "   0.03389141708612442,\n",
       "   0.034645795822143555,\n",
       "   0.037164606153964996,\n",
       "   0.04084719717502594,\n",
       "   0.04048342630267143,\n",
       "   0.04133092984557152,\n",
       "   0.04033708572387695,\n",
       "   0.04015139490365982,\n",
       "   0.03872155025601387,\n",
       "   0.03533495217561722,\n",
       "   0.03360511735081673,\n",
       "   0.0354485884308815,\n",
       "   0.03346443921327591,\n",
       "   0.032573945820331573,\n",
       "   0.03500146418809891,\n",
       "   0.036259956657886505,\n",
       "   0.036426350474357605,\n",
       "   0.03768137842416763,\n",
       "   0.038684722036123276,\n",
       "   0.04194089397788048,\n",
       "   0.04157732054591179,\n",
       "   0.039780568331480026,\n",
       "   0.04317908361554146,\n",
       "   0.04195080325007439,\n",
       "   0.04249655082821846,\n",
       "   0.04621458053588867,\n",
       "   0.04489029571413994,\n",
       "   0.04459002986550331,\n",
       "   0.04047905281186104,\n",
       "   0.04179416224360466,\n",
       "   0.04372471198439598,\n",
       "   0.04856317490339279,\n",
       "   0.045494161546230316,\n",
       "   0.04229917749762535,\n",
       "   0.04146503284573555,\n",
       "   0.04182283952832222,\n",
       "   0.04313214495778084,\n",
       "   0.038375236093997955,\n",
       "   0.03960495814681053,\n",
       "   0.037701960653066635,\n",
       "   0.04248597100377083,\n",
       "   0.04183433577418327,\n",
       "   0.04474169388413429,\n",
       "   0.04389467462897301,\n",
       "   0.04351338744163513,\n",
       "   0.04163459688425064,\n",
       "   0.04445650056004524,\n",
       "   0.04289494827389717,\n",
       "   0.04086277633905411,\n",
       "   0.0387093722820282,\n",
       "   0.03967669606208801,\n",
       "   0.03896132856607437,\n",
       "   0.0392141193151474,\n",
       "   0.03721844032406807,\n",
       "   0.04073278233408928,\n",
       "   0.043052446097135544,\n",
       "   0.04276467114686966,\n",
       "   0.040693365037441254,\n",
       "   0.03711578622460365,\n",
       "   0.03579573333263397,\n",
       "   0.04035038501024246,\n",
       "   0.04288165271282196,\n",
       "   0.04254639893770218,\n",
       "   0.040611397475004196,\n",
       "   0.04322410374879837,\n",
       "   0.04438041150569916,\n",
       "   0.04251067340373993,\n",
       "   0.04101422429084778,\n",
       "   0.04336479678750038,\n",
       "   0.04669824615120888,\n",
       "   0.048686590045690536,\n",
       "   0.04803000018000603,\n",
       "   0.04633444920182228,\n",
       "   0.04513779282569885,\n",
       "   0.04768252745270729,\n",
       "   0.04437072202563286,\n",
       "   0.04664331674575806,\n",
       "   0.04742145910859108,\n",
       "   0.047892097383737564,\n",
       "   0.050315916538238525,\n",
       "   0.04879676550626755,\n",
       "   0.04821540042757988,\n",
       "   0.05068247392773628,\n",
       "   0.050343483686447144,\n",
       "   0.05096801742911339,\n",
       "   0.05079830810427666,\n",
       "   0.05195184797048569,\n",
       "   0.05081263184547424,\n",
       "   0.0548514686524868,\n",
       "   0.05545898526906967,\n",
       "   0.05387226492166519,\n",
       "   0.051669344305992126,\n",
       "   0.05131031200289726,\n",
       "   0.0531710609793663,\n",
       "   0.05486482009291649,\n",
       "   0.05433568358421326,\n",
       "   0.054430898278951645,\n",
       "   0.05338551476597786,\n",
       "   0.055215101689100266,\n",
       "   0.057528186589479446,\n",
       "   0.054392825812101364,\n",
       "   0.05560828372836113,\n",
       "   0.05674956366419792,\n",
       "   0.05747101828455925,\n",
       "   0.0627150610089302,\n",
       "   0.06352978199720383,\n",
       "   0.060165561735630035,\n",
       "   0.06148643046617508,\n",
       "   0.06061704829335213,\n",
       "   0.06072621792554855,\n",
       "   0.06160696595907211,\n",
       "   0.06311219930648804,\n",
       "   0.05954354628920555,\n",
       "   0.05803358927369118,\n",
       "   0.05538426712155342,\n",
       "   0.05128512158989906,\n",
       "   0.052376341074705124,\n",
       "   0.05063973739743233,\n",
       "   0.05368988215923309,\n",
       "   0.053189877420663834,\n",
       "   0.053784094750881195,\n",
       "   0.05621049180626869,\n",
       "   0.05728794261813164,\n",
       "   0.05624176934361458,\n",
       "   0.05223694443702698,\n",
       "   0.055058613419532776,\n",
       "   0.05813436955213547,\n",
       "   0.05685483291745186,\n",
       "   0.06005670502781868,\n",
       "   0.061482056975364685,\n",
       "   0.060607992112636566,\n",
       "   0.05704044550657272,\n",
       "   0.056749891489744186,\n",
       "   0.05858039855957031,\n",
       "   0.061327993869781494,\n",
       "   0.05413294956088066,\n",
       "   0.05295306444168091,\n",
       "   0.05186120420694351,\n",
       "   0.0498119592666626,\n",
       "   0.0491509735584259,\n",
       "   0.04673733562231064,\n",
       "   0.052012525498867035,\n",
       "   0.05127527192234993,\n",
       "   0.048484861850738525,\n",
       "   0.04880664870142937,\n",
       "   0.04981977120041847,\n",
       "   0.048611290752887726,\n",
       "   0.04785087704658508,\n",
       "   0.05027355998754501,\n",
       "   0.054011471569538116,\n",
       "   0.05412215739488602,\n",
       "   0.05492359399795532,\n",
       "   0.057621777057647705,\n",
       "   0.057451892644166946,\n",
       "   0.05858186259865761,\n",
       "   0.05695801228284836,\n",
       "   0.05633283779025078,\n",
       "   0.05631427466869354,\n",
       "   0.05305441468954086,\n",
       "   0.054768405854701996,\n",
       "   0.05624325945973396,\n",
       "   0.05457514151930809,\n",
       "   0.05404796823859215,\n",
       "   0.05303315073251724,\n",
       "   0.04972623288631439,\n",
       "   0.051528140902519226,\n",
       "   0.05411918833851814,\n",
       "   0.055453989654779434,\n",
       "   0.05592067167162895,\n",
       "   0.059932779520750046,\n",
       "   0.05715382471680641,\n",
       "   0.04949042946100235,\n",
       "   0.049437858164310455,\n",
       "   0.0486721470952034,\n",
       "   0.04855934530496597,\n",
       "   0.05186652019619942,\n",
       "   0.05164765194058418,\n",
       "   0.04922885820269585,\n",
       "   0.05212026834487915,\n",
       "   0.05576113238930702,\n",
       "   0.052374646067619324,\n",
       "   0.04905131459236145,\n",
       "   0.05052598938345909,\n",
       "   0.04866982623934746,\n",
       "   0.04808131977915764,\n",
       "   0.04620267078280449,\n",
       "   0.046848610043525696,\n",
       "   0.04796112701296806,\n",
       "   0.048949919641017914,\n",
       "   0.04933474212884903,\n",
       "   0.048124946653842926,\n",
       "   0.04763169214129448,\n",
       "   0.04919545352458954,\n",
       "   0.04657270759344101,\n",
       "   0.04703381285071373,\n",
       "   0.04758087545633316,\n",
       "   0.04664144665002823,\n",
       "   0.04339289292693138,\n",
       "   0.04118702933192253,\n",
       "   0.041956447064876556,\n",
       "   0.040622528642416,\n",
       "   0.041869617998600006,\n",
       "   0.03940247371792793,\n",
       "   0.04040876030921936,\n",
       "   0.04208556190133095,\n",
       "   0.04333383962512016,\n",
       "   0.04380234330892563,\n",
       "   0.04173331335186958,\n",
       "   0.04095425084233284,\n",
       "   0.03779377415776253,\n",
       "   0.039727892726659775,\n",
       "   0.04118877649307251,\n",
       "   0.041180968284606934,\n",
       "   0.03893522918224335,\n",
       "   0.03721734508872032,\n",
       "   0.03680787608027458,\n",
       "   0.035462699830532074,\n",
       "   0.031920794397592545,\n",
       "   0.029785726219415665,\n",
       "   0.03222331777215004,\n",
       "   0.031474772840738297,\n",
       "   0.03238110616803169,\n",
       "   0.0303290244191885,\n",
       "   0.031395621597766876,\n",
       "   0.032457299530506134,\n",
       "   0.030977310612797737,\n",
       "   0.030340855941176414,\n",
       "   0.0325019471347332,\n",
       "   0.032747168093919754,\n",
       "   0.033624257892370224,\n",
       "   0.034136589616537094,\n",
       "   0.03669058531522751,\n",
       "   0.03777221962809563,\n",
       "   0.03991914913058281,\n",
       "   0.04288733750581741,\n",
       "   0.043450094759464264,\n",
       "   0.04444821923971176,\n",
       "   0.045593198388814926,\n",
       "   0.04710563272237778,\n",
       "   0.046658094972372055,\n",
       "   0.04537457227706909,\n",
       "   0.04337667301297188,\n",
       "   0.042115047574043274,\n",
       "   0.0455746091902256,\n",
       "   0.04471098631620407,\n",
       "   0.044275786727666855,\n",
       "   0.04227658361196518,\n",
       "   0.04207689315080643,\n",
       "   0.03940993547439575,\n",
       "   0.0420074500143528,\n",
       "   0.04253433644771576,\n",
       "   0.039736900478601456,\n",
       "   0.040863122791051865,\n",
       "   0.039662111550569534,\n",
       "   0.03682218864560127,\n",
       "   0.03900492563843727,\n",
       "   0.03856495022773743,\n",
       "   0.03953865170478821,\n",
       "   0.04074643924832344,\n",
       "   0.03924940153956413,\n",
       "   0.03979061171412468,\n",
       "   0.03961138799786568,\n",
       "   0.03864704445004463,\n",
       "   0.03789462894201279,\n",
       "   0.03869575262069702,\n",
       "   0.03944658115506172,\n",
       "   0.04049576073884964,\n",
       "   0.04022791609168053,\n",
       "   0.03812798112630844,\n",
       "   0.03868069872260094,\n",
       "   0.04026905819773674,\n",
       "   0.0399533249437809,\n",
       "   0.03967927768826485,\n",
       "   0.04063287749886513,\n",
       "   0.04311646893620491,\n",
       "   0.04434339702129364,\n",
       "   0.04366277530789375,\n",
       "   0.04366403818130493,\n",
       "   0.04760746285319328,\n",
       "   0.050080884248018265,\n",
       "   0.04475772753357887,\n",
       "   0.04165227338671684,\n",
       "   0.042025402188301086,\n",
       "   0.04081294313073158,\n",
       "   0.041629113256931305,\n",
       "   0.043100591748952866,\n",
       "   0.04409293085336685,\n",
       "   0.03935008496046066,\n",
       "   0.038704998791217804,\n",
       "   0.03939635679125786,\n",
       "   0.04116241633892059,\n",
       "   0.042013753205537796,\n",
       "   0.04010065644979477,\n",
       "   0.040064021944999695,\n",
       "   0.038837529718875885,\n",
       "   0.03907746821641922,\n",
       "   0.0391765795648098,\n",
       "   0.038507696241140366,\n",
       "   0.03628939762711525,\n",
       "   0.036648765206336975,\n",
       "   0.036701276898384094,\n",
       "   0.03841443359851837,\n",
       "   0.04192855954170227,\n",
       "   0.04208172485232353,\n",
       "   0.037744469940662384,\n",
       "   0.0397762656211853,\n",
       "   0.03820275887846947,\n",
       "   0.037788547575473785,\n",
       "   0.03515826538205147,\n",
       "   0.03741537407040596,\n",
       "   0.036683276295661926,\n",
       "   0.037075839936733246,\n",
       "   0.03594445809721947,\n",
       "   0.037390876561403275,\n",
       "   0.03278764337301254,\n",
       "   0.03419935330748558,\n",
       "   0.03423721715807915,\n",
       "   0.036882270127534866,\n",
       "   0.03553049638867378,\n",
       "   0.036380596458911896,\n",
       "   0.03469351306557655,\n",
       "   0.033044930547475815,\n",
       "   0.03339444473385811,\n",
       "   0.03376670926809311,\n",
       "   0.0323810838162899,\n",
       "   0.033331312239170074,\n",
       "   0.03443746268749237,\n",
       "   0.035602353513240814,\n",
       "   0.03254442289471626,\n",
       "   0.03391261026263237,\n",
       "   0.033118121325969696,\n",
       "   0.036538053303956985,\n",
       "   0.036688100546598434,\n",
       "   0.03997495025396347,\n",
       "   0.035837963223457336,\n",
       "   0.03518930822610855,\n",
       "   0.034438446164131165,\n",
       "   0.037108443677425385,\n",
       "   0.037205781787633896,\n",
       "   0.03588837385177612,\n",
       "   0.03886319324374199,\n",
       "   0.037178367376327515,\n",
       "   0.03903861716389656,\n",
       "   0.039504218846559525,\n",
       "   0.04168751463294029,\n",
       "   0.04120418801903725,\n",
       "   0.03815223276615143,\n",
       "   0.03961959853768349,\n",
       "   0.039753783494234085,\n",
       "   0.03921043127775192,\n",
       "   0.039673998951911926,\n",
       "   0.03710128366947174,\n",
       "   0.03288482874631882,\n",
       "   0.033790647983551025,\n",
       "   0.03515148162841797,\n",
       "   0.03337131440639496,\n",
       "   0.03246874734759331,\n",
       "   0.03470421954989433,\n",
       "   0.03578275069594383,\n",
       "   0.036476194858551025,\n",
       "   0.031685858964920044,\n",
       "   0.031357575207948685],\n",
       "  'voc_loss': [0.02546423114836216,\n",
       "   0.07139988243579865,\n",
       "   0.1437041461467743,\n",
       "   0.18683508038520813,\n",
       "   0.23421524465084076,\n",
       "   0.2933506369590759,\n",
       "   0.33479732275009155,\n",
       "   0.38320526480674744,\n",
       "   0.42856016755104065,\n",
       "   0.4658025801181793,\n",
       "   0.49729815125465393,\n",
       "   0.5172512531280518,\n",
       "   0.5295930504798889,\n",
       "   0.5424957871437073,\n",
       "   0.5463064312934875,\n",
       "   0.5473799705505371,\n",
       "   0.5475946664810181,\n",
       "   0.547910749912262,\n",
       "   0.5487697124481201,\n",
       "   0.5498624444007874,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5499533414840698,\n",
       "   0.5497120022773743,\n",
       "   0.549125611782074,\n",
       "   0.54866623878479,\n",
       "   0.5484152436256409,\n",
       "   0.5473506450653076,\n",
       "   0.5466600060462952,\n",
       "   0.545610249042511,\n",
       "   0.544597864151001,\n",
       "   0.5440385937690735,\n",
       "   0.5432450175285339,\n",
       "   0.542628288269043,\n",
       "   0.5415070056915283,\n",
       "   0.5407904982566833,\n",
       "   0.5400683283805847,\n",
       "   0.5393995642662048,\n",
       "   0.5388383269309998,\n",
       "   0.5396416783332825,\n",
       "   0.5400583148002625,\n",
       "   0.539326012134552,\n",
       "   0.5384982228279114,\n",
       "   0.538276731967926,\n",
       "   0.5381795167922974,\n",
       "   0.5375546216964722,\n",
       "   0.5369713306427002,\n",
       "   0.5367512106895447,\n",
       "   0.536428689956665,\n",
       "   0.5364023447036743,\n",
       "   0.5351913571357727,\n",
       "   0.5347328186035156,\n",
       "   0.5346630215644836,\n",
       "   0.5345101952552795,\n",
       "   0.5345650315284729,\n",
       "   0.5336092710494995,\n",
       "   0.5246560573577881,\n",
       "   0.5209018588066101,\n",
       "   0.5334627032279968,\n",
       "   0.525581419467926,\n",
       "   0.5142008066177368,\n",
       "   0.5266400575637817,\n",
       "   0.5186310410499573,\n",
       "   0.5248200297355652,\n",
       "   0.47949033975601196,\n",
       "   0.48162001371383667,\n",
       "   0.4719984233379364,\n",
       "   0.47448858618736267,\n",
       "   0.48464593291282654,\n",
       "   0.45962679386138916,\n",
       "   0.46639561653137207,\n",
       "   0.4620438516139984,\n",
       "   0.46359825134277344,\n",
       "   0.4617985486984253,\n",
       "   0.4616248607635498,\n",
       "   0.4742182791233063,\n",
       "   0.4868452847003937,\n",
       "   0.4878073036670685,\n",
       "   0.4790584444999695,\n",
       "   0.45896977186203003,\n",
       "   0.47226572036743164,\n",
       "   0.4783070981502533,\n",
       "   0.46718087792396545,\n",
       "   0.4716748893260956,\n",
       "   0.47317448258399963,\n",
       "   0.4774906635284424,\n",
       "   0.46743422746658325,\n",
       "   0.4845406413078308,\n",
       "   0.47457149624824524,\n",
       "   0.4785912036895752,\n",
       "   0.4772448241710663,\n",
       "   0.4812276065349579,\n",
       "   0.4823855757713318,\n",
       "   0.4870017170906067,\n",
       "   0.4794057011604309,\n",
       "   0.45617201924324036,\n",
       "   0.46703875064849854,\n",
       "   0.4603571891784668,\n",
       "   0.46597471833229065,\n",
       "   0.47487789392471313,\n",
       "   0.46712422370910645,\n",
       "   0.4678879678249359,\n",
       "   0.4647391140460968,\n",
       "   0.46816471219062805,\n",
       "   0.47271424531936646,\n",
       "   0.4813810884952545,\n",
       "   0.4922095537185669,\n",
       "   0.48411816358566284,\n",
       "   0.48934605717658997,\n",
       "   0.47465354204177856,\n",
       "   0.4806610345840454,\n",
       "   0.4736175537109375,\n",
       "   0.48408976197242737,\n",
       "   0.49793532490730286,\n",
       "   0.4963890314102173,\n",
       "   0.48682254552841187,\n",
       "   0.4959653317928314,\n",
       "   0.48220202326774597,\n",
       "   0.49279600381851196,\n",
       "   0.50664883852005,\n",
       "   0.5008016228675842,\n",
       "   0.5120235681533813,\n",
       "   0.49827462434768677,\n",
       "   0.4867243766784668,\n",
       "   0.4914521276950836,\n",
       "   0.4996386468410492,\n",
       "   0.5163273811340332,\n",
       "   0.5234589576721191,\n",
       "   0.5158201456069946,\n",
       "   0.5205186009407043,\n",
       "   0.5294191241264343,\n",
       "   0.5136615633964539,\n",
       "   0.5260933041572571,\n",
       "   0.5334423780441284,\n",
       "   0.5332310199737549,\n",
       "   0.5324212908744812,\n",
       "   0.5326842665672302,\n",
       "   0.5327044725418091,\n",
       "   0.5328691005706787,\n",
       "   0.5322449207305908,\n",
       "   0.5281626582145691,\n",
       "   0.523834764957428,\n",
       "   0.5244993567466736,\n",
       "   0.5307012796401978,\n",
       "   0.5309773087501526,\n",
       "   0.5310668349266052,\n",
       "   0.5310446619987488,\n",
       "   0.5307790040969849,\n",
       "   0.5306620597839355,\n",
       "   0.5301733613014221,\n",
       "   0.5305820107460022,\n",
       "   0.5302696228027344,\n",
       "   0.5304248929023743,\n",
       "   0.5305770039558411,\n",
       "   0.5307294130325317,\n",
       "   0.5305230021476746,\n",
       "   0.5301215052604675,\n",
       "   0.530552864074707,\n",
       "   0.5311768651008606,\n",
       "   0.5313689708709717,\n",
       "   0.531257152557373,\n",
       "   0.5307291150093079,\n",
       "   0.5239987373352051,\n",
       "   0.5082811117172241,\n",
       "   0.5277919173240662,\n",
       "   0.5310114026069641,\n",
       "   0.521550714969635,\n",
       "   0.5303224921226501,\n",
       "   0.5289881229400635,\n",
       "   0.5305830836296082,\n",
       "   0.530571460723877,\n",
       "   0.5302623510360718,\n",
       "   0.5302050113677979,\n",
       "   0.5297937393188477,\n",
       "   0.5265228748321533,\n",
       "   0.5300091505050659,\n",
       "   0.5300911664962769,\n",
       "   0.5298822522163391,\n",
       "   0.5282058715820312,\n",
       "   0.512067973613739,\n",
       "   0.5100997686386108,\n",
       "   0.529309868812561,\n",
       "   0.5296872854232788,\n",
       "   0.5066123008728027,\n",
       "   0.5058882236480713,\n",
       "   0.4943341016769409,\n",
       "   0.502407968044281,\n",
       "   0.5077348947525024,\n",
       "   0.5172967910766602,\n",
       "   0.5177698731422424,\n",
       "   0.5287659764289856,\n",
       "   0.5289695858955383,\n",
       "   0.5291433930397034,\n",
       "   0.5275691747665405,\n",
       "   0.4998365342617035,\n",
       "   0.49582579731941223,\n",
       "   0.471077024936676,\n",
       "   0.4831309914588928,\n",
       "   0.47357529401779175,\n",
       "   0.4379928708076477,\n",
       "   0.44284144043922424,\n",
       "   0.4521295726299286,\n",
       "   0.4541592001914978,\n",
       "   0.46063506603240967,\n",
       "   0.46363726258277893,\n",
       "   0.43499162793159485,\n",
       "   0.4235091805458069,\n",
       "   0.43042129278182983,\n",
       "   0.43333765864372253,\n",
       "   0.44076889753341675,\n",
       "   0.43942636251449585,\n",
       "   0.4384707510471344,\n",
       "   0.421540230512619,\n",
       "   0.4320814907550812,\n",
       "   0.44600316882133484,\n",
       "   0.4566541612148285,\n",
       "   0.43820685148239136,\n",
       "   0.445117324590683,\n",
       "   0.43515634536743164,\n",
       "   0.44573622941970825,\n",
       "   0.43380820751190186,\n",
       "   0.435081422328949,\n",
       "   0.4265322685241699,\n",
       "   0.4352797567844391,\n",
       "   0.4435866177082062,\n",
       "   0.4306894838809967,\n",
       "   0.4458496570587158,\n",
       "   0.430390864610672,\n",
       "   0.43839749693870544,\n",
       "   0.443954735994339,\n",
       "   0.4392259418964386,\n",
       "   0.44994044303894043,\n",
       "   0.44771435856819153,\n",
       "   0.43856915831565857,\n",
       "   0.4480765461921692,\n",
       "   0.4663853943347931,\n",
       "   0.480905681848526,\n",
       "   0.47559821605682373,\n",
       "   0.4934285581111908,\n",
       "   0.48187750577926636,\n",
       "   0.49389106035232544,\n",
       "   0.502049446105957,\n",
       "   0.5036670565605164,\n",
       "   0.5068848133087158,\n",
       "   0.49047738313674927,\n",
       "   0.4850088655948639,\n",
       "   0.4894726276397705,\n",
       "   0.4897734522819519,\n",
       "   0.4807470738887787,\n",
       "   0.4815461039543152,\n",
       "   0.47319045662879944,\n",
       "   0.4731771647930145,\n",
       "   0.4796660542488098,\n",
       "   0.48856019973754883,\n",
       "   0.49690958857536316,\n",
       "   0.4910728335380554,\n",
       "   0.4863973557949066,\n",
       "   0.4393694996833801,\n",
       "   0.44091254472732544,\n",
       "   0.4265765845775604,\n",
       "   0.43708348274230957,\n",
       "   0.41823193430900574,\n",
       "   0.43265265226364136,\n",
       "   0.4376696050167084,\n",
       "   0.4352293908596039,\n",
       "   0.4328213334083557,\n",
       "   0.4364912211894989,\n",
       "   0.43402376770973206,\n",
       "   0.44582656025886536,\n",
       "   0.4409641623497009,\n",
       "   0.4472287595272064,\n",
       "   0.4457795023918152,\n",
       "   0.47095516324043274,\n",
       "   0.4741845428943634,\n",
       "   0.4653092920780182,\n",
       "   0.4777047336101532,\n",
       "   0.471314400434494,\n",
       "   0.47142791748046875,\n",
       "   0.48852524161338806,\n",
       "   0.494096040725708,\n",
       "   0.46327993273735046,\n",
       "   0.4672601521015167,\n",
       "   0.46261173486709595,\n",
       "   0.43202075362205505,\n",
       "   0.4407229423522949,\n",
       "   0.44596779346466064,\n",
       "   0.477236270904541,\n",
       "   0.49939799308776855,\n",
       "   0.5110747218132019,\n",
       "   0.5181000232696533,\n",
       "   0.5175273418426514,\n",
       "   0.5139508843421936,\n",
       "   0.5111275315284729,\n",
       "   0.5103768706321716,\n",
       "   0.5127706527709961,\n",
       "   0.5099103450775146,\n",
       "   0.5137425065040588,\n",
       "   0.5114072561264038,\n",
       "   0.5122173428535461,\n",
       "   0.5045133233070374,\n",
       "   0.5056633353233337,\n",
       "   0.5108261704444885,\n",
       "   0.5160400867462158,\n",
       "   0.5106953978538513,\n",
       "   0.49614623188972473,\n",
       "   0.5056765079498291,\n",
       "   0.48364371061325073,\n",
       "   0.48394575715065,\n",
       "   0.4901426136493683,\n",
       "   0.5038654208183289,\n",
       "   0.5043570399284363,\n",
       "   0.5043967366218567,\n",
       "   0.5087759494781494,\n",
       "   0.5083145499229431,\n",
       "   0.48772192001342773,\n",
       "   0.5021820068359375,\n",
       "   0.5113115906715393,\n",
       "   0.5147981643676758,\n",
       "   0.5091935992240906,\n",
       "   0.5107736587524414,\n",
       "   0.515830397605896,\n",
       "   0.5164689421653748,\n",
       "   0.5189208984375,\n",
       "   0.5150046348571777,\n",
       "   0.5189751982688904,\n",
       "   0.5140087008476257,\n",
       "   0.5153886079788208,\n",
       "   0.516450047492981,\n",
       "   0.5125864148139954,\n",
       "   0.5098173022270203,\n",
       "   0.5137256383895874,\n",
       "   0.518957793712616,\n",
       "   0.5197960734367371,\n",
       "   0.4983196258544922,\n",
       "   0.5003725290298462,\n",
       "   0.49952930212020874,\n",
       "   0.4917193353176117,\n",
       "   0.48983702063560486,\n",
       "   0.4979512393474579,\n",
       "   0.5102709531784058,\n",
       "   0.5257582664489746,\n",
       "   0.5294000506401062,\n",
       "   0.529380738735199,\n",
       "   0.529083788394928,\n",
       "   0.5293663144111633,\n",
       "   0.529058575630188,\n",
       "   0.5291275382041931,\n",
       "   0.529238224029541,\n",
       "   0.5289299488067627,\n",
       "   0.5260283350944519,\n",
       "   0.5285667777061462,\n",
       "   0.5272708535194397,\n",
       "   0.5235820412635803,\n",
       "   0.5224429965019226,\n",
       "   0.5213848948478699,\n",
       "   0.5141727924346924,\n",
       "   0.5159984827041626,\n",
       "   0.5154737234115601,\n",
       "   0.5157762169837952,\n",
       "   0.5184472799301147,\n",
       "   0.5221584439277649,\n",
       "   0.5226535201072693,\n",
       "   0.5208664536476135,\n",
       "   0.5240892171859741,\n",
       "   0.5286049246788025,\n",
       "   0.5290524363517761,\n",
       "   0.528762936592102,\n",
       "   0.5292491316795349,\n",
       "   0.529096245765686,\n",
       "   0.5288361310958862,\n",
       "   0.505689799785614,\n",
       "   0.4987010061740875,\n",
       "   0.49576568603515625,\n",
       "   0.49921804666519165,\n",
       "   0.4799681603908539,\n",
       "   0.4751833975315094,\n",
       "   0.46003997325897217,\n",
       "   0.47559836506843567,\n",
       "   0.46754026412963867,\n",
       "   0.47449034452438354,\n",
       "   0.46898919343948364,\n",
       "   0.48136717081069946,\n",
       "   0.46031904220581055,\n",
       "   0.4694361388683319,\n",
       "   0.4679904580116272,\n",
       "   0.45851200819015503,\n",
       "   0.47396546602249146,\n",
       "   0.45057713985443115,\n",
       "   0.4499019384384155,\n",
       "   0.45788121223449707,\n",
       "   0.4685574471950531,\n",
       "   0.48225584626197815,\n",
       "   0.5051199197769165,\n",
       "   0.5009192228317261,\n",
       "   0.5172680616378784,\n",
       "   0.5153642892837524,\n",
       "   0.5036178231239319,\n",
       "   0.5035393238067627,\n",
       "   0.49332761764526367,\n",
       "   0.49747487902641296,\n",
       "   0.4840841591358185,\n",
       "   0.47490015625953674,\n",
       "   0.46932581067085266,\n",
       "   0.4632723927497864,\n",
       "   0.45941033959388733,\n",
       "   0.42629605531692505,\n",
       "   0.433845579624176,\n",
       "   0.43306416273117065,\n",
       "   0.43786749243736267,\n",
       "   0.44144248962402344,\n",
       "   0.44565993547439575,\n",
       "   0.44842761754989624,\n",
       "   0.4465012848377228,\n",
       "   0.45084887742996216,\n",
       "   0.43150147795677185,\n",
       "   0.4240066707134247,\n",
       "   0.42311495542526245,\n",
       "   0.4260254204273224,\n",
       "   0.44446295499801636,\n",
       "   0.4559916853904724,\n",
       "   0.45439577102661133,\n",
       "   0.4398733973503113,\n",
       "   0.43653517961502075,\n",
       "   0.43855780363082886,\n",
       "   0.4517212510108948,\n",
       "   0.4599345326423645,\n",
       "   0.47308164834976196,\n",
       "   0.4775940775871277,\n",
       "   0.4912324845790863,\n",
       "   0.479770302772522,\n",
       "   0.4569300711154938,\n",
       "   0.45873233675956726,\n",
       "   0.46988600492477417,\n",
       "   0.4829300045967102,\n",
       "   0.4856968820095062,\n",
       "   0.48654767870903015,\n",
       "   0.479057252407074,\n",
       "   0.4552203416824341,\n",
       "   0.46953561902046204,\n",
       "   0.46500495076179504,\n",
       "   0.45711749792099,\n",
       "   0.4659560024738312,\n",
       "   0.44926658272743225,\n",
       "   0.46121761202812195,\n",
       "   0.462101548910141,\n",
       "   0.4587683379650116,\n",
       "   0.4561381936073303,\n",
       "   0.4648715853691101,\n",
       "   0.447756290435791,\n",
       "   0.4557839035987854,\n",
       "   0.47568315267562866,\n",
       "   0.4580008089542389,\n",
       "   0.468564510345459,\n",
       "   0.44657495617866516,\n",
       "   0.4642789661884308,\n",
       "   0.4417690634727478,\n",
       "   0.4480666220188141,\n",
       "   0.4578600227832794,\n",
       "   0.4410574734210968,\n",
       "   0.4238198697566986,\n",
       "   0.4209785461425781,\n",
       "   0.41635847091674805,\n",
       "   0.42900481820106506,\n",
       "   0.41503942012786865,\n",
       "   0.4273609220981598,\n",
       "   0.4357600808143616,\n",
       "   0.4475930333137512,\n",
       "   0.43908393383026123,\n",
       "   0.43945327401161194,\n",
       "   0.4383015036582947,\n",
       "   0.4487028419971466,\n",
       "   0.4543817937374115,\n",
       "   0.4583040773868561,\n",
       "   0.4466949701309204,\n",
       "   0.4476497173309326,\n",
       "   0.4520242512226105,\n",
       "   0.44329121708869934,\n",
       "   0.42614293098449707,\n",
       "   0.4118494987487793,\n",
       "   0.42338043451309204,\n",
       "   0.42518192529678345,\n",
       "   0.42392492294311523,\n",
       "   0.4240954518318176,\n",
       "   0.4201568067073822,\n",
       "   0.43523189425468445,\n",
       "   0.4455042779445648,\n",
       "   0.44644469022750854,\n",
       "   0.4333254396915436,\n",
       "   0.439857542514801,\n",
       "   0.46115097403526306],\n",
       "  'jsc_loss': [0.41994813084602356,\n",
       "   0.574974775314331,\n",
       "   0.6520678400993347,\n",
       "   0.6424922347068787,\n",
       "   0.7181795239448547,\n",
       "   0.7712010145187378,\n",
       "   0.7913064956665039,\n",
       "   0.7915782928466797,\n",
       "   0.813291609287262,\n",
       "   0.8332597613334656,\n",
       "   0.8288897275924683,\n",
       "   0.8043889999389648,\n",
       "   0.7635348439216614,\n",
       "   0.7488098740577698,\n",
       "   0.72125244140625,\n",
       "   0.7210542559623718,\n",
       "   0.7260782718658447,\n",
       "   0.7186969518661499,\n",
       "   0.6995818614959717,\n",
       "   0.6951051950454712,\n",
       "   0.6892403960227966,\n",
       "   0.6961085200309753,\n",
       "   0.6982523202896118,\n",
       "   0.6910863518714905,\n",
       "   0.6950001120567322,\n",
       "   0.6927468776702881,\n",
       "   0.7041051983833313,\n",
       "   0.6988467574119568,\n",
       "   0.704346239566803,\n",
       "   0.6985723376274109,\n",
       "   0.6924299001693726,\n",
       "   0.6884105205535889,\n",
       "   0.6853365302085876,\n",
       "   0.6732754111289978,\n",
       "   0.6754704713821411,\n",
       "   0.6781737208366394,\n",
       "   0.6900668144226074,\n",
       "   0.6906697750091553,\n",
       "   0.6920583248138428,\n",
       "   0.6889528632164001,\n",
       "   0.6742388010025024,\n",
       "   0.6839759945869446,\n",
       "   0.6942433714866638,\n",
       "   0.6923510432243347,\n",
       "   0.683254063129425,\n",
       "   0.6744253039360046,\n",
       "   0.6675764918327332,\n",
       "   0.6546846628189087,\n",
       "   0.6549078226089478,\n",
       "   0.6467206478118896,\n",
       "   0.6487405896186829,\n",
       "   0.629708468914032,\n",
       "   0.6384505033493042,\n",
       "   0.6261415481567383,\n",
       "   0.6206010580062866,\n",
       "   0.6149436831474304,\n",
       "   0.6116275191307068,\n",
       "   0.6135534644126892,\n",
       "   0.5953210592269897,\n",
       "   0.5834323167800903,\n",
       "   0.5821044445037842,\n",
       "   0.579046368598938,\n",
       "   0.5767131447792053,\n",
       "   0.5725317001342773,\n",
       "   0.5702799558639526,\n",
       "   0.5669381618499756,\n",
       "   0.5732695460319519,\n",
       "   0.5711697936058044,\n",
       "   0.575092613697052,\n",
       "   0.5642673969268799,\n",
       "   0.5636552572250366,\n",
       "   0.5518554449081421,\n",
       "   0.545473039150238,\n",
       "   0.5436563491821289,\n",
       "   0.5310378074645996,\n",
       "   0.5323947072029114,\n",
       "   0.5358157157897949,\n",
       "   0.5392448902130127,\n",
       "   0.5407405495643616,\n",
       "   0.5430780649185181,\n",
       "   0.5405498147010803,\n",
       "   0.5422983169555664,\n",
       "   0.5502291321754456,\n",
       "   0.5497584342956543,\n",
       "   0.5498776435852051,\n",
       "   0.5535065531730652,\n",
       "   0.5494916439056396,\n",
       "   0.5421262383460999,\n",
       "   0.5381902456283569,\n",
       "   0.5510650277137756,\n",
       "   0.5553911328315735,\n",
       "   0.5583502650260925,\n",
       "   0.5587953925132751,\n",
       "   0.5554578304290771,\n",
       "   0.550606906414032,\n",
       "   0.5450280904769897,\n",
       "   0.5447924137115479,\n",
       "   0.5453498959541321,\n",
       "   0.5439532995223999,\n",
       "   0.5435308218002319,\n",
       "   0.5414878726005554,\n",
       "   0.5353860259056091,\n",
       "   0.5312225818634033,\n",
       "   0.5314536690711975,\n",
       "   0.5232338309288025,\n",
       "   0.518447995185852,\n",
       "   0.5160790681838989,\n",
       "   0.5160302519798279,\n",
       "   0.5271922945976257,\n",
       "   0.5208005309104919,\n",
       "   0.5283493399620056,\n",
       "   0.5304941534996033,\n",
       "   0.5487264394760132,\n",
       "   0.5548177361488342,\n",
       "   0.5585092902183533,\n",
       "   0.5560826659202576,\n",
       "   0.5604639053344727,\n",
       "   0.5590199828147888,\n",
       "   0.5559316873550415,\n",
       "   0.549701452255249,\n",
       "   0.5441614389419556,\n",
       "   0.5447770357131958,\n",
       "   0.536578357219696,\n",
       "   0.5312915444374084,\n",
       "   0.5293492078781128,\n",
       "   0.5308768153190613,\n",
       "   0.5362672805786133,\n",
       "   0.5376614928245544,\n",
       "   0.5424103140830994,\n",
       "   0.5450834631919861,\n",
       "   0.5554865598678589,\n",
       "   0.5645512342453003,\n",
       "   0.5662912130355835,\n",
       "   0.5508092641830444,\n",
       "   0.5469533801078796,\n",
       "   0.5424078702926636,\n",
       "   0.5471963882446289,\n",
       "   0.5464301109313965,\n",
       "   0.5424520373344421,\n",
       "   0.5477543473243713,\n",
       "   0.5438532829284668,\n",
       "   0.5466743111610413,\n",
       "   0.5504162311553955,\n",
       "   0.5391783714294434,\n",
       "   0.5358350276947021,\n",
       "   0.5444371700286865,\n",
       "   0.5333812832832336,\n",
       "   0.5299468636512756,\n",
       "   0.5299033522605896,\n",
       "   0.5300347208976746,\n",
       "   0.5267036557197571,\n",
       "   0.5212253928184509,\n",
       "   0.5097861289978027,\n",
       "   0.517683207988739,\n",
       "   0.5194349884986877,\n",
       "   0.5180765986442566,\n",
       "   0.5121111273765564,\n",
       "   0.5111652612686157,\n",
       "   0.5214964151382446,\n",
       "   0.5216740965843201,\n",
       "   0.5248780250549316,\n",
       "   0.5193192362785339,\n",
       "   0.5200737714767456,\n",
       "   0.5190291404724121,\n",
       "   0.5287195444107056,\n",
       "   0.5337598919868469,\n",
       "   0.5297402739524841,\n",
       "   0.5294912457466125,\n",
       "   0.5296735763549805,\n",
       "   0.5225915312767029,\n",
       "   0.5150768160820007,\n",
       "   0.5228708982467651,\n",
       "   0.5239823460578918,\n",
       "   0.5247255563735962,\n",
       "   0.5169060826301575,\n",
       "   0.5269243121147156,\n",
       "   0.5290201306343079,\n",
       "   0.5408571362495422,\n",
       "   0.5440256595611572,\n",
       "   0.5396335124969482,\n",
       "   0.5439632534980774,\n",
       "   0.5418664216995239,\n",
       "   0.549393892288208,\n",
       "   0.5504125356674194,\n",
       "   0.5501485466957092,\n",
       "   0.5546807050704956,\n",
       "   0.5452440977096558,\n",
       "   0.5438234210014343,\n",
       "   0.5370547771453857,\n",
       "   0.5429962873458862,\n",
       "   0.5446469187736511,\n",
       "   0.5426268577575684,\n",
       "   0.5474709272384644,\n",
       "   0.5603363513946533,\n",
       "   0.5634388327598572,\n",
       "   0.557510256767273,\n",
       "   0.549346387386322,\n",
       "   0.5514009594917297,\n",
       "   0.5459330677986145,\n",
       "   0.5489246249198914,\n",
       "   0.5509616732597351,\n",
       "   0.5548633933067322,\n",
       "   0.5509291291236877,\n",
       "   0.5525675415992737,\n",
       "   0.5544108152389526,\n",
       "   0.547082245349884,\n",
       "   0.5478944778442383,\n",
       "   0.5333467721939087,\n",
       "   0.5482317209243774,\n",
       "   0.5535569787025452,\n",
       "   0.5480101108551025,\n",
       "   0.554912269115448,\n",
       "   0.5528071522712708,\n",
       "   0.5491652488708496,\n",
       "   0.5497465133666992,\n",
       "   0.5492344498634338,\n",
       "   0.5618917942047119,\n",
       "   0.5697788000106812,\n",
       "   0.5683240294456482,\n",
       "   0.5701698064804077,\n",
       "   0.5646418333053589,\n",
       "   0.5775392651557922,\n",
       "   0.5937296152114868,\n",
       "   0.6013380885124207,\n",
       "   0.6112721562385559,\n",
       "   0.6208078265190125,\n",
       "   0.6220929026603699,\n",
       "   0.6331186294555664,\n",
       "   0.6401634216308594,\n",
       "   0.6284219622612,\n",
       "   0.6295557022094727,\n",
       "   0.6324560642242432,\n",
       "   0.619022011756897,\n",
       "   0.6167525053024292,\n",
       "   0.6103585958480835,\n",
       "   0.606614887714386,\n",
       "   0.6026975512504578,\n",
       "   0.6087822914123535,\n",
       "   0.610601544380188,\n",
       "   0.6138311624526978,\n",
       "   0.6065252423286438,\n",
       "   0.5982875823974609,\n",
       "   0.5955606698989868,\n",
       "   0.6027920246124268,\n",
       "   0.5982496738433838,\n",
       "   0.6032351851463318,\n",
       "   0.6063269376754761,\n",
       "   0.6182794570922852,\n",
       "   0.6118123531341553,\n",
       "   0.6147711277008057,\n",
       "   0.6137046813964844,\n",
       "   0.6063846349716187,\n",
       "   0.5993228554725647,\n",
       "   0.6036109924316406,\n",
       "   0.600919246673584,\n",
       "   0.6066125631332397,\n",
       "   0.607726514339447,\n",
       "   0.6101765632629395,\n",
       "   0.6155202984809875,\n",
       "   0.6110045313835144,\n",
       "   0.609397292137146,\n",
       "   0.6118429899215698,\n",
       "   0.6124038696289062,\n",
       "   0.6042574644088745,\n",
       "   0.5963903665542603,\n",
       "   0.6063169240951538,\n",
       "   0.6069512963294983,\n",
       "   0.608834445476532,\n",
       "   0.6002325415611267,\n",
       "   0.5984188318252563,\n",
       "   0.5991774797439575,\n",
       "   0.6019137501716614,\n",
       "   0.5921316742897034,\n",
       "   0.6065715551376343,\n",
       "   0.6051651239395142,\n",
       "   0.5946253538131714,\n",
       "   0.6028236150741577,\n",
       "   0.6110118627548218,\n",
       "   0.6097874641418457,\n",
       "   0.6086152791976929,\n",
       "   0.6168080568313599,\n",
       "   0.6218742728233337,\n",
       "   0.6261448860168457,\n",
       "   0.6197298765182495,\n",
       "   0.6241603493690491,\n",
       "   0.6154150366783142,\n",
       "   0.5988857746124268,\n",
       "   0.6070886850357056,\n",
       "   0.6085213422775269,\n",
       "   0.614518404006958,\n",
       "   0.6206550598144531,\n",
       "   0.6259724497795105,\n",
       "   0.6261797547340393,\n",
       "   0.6246722936630249,\n",
       "   0.6270780563354492,\n",
       "   0.6269047260284424,\n",
       "   0.619902491569519,\n",
       "   0.6286881566047668,\n",
       "   0.6367120742797852,\n",
       "   0.6365313529968262,\n",
       "   0.6417957544326782,\n",
       "   0.6495458483695984,\n",
       "   0.6531322598457336,\n",
       "   0.6455762982368469,\n",
       "   0.6391202807426453,\n",
       "   0.6320406198501587,\n",
       "   0.6296976208686829,\n",
       "   0.6306054592132568,\n",
       "   0.6253868341445923,\n",
       "   0.6290472149848938,\n",
       "   0.6272676587104797,\n",
       "   0.6336802840232849,\n",
       "   0.6218340992927551,\n",
       "   0.6237643361091614,\n",
       "   0.6342827081680298,\n",
       "   0.6406251788139343,\n",
       "   0.6503334045410156,\n",
       "   0.6504095196723938,\n",
       "   0.6535518765449524,\n",
       "   0.648155927658081,\n",
       "   0.6509779095649719,\n",
       "   0.6515669822692871,\n",
       "   0.6497083902359009,\n",
       "   0.65254807472229,\n",
       "   0.6555361747741699,\n",
       "   0.6638553142547607,\n",
       "   0.6533048748970032,\n",
       "   0.6528257727622986,\n",
       "   0.6502636075019836,\n",
       "   0.6447920203208923,\n",
       "   0.6448232531547546,\n",
       "   0.6388547420501709,\n",
       "   0.6456086039543152,\n",
       "   0.6450079679489136,\n",
       "   0.64058518409729,\n",
       "   0.6423336863517761,\n",
       "   0.6427935361862183,\n",
       "   0.6317259669303894,\n",
       "   0.6172766089439392,\n",
       "   0.6188341379165649,\n",
       "   0.6083774566650391,\n",
       "   0.6065831184387207,\n",
       "   0.6084948182106018,\n",
       "   0.6033618450164795,\n",
       "   0.6107408404350281,\n",
       "   0.6148940920829773,\n",
       "   0.6268996000289917,\n",
       "   0.6307052969932556,\n",
       "   0.6220496892929077,\n",
       "   0.6204556226730347,\n",
       "   0.6286027431488037,\n",
       "   0.6179875135421753,\n",
       "   0.6140724420547485,\n",
       "   0.6170651316642761,\n",
       "   0.6117260456085205,\n",
       "   0.6216177940368652,\n",
       "   0.6303412914276123,\n",
       "   0.6382338404655457,\n",
       "   0.6398154497146606,\n",
       "   0.6568198204040527,\n",
       "   0.6549902558326721,\n",
       "   0.6579114198684692,\n",
       "   0.6627130508422852,\n",
       "   0.6612719893455505,\n",
       "   0.6665093302726746,\n",
       "   0.6726513504981995,\n",
       "   0.6692899465560913,\n",
       "   0.6614309549331665,\n",
       "   0.6687819361686707,\n",
       "   0.6727411150932312,\n",
       "   0.6652312874794006,\n",
       "   0.6681802272796631,\n",
       "   0.6666210293769836,\n",
       "   0.6651592254638672,\n",
       "   0.663352906703949,\n",
       "   0.6666512489318848,\n",
       "   0.6759722828865051,\n",
       "   0.6703944802284241,\n",
       "   0.6636291146278381,\n",
       "   0.6495142579078674,\n",
       "   0.6531867384910583,\n",
       "   0.6553527116775513,\n",
       "   0.6608007550239563,\n",
       "   0.6605850458145142,\n",
       "   0.6517335772514343,\n",
       "   0.6532800793647766,\n",
       "   0.6556516885757446,\n",
       "   0.6524784564971924,\n",
       "   0.6490877866744995,\n",
       "   0.6475226879119873,\n",
       "   0.6559216380119324,\n",
       "   0.6571706533432007,\n",
       "   0.6610738635063171,\n",
       "   0.6771894693374634,\n",
       "   0.6725447773933411,\n",
       "   0.6715970635414124,\n",
       "   0.6752110719680786,\n",
       "   0.6713131070137024,\n",
       "   0.6696779727935791,\n",
       "   0.6669340133666992,\n",
       "   0.6539889574050903,\n",
       "   0.6479637622833252,\n",
       "   0.6524757146835327,\n",
       "   0.6543542742729187,\n",
       "   0.656344473361969,\n",
       "   0.6495006680488586,\n",
       "   0.6432351469993591,\n",
       "   0.648775041103363,\n",
       "   0.6420459747314453,\n",
       "   0.6308422088623047,\n",
       "   0.6317108869552612,\n",
       "   0.6305521726608276,\n",
       "   0.6237366795539856,\n",
       "   0.6279520988464355,\n",
       "   0.6253714561462402,\n",
       "   0.6333627700805664,\n",
       "   0.6307672262191772,\n",
       "   0.6289986371994019,\n",
       "   0.6344716548919678,\n",
       "   0.621789276599884,\n",
       "   0.6153728365898132,\n",
       "   0.6082656979560852,\n",
       "   0.6088584661483765,\n",
       "   0.6113297939300537,\n",
       "   0.5995157361030579,\n",
       "   0.5843243598937988,\n",
       "   0.5859740376472473,\n",
       "   0.5819160342216492,\n",
       "   0.5896894335746765,\n",
       "   0.590106725692749,\n",
       "   0.6044955849647522,\n",
       "   0.6150258779525757,\n",
       "   0.6216476559638977,\n",
       "   0.6215440630912781,\n",
       "   0.6193423271179199,\n",
       "   0.6220882534980774,\n",
       "   0.6115405559539795,\n",
       "   0.6055013537406921,\n",
       "   0.6105604767799377,\n",
       "   0.6088235974311829,\n",
       "   0.6163299679756165,\n",
       "   0.6177552938461304,\n",
       "   0.6204603910446167,\n",
       "   0.6117584705352783,\n",
       "   0.6166592240333557,\n",
       "   0.6269459128379822,\n",
       "   0.6383516788482666,\n",
       "   0.6457045078277588,\n",
       "   0.6526972055435181,\n",
       "   0.6460174322128296,\n",
       "   0.6455564498901367,\n",
       "   0.6498193740844727,\n",
       "   0.6344470977783203,\n",
       "   0.6390581727027893,\n",
       "   0.6417906284332275,\n",
       "   0.644730269908905,\n",
       "   0.6493914723396301,\n",
       "   0.6512166261672974,\n",
       "   0.656821072101593,\n",
       "   0.6601145267486572,\n",
       "   0.6573482155799866,\n",
       "   0.6552721261978149,\n",
       "   0.6437687873840332,\n",
       "   0.6428653001785278,\n",
       "   0.6355111002922058,\n",
       "   0.6458355188369751,\n",
       "   0.6542969346046448,\n",
       "   0.6543418169021606,\n",
       "   0.6575886607170105,\n",
       "   0.655138373374939,\n",
       "   0.6647692918777466,\n",
       "   0.6603250503540039,\n",
       "   0.6519482731819153,\n",
       "   0.6425982713699341,\n",
       "   0.6335864067077637,\n",
       "   0.6532387733459473,\n",
       "   0.6624591946601868,\n",
       "   0.6565533876419067,\n",
       "   0.6713929772377014,\n",
       "   0.6723616719245911,\n",
       "   0.6751747727394104,\n",
       "   0.6811875104904175,\n",
       "   0.6803107857704163,\n",
       "   0.676213264465332,\n",
       "   0.6858088970184326,\n",
       "   0.680611252784729,\n",
       "   0.6745489835739136,\n",
       "   0.6762996912002563,\n",
       "   0.6776997447013855,\n",
       "   0.6907098889350891,\n",
       "   0.6932592391967773,\n",
       "   0.6888825297355652,\n",
       "   0.6841420531272888,\n",
       "   0.6741457581520081,\n",
       "   0.6733438968658447,\n",
       "   0.6711247563362122,\n",
       "   0.6742162108421326,\n",
       "   0.6728353500366211,\n",
       "   0.6690614819526672,\n",
       "   0.6643005609512329],\n",
       "  'ff_loss': [24.244644165039062,\n",
       "   14.712047576904297,\n",
       "   9.55972671508789,\n",
       "   6.444404602050781,\n",
       "   4.479272365570068,\n",
       "   3.1794443130493164,\n",
       "   2.268115520477295,\n",
       "   1.636184811592102,\n",
       "   1.1684458255767822,\n",
       "   0.8708456754684448,\n",
       "   0.6575227379798889,\n",
       "   0.5028623342514038,\n",
       "   0.3980752229690552,\n",
       "   0.32565680146217346,\n",
       "   0.2700529396533966,\n",
       "   0.24264177680015564,\n",
       "   0.22578448057174683,\n",
       "   0.21585272252559662,\n",
       "   0.2182098925113678,\n",
       "   0.22974735498428345,\n",
       "   0.24069085717201233,\n",
       "   0.25281181931495667,\n",
       "   0.26559609174728394,\n",
       "   0.2789616286754608,\n",
       "   0.2960556447505951,\n",
       "   0.3162282109260559,\n",
       "   0.3416869640350342,\n",
       "   0.36420029401779175,\n",
       "   0.38339394330978394,\n",
       "   0.4021162986755371,\n",
       "   0.4222382605075836,\n",
       "   0.45106619596481323,\n",
       "   0.4668790400028229,\n",
       "   0.48607683181762695,\n",
       "   0.5063618421554565,\n",
       "   0.5263493657112122,\n",
       "   0.5501097440719604,\n",
       "   0.586702287197113,\n",
       "   0.6089223623275757,\n",
       "   0.6320279836654663,\n",
       "   0.6547986268997192,\n",
       "   0.6755855679512024,\n",
       "   0.6912508010864258,\n",
       "   0.6978731155395508,\n",
       "   0.7256256341934204,\n",
       "   0.7366904616355896,\n",
       "   0.7471816539764404,\n",
       "   0.7528083324432373,\n",
       "   0.7706985473632812,\n",
       "   0.7816112041473389,\n",
       "   0.8084660768508911,\n",
       "   0.8266798257827759,\n",
       "   0.8301787376403809,\n",
       "   0.8490343689918518,\n",
       "   0.8690243363380432,\n",
       "   0.880506157875061,\n",
       "   0.8959181308746338,\n",
       "   0.9152321815490723,\n",
       "   0.9274159073829651,\n",
       "   0.9387417435646057,\n",
       "   0.9503024220466614,\n",
       "   0.9608068466186523,\n",
       "   0.9669768810272217,\n",
       "   0.9788672924041748,\n",
       "   0.9947596192359924,\n",
       "   1.0049747228622437,\n",
       "   1.0152511596679688,\n",
       "   1.009250283241272,\n",
       "   1.0152640342712402,\n",
       "   1.025894284248352,\n",
       "   1.029056429862976,\n",
       "   1.0311951637268066,\n",
       "   1.0406367778778076,\n",
       "   1.0455461740493774,\n",
       "   1.051409125328064,\n",
       "   1.0435774326324463,\n",
       "   1.0363450050354004,\n",
       "   1.0581531524658203,\n",
       "   1.0647283792495728,\n",
       "   1.0595626831054688,\n",
       "   1.0566139221191406,\n",
       "   1.050606369972229,\n",
       "   1.0531005859375,\n",
       "   1.0644736289978027,\n",
       "   1.0770936012268066,\n",
       "   1.0692886114120483,\n",
       "   1.0617361068725586,\n",
       "   1.0592625141143799,\n",
       "   1.0460715293884277,\n",
       "   1.0504337549209595,\n",
       "   1.0566414594650269,\n",
       "   1.0524429082870483,\n",
       "   1.0600224733352661,\n",
       "   1.0673516988754272,\n",
       "   1.0815125703811646,\n",
       "   1.079110860824585,\n",
       "   1.0812923908233643,\n",
       "   1.0869578123092651,\n",
       "   1.090247392654419,\n",
       "   1.091623306274414,\n",
       "   1.094089388847351,\n",
       "   1.0842955112457275,\n",
       "   1.0779211521148682,\n",
       "   1.097644567489624,\n",
       "   1.0987844467163086,\n",
       "   1.1061699390411377,\n",
       "   1.1017826795578003,\n",
       "   1.1047728061676025,\n",
       "   1.1053218841552734,\n",
       "   1.0948320627212524,\n",
       "   1.0836883783340454,\n",
       "   1.0776796340942383,\n",
       "   1.0731785297393799,\n",
       "   1.1002463102340698,\n",
       "   1.107021689414978,\n",
       "   1.1107004880905151,\n",
       "   1.1246392726898193,\n",
       "   1.120445728302002,\n",
       "   1.1189515590667725,\n",
       "   1.1291592121124268,\n",
       "   1.1392035484313965,\n",
       "   1.1540796756744385,\n",
       "   1.1585055589675903,\n",
       "   1.1557600498199463,\n",
       "   1.165458083152771,\n",
       "   1.1750253438949585,\n",
       "   1.1682544946670532,\n",
       "   1.176047921180725,\n",
       "   1.1971909999847412,\n",
       "   1.1811015605926514,\n",
       "   1.1705329418182373,\n",
       "   1.197103500366211,\n",
       "   1.1956318616867065,\n",
       "   1.1943562030792236,\n",
       "   1.1788811683654785,\n",
       "   1.1782466173171997,\n",
       "   1.1796817779541016,\n",
       "   1.176316261291504,\n",
       "   1.1595155000686646,\n",
       "   1.1448147296905518,\n",
       "   1.159205436706543,\n",
       "   1.1659057140350342,\n",
       "   1.1586542129516602,\n",
       "   1.1663867235183716,\n",
       "   1.1581168174743652,\n",
       "   1.1514439582824707,\n",
       "   1.1478402614593506,\n",
       "   1.1554898023605347,\n",
       "   1.1587111949920654,\n",
       "   1.1711621284484863,\n",
       "   1.1559864282608032,\n",
       "   1.173201560974121,\n",
       "   1.1813353300094604,\n",
       "   1.2024803161621094,\n",
       "   1.1933996677398682,\n",
       "   1.1994954347610474,\n",
       "   1.2028905153274536,\n",
       "   1.1745167970657349,\n",
       "   1.174924373626709,\n",
       "   1.170483112335205,\n",
       "   1.1629868745803833,\n",
       "   1.1661778688430786,\n",
       "   1.1725399494171143,\n",
       "   1.1798548698425293,\n",
       "   1.1852577924728394,\n",
       "   1.1807724237442017,\n",
       "   1.1768596172332764,\n",
       "   1.1859569549560547,\n",
       "   1.2055469751358032,\n",
       "   1.18504798412323,\n",
       "   1.1886091232299805,\n",
       "   1.1778258085250854,\n",
       "   1.1562933921813965,\n",
       "   1.1606075763702393,\n",
       "   1.1765191555023193,\n",
       "   1.1728744506835938,\n",
       "   1.161203384399414,\n",
       "   1.1761404275894165,\n",
       "   1.1751534938812256,\n",
       "   1.167028546333313,\n",
       "   1.1524553298950195,\n",
       "   1.1531113386154175,\n",
       "   1.144222617149353,\n",
       "   1.1602470874786377,\n",
       "   1.1648619174957275,\n",
       "   1.1714756488800049,\n",
       "   1.1554265022277832,\n",
       "   1.145605206489563,\n",
       "   1.1500558853149414,\n",
       "   1.1462337970733643,\n",
       "   1.1449586153030396,\n",
       "   1.1270819902420044,\n",
       "   1.1332614421844482,\n",
       "   1.16130530834198,\n",
       "   1.1665292978286743,\n",
       "   1.1659473180770874,\n",
       "   1.1676700115203857,\n",
       "   1.1609647274017334,\n",
       "   1.1685603857040405,\n",
       "   1.1736410856246948,\n",
       "   1.1851630210876465,\n",
       "   1.1764086484909058,\n",
       "   1.1985113620758057,\n",
       "   1.2089781761169434,\n",
       "   1.2189624309539795,\n",
       "   1.219943642616272,\n",
       "   1.227474570274353,\n",
       "   1.2419179677963257,\n",
       "   1.2338097095489502,\n",
       "   1.2278659343719482,\n",
       "   1.2134875059127808,\n",
       "   1.2119039297103882,\n",
       "   1.196137547492981,\n",
       "   1.1834737062454224,\n",
       "   1.1728016138076782,\n",
       "   1.1721055507659912,\n",
       "   1.170161247253418,\n",
       "   1.1727771759033203,\n",
       "   1.1772512197494507,\n",
       "   1.1799496412277222,\n",
       "   1.1744552850723267,\n",
       "   1.1806888580322266,\n",
       "   1.1907968521118164,\n",
       "   1.2066688537597656,\n",
       "   1.1872745752334595,\n",
       "   1.2079410552978516,\n",
       "   1.195567011833191,\n",
       "   1.2129383087158203,\n",
       "   1.2003099918365479,\n",
       "   1.2015093564987183,\n",
       "   1.2106550931930542,\n",
       "   1.2095597982406616,\n",
       "   1.2074083089828491,\n",
       "   1.221840500831604,\n",
       "   1.217254400253296,\n",
       "   1.2186610698699951,\n",
       "   1.218448281288147,\n",
       "   1.2184419631958008,\n",
       "   1.2306445837020874,\n",
       "   1.2262943983078003,\n",
       "   1.2328852415084839,\n",
       "   1.2436516284942627,\n",
       "   1.2236639261245728,\n",
       "   1.2031794786453247,\n",
       "   1.2083568572998047,\n",
       "   1.1903488636016846,\n",
       "   1.1899374723434448,\n",
       "   1.193968653678894,\n",
       "   1.2090048789978027,\n",
       "   1.2081060409545898,\n",
       "   1.2168952226638794,\n",
       "   1.2153277397155762,\n",
       "   1.2281286716461182,\n",
       "   1.2300710678100586,\n",
       "   1.2082586288452148,\n",
       "   1.2179104089736938,\n",
       "   1.237072229385376,\n",
       "   1.2110872268676758,\n",
       "   1.210699200630188,\n",
       "   1.1796975135803223,\n",
       "   1.1705743074417114,\n",
       "   1.1692533493041992,\n",
       "   1.1855595111846924,\n",
       "   1.193957805633545,\n",
       "   1.1813416481018066,\n",
       "   1.1718519926071167,\n",
       "   1.1844737529754639,\n",
       "   1.1760057210922241,\n",
       "   1.1883114576339722,\n",
       "   1.2011104822158813,\n",
       "   1.2027021646499634,\n",
       "   1.204506754875183,\n",
       "   1.1980177164077759,\n",
       "   1.188140869140625,\n",
       "   1.172938585281372,\n",
       "   1.1769523620605469,\n",
       "   1.1799615621566772,\n",
       "   1.170838475227356,\n",
       "   1.1716244220733643,\n",
       "   1.176644206047058,\n",
       "   1.1818773746490479,\n",
       "   1.1763330698013306,\n",
       "   1.1821171045303345,\n",
       "   1.1724750995635986,\n",
       "   1.1829712390899658,\n",
       "   1.1889019012451172,\n",
       "   1.1754471063613892,\n",
       "   1.1792930364608765,\n",
       "   1.1541744470596313,\n",
       "   1.1561743021011353,\n",
       "   1.1520967483520508,\n",
       "   1.139790415763855,\n",
       "   1.1389546394348145,\n",
       "   1.128607988357544,\n",
       "   1.1253840923309326,\n",
       "   1.1134589910507202,\n",
       "   1.1343539953231812,\n",
       "   1.1176502704620361,\n",
       "   1.1300902366638184,\n",
       "   1.1374224424362183,\n",
       "   1.1399874687194824,\n",
       "   1.1409014463424683,\n",
       "   1.1423076391220093,\n",
       "   1.134857177734375,\n",
       "   1.1243319511413574,\n",
       "   1.1396560668945312,\n",
       "   1.131466269493103,\n",
       "   1.1222978830337524,\n",
       "   1.1214938163757324,\n",
       "   1.1110364198684692,\n",
       "   1.117140769958496,\n",
       "   1.1138969659805298,\n",
       "   1.125040888786316,\n",
       "   1.1242510080337524,\n",
       "   1.1184381246566772,\n",
       "   1.1278260946273804,\n",
       "   1.121819019317627,\n",
       "   1.1216404438018799,\n",
       "   1.1035430431365967,\n",
       "   1.1190941333770752,\n",
       "   1.1074539422988892,\n",
       "   1.1064220666885376,\n",
       "   1.1071445941925049,\n",
       "   1.0921074151992798,\n",
       "   1.080691933631897,\n",
       "   1.0686196088790894,\n",
       "   1.0557810068130493,\n",
       "   1.061330795288086,\n",
       "   1.0492184162139893,\n",
       "   1.0622196197509766,\n",
       "   1.0665870904922485,\n",
       "   1.0630861520767212,\n",
       "   1.068345546722412,\n",
       "   1.0721248388290405,\n",
       "   1.081834077835083,\n",
       "   1.0807175636291504,\n",
       "   1.0779931545257568,\n",
       "   1.0644296407699585,\n",
       "   1.067582368850708,\n",
       "   1.0493431091308594,\n",
       "   1.0616374015808105,\n",
       "   1.081696629524231,\n",
       "   1.0893412828445435,\n",
       "   1.1036295890808105,\n",
       "   1.0949902534484863,\n",
       "   1.0996017456054688,\n",
       "   1.0841583013534546,\n",
       "   1.0849937200546265,\n",
       "   1.0675238370895386,\n",
       "   1.0721837282180786,\n",
       "   1.0677869319915771,\n",
       "   1.0678144693374634,\n",
       "   1.0704997777938843,\n",
       "   1.0668996572494507,\n",
       "   1.055397868156433,\n",
       "   1.0624635219573975,\n",
       "   1.0552726984024048,\n",
       "   1.0479758977890015,\n",
       "   1.0430271625518799,\n",
       "   1.053430438041687,\n",
       "   1.0413661003112793,\n",
       "   1.0257240533828735,\n",
       "   1.0069760084152222,\n",
       "   0.9906576871871948,\n",
       "   0.9967333674430847,\n",
       "   0.995317816734314,\n",
       "   1.015987753868103,\n",
       "   1.0377038717269897,\n",
       "   1.0413790941238403,\n",
       "   1.0541921854019165,\n",
       "   1.0774586200714111,\n",
       "   1.0839800834655762,\n",
       "   1.0864663124084473,\n",
       "   1.0775597095489502,\n",
       "   1.05266535282135,\n",
       "   1.0425220727920532,\n",
       "   1.0396908521652222,\n",
       "   1.030548095703125,\n",
       "   1.0382319688796997,\n",
       "   1.0326741933822632,\n",
       "   1.0240176916122437,\n",
       "   0.9934821128845215,\n",
       "   0.9866060018539429,\n",
       "   0.9985100030899048,\n",
       "   0.997787356376648,\n",
       "   1.0208379030227661,\n",
       "   1.007466197013855,\n",
       "   1.0007038116455078,\n",
       "   1.00356924533844,\n",
       "   0.9999540448188782,\n",
       "   0.9890400767326355,\n",
       "   0.9925563931465149,\n",
       "   0.9924038052558899,\n",
       "   0.9921544790267944,\n",
       "   0.9937888383865356,\n",
       "   0.9884845614433289,\n",
       "   0.9863404035568237,\n",
       "   0.9965631365776062,\n",
       "   1.0065850019454956,\n",
       "   1.0188788175582886,\n",
       "   1.0375677347183228,\n",
       "   1.0375010967254639,\n",
       "   1.0456140041351318,\n",
       "   1.0263469219207764,\n",
       "   1.0342819690704346,\n",
       "   1.0208574533462524,\n",
       "   1.0285776853561401,\n",
       "   1.0261027812957764,\n",
       "   1.0027196407318115,\n",
       "   1.0006458759307861,\n",
       "   0.9864373207092285,\n",
       "   0.969506561756134,\n",
       "   0.9762957692146301,\n",
       "   0.9910693764686584,\n",
       "   0.9877094626426697,\n",
       "   0.9801087379455566,\n",
       "   0.9732585549354553,\n",
       "   0.9728713035583496,\n",
       "   0.9733311533927917,\n",
       "   0.969318687915802,\n",
       "   0.9678199291229248,\n",
       "   0.9713342785835266,\n",
       "   0.9692866206169128,\n",
       "   0.964805006980896,\n",
       "   0.955930233001709,\n",
       "   0.9520227313041687,\n",
       "   0.9661058187484741,\n",
       "   0.9649167656898499,\n",
       "   0.9577060341835022,\n",
       "   0.9495006203651428,\n",
       "   0.9446982145309448,\n",
       "   0.9541282653808594,\n",
       "   0.9602272510528564,\n",
       "   0.9567838311195374,\n",
       "   0.964015543460846,\n",
       "   0.9661852717399597,\n",
       "   0.9470415115356445,\n",
       "   0.9471338391304016,\n",
       "   0.9434942007064819,\n",
       "   0.9420881271362305,\n",
       "   0.9506047368049622,\n",
       "   0.9462445974349976,\n",
       "   0.9335998296737671,\n",
       "   0.9421382546424866,\n",
       "   0.9556580781936646,\n",
       "   0.9604980945587158,\n",
       "   0.9720820784568787,\n",
       "   0.9866904020309448,\n",
       "   0.9886996150016785,\n",
       "   0.9703450798988342,\n",
       "   0.9780851006507874,\n",
       "   0.9783832430839539,\n",
       "   0.9653847217559814,\n",
       "   0.9663149118423462,\n",
       "   0.9660267233848572,\n",
       "   0.9769691228866577,\n",
       "   0.9557660222053528,\n",
       "   0.940569281578064,\n",
       "   0.9396528005599976,\n",
       "   0.953984260559082,\n",
       "   0.9366613626480103,\n",
       "   0.9510147571563721,\n",
       "   0.9504393339157104,\n",
       "   0.9530146718025208,\n",
       "   0.9444460272789001,\n",
       "   0.9341195821762085,\n",
       "   0.9353246092796326,\n",
       "   0.9290500283241272,\n",
       "   0.9421471357345581,\n",
       "   0.9351288080215454,\n",
       "   0.9500536322593689,\n",
       "   0.9651643633842468,\n",
       "   0.9887753129005432,\n",
       "   0.992902398109436,\n",
       "   0.9877501726150513,\n",
       "   0.9886053204536438,\n",
       "   1.0053547620773315,\n",
       "   1.0182280540466309,\n",
       "   1.010463833808899,\n",
       "   1.0018306970596313,\n",
       "   0.998840868473053,\n",
       "   0.9949921369552612,\n",
       "   0.9625816345214844,\n",
       "   0.9747372269630432,\n",
       "   0.9664698243141174,\n",
       "   0.9573876857757568,\n",
       "   0.9471120834350586,\n",
       "   0.970382571220398,\n",
       "   0.9445887207984924,\n",
       "   0.9409536123275757,\n",
       "   0.9473878741264343,\n",
       "   0.9557174444198608,\n",
       "   0.9417456388473511,\n",
       "   0.9467347860336304,\n",
       "   0.9560098648071289,\n",
       "   0.9531182050704956,\n",
       "   0.9425244927406311,\n",
       "   0.9416451454162598,\n",
       "   0.9274721145629883,\n",
       "   0.9217626452445984],\n",
       "  'test_losses': [24.801213147118688,\n",
       "   15.468417018651962,\n",
       "   10.465998351573944,\n",
       "   7.381906256079674,\n",
       "   5.542551673948765,\n",
       "   4.3580256924033165,\n",
       "   3.5064821913838387,\n",
       "   2.9225675463676453,\n",
       "   2.5194453671574593,\n",
       "   2.2798596397042274,\n",
       "   2.095725931227207,\n",
       "   1.9297411665320396,\n",
       "   1.7839208543300629,\n",
       "   1.7013660222291946,\n",
       "   1.6138365492224693,\n",
       "   1.5833549797534943,\n",
       "   1.5696205496788025,\n",
       "   1.552189439535141,\n",
       "   1.535684272646904,\n",
       "   1.541865013539791,\n",
       "   1.5479472056031227,\n",
       "   1.5679092705249786,\n",
       "   1.5847156718373299,\n",
       "   1.593692570924759,\n",
       "   1.612626202404499,\n",
       "   1.6353050842881203,\n",
       "   1.670793741941452,\n",
       "   1.688173420727253,\n",
       "   1.7173100486397743,\n",
       "   1.7313667833805084,\n",
       "   1.7456403449177742,\n",
       "   1.7680988237261772,\n",
       "   1.780588760972023,\n",
       "   1.786813847720623,\n",
       "   1.8073921725153923,\n",
       "   1.8254202604293823,\n",
       "   1.8596359863877296,\n",
       "   1.8983326107263565,\n",
       "   1.9236158952116966,\n",
       "   1.9447580352425575,\n",
       "   1.959445245563984,\n",
       "   1.9923604801297188,\n",
       "   2.01503624022007,\n",
       "   2.0194185823202133,\n",
       "   2.0357297509908676,\n",
       "   2.0380683168768883,\n",
       "   2.042251653969288,\n",
       "   2.038348227739334,\n",
       "   2.0513343811035156,\n",
       "   2.0546059012413025,\n",
       "   2.082154504954815,\n",
       "   2.084861993789673,\n",
       "   2.0956039652228355,\n",
       "   2.0971327424049377,\n",
       "   2.112069323658943,\n",
       "   2.115589275956154,\n",
       "   2.123307704925537,\n",
       "   2.147699259221554,\n",
       "   2.1387490406632423,\n",
       "   2.134437546133995,\n",
       "   2.146448716521263,\n",
       "   2.148998036980629,\n",
       "   2.1546006873250008,\n",
       "   2.158070094883442,\n",
       "   2.1669404804706573,\n",
       "   2.1692292541265488,\n",
       "   2.173307701945305,\n",
       "   2.161736287176609,\n",
       "   2.1828903444111347,\n",
       "   2.176788631826639,\n",
       "   2.164923034608364,\n",
       "   2.163257375359535,\n",
       "   2.157048638910055,\n",
       "   2.1648699045181274,\n",
       "   2.115574859082699,\n",
       "   2.1113302558660507,\n",
       "   2.0951440036296844,\n",
       "   2.1230330280959606,\n",
       "   2.136626645922661,\n",
       "   2.1095749102532864,\n",
       "   2.1113274432718754,\n",
       "   2.104787301272154,\n",
       "   2.1162094436585903,\n",
       "   2.124780334532261,\n",
       "   2.1371257677674294,\n",
       "   2.1436440125107765,\n",
       "   2.147299900650978,\n",
       "   2.1367746368050575,\n",
       "   2.1077310107648373,\n",
       "   2.104232743382454,\n",
       "   2.127920888364315,\n",
       "   2.1333778500556946,\n",
       "   2.129062809050083,\n",
       "   2.137053433805704,\n",
       "   2.145947862416506,\n",
       "   2.142116319388151,\n",
       "   2.1340630874037743,\n",
       "   2.159392263740301,\n",
       "   2.1524969078600407,\n",
       "   2.157456275075674,\n",
       "   2.1560225188732147,\n",
       "   2.142887108027935,\n",
       "   2.131492454558611,\n",
       "   2.1566217951476574,\n",
       "   2.1411097683012486,\n",
       "   2.121866215020418,\n",
       "   2.1244236156344414,\n",
       "   2.119312807917595,\n",
       "   2.139468677341938,\n",
       "   2.130818046629429,\n",
       "   2.1188951954245567,\n",
       "   2.116364311426878,\n",
       "   2.1290599144995213,\n",
       "   2.1658597514033318,\n",
       "   2.182578779757023,\n",
       "   2.192713290452957,\n",
       "   2.22313267365098,\n",
       "   2.2092039212584496,\n",
       "   2.208222310990095,\n",
       "   2.1988906376063824,\n",
       "   2.2098892852663994,\n",
       "   2.2174533121287823,\n",
       "   2.22290563210845,\n",
       "   2.2303741425275803,\n",
       "   2.23453738540411,\n",
       "   2.2342066131532192,\n",
       "   2.238296043127775,\n",
       "   2.2323178611695766,\n",
       "   2.2668916657567024,\n",
       "   2.263771517202258,\n",
       "   2.260743711143732,\n",
       "   2.308789264410734,\n",
       "   2.296064954251051,\n",
       "   2.268503066152334,\n",
       "   2.254013631492853,\n",
       "   2.255415938794613,\n",
       "   2.2761209830641747,\n",
       "   2.2792703174054623,\n",
       "   2.2516791000962257,\n",
       "   2.247733473777771,\n",
       "   2.269642449915409,\n",
       "   2.2670887857675552,\n",
       "   2.275647174566984,\n",
       "   2.280338402837515,\n",
       "   2.267519950866699,\n",
       "   2.2684538140892982,\n",
       "   2.2526273615658283,\n",
       "   2.2534760907292366,\n",
       "   2.2550887651741505,\n",
       "   2.268890358507633,\n",
       "   2.2443171814084053,\n",
       "   2.2508356645703316,\n",
       "   2.2506222799420357,\n",
       "   2.2871247604489326,\n",
       "   2.280238315463066,\n",
       "   2.286320246756077,\n",
       "   2.284731026738882,\n",
       "   2.258401956409216,\n",
       "   2.268660169094801,\n",
       "   2.2621111385524273,\n",
       "   2.2616259939968586,\n",
       "   2.2577175311744213,\n",
       "   2.2655351646244526,\n",
       "   2.275675594806671,\n",
       "   2.2895970456302166,\n",
       "   2.2896453477442265,\n",
       "   2.277200449258089,\n",
       "   2.287795227020979,\n",
       "   2.3101221285760403,\n",
       "   2.2875716611742973,\n",
       "   2.2804372534155846,\n",
       "   2.273724999278784,\n",
       "   2.245739508420229,\n",
       "   2.235437083989382,\n",
       "   2.264349300414324,\n",
       "   2.2691854014992714,\n",
       "   2.2513791881501675,\n",
       "   2.2850220166146755,\n",
       "   2.290653247386217,\n",
       "   2.2790794782340527,\n",
       "   2.271731738001108,\n",
       "   2.269134785979986,\n",
       "   2.267334908246994,\n",
       "   2.2820879593491554,\n",
       "   2.2859898395836353,\n",
       "   2.2990604527294636,\n",
       "   2.27162454277277,\n",
       "   2.2580202519893646,\n",
       "   2.2549932301044464,\n",
       "   2.240259386599064,\n",
       "   2.238919422030449,\n",
       "   2.236237157136202,\n",
       "   2.2511524371802807,\n",
       "   2.2713064067065716,\n",
       "   2.2786210253834724,\n",
       "   2.2584850415587425,\n",
       "   2.2565401531755924,\n",
       "   2.2558963149785995,\n",
       "   2.2721406295895576,\n",
       "   2.2832172363996506,\n",
       "   2.3074370697140694,\n",
       "   2.3008530251681805,\n",
       "   2.321807987987995,\n",
       "   2.3334953039884567,\n",
       "   2.3157204538583755,\n",
       "   2.303865909576416,\n",
       "   2.2898108698427677,\n",
       "   2.305093977600336,\n",
       "   2.30430331453681,\n",
       "   2.267445784062147,\n",
       "   2.25067350640893,\n",
       "   2.2640835642814636,\n",
       "   2.250786427408457,\n",
       "   2.2376447431743145,\n",
       "   2.2328287065029144,\n",
       "   2.203753087669611,\n",
       "   2.2034543193876743,\n",
       "   2.22329318523407,\n",
       "   2.227709673345089,\n",
       "   2.2391037456691265,\n",
       "   2.2292059548199177,\n",
       "   2.2470423579216003,\n",
       "   2.2570347152650356,\n",
       "   2.290886741131544,\n",
       "   2.296501748263836,\n",
       "   2.3362156748771667,\n",
       "   2.310718234628439,\n",
       "   2.3466332480311394,\n",
       "   2.329502023756504,\n",
       "   2.3273368924856186,\n",
       "   2.325329314917326,\n",
       "   2.33026834577322,\n",
       "   2.3078274093568325,\n",
       "   2.3282084465026855,\n",
       "   2.325630512088537,\n",
       "   2.3093509562313557,\n",
       "   2.322210591286421,\n",
       "   2.3151433058083057,\n",
       "   2.334036450833082,\n",
       "   2.339688580483198,\n",
       "   2.335385989397764,\n",
       "   2.3493506722152233,\n",
       "   2.3296540156006813,\n",
       "   2.308070443570614,\n",
       "   2.3148486390709877,\n",
       "   2.3214558735489845,\n",
       "   2.337787140160799,\n",
       "   2.3485725447535515,\n",
       "   2.375852756202221,\n",
       "   2.36786687374115,\n",
       "   2.3840345107018948,\n",
       "   2.381795410066843,\n",
       "   2.3865028508007526,\n",
       "   2.391851995140314,\n",
       "   2.352031599730253,\n",
       "   2.36017157509923,\n",
       "   2.3879612535238266,\n",
       "   2.364227119833231,\n",
       "   2.3607506677508354,\n",
       "   2.3284586407244205,\n",
       "   2.3104499988257885,\n",
       "   2.310515273362398,\n",
       "   2.3298663794994354,\n",
       "   2.341834083199501,\n",
       "   2.3327759727835655,\n",
       "   2.326096583157778,\n",
       "   2.3378791101276875,\n",
       "   2.285691723227501,\n",
       "   2.290064536035061,\n",
       "   2.283146344125271,\n",
       "   2.2957130186259747,\n",
       "   2.2832328379154205,\n",
       "   2.284130036830902,\n",
       "   2.2865149788558483,\n",
       "   2.266286164522171,\n",
       "   2.2562602534890175,\n",
       "   2.2690883576869965,\n",
       "   2.2650250792503357,\n",
       "   2.273975782096386,\n",
       "   2.278236173093319,\n",
       "   2.297189462929964,\n",
       "   2.292471706867218,\n",
       "   2.3280238024890423,\n",
       "   2.31620929017663,\n",
       "   2.321052171289921,\n",
       "   2.3298725485801697,\n",
       "   2.295920841395855,\n",
       "   2.311821110546589,\n",
       "   2.3053431883454323,\n",
       "   2.3197123408317566,\n",
       "   2.293653517961502,\n",
       "   2.290474910289049,\n",
       "   2.2863279916346073,\n",
       "   2.2422590479254723,\n",
       "   2.2495179288089275,\n",
       "   2.242645785212517,\n",
       "   2.284547172486782,\n",
       "   2.3005048260092735,\n",
       "   2.3341202922165394,\n",
       "   2.346628960222006,\n",
       "   2.353358533233404,\n",
       "   2.3574313297867775,\n",
       "   2.35629366338253,\n",
       "   2.342338487505913,\n",
       "   2.330342072993517,\n",
       "   2.337061021476984,\n",
       "   2.3308270685374737,\n",
       "   2.324243377894163,\n",
       "   2.3162518180906773,\n",
       "   2.2940873876214027,\n",
       "   2.29950962215662,\n",
       "   2.3070755675435066,\n",
       "   2.311474420130253,\n",
       "   2.3105772621929646,\n",
       "   2.300514716655016,\n",
       "   2.3233566395938396,\n",
       "   2.3079164028167725,\n",
       "   2.3117568530142307,\n",
       "   2.2996121793985367,\n",
       "   2.3201667964458466,\n",
       "   2.3133148811757565,\n",
       "   2.311055611819029,\n",
       "   2.313710253685713,\n",
       "   2.2991727106273174,\n",
       "   2.2707986384630203,\n",
       "   2.2826180569827557,\n",
       "   2.2693473920226097,\n",
       "   2.2782894745469093,\n",
       "   2.2568005695939064,\n",
       "   2.265416990965605,\n",
       "   2.2764361947774887,\n",
       "   2.264982543885708,\n",
       "   2.279908861964941,\n",
       "   2.279718317091465,\n",
       "   2.2880359068512917,\n",
       "   2.2804528437554836,\n",
       "   2.2773623280227184,\n",
       "   2.2545621022582054,\n",
       "   2.2380679212510586,\n",
       "   2.2198641672730446,\n",
       "   2.223142970353365,\n",
       "   2.247646301984787,\n",
       "   2.2597177363932133,\n",
       "   2.2486448995769024,\n",
       "   2.2499059662222862,\n",
       "   2.2557584531605244,\n",
       "   2.243731487542391,\n",
       "   2.2433298118412495,\n",
       "   2.227252658456564,\n",
       "   2.2440990805625916,\n",
       "   2.2633289098739624,\n",
       "   2.254137262701988,\n",
       "   2.251170303672552,\n",
       "   2.2498564533889294,\n",
       "   2.231952928006649,\n",
       "   2.2450606860220432,\n",
       "   2.244527254253626,\n",
       "   2.247671280056238,\n",
       "   2.2432473339140415,\n",
       "   2.2686596997082233,\n",
       "   2.255252158269286,\n",
       "   2.2423019483685493,\n",
       "   2.225728400051594,\n",
       "   2.2053499836474657,\n",
       "   2.2149684485048056,\n",
       "   2.214643906801939,\n",
       "   2.2340233512222767,\n",
       "   2.2482328079640865,\n",
       "   2.2600738368928432,\n",
       "   2.28207116574049,\n",
       "   2.3026205711066723,\n",
       "   2.3147329799830914,\n",
       "   2.316841132938862,\n",
       "   2.3102582469582558,\n",
       "   2.2890714034438133,\n",
       "   2.283818956464529,\n",
       "   2.291531704366207,\n",
       "   2.276849802583456,\n",
       "   2.276331901550293,\n",
       "   2.2544012553989887,\n",
       "   2.2250092774629593,\n",
       "   2.193110439926386,\n",
       "   2.1878834292292595,\n",
       "   2.2025888822972775,\n",
       "   2.1717656776309013,\n",
       "   2.1913782730698586,\n",
       "   2.1625677943229675,\n",
       "   2.1707880832254887,\n",
       "   2.162731632590294,\n",
       "   2.1617039777338505,\n",
       "   2.1548140309751034,\n",
       "   2.1707563288509846,\n",
       "   2.150618899613619,\n",
       "   2.177785012871027,\n",
       "   2.1728890240192413,\n",
       "   2.1581322848796844,\n",
       "   2.1762633807957172,\n",
       "   2.157702784985304,\n",
       "   2.165955524891615,\n",
       "   2.1833054311573505,\n",
       "   2.198761183768511,\n",
       "   2.20561533421278,\n",
       "   2.241905391216278,\n",
       "   2.221067000180483,\n",
       "   2.2483902648091316,\n",
       "   2.225950326770544,\n",
       "   2.2135586366057396,\n",
       "   2.217097844928503,\n",
       "   2.1783622913062572,\n",
       "   2.1689162887632847,\n",
       "   2.141911644488573,\n",
       "   2.1155917681753635,\n",
       "   2.1124747283756733,\n",
       "   2.126637265086174,\n",
       "   2.116154033690691,\n",
       "   2.083431601524353,\n",
       "   2.085478823632002,\n",
       "   2.0850149877369404,\n",
       "   2.090428028255701,\n",
       "   2.0742027275264263,\n",
       "   2.070878103375435,\n",
       "   2.0688405372202396,\n",
       "   2.0662754848599434,\n",
       "   2.0700842700898647,\n",
       "   2.0310403779149055,\n",
       "   1.9997038468718529,\n",
       "   2.0138998106122017,\n",
       "   2.0122545771300793,\n",
       "   2.0330208390951157,\n",
       "   2.037612784653902,\n",
       "   2.043690226972103,\n",
       "   2.049091562628746,\n",
       "   2.057247616350651,\n",
       "   2.0559631660580635,\n",
       "   2.0742557011544704,\n",
       "   2.086715754121542,\n",
       "   2.0679531134665012,\n",
       "   2.0668780356645584,\n",
       "   2.08198843896389,\n",
       "   2.0690964609384537,\n",
       "   2.0657933354377747,\n",
       "   2.0648139528930187,\n",
       "   2.0616906955838203,\n",
       "   2.0766029953956604,\n",
       "   2.096216943114996,\n",
       "   2.111780233681202,\n",
       "   2.1246492750942707,\n",
       "   2.1250306256115437,\n",
       "   2.1476157158613205,\n",
       "   2.118443302810192,\n",
       "   2.1167035065591335,\n",
       "   2.131549496203661,\n",
       "   2.0818860456347466,\n",
       "   2.100790049880743,\n",
       "   2.104156117886305,\n",
       "   2.117350000888109,\n",
       "   2.096826184540987,\n",
       "   2.0930380895733833,\n",
       "   2.078923676162958,\n",
       "   2.1029276214540005,\n",
       "   2.1030871756374836,\n",
       "   2.098054401576519,\n",
       "   2.0951537154614925,\n",
       "   2.075786240398884,\n",
       "   2.078673556447029,\n",
       "   2.057326517999172,\n",
       "   2.0702325887978077,\n",
       "   2.0751644782721996,\n",
       "   2.073911391198635,\n",
       "   2.05062510445714,\n",
       "   2.072489570826292,\n",
       "   2.0818228349089622,\n",
       "   2.105566367506981,\n",
       "   2.0857293978333473,\n",
       "   2.083135947585106,\n",
       "   2.114712618291378,\n",
       "   2.1526127718389034,\n",
       "   2.149753749370575,\n",
       "   2.1601732783019543,\n",
       "   2.1496722400188446,\n",
       "   2.1617571003735065,\n",
       "   2.1700656600296497,\n",
       "   2.142884012311697,\n",
       "   2.138849649578333,\n",
       "   2.138080671429634,\n",
       "   2.12964278832078,\n",
       "   2.1047060675919056,\n",
       "   2.1120356246829033,\n",
       "   2.073811963200569,\n",
       "   2.0921452194452286,\n",
       "   2.098713867366314,\n",
       "   2.1023155450820923,\n",
       "   2.0851346254348755,\n",
       "   2.0744086652994156,\n",
       "   2.0970544032752514,\n",
       "   2.104451458901167,\n",
       "   2.098968144506216,\n",
       "   2.0842821300029755,\n",
       "   2.0680769979953766,\n",
       "   2.078571755439043],\n",
       "  'pce_acc': [1442.0770263671875,\n",
       "   1439.868408203125,\n",
       "   1440.7259521484375,\n",
       "   1433.288818359375,\n",
       "   1441.464599609375,\n",
       "   1446.868408203125,\n",
       "   1449.507080078125,\n",
       "   1452.578857421875,\n",
       "   1451.493408203125,\n",
       "   1452.533447265625,\n",
       "   1453.7706298828125,\n",
       "   1451.4046630859375,\n",
       "   1446.19091796875,\n",
       "   1442.3509521484375,\n",
       "   1438.094482421875,\n",
       "   1435.580810546875,\n",
       "   1433.9674072265625,\n",
       "   1432.5980224609375,\n",
       "   1430.7120361328125,\n",
       "   1428.7833251953125,\n",
       "   1428.144775390625,\n",
       "   1427.0430908203125,\n",
       "   1427.513671875,\n",
       "   1427.9979248046875,\n",
       "   1425.6690673828125,\n",
       "   1426.379150390625,\n",
       "   1424.8602294921875,\n",
       "   1423.539794921875,\n",
       "   1424.8607177734375,\n",
       "   1425.1519775390625,\n",
       "   1424.9949951171875,\n",
       "   1423.84912109375,\n",
       "   1423.7235107421875,\n",
       "   1423.3387451171875,\n",
       "   1422.7266845703125,\n",
       "   1420.831298828125,\n",
       "   1420.2254638671875,\n",
       "   1421.41748046875,\n",
       "   1422.45166015625,\n",
       "   1423.1971435546875,\n",
       "   1427.0535888671875,\n",
       "   1428.153076171875,\n",
       "   1427.5672607421875,\n",
       "   1428.4105224609375,\n",
       "   1428.081298828125,\n",
       "   1428.596923828125,\n",
       "   1429.1339111328125,\n",
       "   1430.803466796875,\n",
       "   1428.835693359375,\n",
       "   1428.7008056640625,\n",
       "   1427.8001708984375,\n",
       "   1429.144775390625,\n",
       "   1428.4970703125,\n",
       "   1426.507568359375,\n",
       "   1426.71044921875,\n",
       "   1426.39404296875,\n",
       "   1425.5714111328125,\n",
       "   1428.3101806640625,\n",
       "   1426.690185546875,\n",
       "   1425.644287109375,\n",
       "   1427.819580078125,\n",
       "   1425.4970703125,\n",
       "   1426.5245361328125,\n",
       "   1425.05810546875,\n",
       "   1422.1171875,\n",
       "   1420.5543212890625,\n",
       "   1419.0330810546875,\n",
       "   1418.9197998046875,\n",
       "   1417.1915283203125,\n",
       "   1418.4149169921875,\n",
       "   1416.6143798828125,\n",
       "   1414.1888427734375,\n",
       "   1413.4012451171875,\n",
       "   1412.908935546875,\n",
       "   1414.684814453125,\n",
       "   1414.681396484375,\n",
       "   1412.5406494140625,\n",
       "   1412.1134033203125,\n",
       "   1408.66943359375,\n",
       "   1409.1048583984375,\n",
       "   1409.7947998046875,\n",
       "   1411.3602294921875,\n",
       "   1410.5291748046875,\n",
       "   1410.375732421875,\n",
       "   1409.7938232421875,\n",
       "   1408.7083740234375,\n",
       "   1410.9990234375,\n",
       "   1410.8748779296875,\n",
       "   1408.4832763671875,\n",
       "   1408.8890380859375,\n",
       "   1409.4296875,\n",
       "   1410.8450927734375,\n",
       "   1409.8858642578125,\n",
       "   1409.0142822265625,\n",
       "   1407.40771484375,\n",
       "   1407.722900390625,\n",
       "   1407.2412109375,\n",
       "   1408.47509765625,\n",
       "   1409.1982421875,\n",
       "   1408.8045654296875,\n",
       "   1407.5528564453125,\n",
       "   1406.278076171875,\n",
       "   1405.2158203125,\n",
       "   1405.615478515625,\n",
       "   1404.6185302734375,\n",
       "   1406.428466796875,\n",
       "   1404.87158203125,\n",
       "   1403.4007568359375,\n",
       "   1405.2998046875,\n",
       "   1404.85205078125,\n",
       "   1403.7020263671875,\n",
       "   1404.2403564453125,\n",
       "   1405.4886474609375,\n",
       "   1405.8486328125,\n",
       "   1406.9906005859375,\n",
       "   1406.986572265625,\n",
       "   1407.67822265625,\n",
       "   1407.5419921875,\n",
       "   1406.689453125,\n",
       "   1407.583984375,\n",
       "   1407.98193359375,\n",
       "   1407.4652099609375,\n",
       "   1406.7841796875,\n",
       "   1407.8870849609375,\n",
       "   1406.5623779296875,\n",
       "   1405.521728515625,\n",
       "   1403.1038818359375,\n",
       "   1402.2784423828125,\n",
       "   1401.1668701171875,\n",
       "   1398.635498046875,\n",
       "   1401.7171630859375,\n",
       "   1402.715576171875,\n",
       "   1402.7015380859375,\n",
       "   1402.077880859375,\n",
       "   1403.4222412109375,\n",
       "   1401.486572265625,\n",
       "   1400.9864501953125,\n",
       "   1401.57275390625,\n",
       "   1401.562255859375,\n",
       "   1401.1904296875,\n",
       "   1403.0904541015625,\n",
       "   1405.0916748046875,\n",
       "   1404.0848388671875,\n",
       "   1404.7103271484375,\n",
       "   1404.292724609375,\n",
       "   1404.7156982421875,\n",
       "   1403.94091796875,\n",
       "   1401.6094970703125,\n",
       "   1400.68359375,\n",
       "   1402.1207275390625,\n",
       "   1400.6370849609375,\n",
       "   1399.4971923828125,\n",
       "   1401.0814208984375,\n",
       "   1402.0892333984375,\n",
       "   1402.6138916015625,\n",
       "   1402.5565185546875,\n",
       "   1403.1968994140625,\n",
       "   1405.5010986328125,\n",
       "   1405.290771484375,\n",
       "   1404.0638427734375,\n",
       "   1406.02734375,\n",
       "   1405.1031494140625,\n",
       "   1405.338623046875,\n",
       "   1407.568359375,\n",
       "   1406.757080078125,\n",
       "   1406.591552734375,\n",
       "   1403.95361328125,\n",
       "   1404.7978515625,\n",
       "   1406.1170654296875,\n",
       "   1409.08056640625,\n",
       "   1407.2105712890625,\n",
       "   1405.39501953125,\n",
       "   1404.8377685546875,\n",
       "   1405.0733642578125,\n",
       "   1406.0322265625,\n",
       "   1402.9891357421875,\n",
       "   1403.900390625,\n",
       "   1402.693115234375,\n",
       "   1405.798095703125,\n",
       "   1405.3909912109375,\n",
       "   1407.636474609375,\n",
       "   1406.9407958984375,\n",
       "   1406.626953125,\n",
       "   1405.35693359375,\n",
       "   1406.9373779296875,\n",
       "   1405.9105224609375,\n",
       "   1404.5439453125,\n",
       "   1403.0576171875,\n",
       "   1403.8765869140625,\n",
       "   1403.4407958984375,\n",
       "   1403.6109619140625,\n",
       "   1402.0460205078125,\n",
       "   1404.313720703125,\n",
       "   1405.8321533203125,\n",
       "   1405.7894287109375,\n",
       "   1404.6942138671875,\n",
       "   1402.22119140625,\n",
       "   1401.2593994140625,\n",
       "   1405.374755859375,\n",
       "   1406.7197265625,\n",
       "   1406.353515625,\n",
       "   1404.5052490234375,\n",
       "   1405.5531005859375,\n",
       "   1406.5072021484375,\n",
       "   1405.4266357421875,\n",
       "   1404.646728515625,\n",
       "   1406.0069580078125,\n",
       "   1408.406494140625,\n",
       "   1408.7608642578125,\n",
       "   1408.572265625,\n",
       "   1407.74169921875,\n",
       "   1406.9832763671875,\n",
       "   1408.739990234375,\n",
       "   1407.1082763671875,\n",
       "   1407.7801513671875,\n",
       "   1408.8848876953125,\n",
       "   1409.12890625,\n",
       "   1410.452880859375,\n",
       "   1409.7127685546875,\n",
       "   1409.867919921875,\n",
       "   1411.2176513671875,\n",
       "   1411.31884765625,\n",
       "   1411.0521240234375,\n",
       "   1410.956787109375,\n",
       "   1411.84716796875,\n",
       "   1411.3392333984375,\n",
       "   1414.2532958984375,\n",
       "   1414.5582275390625,\n",
       "   1413.1474609375,\n",
       "   1412.5982666015625,\n",
       "   1412.389892578125,\n",
       "   1413.0943603515625,\n",
       "   1414.63623046875,\n",
       "   1413.928466796875,\n",
       "   1413.2802734375,\n",
       "   1413.2554931640625,\n",
       "   1414.203369140625,\n",
       "   1416.51904296875,\n",
       "   1414.4488525390625,\n",
       "   1414.9132080078125,\n",
       "   1415.0797119140625,\n",
       "   1415.5904541015625,\n",
       "   1417.9246826171875,\n",
       "   1417.6094970703125,\n",
       "   1415.9334716796875,\n",
       "   1416.44873046875,\n",
       "   1415.9686279296875,\n",
       "   1415.9403076171875,\n",
       "   1416.5147705078125,\n",
       "   1417.0511474609375,\n",
       "   1415.270751953125,\n",
       "   1415.06201171875,\n",
       "   1413.3743896484375,\n",
       "   1411.2928466796875,\n",
       "   1412.8582763671875,\n",
       "   1411.4718017578125,\n",
       "   1413.481201171875,\n",
       "   1413.4171142578125,\n",
       "   1412.9622802734375,\n",
       "   1414.3106689453125,\n",
       "   1414.665283203125,\n",
       "   1414.250732421875,\n",
       "   1411.9825439453125,\n",
       "   1413.56396484375,\n",
       "   1414.9447021484375,\n",
       "   1414.099609375,\n",
       "   1415.706787109375,\n",
       "   1417.3037109375,\n",
       "   1416.8077392578125,\n",
       "   1414.8267822265625,\n",
       "   1414.7501220703125,\n",
       "   1414.8636474609375,\n",
       "   1416.4810791015625,\n",
       "   1412.3572998046875,\n",
       "   1412.1810302734375,\n",
       "   1411.578857421875,\n",
       "   1410.6771240234375,\n",
       "   1410.348388671875,\n",
       "   1408.508056640625,\n",
       "   1411.101318359375,\n",
       "   1410.8160400390625,\n",
       "   1409.525634765625,\n",
       "   1409.4437255859375,\n",
       "   1410.0233154296875,\n",
       "   1409.390625,\n",
       "   1408.6221923828125,\n",
       "   1410.0224609375,\n",
       "   1412.0489501953125,\n",
       "   1412.3428955078125,\n",
       "   1412.7303466796875,\n",
       "   1414.205810546875,\n",
       "   1414.07177734375,\n",
       "   1414.7109375,\n",
       "   1414.1522216796875,\n",
       "   1413.77099609375,\n",
       "   1413.8828125,\n",
       "   1411.943603515625,\n",
       "   1412.7916259765625,\n",
       "   1413.530029296875,\n",
       "   1412.494873046875,\n",
       "   1412.2183837890625,\n",
       "   1411.6676025390625,\n",
       "   1409.744873046875,\n",
       "   1410.9324951171875,\n",
       "   1412.324951171875,\n",
       "   1413.2125244140625,\n",
       "   1413.6278076171875,\n",
       "   1415.65283203125,\n",
       "   1414.3060302734375,\n",
       "   1410.2109375,\n",
       "   1410.14111328125,\n",
       "   1409.7353515625,\n",
       "   1409.615478515625,\n",
       "   1411.664306640625,\n",
       "   1411.4881591796875,\n",
       "   1410.0919189453125,\n",
       "   1411.941162109375,\n",
       "   1413.862060546875,\n",
       "   1412.029052734375,\n",
       "   1410.046142578125,\n",
       "   1410.86328125,\n",
       "   1409.68896484375,\n",
       "   1409.2950439453125,\n",
       "   1408.2200927734375,\n",
       "   1408.646240234375,\n",
       "   1409.4139404296875,\n",
       "   1410.2955322265625,\n",
       "   1410.7471923828125,\n",
       "   1410.485595703125,\n",
       "   1409.89794921875,\n",
       "   1411.1317138671875,\n",
       "   1409.2918701171875,\n",
       "   1409.70166015625,\n",
       "   1410.0518798828125,\n",
       "   1409.1195068359375,\n",
       "   1406.9132080078125,\n",
       "   1405.47998046875,\n",
       "   1406.037353515625,\n",
       "   1404.959228515625,\n",
       "   1405.668701171875,\n",
       "   1404.0477294921875,\n",
       "   1404.7232666015625,\n",
       "   1405.985595703125,\n",
       "   1406.9791259765625,\n",
       "   1407.24169921875,\n",
       "   1405.8206787109375,\n",
       "   1405.2564697265625,\n",
       "   1403.0618896484375,\n",
       "   1404.670654296875,\n",
       "   1405.349609375,\n",
       "   1405.2696533203125,\n",
       "   1403.8067626953125,\n",
       "   1402.9193115234375,\n",
       "   1402.917236328125,\n",
       "   1401.8577880859375,\n",
       "   1398.796142578125,\n",
       "   1397.003662109375,\n",
       "   1398.8992919921875,\n",
       "   1398.3282470703125,\n",
       "   1398.9908447265625,\n",
       "   1397.4580078125,\n",
       "   1398.3013916015625,\n",
       "   1399.0281982421875,\n",
       "   1398.065185546875,\n",
       "   1397.512451171875,\n",
       "   1399.151123046875,\n",
       "   1399.3448486328125,\n",
       "   1399.994384765625,\n",
       "   1400.31103515625,\n",
       "   1402.108642578125,\n",
       "   1402.910400390625,\n",
       "   1404.485595703125,\n",
       "   1406.420654296875,\n",
       "   1406.7061767578125,\n",
       "   1407.3642578125,\n",
       "   1408.1170654296875,\n",
       "   1409.0111083984375,\n",
       "   1408.80419921875,\n",
       "   1407.962158203125,\n",
       "   1406.598876953125,\n",
       "   1405.82373046875,\n",
       "   1408.005859375,\n",
       "   1407.4573974609375,\n",
       "   1407.4931640625,\n",
       "   1406.0260009765625,\n",
       "   1406.0048828125,\n",
       "   1404.1466064453125,\n",
       "   1405.92626953125,\n",
       "   1406.0244140625,\n",
       "   1404.955078125,\n",
       "   1406.1513671875,\n",
       "   1405.8822021484375,\n",
       "   1403.6138916015625,\n",
       "   1404.4971923828125,\n",
       "   1404.0565185546875,\n",
       "   1405.1181640625,\n",
       "   1406.2171630859375,\n",
       "   1404.1275634765625,\n",
       "   1404.384765625,\n",
       "   1404.18505859375,\n",
       "   1403.5189208984375,\n",
       "   1402.978759765625,\n",
       "   1403.4735107421875,\n",
       "   1403.93994140625,\n",
       "   1404.610107421875,\n",
       "   1404.3463134765625,\n",
       "   1402.8709716796875,\n",
       "   1403.252197265625,\n",
       "   1404.398193359375,\n",
       "   1404.6190185546875,\n",
       "   1404.8455810546875,\n",
       "   1405.42919921875,\n",
       "   1407.214111328125,\n",
       "   1407.9775390625,\n",
       "   1407.7066650390625,\n",
       "   1407.156494140625,\n",
       "   1409.658447265625,\n",
       "   1410.8623046875,\n",
       "   1407.6851806640625,\n",
       "   1405.62841796875,\n",
       "   1406.1651611328125,\n",
       "   1405.304443359375,\n",
       "   1406.7430419921875,\n",
       "   1407.7554931640625,\n",
       "   1408.2679443359375,\n",
       "   1405.4156494140625,\n",
       "   1405.08740234375,\n",
       "   1406.0631103515625,\n",
       "   1407.4998779296875,\n",
       "   1408.091796875,\n",
       "   1406.73193359375,\n",
       "   1406.6353759765625,\n",
       "   1405.732177734375,\n",
       "   1406.306884765625,\n",
       "   1406.7838134765625,\n",
       "   1406.7730712890625,\n",
       "   1405.208251953125,\n",
       "   1405.1597900390625,\n",
       "   1405.1353759765625,\n",
       "   1406.6082763671875,\n",
       "   1408.865966796875,\n",
       "   1408.9849853515625,\n",
       "   1406.0513916015625,\n",
       "   1407.4080810546875,\n",
       "   1406.217529296875,\n",
       "   1406.2667236328125,\n",
       "   1404.556884765625,\n",
       "   1406.603759765625,\n",
       "   1405.9500732421875,\n",
       "   1406.3865966796875,\n",
       "   1405.4727783203125,\n",
       "   1406.4827880859375,\n",
       "   1402.8663330078125,\n",
       "   1403.912353515625,\n",
       "   1403.7978515625,\n",
       "   1405.8046875,\n",
       "   1404.97998046875,\n",
       "   1405.5157470703125,\n",
       "   1404.2674560546875,\n",
       "   1403.0533447265625,\n",
       "   1403.2447509765625,\n",
       "   1403.3175048828125,\n",
       "   1402.2603759765625,\n",
       "   1402.8248291015625,\n",
       "   1403.6494140625,\n",
       "   1404.6044921875,\n",
       "   1402.289794921875,\n",
       "   1403.16162109375,\n",
       "   1402.880615234375,\n",
       "   1405.8404541015625,\n",
       "   1406.1064453125,\n",
       "   1408.142333984375,\n",
       "   1405.276123046875,\n",
       "   1404.8011474609375,\n",
       "   1404.08935546875,\n",
       "   1405.9267578125,\n",
       "   1405.770263671875,\n",
       "   1404.789306640625,\n",
       "   1407.2021484375,\n",
       "   1405.7542724609375,\n",
       "   1407.0750732421875,\n",
       "   1407.4696044921875,\n",
       "   1408.395263671875,\n",
       "   1407.93408203125,\n",
       "   1405.8116455078125,\n",
       "   1407.1121826171875,\n",
       "   1407.6104736328125,\n",
       "   1406.6861572265625,\n",
       "   1407.1199951171875,\n",
       "   1405.5582275390625,\n",
       "   1402.0775146484375,\n",
       "   1402.969970703125,\n",
       "   1403.951416015625,\n",
       "   1402.6494140625,\n",
       "   1402.224609375,\n",
       "   1404.3255615234375,\n",
       "   1405.3001708984375,\n",
       "   1405.7701416015625,\n",
       "   1401.855712890625,\n",
       "   1401.35205078125],\n",
       "  'voc_acc': [1365.193115234375,\n",
       "   1383.015380859375,\n",
       "   1399.5035400390625,\n",
       "   1406.2271728515625,\n",
       "   1413.2462158203125,\n",
       "   1420.8243408203125,\n",
       "   1425.619873046875,\n",
       "   1430.8331298828125,\n",
       "   1435.99267578125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1447.359619140625,\n",
       "   1447.134765625,\n",
       "   nan,\n",
       "   1447.4344482421875,\n",
       "   1446.7235107421875,\n",
       "   1447.5289306640625,\n",
       "   1447.0543212890625,\n",
       "   1447.3994140625,\n",
       "   1444.417236328125,\n",
       "   1444.5579833984375,\n",
       "   1443.8743896484375,\n",
       "   1444.052001953125,\n",
       "   1444.791015625,\n",
       "   1442.86474609375,\n",
       "   1443.4134521484375,\n",
       "   1443.036376953125,\n",
       "   1443.0264892578125,\n",
       "   1442.9515380859375,\n",
       "   1442.6767578125,\n",
       "   1443.6695556640625,\n",
       "   1444.6397705078125,\n",
       "   1444.828369140625,\n",
       "   1444.1610107421875,\n",
       "   1442.6153564453125,\n",
       "   1443.76416015625,\n",
       "   1444.195556640625,\n",
       "   1443.425537109375,\n",
       "   1443.7587890625,\n",
       "   1443.8131103515625,\n",
       "   1444.1217041015625,\n",
       "   1443.1795654296875,\n",
       "   1444.6173095703125,\n",
       "   1443.8653564453125,\n",
       "   1444.0816650390625,\n",
       "   1443.8372802734375,\n",
       "   1444.1934814453125,\n",
       "   1444.085693359375,\n",
       "   1444.246337890625,\n",
       "   1443.637451171875,\n",
       "   1442.1053466796875,\n",
       "   1442.8134765625,\n",
       "   1442.212158203125,\n",
       "   1442.748779296875,\n",
       "   1443.326171875,\n",
       "   1442.9552001953125,\n",
       "   1443.1767578125,\n",
       "   1442.91064453125,\n",
       "   1443.2354736328125,\n",
       "   1443.6673583984375,\n",
       "   1444.3714599609375,\n",
       "   1445.0972900390625,\n",
       "   1444.56640625,\n",
       "   1444.95654296875,\n",
       "   1443.88525390625,\n",
       "   1444.33544921875,\n",
       "   1443.8287353515625,\n",
       "   1444.5689697265625,\n",
       "   1445.541259765625,\n",
       "   1445.4888916015625,\n",
       "   1444.842041015625,\n",
       "   1445.422119140625,\n",
       "   1444.45166015625,\n",
       "   1445.1822509765625,\n",
       "   1446.1260986328125,\n",
       "   1445.805419921875,\n",
       "   1446.5064697265625,\n",
       "   1445.615234375,\n",
       "   1444.8470458984375,\n",
       "   1445.1553955078125,\n",
       "   1445.7171630859375,\n",
       "   1446.81591796875,\n",
       "   1447.2276611328125,\n",
       "   1446.7904052734375,\n",
       "   1447.0989990234375,\n",
       "   1447.6583251953125,\n",
       "   1446.6881103515625,\n",
       "   1447.453369140625,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1447.342529296875,\n",
       "   1447.0513916015625,\n",
       "   1447.0496826171875,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1447.3802490234375,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1447.0015869140625,\n",
       "   1446.0374755859375,\n",
       "   1447.240234375,\n",
       "   nan,\n",
       "   1446.8255615234375,\n",
       "   nan,\n",
       "   1447.2611083984375,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1447.075439453125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1447.0924072265625,\n",
       "   1446.1087646484375,\n",
       "   1445.8994140625,\n",
       "   nan,\n",
       "   nan,\n",
       "   1445.791259765625,\n",
       "   1445.71337890625,\n",
       "   1444.796875,\n",
       "   1445.3111572265625,\n",
       "   1445.765625,\n",
       "   1446.3076171875,\n",
       "   1446.166748046875,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1444.840576171875,\n",
       "   1444.1754150390625,\n",
       "   1442.5089111328125,\n",
       "   1443.228271484375,\n",
       "   1442.28564453125,\n",
       "   1439.9881591796875,\n",
       "   1440.39111328125,\n",
       "   1441.039794921875,\n",
       "   1441.00439453125,\n",
       "   1441.272216796875,\n",
       "   1441.37158203125,\n",
       "   1439.1885986328125,\n",
       "   1438.019287109375,\n",
       "   1438.687744140625,\n",
       "   1438.85498046875,\n",
       "   1439.6063232421875,\n",
       "   1439.4666748046875,\n",
       "   1439.392333984375,\n",
       "   1437.971923828125,\n",
       "   1438.846923828125,\n",
       "   1440.22216796875,\n",
       "   1441.0294189453125,\n",
       "   1439.287841796875,\n",
       "   1440.048828125,\n",
       "   1439.116455078125,\n",
       "   1439.9158935546875,\n",
       "   1439.0439453125,\n",
       "   1439.3193359375,\n",
       "   1438.6976318359375,\n",
       "   1439.2061767578125,\n",
       "   1439.7314453125,\n",
       "   1438.53759765625,\n",
       "   1439.85302734375,\n",
       "   1438.84326171875,\n",
       "   1439.4986572265625,\n",
       "   1439.9329833984375,\n",
       "   1439.58447265625,\n",
       "   1440.4915771484375,\n",
       "   1440.466064453125,\n",
       "   1439.8187255859375,\n",
       "   1440.7821044921875,\n",
       "   1442.2186279296875,\n",
       "   1443.5413818359375,\n",
       "   1442.9488525390625,\n",
       "   1444.5086669921875,\n",
       "   1443.615966796875,\n",
       "   1444.6221923828125,\n",
       "   1445.3751220703125,\n",
       "   1445.5089111328125,\n",
       "   1445.73876953125,\n",
       "   1444.697998046875,\n",
       "   1444.3453369140625,\n",
       "   1444.6688232421875,\n",
       "   1444.658447265625,\n",
       "   1444.0772705078125,\n",
       "   1444.0904541015625,\n",
       "   1443.4307861328125,\n",
       "   1443.3077392578125,\n",
       "   1443.855224609375,\n",
       "   1444.5706787109375,\n",
       "   1444.790771484375,\n",
       "   1444.1668701171875,\n",
       "   1443.8707275390625,\n",
       "   1440.15966796875,\n",
       "   1440.3861083984375,\n",
       "   1439.081298828125,\n",
       "   1440.01123046875,\n",
       "   1438.5396728515625,\n",
       "   1439.831298828125,\n",
       "   1440.197998046875,\n",
       "   1439.938720703125,\n",
       "   1439.52783203125,\n",
       "   1439.8297119140625,\n",
       "   1439.468017578125,\n",
       "   1440.38232421875,\n",
       "   1439.8740234375,\n",
       "   1440.3692626953125,\n",
       "   1439.9991455078125,\n",
       "   1441.8870849609375,\n",
       "   1442.029052734375,\n",
       "   1441.692626953125,\n",
       "   1442.4619140625,\n",
       "   1442.004150390625,\n",
       "   1442.0987548828125,\n",
       "   1443.326416015625,\n",
       "   1444.12890625,\n",
       "   1442.2379150390625,\n",
       "   1442.3048095703125,\n",
       "   1441.87890625,\n",
       "   1439.4033203125,\n",
       "   1440.2025146484375,\n",
       "   1440.30908203125,\n",
       "   1442.666748046875,\n",
       "   1444.2244873046875,\n",
       "   1445.1309814453125,\n",
       "   nan,\n",
       "   nan,\n",
       "   1445.292724609375,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1444.878662109375,\n",
       "   1443.9197998046875,\n",
       "   1444.560791015625,\n",
       "   1442.8408203125,\n",
       "   1443.1016845703125,\n",
       "   1443.618408203125,\n",
       "   1444.760498046875,\n",
       "   1444.684814453125,\n",
       "   1444.4786376953125,\n",
       "   nan,\n",
       "   nan,\n",
       "   1443.3272705078125,\n",
       "   1444.2987060546875,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1444.9468994140625,\n",
       "   1445.2955322265625,\n",
       "   1445.0823974609375,\n",
       "   1444.4376220703125,\n",
       "   1444.0740966796875,\n",
       "   1444.7645263671875,\n",
       "   1445.8175048828125,\n",
       "   1446.8890380859375,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   1445.61669921875,\n",
       "   1445.174072265625,\n",
       "   1445.05859375,\n",
       "   1445.218994140625,\n",
       "   1443.9560546875,\n",
       "   1443.586181640625,\n",
       "   1442.4544677734375,\n",
       "   1443.666015625,\n",
       "   1443.0849609375,\n",
       "   1443.5662841796875,\n",
       "   1443.1795654296875,\n",
       "   1444.027587890625,\n",
       "   1442.500244140625,\n",
       "   1443.1846923828125,\n",
       "   1442.8812255859375,\n",
       "   1442.1500244140625,\n",
       "   1443.466064453125,\n",
       "   1441.697265625,\n",
       "   1441.565185546875,\n",
       "   1442.1583251953125,\n",
       "   1442.7530517578125,\n",
       "   1443.66748046875,\n",
       "   1445.55615234375,\n",
       "   1444.867919921875,\n",
       "   nan,\n",
       "   nan,\n",
       "   1444.186767578125,\n",
       "   nan,\n",
       "   1443.151611328125,\n",
       "   nan,\n",
       "   1442.394775390625,\n",
       "   1441.9072265625,\n",
       "   1441.585205078125,\n",
       "   1440.909423828125,\n",
       "   1440.702392578125,\n",
       "   1438.244140625,\n",
       "   1438.8133544921875,\n",
       "   1438.7864990234375,\n",
       "   1439.1207275390625,\n",
       "   1439.3778076171875,\n",
       "   1439.6368408203125,\n",
       "   1440.0220947265625,\n",
       "   1439.6712646484375,\n",
       "   1440.035888671875,\n",
       "   1438.4102783203125,\n",
       "   1438.24560546875,\n",
       "   1438.0174560546875,\n",
       "   1438.3443603515625,\n",
       "   1439.8643798828125,\n",
       "   1440.9134521484375,\n",
       "   1440.743408203125,\n",
       "   1439.605224609375,\n",
       "   1439.30615234375,\n",
       "   1439.575439453125,\n",
       "   1440.7825927734375,\n",
       "   1441.4400634765625,\n",
       "   1442.4354248046875,\n",
       "   1442.846923828125,\n",
       "   1444.067626953125,\n",
       "   1443.21630859375,\n",
       "   1441.476806640625,\n",
       "   1441.6466064453125,\n",
       "   1442.5467529296875,\n",
       "   1443.6622314453125,\n",
       "   1443.7415771484375,\n",
       "   1443.5535888671875,\n",
       "   1443.083740234375,\n",
       "   1441.338134765625,\n",
       "   1442.541748046875,\n",
       "   1442.2275390625,\n",
       "   1441.4598388671875,\n",
       "   1442.05908203125,\n",
       "   1440.4599609375,\n",
       "   1441.6240234375,\n",
       "   1441.6817626953125,\n",
       "   1441.4727783203125,\n",
       "   1440.996826171875,\n",
       "   1441.56298828125,\n",
       "   1440.1986083984375,\n",
       "   1440.730712890625,\n",
       "   1442.0484619140625,\n",
       "   1440.729248046875,\n",
       "   1441.762939453125,\n",
       "   1440.066650390625,\n",
       "   1441.4376220703125,\n",
       "   1439.5301513671875,\n",
       "   1439.97607421875,\n",
       "   1440.3641357421875,\n",
       "   1439.264892578125,\n",
       "   1437.8287353515625,\n",
       "   1437.4599609375,\n",
       "   1436.97802734375,\n",
       "   1437.9293212890625,\n",
       "   1436.645751953125,\n",
       "   1437.5411376953125,\n",
       "   1438.239013671875,\n",
       "   1438.7457275390625,\n",
       "   1438.115478515625,\n",
       "   1438.1307373046875,\n",
       "   1437.9583740234375,\n",
       "   1438.908935546875,\n",
       "   1439.477783203125,\n",
       "   1439.7989501953125,\n",
       "   1438.923583984375,\n",
       "   1438.7808837890625,\n",
       "   1439.2540283203125,\n",
       "   1438.8714599609375,\n",
       "   1437.378173828125,\n",
       "   1436.3875732421875,\n",
       "   1437.5865478515625,\n",
       "   1438.0262451171875,\n",
       "   1437.7099609375,\n",
       "   1437.689697265625,\n",
       "   1437.353515625,\n",
       "   1438.7830810546875,\n",
       "   1439.56201171875,\n",
       "   1439.575439453125,\n",
       "   1438.4874267578125,\n",
       "   1438.9722900390625,\n",
       "   1440.5870361328125],\n",
       "  'jsc_acc': [1888.4403076171875,\n",
       "   2039.4891357421875,\n",
       "   2101.83056640625,\n",
       "   2094.412109375,\n",
       "   2150.93115234375,\n",
       "   2187.973876953125,\n",
       "   2201.804443359375,\n",
       "   2203.298095703125,\n",
       "   2219.6787109375,\n",
       "   2235.373046875,\n",
       "   2240.796875,\n",
       "   2235.49072265625,\n",
       "   2215.394775390625,\n",
       "   2208.91162109375,\n",
       "   2195.599365234375,\n",
       "   2203.78955078125,\n",
       "   2214.582275390625,\n",
       "   2214.567138671875,\n",
       "   2207.827392578125,\n",
       "   2207.95849609375,\n",
       "   2207.275390625,\n",
       "   2213.35498046875,\n",
       "   2216.650634765625,\n",
       "   2213.611328125,\n",
       "   2217.142333984375,\n",
       "   2216.619140625,\n",
       "   2224.137939453125,\n",
       "   2221.26171875,\n",
       "   2225.107177734375,\n",
       "   2221.842041015625,\n",
       "   2218.36865234375,\n",
       "   2216.33349609375,\n",
       "   2214.442138671875,\n",
       "   2206.9140625,\n",
       "   2208.413818359375,\n",
       "   2210.145263671875,\n",
       "   2217.482177734375,\n",
       "   2217.9072265625,\n",
       "   2218.031005859375,\n",
       "   2215.699462890625,\n",
       "   2205.29638671875,\n",
       "   2211.033447265625,\n",
       "   2216.88720703125,\n",
       "   2215.151123046875,\n",
       "   2209.156494140625,\n",
       "   2202.857421875,\n",
       "   2198.279296875,\n",
       "   2189.710693359375,\n",
       "   2190.50244140625,\n",
       "   2184.58154296875,\n",
       "   2186.092529296875,\n",
       "   2175.254638671875,\n",
       "   2181.974365234375,\n",
       "   2175.2880859375,\n",
       "   2172.789306640625,\n",
       "   2169.2861328125,\n",
       "   2167.266845703125,\n",
       "   2168.80078125,\n",
       "   2156.664794921875,\n",
       "   2148.620361328125,\n",
       "   2147.756591796875,\n",
       "   2145.583984375,\n",
       "   2143.75244140625,\n",
       "   2140.749267578125,\n",
       "   2139.1533203125,\n",
       "   2136.554443359375,\n",
       "   2140.646728515625,\n",
       "   2138.50390625,\n",
       "   2140.61572265625,\n",
       "   2132.79541015625,\n",
       "   2132.461669921875,\n",
       "   2124.489501953125,\n",
       "   2119.478759765625,\n",
       "   2117.8251953125,\n",
       "   2109.007568359375,\n",
       "   2110.3046875,\n",
       "   2112.59130859375,\n",
       "   2114.78173828125,\n",
       "   2115.222412109375,\n",
       "   2116.648193359375,\n",
       "   2114.4072265625,\n",
       "   2116.1884765625,\n",
       "   2122.2099609375,\n",
       "   2122.20068359375,\n",
       "   2121.620849609375,\n",
       "   2124.491455078125,\n",
       "   2121.180419921875,\n",
       "   2116.638427734375,\n",
       "   2113.424072265625,\n",
       "   2121.862548828125,\n",
       "   2125.023193359375,\n",
       "   2126.726318359375,\n",
       "   2127.55859375,\n",
       "   2124.989990234375,\n",
       "   2121.222900390625,\n",
       "   2117.089111328125,\n",
       "   2116.483642578125,\n",
       "   2116.489501953125,\n",
       "   2115.9150390625,\n",
       "   2115.981201171875,\n",
       "   2113.822509765625,\n",
       "   2109.109375,\n",
       "   2105.164306640625,\n",
       "   2106.11767578125,\n",
       "   2100.58154296875,\n",
       "   2097.526611328125,\n",
       "   2095.929931640625,\n",
       "   2095.89111328125,\n",
       "   2103.96337890625,\n",
       "   2099.481689453125,\n",
       "   2104.233642578125,\n",
       "   2105.260009765625,\n",
       "   2118.18017578125,\n",
       "   2122.7275390625,\n",
       "   2125.68017578125,\n",
       "   2124.655517578125,\n",
       "   2127.959228515625,\n",
       "   2127.59716796875,\n",
       "   2125.4560546875,\n",
       "   2120.80615234375,\n",
       "   2117.106201171875,\n",
       "   2117.710693359375,\n",
       "   2111.76953125,\n",
       "   2108.17578125,\n",
       "   2106.7587890625,\n",
       "   2107.830078125,\n",
       "   2111.4765625,\n",
       "   2112.21630859375,\n",
       "   2115.822998046875,\n",
       "   2117.794921875,\n",
       "   2125.40234375,\n",
       "   2131.63330078125,\n",
       "   2133.514404296875,\n",
       "   2123.246826171875,\n",
       "   2121.58544921875,\n",
       "   2118.707275390625,\n",
       "   2122.041015625,\n",
       "   2121.455078125,\n",
       "   2118.529296875,\n",
       "   2122.0654296875,\n",
       "   2119.645751953125,\n",
       "   2121.250244140625,\n",
       "   2123.8056640625,\n",
       "   2115.844970703125,\n",
       "   2113.09619140625,\n",
       "   2119.651611328125,\n",
       "   2111.29345703125,\n",
       "   2108.685791015625,\n",
       "   2108.504150390625,\n",
       "   2108.814208984375,\n",
       "   2104.946044921875,\n",
       "   2101.3291015625,\n",
       "   2092.49560546875,\n",
       "   2098.08251953125,\n",
       "   2099.631591796875,\n",
       "   2098.05126953125,\n",
       "   2093.955810546875,\n",
       "   2093.177490234375,\n",
       "   2100.792236328125,\n",
       "   2100.64404296875,\n",
       "   2102.943359375,\n",
       "   2098.8642578125,\n",
       "   2099.04833984375,\n",
       "   2098.7333984375,\n",
       "   2105.3583984375,\n",
       "   2108.91748046875,\n",
       "   2106.083984375,\n",
       "   2106.1552734375,\n",
       "   2106.610595703125,\n",
       "   2101.956787109375,\n",
       "   2097.321044921875,\n",
       "   2103.19482421875,\n",
       "   2103.884765625,\n",
       "   2103.635986328125,\n",
       "   2098.53271484375,\n",
       "   2105.8134765625,\n",
       "   2107.796142578125,\n",
       "   2116.1337890625,\n",
       "   2118.105224609375,\n",
       "   2114.716064453125,\n",
       "   2117.815185546875,\n",
       "   2116.211669921875,\n",
       "   2121.26416015625,\n",
       "   2121.91064453125,\n",
       "   2121.798828125,\n",
       "   2125.726806640625,\n",
       "   2119.112060546875,\n",
       "   2118.66650390625,\n",
       "   2113.525146484375,\n",
       "   2117.69384765625,\n",
       "   2119.2109375,\n",
       "   2117.907470703125,\n",
       "   2121.857666015625,\n",
       "   2130.857421875,\n",
       "   2133.623046875,\n",
       "   2129.63916015625,\n",
       "   2124.31591796875,\n",
       "   2126.079345703125,\n",
       "   2122.0146484375,\n",
       "   2123.4697265625,\n",
       "   2125.447998046875,\n",
       "   2127.718505859375,\n",
       "   2124.822509765625,\n",
       "   2125.965087890625,\n",
       "   2127.570068359375,\n",
       "   2122.77783203125,\n",
       "   2123.606689453125,\n",
       "   2113.177001953125,\n",
       "   2123.830078125,\n",
       "   2127.551513671875,\n",
       "   2123.6650390625,\n",
       "   2128.138916015625,\n",
       "   2127.0146484375,\n",
       "   2124.30029296875,\n",
       "   2123.5126953125,\n",
       "   2122.944091796875,\n",
       "   2132.63671875,\n",
       "   2138.288818359375,\n",
       "   2136.92333984375,\n",
       "   2138.48193359375,\n",
       "   2134.573974609375,\n",
       "   2143.412353515625,\n",
       "   2154.685302734375,\n",
       "   2159.57568359375,\n",
       "   2166.0576171875,\n",
       "   2172.8203125,\n",
       "   2173.846435546875,\n",
       "   2181.25439453125,\n",
       "   2185.72265625,\n",
       "   2178.2197265625,\n",
       "   2179.677001953125,\n",
       "   2181.537353515625,\n",
       "   2172.48828125,\n",
       "   2170.820556640625,\n",
       "   2166.054443359375,\n",
       "   2164.05078125,\n",
       "   2161.31787109375,\n",
       "   2165.474853515625,\n",
       "   2166.70849609375,\n",
       "   2168.7451171875,\n",
       "   2163.61083984375,\n",
       "   2158.150390625,\n",
       "   2156.611572265625,\n",
       "   2161.5947265625,\n",
       "   2158.645263671875,\n",
       "   2162.318603515625,\n",
       "   2164.3642578125,\n",
       "   2172.27734375,\n",
       "   2167.669189453125,\n",
       "   2169.405029296875,\n",
       "   2168.619140625,\n",
       "   2164.096435546875,\n",
       "   2159.68212890625,\n",
       "   2162.492431640625,\n",
       "   2160.869140625,\n",
       "   2164.615966796875,\n",
       "   2165.021240234375,\n",
       "   2166.556396484375,\n",
       "   2170.26318359375,\n",
       "   2167.368896484375,\n",
       "   2166.422119140625,\n",
       "   2168.050048828125,\n",
       "   2168.47412109375,\n",
       "   2162.862548828125,\n",
       "   2157.722900390625,\n",
       "   2164.69677734375,\n",
       "   2165.090576171875,\n",
       "   2166.19384765625,\n",
       "   2160.370361328125,\n",
       "   2158.966552734375,\n",
       "   2159.1513671875,\n",
       "   2161.14892578125,\n",
       "   2154.45751953125,\n",
       "   2164.131591796875,\n",
       "   2163.270263671875,\n",
       "   2155.710693359375,\n",
       "   2160.714111328125,\n",
       "   2166.68212890625,\n",
       "   2165.741455078125,\n",
       "   2165.5068359375,\n",
       "   2170.903076171875,\n",
       "   2174.515869140625,\n",
       "   2177.542724609375,\n",
       "   2173.47265625,\n",
       "   2176.164794921875,\n",
       "   2170.39501953125,\n",
       "   2158.95361328125,\n",
       "   2164.0185546875,\n",
       "   2165.1005859375,\n",
       "   2169.110595703125,\n",
       "   2173.396240234375,\n",
       "   2177.10595703125,\n",
       "   2177.57568359375,\n",
       "   2176.635986328125,\n",
       "   2178.486328125,\n",
       "   2178.258056640625,\n",
       "   2174.012451171875,\n",
       "   2180.101806640625,\n",
       "   2185.43505859375,\n",
       "   2185.4609375,\n",
       "   2188.677490234375,\n",
       "   2193.778564453125,\n",
       "   2196.259765625,\n",
       "   2191.18212890625,\n",
       "   2186.89111328125,\n",
       "   2182.498291015625,\n",
       "   2180.701904296875,\n",
       "   2181.213623046875,\n",
       "   2177.8544921875,\n",
       "   2180.4482421875,\n",
       "   2179.292724609375,\n",
       "   2183.6669921875,\n",
       "   2175.873046875,\n",
       "   2177.2255859375,\n",
       "   2184.107421875,\n",
       "   2188.18408203125,\n",
       "   2194.4541015625,\n",
       "   2194.472412109375,\n",
       "   2196.43701171875,\n",
       "   2192.984619140625,\n",
       "   2194.65380859375,\n",
       "   2194.982421875,\n",
       "   2193.875,\n",
       "   2195.647216796875,\n",
       "   2197.40380859375,\n",
       "   2202.739990234375,\n",
       "   2196.077392578125,\n",
       "   2195.79248046875,\n",
       "   2194.121826171875,\n",
       "   2190.6689453125,\n",
       "   2190.63525390625,\n",
       "   2186.768798828125,\n",
       "   2191.0390625,\n",
       "   2190.8515625,\n",
       "   2187.932861328125,\n",
       "   2189.162109375,\n",
       "   2189.38525390625,\n",
       "   2182.25634765625,\n",
       "   2172.91162109375,\n",
       "   2173.90673828125,\n",
       "   2167.032958984375,\n",
       "   2165.774169921875,\n",
       "   2167.136962890625,\n",
       "   2163.727294921875,\n",
       "   2168.504638671875,\n",
       "   2171.276611328125,\n",
       "   2179.265869140625,\n",
       "   2181.815673828125,\n",
       "   2176.136474609375,\n",
       "   2175.082275390625,\n",
       "   2180.425537109375,\n",
       "   2173.465087890625,\n",
       "   2170.92333984375,\n",
       "   2172.939453125,\n",
       "   2169.4443359375,\n",
       "   2175.91943359375,\n",
       "   2181.73779296875,\n",
       "   2186.87158203125,\n",
       "   2187.864990234375,\n",
       "   2198.754150390625,\n",
       "   2197.5869140625,\n",
       "   2199.43017578125,\n",
       "   2202.454345703125,\n",
       "   2201.5166015625,\n",
       "   2204.893798828125,\n",
       "   2208.818603515625,\n",
       "   2206.70458984375,\n",
       "   2201.619873046875,\n",
       "   2206.251220703125,\n",
       "   2208.758056640625,\n",
       "   2204.012939453125,\n",
       "   2205.797607421875,\n",
       "   2204.745849609375,\n",
       "   2203.88720703125,\n",
       "   2202.844482421875,\n",
       "   2204.976318359375,\n",
       "   2210.87255859375,\n",
       "   2207.36669921875,\n",
       "   2203.08154296875,\n",
       "   2194.081787109375,\n",
       "   2196.352783203125,\n",
       "   2197.67333984375,\n",
       "   2201.19189453125,\n",
       "   2201.034912109375,\n",
       "   2195.332763671875,\n",
       "   2196.359619140625,\n",
       "   2197.7783203125,\n",
       "   2195.92138671875,\n",
       "   2193.82275390625,\n",
       "   2192.88037109375,\n",
       "   2198.216552734375,\n",
       "   2198.978515625,\n",
       "   2201.456298828125,\n",
       "   2211.5986328125,\n",
       "   2208.658447265625,\n",
       "   2208.037841796875,\n",
       "   2210.3408203125,\n",
       "   2207.97900390625,\n",
       "   2206.93408203125,\n",
       "   2205.150146484375,\n",
       "   2196.88134765625,\n",
       "   2192.967041015625,\n",
       "   2195.796630859375,\n",
       "   2197.01953125,\n",
       "   2198.234375,\n",
       "   2193.85888671875,\n",
       "   2189.7958984375,\n",
       "   2193.421630859375,\n",
       "   2189.05078125,\n",
       "   2181.771240234375,\n",
       "   2182.35107421875,\n",
       "   2181.681640625,\n",
       "   2177.247314453125,\n",
       "   2180.07958984375,\n",
       "   2178.418212890625,\n",
       "   2183.572265625,\n",
       "   2181.83642578125,\n",
       "   2180.6376953125,\n",
       "   2184.13134765625,\n",
       "   2175.85546875,\n",
       "   2171.797119140625,\n",
       "   2167.126953125,\n",
       "   2167.445068359375,\n",
       "   2169.25244140625,\n",
       "   2161.408447265625,\n",
       "   2151.187255859375,\n",
       "   2152.28515625,\n",
       "   2149.57275390625,\n",
       "   2154.816162109375,\n",
       "   2155.12841796875,\n",
       "   2164.754150390625,\n",
       "   2171.727294921875,\n",
       "   2176.065673828125,\n",
       "   2175.9716796875,\n",
       "   2174.52197265625,\n",
       "   2176.231689453125,\n",
       "   2169.363525390625,\n",
       "   2165.33251953125,\n",
       "   2168.52197265625,\n",
       "   2167.412109375,\n",
       "   2172.255615234375,\n",
       "   2173.27099609375,\n",
       "   2175.102783203125,\n",
       "   2169.423828125,\n",
       "   2172.7333984375,\n",
       "   2179.51220703125,\n",
       "   2187.013427734375,\n",
       "   2191.810302734375,\n",
       "   2196.287109375,\n",
       "   2191.995361328125,\n",
       "   2191.669189453125,\n",
       "   2194.388916015625,\n",
       "   2184.389404296875,\n",
       "   2187.423828125,\n",
       "   2189.20556640625,\n",
       "   2191.140625,\n",
       "   2194.166748046875,\n",
       "   2195.313720703125,\n",
       "   2198.8671875,\n",
       "   2201.011962890625,\n",
       "   2199.22705078125,\n",
       "   2197.92431640625,\n",
       "   2190.428955078125,\n",
       "   2189.789306640625,\n",
       "   2185.083984375,\n",
       "   2191.7119140625,\n",
       "   2197.07861328125,\n",
       "   2197.197021484375,\n",
       "   2199.2802734375,\n",
       "   2197.764404296875,\n",
       "   2203.98291015625,\n",
       "   2201.19921875,\n",
       "   2195.859375,\n",
       "   2189.862548828125,\n",
       "   2183.999755859375,\n",
       "   2196.712158203125,\n",
       "   2202.570068359375,\n",
       "   2198.7919921875,\n",
       "   2208.240966796875,\n",
       "   2208.8251953125,\n",
       "   2210.528564453125,\n",
       "   2214.35693359375,\n",
       "   2213.810302734375,\n",
       "   2211.240478515625,\n",
       "   2217.261474609375,\n",
       "   2213.905517578125,\n",
       "   2209.8115234375,\n",
       "   2210.805908203125,\n",
       "   2211.607177734375,\n",
       "   2219.829833984375,\n",
       "   2221.37255859375,\n",
       "   2218.65625,\n",
       "   2215.72314453125,\n",
       "   2209.710205078125,\n",
       "   2209.211669921875,\n",
       "   2207.749755859375,\n",
       "   2209.65869140625,\n",
       "   2208.82958984375,\n",
       "   2206.35693359375,\n",
       "   2203.47607421875],\n",
       "  'ff_acc': [2368.80419921875,\n",
       "   2136.444580078125,\n",
       "   1975.4482421875,\n",
       "   1860.9222412109375,\n",
       "   1778.608642578125,\n",
       "   1713.3079833984375,\n",
       "   1659.285888671875,\n",
       "   1614.890625,\n",
       "   1577.2015380859375,\n",
       "   1548.816162109375,\n",
       "   1524.7596435546875,\n",
       "   1504.662841796875,\n",
       "   1488.439697265625,\n",
       "   1474.576416015625,\n",
       "   1462.871826171875,\n",
       "   1460.6068115234375,\n",
       "   1459.832275390625,\n",
       "   1458.8262939453125,\n",
       "   1459.6722412109375,\n",
       "   1461.6810302734375,\n",
       "   1463.01123046875,\n",
       "   1464.40380859375,\n",
       "   1465.2210693359375,\n",
       "   1466.593505859375,\n",
       "   1468.3046875,\n",
       "   1470.5245361328125,\n",
       "   1473.3985595703125,\n",
       "   1476.01318359375,\n",
       "   1477.49609375,\n",
       "   1478.8719482421875,\n",
       "   1480.571044921875,\n",
       "   1483.4317626953125,\n",
       "   1484.5753173828125,\n",
       "   1486.193603515625,\n",
       "   1487.9593505859375,\n",
       "   1489.72314453125,\n",
       "   1491.3094482421875,\n",
       "   1493.7340087890625,\n",
       "   1495.702880859375,\n",
       "   1497.56884765625,\n",
       "   1499.490478515625,\n",
       "   1501.579345703125,\n",
       "   1502.49072265625,\n",
       "   1503.00634765625,\n",
       "   1505.7362060546875,\n",
       "   1506.815673828125,\n",
       "   1507.7359619140625,\n",
       "   1508.49658203125,\n",
       "   1510.3277587890625,\n",
       "   1511.645263671875,\n",
       "   1514.1072998046875,\n",
       "   1515.5445556640625,\n",
       "   1515.70703125,\n",
       "   1517.2596435546875,\n",
       "   1518.90771484375,\n",
       "   1519.9146728515625,\n",
       "   1520.9771728515625,\n",
       "   1522.5482177734375,\n",
       "   1523.4871826171875,\n",
       "   1524.3402099609375,\n",
       "   1525.32666015625,\n",
       "   1526.14111328125,\n",
       "   1526.8365478515625,\n",
       "   1528.1881103515625,\n",
       "   1529.419189453125,\n",
       "   1530.599853515625,\n",
       "   1531.38232421875,\n",
       "   1530.84814453125,\n",
       "   1531.2833251953125,\n",
       "   1532.2647705078125,\n",
       "   1532.8790283203125,\n",
       "   1532.975341796875,\n",
       "   1533.7158203125,\n",
       "   1533.9100341796875,\n",
       "   1534.2357177734375,\n",
       "   1533.5576171875,\n",
       "   1533.068603515625,\n",
       "   1534.6390380859375,\n",
       "   1534.927001953125,\n",
       "   1534.491943359375,\n",
       "   1534.4625244140625,\n",
       "   1534.00830078125,\n",
       "   1534.0592041015625,\n",
       "   1535.05908203125,\n",
       "   1535.9281005859375,\n",
       "   1535.4564208984375,\n",
       "   1534.7784423828125,\n",
       "   1534.638427734375,\n",
       "   1533.6197509765625,\n",
       "   1533.8778076171875,\n",
       "   1534.247802734375,\n",
       "   1534.060791015625,\n",
       "   1534.6185302734375,\n",
       "   1535.088134765625,\n",
       "   1536.2237548828125,\n",
       "   1535.815673828125,\n",
       "   1536.0648193359375,\n",
       "   1536.4652099609375,\n",
       "   1536.9088134765625,\n",
       "   1537.003662109375,\n",
       "   1537.214111328125,\n",
       "   1536.5950927734375,\n",
       "   1536.1710205078125,\n",
       "   1537.4193115234375,\n",
       "   1537.7447509765625,\n",
       "   1538.43359375,\n",
       "   1538.3096923828125,\n",
       "   1538.7464599609375,\n",
       "   1538.9139404296875,\n",
       "   1538.3070068359375,\n",
       "   1537.25439453125,\n",
       "   1536.957763671875,\n",
       "   1536.8375244140625,\n",
       "   1538.581787109375,\n",
       "   1539.1591796875,\n",
       "   1539.350341796875,\n",
       "   1540.3353271484375,\n",
       "   1539.572265625,\n",
       "   1539.587646484375,\n",
       "   1540.4814453125,\n",
       "   1541.064453125,\n",
       "   1541.641845703125,\n",
       "   1541.9229736328125,\n",
       "   1541.94970703125,\n",
       "   1542.39697265625,\n",
       "   1542.9588623046875,\n",
       "   1542.666015625,\n",
       "   1543.2183837890625,\n",
       "   1544.634521484375,\n",
       "   1543.6231689453125,\n",
       "   1542.856201171875,\n",
       "   1544.8385009765625,\n",
       "   1545.1522216796875,\n",
       "   1545.27001953125,\n",
       "   1544.3280029296875,\n",
       "   1544.09033203125,\n",
       "   1544.063232421875,\n",
       "   1544.1719970703125,\n",
       "   1543.34912109375,\n",
       "   1542.19091796875,\n",
       "   1543.3992919921875,\n",
       "   1543.8243408203125,\n",
       "   1543.365966796875,\n",
       "   1544.0169677734375,\n",
       "   1543.1585693359375,\n",
       "   1542.9610595703125,\n",
       "   1542.5894775390625,\n",
       "   1542.7379150390625,\n",
       "   1543.0203857421875,\n",
       "   1543.7384033203125,\n",
       "   1542.9002685546875,\n",
       "   1543.7398681640625,\n",
       "   1544.3232421875,\n",
       "   1545.715576171875,\n",
       "   1545.1552734375,\n",
       "   1545.8101806640625,\n",
       "   1546.0533447265625,\n",
       "   1544.372314453125,\n",
       "   1544.67529296875,\n",
       "   1544.34765625,\n",
       "   1544.1064453125,\n",
       "   1544.3670654296875,\n",
       "   1544.6517333984375,\n",
       "   1544.76953125,\n",
       "   1545.3780517578125,\n",
       "   1545.27734375,\n",
       "   1545.16259765625,\n",
       "   1545.7657470703125,\n",
       "   1546.99365234375,\n",
       "   1545.575927734375,\n",
       "   1546.02294921875,\n",
       "   1545.3289794921875,\n",
       "   1543.86572265625,\n",
       "   1544.1217041015625,\n",
       "   1545.2646484375,\n",
       "   1545.1514892578125,\n",
       "   1544.5035400390625,\n",
       "   1545.593017578125,\n",
       "   1545.3746337890625,\n",
       "   1544.790283203125,\n",
       "   1543.539794921875,\n",
       "   1543.5806884765625,\n",
       "   1542.916748046875,\n",
       "   1543.9368896484375,\n",
       "   1544.254150390625,\n",
       "   1544.6336669921875,\n",
       "   1543.47509765625,\n",
       "   1542.8704833984375,\n",
       "   1543.18701171875,\n",
       "   1542.9490966796875,\n",
       "   1542.656494140625,\n",
       "   1541.52685546875,\n",
       "   1541.946533203125,\n",
       "   1544.0662841796875,\n",
       "   1544.426025390625,\n",
       "   1544.5667724609375,\n",
       "   1544.4552001953125,\n",
       "   1544.144287109375,\n",
       "   1544.50048828125,\n",
       "   1544.779052734375,\n",
       "   1545.7718505859375,\n",
       "   1545.0205078125,\n",
       "   1546.126953125,\n",
       "   1546.4185791015625,\n",
       "   1547.0694580078125,\n",
       "   1547.1143798828125,\n",
       "   1547.600830078125,\n",
       "   1548.281494140625,\n",
       "   1547.7266845703125,\n",
       "   1547.45751953125,\n",
       "   1546.3543701171875,\n",
       "   1546.2999267578125,\n",
       "   1545.6328125,\n",
       "   1545.165771484375,\n",
       "   1544.661376953125,\n",
       "   1544.843017578125,\n",
       "   1544.66796875,\n",
       "   1544.84619140625,\n",
       "   1545.2119140625,\n",
       "   1545.340576171875,\n",
       "   1544.5701904296875,\n",
       "   1544.5474853515625,\n",
       "   1545.2408447265625,\n",
       "   1546.0411376953125,\n",
       "   1544.8951416015625,\n",
       "   1546.21630859375,\n",
       "   1545.6685791015625,\n",
       "   1547.02734375,\n",
       "   1546.16748046875,\n",
       "   1546.078125,\n",
       "   1546.77099609375,\n",
       "   1546.7613525390625,\n",
       "   1546.3673095703125,\n",
       "   1547.5899658203125,\n",
       "   1547.4951171875,\n",
       "   1547.6678466796875,\n",
       "   1547.416259765625,\n",
       "   1546.981201171875,\n",
       "   1547.93896484375,\n",
       "   1547.4310302734375,\n",
       "   1548.1058349609375,\n",
       "   1548.8756103515625,\n",
       "   1547.570556640625,\n",
       "   1546.52734375,\n",
       "   1546.6722412109375,\n",
       "   1545.3436279296875,\n",
       "   1545.327880859375,\n",
       "   1545.66064453125,\n",
       "   1546.2967529296875,\n",
       "   1546.16650390625,\n",
       "   1546.822021484375,\n",
       "   1546.7135009765625,\n",
       "   1547.57080078125,\n",
       "   1548.1168212890625,\n",
       "   1546.39306640625,\n",
       "   1547.4542236328125,\n",
       "   1548.5550537109375,\n",
       "   1547.024169921875,\n",
       "   1547.13037109375,\n",
       "   1545.165771484375,\n",
       "   1544.4180908203125,\n",
       "   1544.472900390625,\n",
       "   1545.803466796875,\n",
       "   1546.2166748046875,\n",
       "   1545.57470703125,\n",
       "   1545.025634765625,\n",
       "   1546.012451171875,\n",
       "   1545.593505859375,\n",
       "   1546.755859375,\n",
       "   1547.58935546875,\n",
       "   1547.794189453125,\n",
       "   1547.861328125,\n",
       "   1547.196044921875,\n",
       "   1546.6285400390625,\n",
       "   1545.520263671875,\n",
       "   1545.9405517578125,\n",
       "   1546.3363037109375,\n",
       "   1545.6739501953125,\n",
       "   1545.5059814453125,\n",
       "   1545.908203125,\n",
       "   1546.2471923828125,\n",
       "   1546.13916015625,\n",
       "   1546.4583740234375,\n",
       "   1545.694091796875,\n",
       "   1546.4246826171875,\n",
       "   1546.533447265625,\n",
       "   1545.590087890625,\n",
       "   1545.824951171875,\n",
       "   1544.4853515625,\n",
       "   1544.5166015625,\n",
       "   1544.326171875,\n",
       "   1543.4931640625,\n",
       "   1543.4603271484375,\n",
       "   1542.9095458984375,\n",
       "   1542.6768798828125,\n",
       "   1541.84619140625,\n",
       "   1542.9334716796875,\n",
       "   1542.093505859375,\n",
       "   1542.9071044921875,\n",
       "   1543.39404296875,\n",
       "   1543.9522705078125,\n",
       "   1543.9217529296875,\n",
       "   1544.079345703125,\n",
       "   1543.379638671875,\n",
       "   1542.54296875,\n",
       "   1543.3739013671875,\n",
       "   1542.7840576171875,\n",
       "   1542.3555908203125,\n",
       "   1542.352294921875,\n",
       "   1541.9537353515625,\n",
       "   1542.2532958984375,\n",
       "   1542.083740234375,\n",
       "   1542.737060546875,\n",
       "   1542.4947509765625,\n",
       "   1541.88818359375,\n",
       "   1542.8912353515625,\n",
       "   1542.659423828125,\n",
       "   1542.724853515625,\n",
       "   1541.216064453125,\n",
       "   1541.84912109375,\n",
       "   1540.9127197265625,\n",
       "   1540.937255859375,\n",
       "   1541.2650146484375,\n",
       "   1540.5472412109375,\n",
       "   1539.953857421875,\n",
       "   1539.2681884765625,\n",
       "   1538.4073486328125,\n",
       "   1538.9298095703125,\n",
       "   1538.299560546875,\n",
       "   1539.177001953125,\n",
       "   1539.4334716796875,\n",
       "   1539.267578125,\n",
       "   1539.931884765625,\n",
       "   1540.203369140625,\n",
       "   1540.6995849609375,\n",
       "   1540.9002685546875,\n",
       "   1540.7803955078125,\n",
       "   1539.9935302734375,\n",
       "   1540.23681640625,\n",
       "   1539.17578125,\n",
       "   1540.005126953125,\n",
       "   1541.427490234375,\n",
       "   1541.818115234375,\n",
       "   1543.0792236328125,\n",
       "   1542.2933349609375,\n",
       "   1542.351806640625,\n",
       "   1540.9293212890625,\n",
       "   1540.6531982421875,\n",
       "   1539.7545166015625,\n",
       "   1540.0533447265625,\n",
       "   1539.8291015625,\n",
       "   1539.9813232421875,\n",
       "   1539.8751220703125,\n",
       "   1539.5489501953125,\n",
       "   1538.5634765625,\n",
       "   1538.895263671875,\n",
       "   1538.522705078125,\n",
       "   1538.0333251953125,\n",
       "   1537.581298828125,\n",
       "   1538.153564453125,\n",
       "   1537.392822265625,\n",
       "   1536.36572265625,\n",
       "   1535.1143798828125,\n",
       "   1533.931884765625,\n",
       "   1534.283935546875,\n",
       "   1534.40234375,\n",
       "   1535.4669189453125,\n",
       "   1537.1435546875,\n",
       "   1537.347412109375,\n",
       "   1537.84326171875,\n",
       "   1539.682373046875,\n",
       "   1540.3177490234375,\n",
       "   1540.383544921875,\n",
       "   1540.0179443359375,\n",
       "   1538.3658447265625,\n",
       "   1537.463623046875,\n",
       "   1537.2396240234375,\n",
       "   1536.5328369140625,\n",
       "   1536.978759765625,\n",
       "   1536.755126953125,\n",
       "   1536.2481689453125,\n",
       "   1534.30517578125,\n",
       "   1533.711669921875,\n",
       "   1534.321533203125,\n",
       "   1534.177734375,\n",
       "   1535.7279052734375,\n",
       "   1534.91796875,\n",
       "   1534.590576171875,\n",
       "   1534.634521484375,\n",
       "   1534.5692138671875,\n",
       "   1533.95361328125,\n",
       "   1534.1378173828125,\n",
       "   1534.4219970703125,\n",
       "   1534.7247314453125,\n",
       "   1534.7518310546875,\n",
       "   1534.1632080078125,\n",
       "   1533.844482421875,\n",
       "   1534.552978515625,\n",
       "   1535.2896728515625,\n",
       "   1535.937744140625,\n",
       "   1536.935546875,\n",
       "   1536.8238525390625,\n",
       "   1537.0572509765625,\n",
       "   1536.00048828125,\n",
       "   1536.5430908203125,\n",
       "   1535.6405029296875,\n",
       "   1536.2384033203125,\n",
       "   1536.23388671875,\n",
       "   1534.6181640625,\n",
       "   1534.6146240234375,\n",
       "   1533.623779296875,\n",
       "   1532.364501953125,\n",
       "   1532.7847900390625,\n",
       "   1533.8736572265625,\n",
       "   1533.55859375,\n",
       "   1532.885986328125,\n",
       "   1532.4544677734375,\n",
       "   1532.5179443359375,\n",
       "   1532.09033203125,\n",
       "   1531.7369384765625,\n",
       "   1531.76123046875,\n",
       "   1531.927001953125,\n",
       "   1531.6361083984375,\n",
       "   1531.439453125,\n",
       "   1530.761474609375,\n",
       "   1530.5162353515625,\n",
       "   1531.5601806640625,\n",
       "   1531.3743896484375,\n",
       "   1530.9500732421875,\n",
       "   1530.2196044921875,\n",
       "   1530.2646484375,\n",
       "   1531.06591796875,\n",
       "   1531.5087890625,\n",
       "   1531.45703125,\n",
       "   1531.89794921875,\n",
       "   1532.1378173828125,\n",
       "   1530.6612548828125,\n",
       "   1530.3636474609375,\n",
       "   1529.99365234375,\n",
       "   1530.0362548828125,\n",
       "   1530.6043701171875,\n",
       "   1530.354736328125,\n",
       "   1529.6231689453125,\n",
       "   1530.1995849609375,\n",
       "   1531.110107421875,\n",
       "   1531.2509765625,\n",
       "   1531.89501953125,\n",
       "   1532.942626953125,\n",
       "   1532.901611328125,\n",
       "   1531.6246337890625,\n",
       "   1532.46435546875,\n",
       "   1532.7957763671875,\n",
       "   1532.046630859375,\n",
       "   1532.3138427734375,\n",
       "   1532.1541748046875,\n",
       "   1532.94091796875,\n",
       "   1531.5421142578125,\n",
       "   1530.4246826171875,\n",
       "   1530.474853515625,\n",
       "   1531.401611328125,\n",
       "   1530.3543701171875,\n",
       "   1531.357177734375,\n",
       "   1531.41845703125,\n",
       "   1531.5438232421875,\n",
       "   1531.085205078125,\n",
       "   1530.3721923828125,\n",
       "   1530.392822265625,\n",
       "   1529.6192626953125,\n",
       "   1530.692138671875,\n",
       "   1530.3363037109375,\n",
       "   1531.3505859375,\n",
       "   1532.290771484375,\n",
       "   1533.585205078125,\n",
       "   1533.7139892578125,\n",
       "   1533.3603515625,\n",
       "   1533.1715087890625,\n",
       "   1534.149169921875,\n",
       "   1534.96728515625,\n",
       "   1534.4315185546875,\n",
       "   1533.6754150390625,\n",
       "   1533.780517578125,\n",
       "   1533.7099609375,\n",
       "   1531.6448974609375,\n",
       "   1532.6180419921875,\n",
       "   1532.1790771484375,\n",
       "   1531.3653564453125,\n",
       "   1530.3455810546875,\n",
       "   1531.771240234375,\n",
       "   1529.83740234375,\n",
       "   1529.4345703125,\n",
       "   1529.8111572265625,\n",
       "   1530.46826171875,\n",
       "   1529.3048095703125,\n",
       "   1529.95361328125,\n",
       "   1530.5963134765625,\n",
       "   1530.5621337890625,\n",
       "   1529.9532470703125,\n",
       "   1529.9034423828125,\n",
       "   1529.09033203125,\n",
       "   1528.555908203125],\n",
       "  'test_accs': [7064.5146484375,\n",
       "   6998.8175048828125,\n",
       "   6917.50830078125,\n",
       "   6794.850341796875,\n",
       "   6784.2506103515625,\n",
       "   6768.974609375,\n",
       "   6736.21728515625,\n",
       "   6701.6007080078125,\n",
       "   6684.3663330078125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6538.4217529296875,\n",
       "   6535.4066162109375,\n",
       "   nan,\n",
       "   6530.9095458984375,\n",
       "   6528.6785888671875,\n",
       "   6519.1826171875,\n",
       "   6513.650146484375,\n",
       "   6512.0435791015625,\n",
       "   6502.3453369140625,\n",
       "   6503.1016845703125,\n",
       "   6502.074951171875,\n",
       "   6505.586181640625,\n",
       "   6503.60986328125,\n",
       "   6503.1097412109375,\n",
       "   6502.0780029296875,\n",
       "   6504.5933837890625,\n",
       "   6509.8248291015625,\n",
       "   6510.5870361328125,\n",
       "   6510.01953125,\n",
       "   6512.3258056640625,\n",
       "   6511.59765625,\n",
       "   6506.9801025390625,\n",
       "   6499.6881103515625,\n",
       "   6507.2447509765625,\n",
       "   6512.46484375,\n",
       "   6515.8277587890625,\n",
       "   6515.488525390625,\n",
       "   6512.8511962890625,\n",
       "   6508.66748046875,\n",
       "   6504.7493896484375,\n",
       "   6502.96923828125,\n",
       "   6506.047119140625,\n",
       "   6505.887451171875,\n",
       "   6505.87109375,\n",
       "   6502.4267578125,\n",
       "   6496.176025390625,\n",
       "   6490.6368408203125,\n",
       "   6493.3988037109375,\n",
       "   6486.582275390625,\n",
       "   6484.4940185546875,\n",
       "   6481.9246826171875,\n",
       "   6480.25048828125,\n",
       "   6490.9259033203125,\n",
       "   6485.9669189453125,\n",
       "   6488.145263671875,\n",
       "   6489.6348876953125,\n",
       "   6503.4169921875,\n",
       "   6510.3934326171875,\n",
       "   6515.497314453125,\n",
       "   6515.3638916015625,\n",
       "   6521.070068359375,\n",
       "   6519.27783203125,\n",
       "   6516.689697265625,\n",
       "   6512.7568359375,\n",
       "   6510.488037109375,\n",
       "   6510.646484375,\n",
       "   6505.045654296875,\n",
       "   6503.5538330078125,\n",
       "   6501.20703125,\n",
       "   6501.1527099609375,\n",
       "   6502.6685791015625,\n",
       "   6502.164794921875,\n",
       "   6506.806640625,\n",
       "   6506.1796875,\n",
       "   6515.7811279296875,\n",
       "   6525.69384765625,\n",
       "   6526.9833984375,\n",
       "   6515.4417724609375,\n",
       "   6514.4910888671875,\n",
       "   6510.0013427734375,\n",
       "   6513.9066162109375,\n",
       "   6514.427490234375,\n",
       "   6510.2310791015625,\n",
       "   6512.5457763671875,\n",
       "   6513.7938232421875,\n",
       "   6516.8543701171875,\n",
       "   6518.7098388671875,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6495.825927734375,\n",
       "   6491.6175537109375,\n",
       "   6484.949951171875,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6496.435791015625,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6499.58984375,\n",
       "   6498.8685302734375,\n",
       "   6497.06982421875,\n",
       "   nan,\n",
       "   6503.025634765625,\n",
       "   nan,\n",
       "   6516.5390625,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6520.0657958984375,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6507.68115234375,\n",
       "   6510.1925048828125,\n",
       "   6511.3778076171875,\n",
       "   nan,\n",
       "   nan,\n",
       "   6526.547119140625,\n",
       "   6529.5518798828125,\n",
       "   6523.697021484375,\n",
       "   6516.303466796875,\n",
       "   6517.2486572265625,\n",
       "   6518.197509765625,\n",
       "   6521.13525390625,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6524.90673828125,\n",
       "   6518.71435546875,\n",
       "   6519.723388671875,\n",
       "   6513.09326171875,\n",
       "   6522.603271484375,\n",
       "   6523.5694580078125,\n",
       "   6518.1522216796875,\n",
       "   6522.4619140625,\n",
       "   6522.391845703125,\n",
       "   6517.8465576171875,\n",
       "   6517.3258056640625,\n",
       "   6515.860595703125,\n",
       "   6524.452880859375,\n",
       "   6532.275634765625,\n",
       "   6530.7030029296875,\n",
       "   6533.2967529296875,\n",
       "   6529.8284912109375,\n",
       "   6538.6710205078125,\n",
       "   6548.9501953125,\n",
       "   6555.4205322265625,\n",
       "   6563.0220947265625,\n",
       "   6571.4052734375,\n",
       "   6573.05615234375,\n",
       "   6582.8887939453125,\n",
       "   6584.154052734375,\n",
       "   6576.81201171875,\n",
       "   6577.8818359375,\n",
       "   6580.71240234375,\n",
       "   6572.189453125,\n",
       "   6571.545166015625,\n",
       "   6566.561279296875,\n",
       "   6563.51171875,\n",
       "   6562.79052734375,\n",
       "   6567.818359375,\n",
       "   6568.594970703125,\n",
       "   6571.0223388671875,\n",
       "   6566.380859375,\n",
       "   6563.1080322265625,\n",
       "   6562.5728759765625,\n",
       "   6565.55029296875,\n",
       "   6562.0330810546875,\n",
       "   6566.32958984375,\n",
       "   6569.2021484375,\n",
       "   6576.8271484375,\n",
       "   6574.9893798828125,\n",
       "   6576.2386474609375,\n",
       "   6575.3341064453125,\n",
       "   6571.2470703125,\n",
       "   6566.13623046875,\n",
       "   6567.640869140625,\n",
       "   6564.8184814453125,\n",
       "   6567.8873291015625,\n",
       "   6571.726318359375,\n",
       "   6571.6561279296875,\n",
       "   6574.43310546875,\n",
       "   6570.935791015625,\n",
       "   6568.936279296875,\n",
       "   6570.0814208984375,\n",
       "   6570.1153564453125,\n",
       "   6567.2138671875,\n",
       "   6563.0330810546875,\n",
       "   6567.9888916015625,\n",
       "   6570.6805419921875,\n",
       "   6569.250732421875,\n",
       "   6564.320068359375,\n",
       "   6560.4639892578125,\n",
       "   6561.7069091796875,\n",
       "   6562.41357421875,\n",
       "   6557.9659423828125,\n",
       "   6563.3154296875,\n",
       "   6560.9102783203125,\n",
       "   6552.7579345703125,\n",
       "   6557.5572509765625,\n",
       "   6562.1724853515625,\n",
       "   6560.1378173828125,\n",
       "   6562.390380859375,\n",
       "   6568.3355712890625,\n",
       "   6570.1798095703125,\n",
       "   6575.3319091796875,\n",
       "   6571.2191162109375,\n",
       "   6573.6727294921875,\n",
       "   6568.0125732421875,\n",
       "   6556.5703125,\n",
       "   6563.9912109375,\n",
       "   6565.2552490234375,\n",
       "   6570.4864501953125,\n",
       "   6574.1661376953125,\n",
       "   6576.9757080078125,\n",
       "   6577.6258544921875,\n",
       "   6573.10107421875,\n",
       "   6575.13671875,\n",
       "   6574.296142578125,\n",
       "   6571.5562744140625,\n",
       "   6579.21142578125,\n",
       "   6587.003173828125,\n",
       "   nan,\n",
       "   nan,\n",
       "   6594.66064453125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6576.2633056640625,\n",
       "   6581.403564453125,\n",
       "   6585.72802734375,\n",
       "   6591.8955078125,\n",
       "   6594.1610107421875,\n",
       "   6593.300537109375,\n",
       "   6589.640380859375,\n",
       "   6591.1146240234375,\n",
       "   6590.0872802734375,\n",
       "   nan,\n",
       "   nan,\n",
       "   6589.3311767578125,\n",
       "   6595.7208251953125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6558.7325439453125,\n",
       "   6563.335205078125,\n",
       "   6564.531494140625,\n",
       "   6569.8892822265625,\n",
       "   6569.6048583984375,\n",
       "   6565.326171875,\n",
       "   6566.302734375,\n",
       "   6572.413330078125,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   nan,\n",
       "   6584.0413818359375,\n",
       "   6585.158447265625,\n",
       "   6587.4195556640625,\n",
       "   6588.068603515625,\n",
       "   6579.4925537109375,\n",
       "   6581.6785888671875,\n",
       "   6579.29736328125,\n",
       "   6580.104248046875,\n",
       "   6577.566650390625,\n",
       "   6575.970947265625,\n",
       "   6581.5010986328125,\n",
       "   6583.026123046875,\n",
       "   6581.992431640625,\n",
       "   6594.0052490234375,\n",
       "   6590.3480224609375,\n",
       "   6589.46923828125,\n",
       "   6593.8685302734375,\n",
       "   6588.3568115234375,\n",
       "   6588.1737060546875,\n",
       "   6587.4312744140625,\n",
       "   6580.0888671875,\n",
       "   6576.4371337890625,\n",
       "   6581.883544921875,\n",
       "   6581.827880859375,\n",
       "   nan,\n",
       "   nan,\n",
       "   6573.092041015625,\n",
       "   nan,\n",
       "   6571.21875,\n",
       "   nan,\n",
       "   6563.2152099609375,\n",
       "   6561.382568359375,\n",
       "   6558.8314208984375,\n",
       "   6562.8402099609375,\n",
       "   6560.3858642578125,\n",
       "   6561.85888671875,\n",
       "   6562.7626953125,\n",
       "   6562.804443359375,\n",
       "   6563.027587890625,\n",
       "   6552.5986328125,\n",
       "   6549.3603515625,\n",
       "   6544.3804931640625,\n",
       "   6545.4954833984375,\n",
       "   6548.4832763671875,\n",
       "   6538.84814453125,\n",
       "   6525.36474609375,\n",
       "   6526.9501953125,\n",
       "   6525.3546142578125,\n",
       "   6533.1304931640625,\n",
       "   6534.353271484375,\n",
       "   6542.494140625,\n",
       "   6549.0338134765625,\n",
       "   6552.61279296875,\n",
       "   6553.31103515625,\n",
       "   6553.986328125,\n",
       "   6556.5826416015625,\n",
       "   6547.66845703125,\n",
       "   6543.702880859375,\n",
       "   6547.7186279296875,\n",
       "   6547.27294921875,\n",
       "   6553.2027587890625,\n",
       "   6554.25732421875,\n",
       "   6553.3240966796875,\n",
       "   6550.6937255859375,\n",
       "   6553.8026123046875,\n",
       "   6560.58349609375,\n",
       "   6566.549072265625,\n",
       "   6572.69482421875,\n",
       "   6577.6805419921875,\n",
       "   6572.234130859375,\n",
       "   6571.066162109375,\n",
       "   6575.7265625,\n",
       "   6559.7623291015625,\n",
       "   6565.2740478515625,\n",
       "   6566.83935546875,\n",
       "   6571.3590087890625,\n",
       "   6571.6856689453125,\n",
       "   6572.817138671875,\n",
       "   6573.80810546875,\n",
       "   6576.1976318359375,\n",
       "   6574.8746337890625,\n",
       "   6573.3282470703125,\n",
       "   6565.8707275390625,\n",
       "   6564.224609375,\n",
       "   6561.2562255859375,\n",
       "   6566.21875,\n",
       "   6569.7373046875,\n",
       "   6570.342041015625,\n",
       "   6572.117919921875,\n",
       "   6571.7698974609375,\n",
       "   6578.89990234375,\n",
       "   6578.6103515625,\n",
       "   6572.6500244140625,\n",
       "   6565.0234375,\n",
       "   6558.9906005859375,\n",
       "   6574.0494384765625,\n",
       "   6581.2352294921875,\n",
       "   6576.6640625,\n",
       "   6588.00537109375,\n",
       "   6586.2132568359375,\n",
       "   6590.2930908203125,\n",
       "   6595.0142822265625,\n",
       "   6593.6494140625,\n",
       "   6590.7161865234375,\n",
       "   6594.0330810546875,\n",
       "   6591.6370849609375,\n",
       "   6586.6390380859375,\n",
       "   6586.6414794921875,\n",
       "   6584.9521484375,\n",
       "   6592.4091796875,\n",
       "   6591.2874755859375,\n",
       "   6589.804443359375,\n",
       "   6586.6690673828125,\n",
       "   6579.666748046875,\n",
       "   6580.815673828125,\n",
       "   6582.199462890625,\n",
       "   6584.487548828125,\n",
       "   6582.9906005859375,\n",
       "   6576.2752685546875,\n",
       "   6573.9710693359375],\n",
       "  'pce_r2': [-111.27161990065834,\n",
       "   -61.7206221588606,\n",
       "   -79.52120703727729,\n",
       "   -12.980084565945376,\n",
       "   -5.863334909576775,\n",
       "   -4.166254502531615,\n",
       "   -3.185441227750937,\n",
       "   -2.237790710689601,\n",
       "   -2.0445258874692103,\n",
       "   -1.6106011427725884,\n",
       "   -1.37077095476246,\n",
       "   -1.153380022263399,\n",
       "   -0.9482099325035245,\n",
       "   -0.8279633352844347,\n",
       "   -0.7824111081542662,\n",
       "   -0.7331780386142752,\n",
       "   -0.7165446279901277,\n",
       "   -0.712918870555995,\n",
       "   -0.7318463144920933,\n",
       "   -0.7567047278750998,\n",
       "   -0.7791561681142545,\n",
       "   -0.8188091760807341,\n",
       "   -0.8251578021204071,\n",
       "   -0.8435667189357461,\n",
       "   -0.8946385070160789,\n",
       "   -0.9389700185093388,\n",
       "   -0.9818022004165707,\n",
       "   -1.0419852197179935,\n",
       "   -1.064730933594014,\n",
       "   -1.074779571606371,\n",
       "   -1.126182759224461,\n",
       "   -1.1834999799837096,\n",
       "   -1.189902553682228,\n",
       "   -1.1777380101991328,\n",
       "   -1.182880242989628,\n",
       "   -1.2161852169791865,\n",
       "   -1.2271046624185198,\n",
       "   -1.229354083025414,\n",
       "   -1.2229941671826614,\n",
       "   -1.2074514019310651,\n",
       "   -1.2185243958179934,\n",
       "   -1.1817058271025598,\n",
       "   -1.224270442649002,\n",
       "   -1.2805449703597853,\n",
       "   -1.3098169236274368,\n",
       "   -1.3142490705047147,\n",
       "   -1.3079688649601087,\n",
       "   -1.2770906354174518,\n",
       "   -1.2912950692576781,\n",
       "   -1.2795752940029348,\n",
       "   -1.2634008606864713,\n",
       "   -1.1975747297951118,\n",
       "   -1.1446715656678084,\n",
       "   -1.1733032505595835,\n",
       "   -1.1326332347905859,\n",
       "   -1.0807278447687505,\n",
       "   -1.0540774445535752,\n",
       "   -0.9856871808652503,\n",
       "   -1.0143340113390713,\n",
       "   -0.9980246560004908,\n",
       "   -0.9512475605614277,\n",
       "   -0.9793021984666033,\n",
       "   -0.9668043419591712,\n",
       "   -0.9658198717656845,\n",
       "   -1.0112570656214404,\n",
       "   -1.0125505911667,\n",
       "   -1.0143095472245172,\n",
       "   -1.0246247884330915,\n",
       "   -1.0798616478128698,\n",
       "   -1.0619689234436431,\n",
       "   -1.0861317206756822,\n",
       "   -1.108663279366929,\n",
       "   -1.1200012432740718,\n",
       "   -1.1115329016298214,\n",
       "   -1.0836481879301982,\n",
       "   -1.0882435131845436,\n",
       "   -1.134710395024101,\n",
       "   -1.157231576544127,\n",
       "   -1.2278082424255063,\n",
       "   -1.2260092887191525,\n",
       "   -1.198981489728025,\n",
       "   -1.1686229872536855,\n",
       "   -1.1988132649549144,\n",
       "   -1.1924614075597488,\n",
       "   -1.2224243238909125,\n",
       "   -1.2273645705004714,\n",
       "   -1.167977090909345,\n",
       "   -1.136792685263051,\n",
       "   -1.1799574368353736,\n",
       "   -1.145661453258112,\n",
       "   -1.118686565272558,\n",
       "   -1.0714353974846662,\n",
       "   -1.0885907850310375,\n",
       "   -1.1150058419932622,\n",
       "   -1.1461776147459233,\n",
       "   -1.1266293132320828,\n",
       "   -1.150463432950375,\n",
       "   -1.1379840410737012,\n",
       "   -1.1304608008962709,\n",
       "   -1.1471359027341843,\n",
       "   -1.1946906837344478,\n",
       "   -1.2272357190665546,\n",
       "   -1.2262634718652419,\n",
       "   -1.2252497218702203,\n",
       "   -1.2565705808208039,\n",
       "   -1.1950526771278396,\n",
       "   -1.22706683740576,\n",
       "   -1.2665183310589962,\n",
       "   -1.2457187628479645,\n",
       "   -1.2523627489545897,\n",
       "   -1.3280571151727374,\n",
       "   -1.3007996886316557,\n",
       "   -1.317291677192066,\n",
       "   -1.2871181185729448,\n",
       "   -1.2747359821450464,\n",
       "   -1.3015536115600472,\n",
       "   -1.3386218494580517,\n",
       "   -1.3068639031864744,\n",
       "   -1.3087629473493902,\n",
       "   -1.3000574223046937,\n",
       "   -1.311948079077502,\n",
       "   -1.3076967310849863,\n",
       "   -1.2703295306491715,\n",
       "   -1.256206957121698,\n",
       "   -1.3072404566017828,\n",
       "   -1.2581161292412388,\n",
       "   -1.2605364877713354,\n",
       "   -1.2480987339080514,\n",
       "   -1.239052497398459,\n",
       "   -1.2399571241519545,\n",
       "   -1.1910707085620058,\n",
       "   -1.1777247741475407,\n",
       "   -1.2077283666164824,\n",
       "   -1.2713985501837586,\n",
       "   -1.1991336217002204,\n",
       "   -1.247071741278389,\n",
       "   -1.2004429333952054,\n",
       "   -1.176243211187979,\n",
       "   -1.1976547781596718,\n",
       "   -1.2512977900150393,\n",
       "   -1.236960893487772,\n",
       "   -1.2440477682044713,\n",
       "   -1.296397028594817,\n",
       "   -1.2828425008383144,\n",
       "   -1.2727848341291885,\n",
       "   -1.2428838428604,\n",
       "   -1.2442751414525555,\n",
       "   -1.251239479316716,\n",
       "   -1.2399631254569141,\n",
       "   -1.2357540099156816,\n",
       "   -1.2359783002677411,\n",
       "   -1.262898975501432,\n",
       "   -1.2750573356964847,\n",
       "   -1.2645506691209003,\n",
       "   -1.2416253224991975,\n",
       "   -1.2844987630404794,\n",
       "   -1.2806290903046254,\n",
       "   -1.2527472674553581,\n",
       "   -1.2442286188475862,\n",
       "   -1.2545726451916113,\n",
       "   -1.2673326345404616,\n",
       "   -1.319834281445131,\n",
       "   -1.341720666693246,\n",
       "   -1.3105414347841493,\n",
       "   -1.298049171070272,\n",
       "   -1.3497190781590072,\n",
       "   -1.360780201790397,\n",
       "   -1.3832988327215028,\n",
       "   -1.3706841626961568,\n",
       "   -1.3654495787396987,\n",
       "   -1.3492459023168095,\n",
       "   -1.3382018489949883,\n",
       "   -1.3633722197717333,\n",
       "   -1.4153582898308685,\n",
       "   -1.3477868897753562,\n",
       "   -1.3517251156225063,\n",
       "   -1.3476153350163016,\n",
       "   -1.4394107980684123,\n",
       "   -1.4350572101555348,\n",
       "   -1.4297157936158058,\n",
       "   -1.4725889421357947,\n",
       "   -1.4420389003076082,\n",
       "   -1.3994745973317952,\n",
       "   -1.3835089920677377,\n",
       "   -1.3830749477706026,\n",
       "   -1.3705439185181727,\n",
       "   -1.3537361468234295,\n",
       "   -1.3474200691325469,\n",
       "   -1.3083232124895972,\n",
       "   -1.3056828743135873,\n",
       "   -1.3093377160609827,\n",
       "   -1.3614859678580484,\n",
       "   -1.369531848802748,\n",
       "   -1.3532327482540731,\n",
       "   -1.3292614184687341,\n",
       "   -1.2881818311847213,\n",
       "   -1.3114546358244663,\n",
       "   -1.3206788298309933,\n",
       "   -1.231212894305195,\n",
       "   -1.2302501371052612,\n",
       "   -1.242426313357306,\n",
       "   -1.2890430417221137,\n",
       "   -1.3171190475374344,\n",
       "   -1.276772143602407,\n",
       "   -1.280877476040887,\n",
       "   -1.2840787062042018,\n",
       "   -1.2717195432234827,\n",
       "   -1.2259993393868145,\n",
       "   -1.272524595642475,\n",
       "   -1.253937929886039,\n",
       "   -1.2589023152307734,\n",
       "   -1.2700002895410476,\n",
       "   -1.2398946706525598,\n",
       "   -1.2431990332782998,\n",
       "   -1.2726939889097206,\n",
       "   -1.2181901039714957,\n",
       "   -1.2197650008892915,\n",
       "   -1.200571448166004,\n",
       "   -1.205575355379498,\n",
       "   -1.1867725967246798,\n",
       "   -1.1771357871995316,\n",
       "   -1.1638741045340968,\n",
       "   -1.1917150645746029,\n",
       "   -1.1941166242329686,\n",
       "   -1.1741369637169217,\n",
       "   -1.1799600228156413,\n",
       "   -1.1231598755162637,\n",
       "   -1.1201798053326373,\n",
       "   -1.1585470735876031,\n",
       "   -1.1380646896123254,\n",
       "   -1.1424028855513382,\n",
       "   -1.1476030070251086,\n",
       "   -1.1088459343200796,\n",
       "   -1.129508246532851,\n",
       "   -1.1603814602574625,\n",
       "   -1.1407364955027748,\n",
       "   -1.1289106789591057,\n",
       "   -1.069354773830073,\n",
       "   -1.1030661491243552,\n",
       "   -1.106099539766297,\n",
       "   -1.1261956084871803,\n",
       "   -1.1130387297147628,\n",
       "   -1.1038276598533119,\n",
       "   -1.1417304563138084,\n",
       "   -1.1699554734733941,\n",
       "   -1.187578320565803,\n",
       "   -1.2010001424196988,\n",
       "   -1.2065572822761221,\n",
       "   -1.1785404133919313,\n",
       "   -1.1890855542567857,\n",
       "   -1.1966343575462357,\n",
       "   -1.158018291227267,\n",
       "   -1.1895874485170594,\n",
       "   -1.1972062259535114,\n",
       "   -1.1386222840997862,\n",
       "   -1.1666423184302945,\n",
       "   -1.1294949230207512,\n",
       "   -1.1214050913071203,\n",
       "   -1.1567481234040446,\n",
       "   -1.1437616978348828,\n",
       "   -1.149176198839852,\n",
       "   -1.1475274010904148,\n",
       "   -1.1713358312789883,\n",
       "   -1.1508227173559926,\n",
       "   -1.1467065711277287,\n",
       "   -1.1604514188639166,\n",
       "   -1.14792169515481,\n",
       "   -1.0932741697404107,\n",
       "   -1.0987766232931402,\n",
       "   -1.1255323538367263,\n",
       "   -1.121310826883327,\n",
       "   -1.1588554566709126,\n",
       "   -1.1329487078236253,\n",
       "   -1.1997681245261362,\n",
       "   -1.1818744485902113,\n",
       "   -1.1878051083821806,\n",
       "   -1.1855537238905312,\n",
       "   -1.188048289831154,\n",
       "   -1.2241802744729715,\n",
       "   -1.2860438925465956,\n",
       "   -1.2431781454584878,\n",
       "   -1.2292388349939793,\n",
       "   -1.2727965455710044,\n",
       "   -1.2872677941334945,\n",
       "   -1.2526349215783,\n",
       "   -1.3110220549093485,\n",
       "   -1.3419228225279607,\n",
       "   -1.320841553280237,\n",
       "   -1.2610829676134778,\n",
       "   -1.2475967038536644,\n",
       "   -1.212786029494564,\n",
       "   -1.2433722658039081,\n",
       "   -1.2451513384314654,\n",
       "   -1.2187060557743776,\n",
       "   -1.2180528752881434,\n",
       "   -1.2091378072000931,\n",
       "   -1.2553439498992334,\n",
       "   -1.2443162223180213,\n",
       "   -1.2312806282277817,\n",
       "   -1.2481834142795014,\n",
       "   -1.2614940691641454,\n",
       "   -1.3500126858845203,\n",
       "   -1.316387692877429,\n",
       "   -1.2645751938535739,\n",
       "   -1.2651936443890732,\n",
       "   -1.2319007232814987,\n",
       "   -1.2398060124685175,\n",
       "   -1.2517507449066034,\n",
       "   -1.248989979217805,\n",
       "   -1.2568414019244525,\n",
       "   -1.2755638305956145,\n",
       "   -1.2873350413422018,\n",
       "   -1.3189470918097155,\n",
       "   -1.2469065579179115,\n",
       "   -1.2540487188540168,\n",
       "   -1.2597433642366225,\n",
       "   -1.2291385225584426,\n",
       "   -1.2359687181343393,\n",
       "   -1.2289041706461483,\n",
       "   -1.250491956085459,\n",
       "   -1.2547146310676132,\n",
       "   -1.3079436601070342,\n",
       "   -1.3379378320305362,\n",
       "   -1.3942900342036113,\n",
       "   -1.4100924784836,\n",
       "   -1.4365905283239022,\n",
       "   -1.4558363887977506,\n",
       "   -1.4650034636456337,\n",
       "   -1.506448433889822,\n",
       "   -1.466328967311672,\n",
       "   -1.47906099143853,\n",
       "   -1.440116839011489,\n",
       "   -1.4471485600024883,\n",
       "   -1.4508134764093321,\n",
       "   -1.4254885592216984,\n",
       "   -1.388423098545053,\n",
       "   -1.3970409644960209,\n",
       "   -1.419050057285646,\n",
       "   -1.3513175089489504,\n",
       "   -1.358237853035594,\n",
       "   -1.3976920522809313,\n",
       "   -1.302287193936603,\n",
       "   -1.255396123505415,\n",
       "   -1.2270482870150277,\n",
       "   -1.229774411611492,\n",
       "   -1.2454938207628752,\n",
       "   -1.2471481041184762,\n",
       "   -1.267333454631871,\n",
       "   -1.2152015522480148,\n",
       "   -1.2840163622141678,\n",
       "   -1.355465260623447,\n",
       "   -1.3599864261787236,\n",
       "   -1.4033719628785648,\n",
       "   -1.421104493318805,\n",
       "   -1.4028262284163993,\n",
       "   -1.344248631079143,\n",
       "   -1.3201027712106868,\n",
       "   -1.3085552397960183,\n",
       "   -1.3045832485203448,\n",
       "   -1.3321770259193961,\n",
       "   -1.3001552688409928,\n",
       "   -1.3233395790634646,\n",
       "   -1.3623308195736321,\n",
       "   -1.2957661751483616,\n",
       "   -1.3061088019395664,\n",
       "   -1.3477859614164243,\n",
       "   -1.383356093901781,\n",
       "   -1.4080108946952974,\n",
       "   -1.378546318022881,\n",
       "   -1.3583893301380376,\n",
       "   -1.3324959080758747,\n",
       "   -1.2836652807821816,\n",
       "   -1.3426202911243563,\n",
       "   -1.3184562416333145,\n",
       "   -1.3318342045592284,\n",
       "   -1.3011116760645955,\n",
       "   -1.3402797443484764,\n",
       "   -1.2684734849273198,\n",
       "   -1.3021947938254033,\n",
       "   -1.34863569396006,\n",
       "   -1.3238883958257133,\n",
       "   -1.339701152505004,\n",
       "   -1.3901482878090854,\n",
       "   -1.439644866329016,\n",
       "   -1.4208942146812014,\n",
       "   -1.4526941545574683,\n",
       "   -1.452115796328488,\n",
       "   -1.4572592571153469,\n",
       "   -1.4200693467685386,\n",
       "   -1.4980790293864383,\n",
       "   -1.5469985646008348,\n",
       "   -1.5929356459608441,\n",
       "   -1.5384764666297475,\n",
       "   -1.4939291797103547,\n",
       "   -1.4729621521159366,\n",
       "   -1.5088122366053547,\n",
       "   -1.545202242885039,\n",
       "   -1.3919890507724006,\n",
       "   -1.3561421419508002,\n",
       "   -1.3486848877449966,\n",
       "   -1.347986367545495,\n",
       "   -1.295269643168257,\n",
       "   -1.347643315209588,\n",
       "   -1.4002372969455736,\n",
       "   -1.375450001686509,\n",
       "   -1.358359910407275,\n",
       "   -1.380135299117387,\n",
       "   -1.3752146596750245,\n",
       "   -1.420776283076504,\n",
       "   -1.4673034852098672,\n",
       "   -1.4825215406051235,\n",
       "   -1.4782596303630475,\n",
       "   -1.497583655160991,\n",
       "   -1.5026076837651008,\n",
       "   -1.5399539303478864,\n",
       "   -1.4995991426228534,\n",
       "   -1.501164979738704,\n",
       "   -1.4608639721733647,\n",
       "   -1.4723661534041272,\n",
       "   -1.456088423770042,\n",
       "   -1.4880380227705605,\n",
       "   -1.4730248862502875,\n",
       "   -1.549501866277195,\n",
       "   -1.5489286410979126,\n",
       "   -1.5437828759699204,\n",
       "   -1.5513852675747284,\n",
       "   -1.554319234137854,\n",
       "   -1.5769986792194928,\n",
       "   -1.5876847335576816,\n",
       "   -1.5970153778382823,\n",
       "   -1.5990734640979811,\n",
       "   -1.5616035343383694,\n",
       "   -1.5574171745855816,\n",
       "   -1.577442099935181,\n",
       "   -1.5907430783560748,\n",
       "   -1.614339499865443,\n",
       "   -1.5577663760945684,\n",
       "   -1.533940569099908,\n",
       "   -1.5485068228368193,\n",
       "   -1.5925045533544622,\n",
       "   -1.6300797555228295,\n",
       "   -1.61832182723145,\n",
       "   -1.576403946242039,\n",
       "   -1.601074858864914,\n",
       "   -1.55533063716087,\n",
       "   -1.575913098900069,\n",
       "   -1.5490622988849245,\n",
       "   -1.6144581797198616,\n",
       "   -1.5916216902242217,\n",
       "   -1.6060034251315853,\n",
       "   -1.5557114358644673,\n",
       "   -1.5873013088682026,\n",
       "   -1.468225353334327,\n",
       "   -1.4909860151624956,\n",
       "   -1.4851080430990518,\n",
       "   -1.5283648879211347,\n",
       "   -1.4914229417268867,\n",
       "   -1.51534231891881,\n",
       "   -1.4955427399928625,\n",
       "   -1.4437967454025284,\n",
       "   -1.46813942132045,\n",
       "   -1.473673456494602,\n",
       "   -1.4366235930863707,\n",
       "   -1.4593820403640945,\n",
       "   -1.4845263130466524,\n",
       "   -1.494953389915778,\n",
       "   -1.4136353060840094,\n",
       "   -1.437799261486298,\n",
       "   -1.4293743590508314,\n",
       "   -1.5404615320749455,\n",
       "   -1.5383474786131548,\n",
       "   -1.5804556996926298,\n",
       "   -1.5260445199489188,\n",
       "   -1.4904122224366505,\n",
       "   -1.461964829376314,\n",
       "   -1.5191503205819301,\n",
       "   -1.513647792309397,\n",
       "   -1.46393020976152,\n",
       "   -1.5140370621702792,\n",
       "   -1.455626809574457,\n",
       "   -1.478198115734319,\n",
       "   -1.5010478477367188,\n",
       "   -1.4508689404514135,\n",
       "   -1.4324913449257655,\n",
       "   -1.3918840089379296,\n",
       "   -1.42997968702536,\n",
       "   -1.4837797636471426,\n",
       "   -1.4174624917930583,\n",
       "   -1.4313179529758786,\n",
       "   -1.4011038912195288,\n",
       "   -1.2823837665442048,\n",
       "   -1.3272146360736148,\n",
       "   -1.3399870676670216,\n",
       "   -1.2752127610311152,\n",
       "   -1.2496321854881374,\n",
       "   -1.355039418407824,\n",
       "   -1.4160769500364525,\n",
       "   -1.4425028100190902,\n",
       "   -1.2955708131426453,\n",
       "   -1.2986842383019286],\n",
       "  'voc_r2': [-6.742752889976094,\n",
       "   -28.9942060160564,\n",
       "   -84.67052371700883,\n",
       "   -183.3709263821056,\n",
       "   -216.89965228392904,\n",
       "   -186.7062459167528,\n",
       "   -152.99024527772906,\n",
       "   -122.96306149176586,\n",
       "   -151.39933708044296,\n",
       "   -268.2274266061659,\n",
       "   -552.4003704702031,\n",
       "   -814.1377897102602,\n",
       "   -1197.5962484341446,\n",
       "   -1986.1888495395897,\n",
       "   -4002.7944891060592,\n",
       "   -7491.2739954562,\n",
       "   -8314.941180131455,\n",
       "   -11011.031773776467,\n",
       "   -33561.95064558236,\n",
       "   -5851547.986416789,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -827187.207347692,\n",
       "   -69268.0197252453,\n",
       "   -28302.94484115194,\n",
       "   -19687.027319897577,\n",
       "   -6677.306604817474,\n",
       "   -4089.2303207040277,\n",
       "   -2279.871040719767,\n",
       "   -1453.3923045678252,\n",
       "   -1170.4837436569474,\n",
       "   -886.4662306508088,\n",
       "   -728.0280274532475,\n",
       "   -546.1604186509988,\n",
       "   -473.80397348154895,\n",
       "   -408.85250295456507,\n",
       "   -360.4871528699941,\n",
       "   -324.2610874986021,\n",
       "   -376.20531890380687,\n",
       "   -403.8252017847769,\n",
       "   -350.3999278647273,\n",
       "   -301.2241600888922,\n",
       "   -290.6760638911455,\n",
       "   -284.43100987521274,\n",
       "   -254.8432360654732,\n",
       "   -230.52254060572736,\n",
       "   -221.52934558190705,\n",
       "   -210.5263483604543,\n",
       "   -209.5982571149431,\n",
       "   -179.061676619533,\n",
       "   -168.39510731899023,\n",
       "   -167.97496536668083,\n",
       "   -166.82811199490737,\n",
       "   -168.07911488210942,\n",
       "   -147.6984813681178,\n",
       "   -140.83927748948926,\n",
       "   -140.65846945071925,\n",
       "   -146.25802606416977,\n",
       "   -147.43224701164544,\n",
       "   -133.2320546269856,\n",
       "   -152.84561245556762,\n",
       "   -148.2916978375405,\n",
       "   -149.00332242948556,\n",
       "   -68.40649764092095,\n",
       "   -71.43071486105158,\n",
       "   -56.60913468604067,\n",
       "   -60.12488677191989,\n",
       "   -77.0691637460946,\n",
       "   -40.894045219371776,\n",
       "   -48.57587907012561,\n",
       "   -43.311364795022186,\n",
       "   -46.90264494689306,\n",
       "   -42.69224674560419,\n",
       "   -49.16604957688942,\n",
       "   -63.4573256990696,\n",
       "   -77.48814348242288,\n",
       "   -75.35135527556464,\n",
       "   -63.72351140166744,\n",
       "   -42.31615454851989,\n",
       "   -54.44709513446705,\n",
       "   -62.44737078628861,\n",
       "   -48.86802940649934,\n",
       "   -54.42920262796849,\n",
       "   -55.6077821195893,\n",
       "   -61.067539061737754,\n",
       "   -54.67598216605389,\n",
       "   -71.24457384321953,\n",
       "   -58.720098022287196,\n",
       "   -66.79195002266944,\n",
       "   -69.41394712482224,\n",
       "   -71.91884597569337,\n",
       "   -77.88637691226735,\n",
       "   -86.76185545115398,\n",
       "   -77.38772548775694,\n",
       "   -46.28270814178358,\n",
       "   -62.15653976151097,\n",
       "   -55.98804579417573,\n",
       "   -58.71186244158411,\n",
       "   -70.39960318371524,\n",
       "   -56.16806453453504,\n",
       "   -52.986563952716715,\n",
       "   -50.036126421844116,\n",
       "   -51.557109064250675,\n",
       "   -54.26923673605493,\n",
       "   -64.61361209923352,\n",
       "   -79.11619439881154,\n",
       "   -68.35138048451591,\n",
       "   -76.6896031628404,\n",
       "   -55.574105103645095,\n",
       "   -63.92310189550051,\n",
       "   -55.010632108784684,\n",
       "   -68.75746327745647,\n",
       "   -91.10449683500153,\n",
       "   -91.98591416096372,\n",
       "   -75.94538897640491,\n",
       "   -88.82835710634897,\n",
       "   -66.82349771265478,\n",
       "   -82.80907179292987,\n",
       "   -106.72268115040843,\n",
       "   -101.68585962612781,\n",
       "   -116.59768963860591,\n",
       "   -94.83342990845031,\n",
       "   -76.0035122779138,\n",
       "   -83.63943725182341,\n",
       "   -98.16747787659739,\n",
       "   -125.84073352201585,\n",
       "   -129.29523141042355,\n",
       "   -124.92953748401585,\n",
       "   -131.6111948150718,\n",
       "   -139.14911460183993,\n",
       "   -125.94795126604815,\n",
       "   -137.4141348903715,\n",
       "   -138.34805382628045,\n",
       "   -134.03978077142693,\n",
       "   -121.15563290962513,\n",
       "   -124.25876491357658,\n",
       "   -125.17938722794032,\n",
       "   -127.21264236237349,\n",
       "   -118.16580339277581,\n",
       "   -107.71856471526173,\n",
       "   -104.83178913049058,\n",
       "   -100.19844228146528,\n",
       "   -99.60813469495493,\n",
       "   -102.04209152690636,\n",
       "   -102.7442404013326,\n",
       "   -103.46055063825743,\n",
       "   -99.92418249267368,\n",
       "   -98.228672027632,\n",
       "   -99.38873355069857,\n",
       "   -96.42512947049316,\n",
       "   -93.37158194085711,\n",
       "   -95.24398922817913,\n",
       "   -97.35725265378537,\n",
       "   -99.19583017463948,\n",
       "   -96.90797088465611,\n",
       "   -92.82832703103588,\n",
       "   -97.65143447970343,\n",
       "   -105.10968848110912,\n",
       "   -106.64210023037609,\n",
       "   -104.20094707926833,\n",
       "   -97.92753133802756,\n",
       "   -96.70970073601623,\n",
       "   -88.85228229521454,\n",
       "   -98.71153115363856,\n",
       "   -102.26386365583471,\n",
       "   -95.02316533996306,\n",
       "   -94.07303913526577,\n",
       "   -94.07175267126075,\n",
       "   -95.51539484490442,\n",
       "   -95.44235971136932,\n",
       "   -93.29585895534846,\n",
       "   -92.3512296339255,\n",
       "   -87.3201839211483,\n",
       "   -91.60729375948729,\n",
       "   -89.59602937830482,\n",
       "   -90.27234920194348,\n",
       "   -87.65338896811471,\n",
       "   -84.69146917524621,\n",
       "   -80.12380027536463,\n",
       "   -78.19404960845249,\n",
       "   -82.27955028508374,\n",
       "   -85.74408292226359,\n",
       "   -78.06857652275535,\n",
       "   -75.9680355275651,\n",
       "   -66.24263251439272,\n",
       "   -69.46096497409452,\n",
       "   -72.71125041080381,\n",
       "   -73.55566243897786,\n",
       "   -72.29834676430927,\n",
       "   -75.28585358406589,\n",
       "   -77.76308363942289,\n",
       "   -80.32027712531314,\n",
       "   -79.77771834090336,\n",
       "   -74.51483605922749,\n",
       "   -74.13579328038324,\n",
       "   -60.88784067664411,\n",
       "   -69.5549136152728,\n",
       "   -65.15004885039693,\n",
       "   -36.470573360169695,\n",
       "   -40.313959615067766,\n",
       "   -49.242159170665936,\n",
       "   -54.26581202322143,\n",
       "   -61.63530508765695,\n",
       "   -64.3855713070266,\n",
       "   -43.768475154734794,\n",
       "   -38.87188489658045,\n",
       "   -42.490608132968404,\n",
       "   -44.9490238139717,\n",
       "   -47.8499564595451,\n",
       "   -47.20008092109144,\n",
       "   -45.72162891806429,\n",
       "   -34.791187896913286,\n",
       "   -41.232730243807204,\n",
       "   -47.2045291177056,\n",
       "   -54.79466692565502,\n",
       "   -45.28164518832607,\n",
       "   -48.343705706766045,\n",
       "   -43.04908958480423,\n",
       "   -51.05254698841081,\n",
       "   -41.86600505518574,\n",
       "   -40.27535084785752,\n",
       "   -34.190171459785944,\n",
       "   -43.26282497841055,\n",
       "   -50.51153529671495,\n",
       "   -42.98446591998104,\n",
       "   -51.8526557342934,\n",
       "   -39.02336009124091,\n",
       "   -44.329646264708344,\n",
       "   -48.23461761682673,\n",
       "   -43.94131961897926,\n",
       "   -50.80935428421085,\n",
       "   -47.65698814721628,\n",
       "   -39.87629040764247,\n",
       "   -43.344444199236975,\n",
       "   -57.26059582525671,\n",
       "   -63.7930300437597,\n",
       "   -62.23069958654334,\n",
       "   -70.70651884183147,\n",
       "   -63.518658103061654,\n",
       "   -69.68117123218991,\n",
       "   -71.2632745485649,\n",
       "   -71.9629152552871,\n",
       "   -75.23730217058406,\n",
       "   -63.54811658772316,\n",
       "   -58.98900952248187,\n",
       "   -63.98693039019517,\n",
       "   -63.059730880283084,\n",
       "   -55.79740705789354,\n",
       "   -55.5463929870648,\n",
       "   -48.67336801323797,\n",
       "   -51.36393054178019,\n",
       "   -56.21860671509363,\n",
       "   -62.076502344638534,\n",
       "   -73.13917710990354,\n",
       "   -70.1355158771097,\n",
       "   -68.2349461949165,\n",
       "   -35.83238715988405,\n",
       "   -35.744977157088655,\n",
       "   -28.51146952879032,\n",
       "   -34.73653857541492,\n",
       "   -21.266241311992047,\n",
       "   -28.810940765179623,\n",
       "   -32.52564091073226,\n",
       "   -31.472131032547026,\n",
       "   -33.63658898937138,\n",
       "   -35.97177610810127,\n",
       "   -36.39047994352652,\n",
       "   -46.422398818844535,\n",
       "   -45.024959352526956,\n",
       "   -49.25945791320368,\n",
       "   -50.99225898283588,\n",
       "   -67.10501312530815,\n",
       "   -68.49739922234633,\n",
       "   -62.80953065888904,\n",
       "   -68.99671916877726,\n",
       "   -66.15837300545873,\n",
       "   -65.92059787672126,\n",
       "   -71.3719001303313,\n",
       "   -73.2934264246307,\n",
       "   -50.07967427045232,\n",
       "   -56.13132344761122,\n",
       "   -54.57942587843655,\n",
       "   -33.397157952450314,\n",
       "   -37.944489031709786,\n",
       "   -45.667387567627145,\n",
       "   -68.35292362735132,\n",
       "   -75.6489125596288,\n",
       "   -76.82776248865534,\n",
       "   -80.09384841894025,\n",
       "   -80.82885567228091,\n",
       "   -78.18515711522318,\n",
       "   -75.66134577466737,\n",
       "   -74.0849431440259,\n",
       "   -75.53520144076082,\n",
       "   -72.1609559640116,\n",
       "   -73.91431500939439,\n",
       "   -72.94313750714689,\n",
       "   -75.18765380734973,\n",
       "   -70.01169449264573,\n",
       "   -70.05669151667104,\n",
       "   -73.37741768873758,\n",
       "   -76.51684393802728,\n",
       "   -73.66177660755127,\n",
       "   -72.96717172126002,\n",
       "   -74.30833415601833,\n",
       "   -71.57485608845799,\n",
       "   -73.57321197439789,\n",
       "   -76.65117405009623,\n",
       "   -78.86323247195408,\n",
       "   -79.26471983047969,\n",
       "   -75.41552889781859,\n",
       "   -73.64952483752123,\n",
       "   -73.51758070001905,\n",
       "   -73.11877981086327,\n",
       "   -74.30194768666418,\n",
       "   -73.08497267011532,\n",
       "   -74.53747865108208,\n",
       "   -73.18094576432107,\n",
       "   -74.77483680309462,\n",
       "   -77.45991755344875,\n",
       "   -78.89326651147813,\n",
       "   -78.5065882728654,\n",
       "   -75.43089543011278,\n",
       "   -74.49402650096162,\n",
       "   -69.75863227563657,\n",
       "   -72.1884129683614,\n",
       "   -73.79199964970837,\n",
       "   -71.23341202135165,\n",
       "   -69.74652536767687,\n",
       "   -71.91880828346478,\n",
       "   -75.03064037593354,\n",
       "   -74.41826677622059,\n",
       "   -70.4362285066685,\n",
       "   -70.34511387513531,\n",
       "   -72.36138006624795,\n",
       "   -69.5570488327948,\n",
       "   -69.56676964404375,\n",
       "   -72.1123903070212,\n",
       "   -75.19580450140572,\n",
       "   -81.23161965236913,\n",
       "   -83.83061335479896,\n",
       "   -83.2718313834333,\n",
       "   -79.904769734887,\n",
       "   -82.04466195298933,\n",
       "   -79.30086912094029,\n",
       "   -79.59694443309151,\n",
       "   -81.69534670385526,\n",
       "   -79.35665395450295,\n",
       "   -76.62481072288905,\n",
       "   -77.6068267698551,\n",
       "   -75.42342761516352,\n",
       "   -72.46849058639631,\n",
       "   -73.59542031817709,\n",
       "   -73.35042038080645,\n",
       "   -70.50113727090367,\n",
       "   -71.15886381131718,\n",
       "   -69.96257106284557,\n",
       "   -70.14709707122385,\n",
       "   -71.02476453731911,\n",
       "   -71.27137219986128,\n",
       "   -71.60947056629826,\n",
       "   -71.98524214615833,\n",
       "   -73.56681652042096,\n",
       "   -76.75382915218984,\n",
       "   -79.8493306988242,\n",
       "   -76.86238480757676,\n",
       "   -81.31342038194776,\n",
       "   -80.21415483619825,\n",
       "   -77.3484663247895,\n",
       "   -71.3237360953534,\n",
       "   -67.43884872627905,\n",
       "   -68.3850287534906,\n",
       "   -68.45365136467262,\n",
       "   -53.18316088916519,\n",
       "   -48.470465739468125,\n",
       "   -36.39783011888193,\n",
       "   -49.93363369637325,\n",
       "   -42.85126237153873,\n",
       "   -48.34857558135762,\n",
       "   -43.81246257251623,\n",
       "   -53.79356326118535,\n",
       "   -36.860456847075646,\n",
       "   -44.12921408625084,\n",
       "   -44.76333952823771,\n",
       "   -37.96102546296733,\n",
       "   -46.718389796563855,\n",
       "   -29.703588365465347,\n",
       "   -29.996145652639306,\n",
       "   -35.93673290830971,\n",
       "   -47.37477305246136,\n",
       "   -59.723946424773565,\n",
       "   -71.27752696592562,\n",
       "   -72.33779442874636,\n",
       "   -75.00515745571744,\n",
       "   -74.93588779495417,\n",
       "   -71.04694848694055,\n",
       "   -67.53942636738802,\n",
       "   -67.75233786483335,\n",
       "   -63.95206921464707,\n",
       "   -65.72960949409516,\n",
       "   -63.87220091396905,\n",
       "   -61.77754875750697,\n",
       "   -59.84464549019387,\n",
       "   -58.02998171976607,\n",
       "   -37.701138035881655,\n",
       "   -42.949113438063165,\n",
       "   -42.20464673529206,\n",
       "   -45.13061344008479,\n",
       "   -47.80916181484811,\n",
       "   -51.60702308132343,\n",
       "   -51.582494676345945,\n",
       "   -52.60983350685524,\n",
       "   -54.76879235424174,\n",
       "   -43.824580092530624,\n",
       "   -34.44325763971548,\n",
       "   -35.88823235206783,\n",
       "   -36.9295252707424,\n",
       "   -49.57349520928047,\n",
       "   -55.64687126951389,\n",
       "   -55.01064120288673,\n",
       "   -44.621140592185,\n",
       "   -42.554309794422764,\n",
       "   -42.209102533173144,\n",
       "   -48.86315675446233,\n",
       "   -53.61213478984131,\n",
       "   -62.72994129413032,\n",
       "   -64.38415416279965,\n",
       "   -69.51498650149179,\n",
       "   -63.524210699022916,\n",
       "   -48.35601808703639,\n",
       "   -49.4421358681884,\n",
       "   -56.579570665254636,\n",
       "   -62.09431747196251,\n",
       "   -65.72848387256286,\n",
       "   -68.16866072284891,\n",
       "   -63.439615896266076,\n",
       "   -46.38496153592806,\n",
       "   -55.00393160415686,\n",
       "   -50.711711301746355,\n",
       "   -48.30176968044688,\n",
       "   -55.92971963973254,\n",
       "   -47.86649199329578,\n",
       "   -53.28293844402244,\n",
       "   -52.746642202006676,\n",
       "   -50.575241497621526,\n",
       "   -52.02012558106838,\n",
       "   -58.35739308345739,\n",
       "   -47.46427031631877,\n",
       "   -53.020263358735896,\n",
       "   -63.54540131072201,\n",
       "   -55.60568514602589,\n",
       "   -60.48284353426744,\n",
       "   -47.78782152119705,\n",
       "   -59.67645020958337,\n",
       "   -46.69064043882158,\n",
       "   -51.05473516942633,\n",
       "   -58.25745670837211,\n",
       "   -48.844277428663844,\n",
       "   -39.0178117817569,\n",
       "   -38.34830049417577,\n",
       "   -36.792232650282614,\n",
       "   -45.275499490622515,\n",
       "   -38.10720526457255,\n",
       "   -46.15320845817422,\n",
       "   -49.670823924319045,\n",
       "   -54.72735071055041,\n",
       "   -51.51393895550494,\n",
       "   -52.58386520327485,\n",
       "   -51.232821242440345,\n",
       "   -53.772475039211514,\n",
       "   -54.94956512433235,\n",
       "   -57.212042067584214,\n",
       "   -53.934800852139226,\n",
       "   -53.61138325319088,\n",
       "   -55.289093030837115,\n",
       "   -52.61438089815155,\n",
       "   -44.705770083201244,\n",
       "   -36.67877762956111,\n",
       "   -40.72836940226575,\n",
       "   -38.81383211616005,\n",
       "   -39.76167158347949,\n",
       "   -40.54548538137671,\n",
       "   -38.12631682804832,\n",
       "   -45.52827276663338,\n",
       "   -52.53013724070682,\n",
       "   -52.5889055869963,\n",
       "   -46.10757231859654,\n",
       "   -49.655966085695034,\n",
       "   -60.71605597374961],\n",
       "  'jsc_r2': [-1.3697298981208559,\n",
       "   -0.8752784957730035,\n",
       "   -0.7609792608537869,\n",
       "   -0.7729084749522872,\n",
       "   -0.6911551500819895,\n",
       "   -0.647303938931544,\n",
       "   -0.6326076421549907,\n",
       "   -0.6307786660633496,\n",
       "   -0.6120141305880669,\n",
       "   -0.5941208708639945,\n",
       "   -0.585509929469386,\n",
       "   -0.6190898886178351,\n",
       "   -0.6750947805034895,\n",
       "   -0.7301418280005707,\n",
       "   -0.8002210730892596,\n",
       "   -0.8856836578745559,\n",
       "   -0.9583302133312357,\n",
       "   -1.0260373072497013,\n",
       "   -1.1267710283012882,\n",
       "   -1.1900250771110215,\n",
       "   -1.2774135725388538,\n",
       "   -1.3262950981026362,\n",
       "   -1.4068552351883792,\n",
       "   -1.486069723784603,\n",
       "   -1.543977655938047,\n",
       "   -1.6183716999176814,\n",
       "   -1.623571763837497,\n",
       "   -1.6652427626997475,\n",
       "   -1.693682732732389,\n",
       "   -1.7394623851927582,\n",
       "   -1.7661332645209145,\n",
       "   -1.8461969289890585,\n",
       "   -1.9022426610993297,\n",
       "   -1.9821659126225515,\n",
       "   -2.002986192179571,\n",
       "   -2.0367908582468637,\n",
       "   -2.10830600203441,\n",
       "   -2.1289388807190117,\n",
       "   -2.275781869798408,\n",
       "   -2.3452923791795937,\n",
       "   -2.512576372753221,\n",
       "   -2.544783455472105,\n",
       "   -2.591481020664588,\n",
       "   -2.6651021490570868,\n",
       "   -2.7240244145920456,\n",
       "   -2.8198890759837965,\n",
       "   -2.8615409104826055,\n",
       "   -2.9258121928111342,\n",
       "   -2.8612513644370674,\n",
       "   -2.9578980647657476,\n",
       "   -2.957502815096116,\n",
       "   -2.827034377306774,\n",
       "   -2.6970015210258382,\n",
       "   -2.5219342640683364,\n",
       "   -2.324302297427877,\n",
       "   -2.287593504190589,\n",
       "   -2.2567727374123114,\n",
       "   -2.2271418571338644,\n",
       "   -2.187671935504365,\n",
       "   -2.0901123666355725,\n",
       "   -2.021306540797741,\n",
       "   -1.9864591012160644,\n",
       "   -1.905348989662846,\n",
       "   -1.8940023949802218,\n",
       "   -1.880846038859663,\n",
       "   -1.8423079017990025,\n",
       "   -1.7898481950462966,\n",
       "   -1.714930756188605,\n",
       "   -1.65780966660658,\n",
       "   -1.662438761311285,\n",
       "   -1.6689435056595854,\n",
       "   -1.7161445037208432,\n",
       "   -1.6877065103838231,\n",
       "   -1.6651100121265223,\n",
       "   -1.6989758878355783,\n",
       "   -1.724374457759796,\n",
       "   -1.7047759413993138,\n",
       "   -1.6811622877502272,\n",
       "   -1.6272241433467376,\n",
       "   -1.6081983063877048,\n",
       "   -1.587926726584492,\n",
       "   -1.6224134967301818,\n",
       "   -1.6375258094447158,\n",
       "   -1.6553673307048502,\n",
       "   -1.616339249718778,\n",
       "   -1.625269967820096,\n",
       "   -1.599491236227093,\n",
       "   -1.6553336528734843,\n",
       "   -1.6428174725208002,\n",
       "   -1.5698927326345786,\n",
       "   -1.570181957818459,\n",
       "   -1.5468997990084983,\n",
       "   -1.5707621954213358,\n",
       "   -1.5699886880417457,\n",
       "   -1.5612594046552304,\n",
       "   -1.566043741368718,\n",
       "   -1.5431386030352887,\n",
       "   -1.5171934510539957,\n",
       "   -1.5338557738706364,\n",
       "   -1.5544195207784144,\n",
       "   -1.521999399046389,\n",
       "   -1.5126904580456348,\n",
       "   -1.4733856496611182,\n",
       "   -1.5133462981186039,\n",
       "   -1.5372319776881338,\n",
       "   -1.5639711136491878,\n",
       "   -1.570113653213589,\n",
       "   -1.5651825916741315,\n",
       "   -1.5402541648541055,\n",
       "   -1.5561710769079906,\n",
       "   -1.5079172704821513,\n",
       "   -1.4818812374303336,\n",
       "   -1.449044357110802,\n",
       "   -1.4491795873681435,\n",
       "   -1.4581078950222612,\n",
       "   -1.500230165869691,\n",
       "   -1.5039746999881887,\n",
       "   -1.5391037087124237,\n",
       "   -1.5429626056458337,\n",
       "   -1.5415998526831993,\n",
       "   -1.557720426698765,\n",
       "   -1.5616954384837207,\n",
       "   -1.5674073572179608,\n",
       "   -1.5828374193043149,\n",
       "   -1.5887188381334623,\n",
       "   -1.5884569535156752,\n",
       "   -1.5693192641024587,\n",
       "   -1.5498520245650291,\n",
       "   -1.5551623222805286,\n",
       "   -1.5539909702559465,\n",
       "   -1.5477095764908246,\n",
       "   -1.530086425722975,\n",
       "   -1.5653411984140764,\n",
       "   -1.623672698364686,\n",
       "   -1.7113303267874587,\n",
       "   -1.756661244209536,\n",
       "   -1.7422531962239018,\n",
       "   -1.7406298012497912,\n",
       "   -1.7420412062067,\n",
       "   -1.7099060189860653,\n",
       "   -1.7456504828544337,\n",
       "   -1.7103054815898755,\n",
       "   -1.6997025997878525,\n",
       "   -1.7174614746792667,\n",
       "   -1.6942018876940401,\n",
       "   -1.7122794479976107,\n",
       "   -1.699051849662582,\n",
       "   -1.6958071448303067,\n",
       "   -1.6837115714476427,\n",
       "   -1.6984831546739398,\n",
       "   -1.6074907184717664,\n",
       "   -1.6368192131107104,\n",
       "   -1.6277431195230823,\n",
       "   -1.5938899521210557,\n",
       "   -1.6096795649063758,\n",
       "   -1.5745185266028914,\n",
       "   -1.6050236262904543,\n",
       "   -1.594639042982915,\n",
       "   -1.5874055271342082,\n",
       "   -1.5781082039298573,\n",
       "   -1.565219541714574,\n",
       "   -1.5681821476237503,\n",
       "   -1.5475335876204492,\n",
       "   -1.5762353945565333,\n",
       "   -1.5362024431183423,\n",
       "   -1.5237098223247045,\n",
       "   -1.5316260785741558,\n",
       "   -1.5429763864090602,\n",
       "   -1.5632653328889865,\n",
       "   -1.6086625235842669,\n",
       "   -1.682989638170724,\n",
       "   -1.6823449074674848,\n",
       "   -1.6724671387486465,\n",
       "   -1.61737007968771,\n",
       "   -1.6673392624363639,\n",
       "   -1.645824298844357,\n",
       "   -1.6769569208052588,\n",
       "   -1.6397064456345696,\n",
       "   -1.616862634361064,\n",
       "   -1.6037400844331602,\n",
       "   -1.5932056459270387,\n",
       "   -1.5842008066078472,\n",
       "   -1.5531326013601623,\n",
       "   -1.5498699654983121,\n",
       "   -1.5511968679538612,\n",
       "   -1.5912752949502726,\n",
       "   -1.6065223440024035,\n",
       "   -1.6440401645955705,\n",
       "   -1.6250347268812844,\n",
       "   -1.6163160112314703,\n",
       "   -1.6396253776687124,\n",
       "   -1.634412992157376,\n",
       "   -1.6650302139613387,\n",
       "   -1.6474325838022552,\n",
       "   -1.7057618763437365,\n",
       "   -1.716112056014444,\n",
       "   -1.7717495703523305,\n",
       "   -1.7971656479933156,\n",
       "   -1.7803296734685023,\n",
       "   -1.6967462225907517,\n",
       "   -1.7390650596070008,\n",
       "   -1.6867598920246119,\n",
       "   -1.6794442180937756,\n",
       "   -1.674011959624917,\n",
       "   -1.6977238222564117,\n",
       "   -1.749660382743854,\n",
       "   -1.7655447631582843,\n",
       "   -1.7744321954864928,\n",
       "   -1.7557083504926179,\n",
       "   -1.7424901731988918,\n",
       "   -1.744507864624985,\n",
       "   -1.6984514006994842,\n",
       "   -1.7390643852348155,\n",
       "   -1.7295037898005154,\n",
       "   -1.6259160777672514,\n",
       "   -1.6142528570376085,\n",
       "   -1.6545372107730367,\n",
       "   -1.6497001862263794,\n",
       "   -1.6206026675086669,\n",
       "   -1.632107219503761,\n",
       "   -1.634780406482712,\n",
       "   -1.612607400123446,\n",
       "   -1.6012877648022483,\n",
       "   -1.5697634893109242,\n",
       "   -1.5438576553130385,\n",
       "   -1.5638314803108289,\n",
       "   -1.5776496565581888,\n",
       "   -1.5750653290102057,\n",
       "   -1.5567835486164778,\n",
       "   -1.582363066151541,\n",
       "   -1.6519489887423666,\n",
       "   -1.6407695333835455,\n",
       "   -1.638310325796998,\n",
       "   -1.6220177342322035,\n",
       "   -1.5908405068240032,\n",
       "   -1.6277537604308057,\n",
       "   -1.6252749494200676,\n",
       "   -1.6262042570951252,\n",
       "   -1.6255046707312442,\n",
       "   -1.6095638799192953,\n",
       "   -1.5983882820360127,\n",
       "   -1.6152262682089003,\n",
       "   -1.6486363069587986,\n",
       "   -1.6449845513144323,\n",
       "   -1.663044676703569,\n",
       "   -1.6908268752912567,\n",
       "   -1.6869760663937736,\n",
       "   -1.6715097584588605,\n",
       "   -1.6368528666348783,\n",
       "   -1.6176282881335982,\n",
       "   -1.613834423777197,\n",
       "   -1.65588645777501,\n",
       "   -1.7009609547544122,\n",
       "   -1.687066930146564,\n",
       "   -1.7093181346153714,\n",
       "   -1.6938249084684296,\n",
       "   -1.6626685880755732,\n",
       "   -1.6538169654489692,\n",
       "   -1.6630445220249306,\n",
       "   -1.682929654915796,\n",
       "   -1.6988733364816073,\n",
       "   -1.687633455055615,\n",
       "   -1.6950264221031444,\n",
       "   -1.6806259935804628,\n",
       "   -1.702725758459175,\n",
       "   -1.747209806480301,\n",
       "   -1.743153944191501,\n",
       "   -1.709860798900341,\n",
       "   -1.7077523057489814,\n",
       "   -1.6989692382795871,\n",
       "   -1.6553596088583737,\n",
       "   -1.6604154072276316,\n",
       "   -1.6554812389077092,\n",
       "   -1.630226158701718,\n",
       "   -1.6423873013483554,\n",
       "   -1.605195542066899,\n",
       "   -1.5534103909242103,\n",
       "   -1.5802777137622726,\n",
       "   -1.5711682941564815,\n",
       "   -1.6173430929340187,\n",
       "   -1.5951501487775914,\n",
       "   -1.6091629490389305,\n",
       "   -1.621919290159627,\n",
       "   -1.64337977242903,\n",
       "   -1.6107795626657548,\n",
       "   -1.6289304167234735,\n",
       "   -1.620524539501817,\n",
       "   -1.5734714886479506,\n",
       "   -1.5755423952061451,\n",
       "   -1.5669497522887208,\n",
       "   -1.573424454720913,\n",
       "   -1.580570572121332,\n",
       "   -1.605530110003218,\n",
       "   -1.6076363419689206,\n",
       "   -1.621980482353536,\n",
       "   -1.6166849199588151,\n",
       "   -1.6690553974401143,\n",
       "   -1.6983351095720587,\n",
       "   -1.6964043337778096,\n",
       "   -1.7183809355051323,\n",
       "   -1.6748676677960401,\n",
       "   -1.6835294192991141,\n",
       "   -1.7127716969249134,\n",
       "   -1.6955487988968634,\n",
       "   -1.684621008392667,\n",
       "   -1.7297135335164509,\n",
       "   -1.6812249147236211,\n",
       "   -1.6795973830052495,\n",
       "   -1.687835649480251,\n",
       "   -1.7225086446962496,\n",
       "   -1.7370342345184175,\n",
       "   -1.7844112282672984,\n",
       "   -1.7951796782407938,\n",
       "   -1.827975067725386,\n",
       "   -1.7970727669362807,\n",
       "   -1.7641950611164439,\n",
       "   -1.7789060476441438,\n",
       "   -1.761440874793382,\n",
       "   -1.7369648767057355,\n",
       "   -1.7325262230378757,\n",
       "   -1.7145023078905215,\n",
       "   -1.6962114892699622,\n",
       "   -1.7352688531979967,\n",
       "   -1.7191656054314848,\n",
       "   -1.6839649727437997,\n",
       "   -1.688475638128279,\n",
       "   -1.7179695280820448,\n",
       "   -1.7265539928151208,\n",
       "   -1.7149825008621518,\n",
       "   -1.7300054367082063,\n",
       "   -1.7066368502133606,\n",
       "   -1.7161046654736394,\n",
       "   -1.67806836206171,\n",
       "   -1.7038906880962474,\n",
       "   -1.6997587533611687,\n",
       "   -1.723495673030118,\n",
       "   -1.7105191611479489,\n",
       "   -1.742717777739391,\n",
       "   -1.7910886392119294,\n",
       "   -1.7979239724584741,\n",
       "   -1.833062244617233,\n",
       "   -1.8155463720435905,\n",
       "   -1.8357280422055489,\n",
       "   -1.8513322022022773,\n",
       "   -1.818818464221685,\n",
       "   -1.828493505966717,\n",
       "   -1.8682436091075405,\n",
       "   -1.8966751103662949,\n",
       "   -1.8983502599340443,\n",
       "   -1.9198674142433214,\n",
       "   -1.9253166615850459,\n",
       "   -1.9397716282011581,\n",
       "   -1.9576496752990402,\n",
       "   -1.9323675949119776,\n",
       "   -1.8848960589127532,\n",
       "   -1.8088413564245398,\n",
       "   -1.8184332430548675,\n",
       "   -1.8273651055612103,\n",
       "   -1.853690299784406,\n",
       "   -1.8358341395910664,\n",
       "   -1.8508034347865188,\n",
       "   -1.8828217870749255,\n",
       "   -1.9226585974410697,\n",
       "   -1.9307847577524866,\n",
       "   -1.888690457591876,\n",
       "   -1.8429597321495041,\n",
       "   -1.7956707539880692,\n",
       "   -1.7639828857980833,\n",
       "   -1.7705199194258356,\n",
       "   -1.765674120898312,\n",
       "   -1.7848826341079471,\n",
       "   -1.7730852861922277,\n",
       "   -1.7363593004950135,\n",
       "   -1.7496154687856045,\n",
       "   -1.7776713665490278,\n",
       "   -1.766976713984668,\n",
       "   -1.7669954470743012,\n",
       "   -1.774414769307508,\n",
       "   -1.7591337656286359,\n",
       "   -1.7752567280471228,\n",
       "   -1.7481600359182083,\n",
       "   -1.7229294340246262,\n",
       "   -1.719280949396993,\n",
       "   -1.711131926556857,\n",
       "   -1.7024834216461637,\n",
       "   -1.703894922520571,\n",
       "   -1.6791904517806633,\n",
       "   -1.7244956611206477,\n",
       "   -1.7513346389409765,\n",
       "   -1.8112023877923513,\n",
       "   -1.793429551444825,\n",
       "   -1.7780754989728793,\n",
       "   -1.7587325727175278,\n",
       "   -1.7822794899398406,\n",
       "   -1.7848881859649879,\n",
       "   -1.753684530134203,\n",
       "   -1.7511310061713194,\n",
       "   -1.7897165631721048,\n",
       "   -1.803652224741671,\n",
       "   -1.8460377982224907,\n",
       "   -1.8689160002062057,\n",
       "   -1.9183416678339138,\n",
       "   -1.9529738478523893,\n",
       "   -1.9305272187645919,\n",
       "   -1.9578483917605607,\n",
       "   -1.9697392914933416,\n",
       "   -1.9947920099846557,\n",
       "   -1.9432389079742385,\n",
       "   -1.973219308094253,\n",
       "   -2.0119129075878397,\n",
       "   -1.9975930991057314,\n",
       "   -1.9589171789308155,\n",
       "   -1.9726894240803627,\n",
       "   -1.9243522036879188,\n",
       "   -1.9255444770509387,\n",
       "   -1.9432715130639542,\n",
       "   -1.974624679264236,\n",
       "   -1.9855775829721098,\n",
       "   -1.997231690693726,\n",
       "   -2.03234805361888,\n",
       "   -1.9777707900495396,\n",
       "   -1.9847887983715666,\n",
       "   -2.0296277662282756,\n",
       "   -1.9366729588583995,\n",
       "   -1.9530995579015804,\n",
       "   -1.9583125311459018,\n",
       "   -1.9733213456400511,\n",
       "   -1.930529483002751,\n",
       "   -1.881613898897525,\n",
       "   -1.9131989648761536,\n",
       "   -1.8706657759531584,\n",
       "   -1.845353617471925,\n",
       "   -1.8231265437421893,\n",
       "   -1.8009646529480299,\n",
       "   -1.7939253174857677,\n",
       "   -1.7558982760836117,\n",
       "   -1.7961119447650784,\n",
       "   -1.8030433703712672,\n",
       "   -1.753731412291864,\n",
       "   -1.762597879594014,\n",
       "   -1.7276569477254395,\n",
       "   -1.7341886538943871,\n",
       "   -1.7414391488637522,\n",
       "   -1.763875040581588,\n",
       "   -1.7797448966832192,\n",
       "   -1.771094622809732,\n",
       "   -1.782686452746137,\n",
       "   -1.8245459752446318,\n",
       "   -1.816784539457236,\n",
       "   -1.860707998106148,\n",
       "   -1.8092085830444558,\n",
       "   -1.8006887786060046,\n",
       "   -1.7832580971658412,\n",
       "   -1.8051707400329486,\n",
       "   -1.8087402842749896,\n",
       "   -1.8103894292997595,\n",
       "   -1.8457957343638514,\n",
       "   -1.7968983908358998,\n",
       "   -1.766034305361913,\n",
       "   -1.7911777987598225,\n",
       "   -1.7895001616504032,\n",
       "   -1.802808350995699,\n",
       "   -1.7746820981766671,\n",
       "   -1.7626920683848697,\n",
       "   -1.7895307164699732,\n",
       "   -1.7589128092004929,\n",
       "   -1.7340254467908567,\n",
       "   -1.7483121472079448,\n",
       "   -1.7318935622743021,\n",
       "   -1.749279813562258,\n",
       "   -1.74613176491349,\n",
       "   -1.7895630151918156,\n",
       "   -1.8046071287879895,\n",
       "   -1.8546135126502254,\n",
       "   -1.8952450386150939,\n",
       "   -1.8443615518422165,\n",
       "   -1.8365384098003612,\n",
       "   -1.8634329022506413,\n",
       "   -1.8006271703355567,\n",
       "   -1.817893846227637,\n",
       "   -1.8344032422934942,\n",
       "   -1.8040527276795584,\n",
       "   -1.7702147535229598,\n",
       "   -1.75218841456152,\n",
       "   -1.765784484326292,\n",
       "   -1.7072920826199436,\n",
       "   -1.650182222809676,\n",
       "   -1.628424555825342,\n",
       "   -1.6184709511346926,\n",
       "   -1.616804321152896,\n",
       "   -1.6100076273955355,\n",
       "   -1.615767183107696,\n",
       "   -1.6255438413967975,\n",
       "   -1.6964204878018943,\n",
       "   -1.703768433063737,\n",
       "   -1.6881714412199744,\n",
       "   -1.6787464330609203,\n",
       "   -1.6998325156316576,\n",
       "   -1.685777006772903,\n",
       "   -1.7324205374281765],\n",
       "  'ff_r2': [-8.078020262767149,\n",
       "   -6.242494140469326,\n",
       "   -4.685397967538876,\n",
       "   -4.014481013014846,\n",
       "   -3.7962788785832355,\n",
       "   -3.574760898491192,\n",
       "   -3.3924499744112637,\n",
       "   -3.2265050697072226,\n",
       "   -3.070012456415636,\n",
       "   -2.999537415091865,\n",
       "   -2.9451628366495264,\n",
       "   -2.8847821759477466,\n",
       "   -2.778829819671111,\n",
       "   -2.601438131533644,\n",
       "   -2.564761296257319,\n",
       "   -2.5065864671606986,\n",
       "   -2.3270013355428345,\n",
       "   -2.1117806903473766,\n",
       "   -1.8419715563705568,\n",
       "   -1.6303144346816234,\n",
       "   -1.4454011266820652,\n",
       "   -1.3077810968148222,\n",
       "   -1.165613380971772,\n",
       "   -1.0672422784429756,\n",
       "   -0.9693494126422044,\n",
       "   -0.8850756420399555,\n",
       "   -0.8041467175095189,\n",
       "   -0.7511213386848612,\n",
       "   -0.7090553807568123,\n",
       "   -0.6696047601050881,\n",
       "   -0.6383422334263262,\n",
       "   -0.6037382933696578,\n",
       "   -0.5817441482417596,\n",
       "   -0.5645743103500369,\n",
       "   -0.5527280655773481,\n",
       "   -0.5417224578447695,\n",
       "   -0.5230624303899158,\n",
       "   -0.4970969612486198,\n",
       "   -0.4907647092106804,\n",
       "   -0.4825935846789109,\n",
       "   -0.4746239888786483,\n",
       "   -0.4771989346289798,\n",
       "   -0.46694975262576355,\n",
       "   -0.4664811713727728,\n",
       "   -0.46536662035071363,\n",
       "   -0.4668873255113408,\n",
       "   -0.4685031545263454,\n",
       "   -0.4743775436948954,\n",
       "   -0.48012233222855194,\n",
       "   -0.48703528125660256,\n",
       "   -0.4890439870905463,\n",
       "   -0.4854843939439599,\n",
       "   -0.4834782188399871,\n",
       "   -0.48331330852291754,\n",
       "   -0.4823671316557545,\n",
       "   -0.481828099005825,\n",
       "   -0.4777454781707,\n",
       "   -0.47870879441578307,\n",
       "   -0.4766663218329552,\n",
       "   -0.4740847824117136,\n",
       "   -0.4728049866478312,\n",
       "   -0.4706573823051874,\n",
       "   -0.4713269678404657,\n",
       "   -0.478619868849232,\n",
       "   -0.47873603126243824,\n",
       "   -0.48777431335591803,\n",
       "   -0.48685362795521403,\n",
       "   -0.48510598540216554,\n",
       "   -0.4832954256128883,\n",
       "   -0.48567196395826007,\n",
       "   -0.4945212131536274,\n",
       "   -0.4934736906463544,\n",
       "   -0.495363035301539,\n",
       "   -0.49299780520309655,\n",
       "   -0.4883791558337027,\n",
       "   -0.48920604204695395,\n",
       "   -0.48986692449548164,\n",
       "   -0.48927023271299275,\n",
       "   -0.4867014510873897,\n",
       "   -0.49125195399620747,\n",
       "   -0.49538240377078635,\n",
       "   -0.49531963850752514,\n",
       "   -0.49460231190893067,\n",
       "   -0.4968618251188974,\n",
       "   -0.4979144070719834,\n",
       "   -0.49892175158873253,\n",
       "   -0.4941997386018031,\n",
       "   -0.4972684236193412,\n",
       "   -0.4947556420873622,\n",
       "   -0.4908185560722784,\n",
       "   -0.48851957547316993,\n",
       "   -0.4919329822740188,\n",
       "   -0.4912804510291797,\n",
       "   -0.48947666550742097,\n",
       "   -0.48829027048946116,\n",
       "   -0.4801110608577306,\n",
       "   -0.4825569318261915,\n",
       "   -0.48040845331566095,\n",
       "   -0.48549871682119394,\n",
       "   -0.4839531649507578,\n",
       "   -0.48763114724853396,\n",
       "   -0.4905053976318883,\n",
       "   -0.4896877527190622,\n",
       "   -0.48151447885490906,\n",
       "   -0.4873369895227735,\n",
       "   -0.49031832578338985,\n",
       "   -0.49505700662476926,\n",
       "   -0.4992731563447901,\n",
       "   -0.504050477710114,\n",
       "   -0.5067617108124292,\n",
       "   -0.5044265330377464,\n",
       "   -0.5072738584338161,\n",
       "   -0.5095035786753563,\n",
       "   -0.5052354900070708,\n",
       "   -0.5046874350755632,\n",
       "   -0.5007706118445447,\n",
       "   -0.5001791328187428,\n",
       "   -0.49291240519285573,\n",
       "   -0.4966673707810427,\n",
       "   -0.49970635861593005,\n",
       "   -0.4943212628772582,\n",
       "   -0.4858419993647525,\n",
       "   -0.4881197612832584,\n",
       "   -0.4916520875155599,\n",
       "   -0.4884839536937682,\n",
       "   -0.4851508183091109,\n",
       "   -0.4910211408451888,\n",
       "   -0.4918130628014157,\n",
       "   -0.4921616342592998,\n",
       "   -0.49479000607796086,\n",
       "   -0.4952858878602613,\n",
       "   -0.49673760630017405,\n",
       "   -0.5041641105094823,\n",
       "   -0.5065598613021252,\n",
       "   -0.5087102496201106,\n",
       "   -0.4998859277699168,\n",
       "   -0.49797062483512833,\n",
       "   -0.5032003159381371,\n",
       "   -0.5065104279316548,\n",
       "   -0.5097263876713924,\n",
       "   -0.5113498000338579,\n",
       "   -0.5092655484818438,\n",
       "   -0.5101070133617198,\n",
       "   -0.5141971186718912,\n",
       "   -0.5109745922708391,\n",
       "   -0.5126085916957535,\n",
       "   -0.5106231323758841,\n",
       "   -0.5015967281141958,\n",
       "   -0.5027340609482729,\n",
       "   -0.5005603577897852,\n",
       "   -0.5049039142404397,\n",
       "   -0.49769634984628097,\n",
       "   -0.4971757375737338,\n",
       "   -0.4958657670175457,\n",
       "   -0.4942136080369588,\n",
       "   -0.4963161139490755,\n",
       "   -0.4944720703503458,\n",
       "   -0.5033783779013028,\n",
       "   -0.5099798774560098,\n",
       "   -0.5094223865783172,\n",
       "   -0.5181570587754216,\n",
       "   -0.5235996845457818,\n",
       "   -0.5222431790233861,\n",
       "   -0.5137731137333732,\n",
       "   -0.5155849684630771,\n",
       "   -0.5191414044207829,\n",
       "   -0.5213917476430461,\n",
       "   -0.5200896503187382,\n",
       "   -0.5139560141592188,\n",
       "   -0.5115422585090055,\n",
       "   -0.5131579176211503,\n",
       "   -0.5165250646326005,\n",
       "   -0.5206924452553794,\n",
       "   -0.5221528802509552,\n",
       "   -0.5160053192098524,\n",
       "   -0.5206046398826565,\n",
       "   -0.5273388212759313,\n",
       "   -0.5305185210026675,\n",
       "   -0.5300102511339602,\n",
       "   -0.5320848856862579,\n",
       "   -0.5276093284231478,\n",
       "   -0.5296048377061258,\n",
       "   -0.5292672612788334,\n",
       "   -0.5250391854441969,\n",
       "   -0.5278546404592583,\n",
       "   -0.5263295955471061,\n",
       "   -0.5277313417516949,\n",
       "   -0.5315208084395768,\n",
       "   -0.5319216256773023,\n",
       "   -0.531773913209713,\n",
       "   -0.5269309233753148,\n",
       "   -0.533852996250826,\n",
       "   -0.5327174400885053,\n",
       "   -0.5297080678167603,\n",
       "   -0.5269914689395137,\n",
       "   -0.5299547576495538,\n",
       "   -0.5259867037267663,\n",
       "   -0.5285389987876761,\n",
       "   -0.5244319190013604,\n",
       "   -0.5239472870406914,\n",
       "   -0.5270447480884726,\n",
       "   -0.525189220291798,\n",
       "   -0.5143868406345293,\n",
       "   -0.504197587430995,\n",
       "   -0.5054087193382135,\n",
       "   -0.5068357943688924,\n",
       "   -0.5059437532492292,\n",
       "   -0.4996369334817792,\n",
       "   -0.4973816008734053,\n",
       "   -0.4983965812077258,\n",
       "   -0.49802619615979604,\n",
       "   -0.5012340864654259,\n",
       "   -0.5103991519554083,\n",
       "   -0.5120649149258107,\n",
       "   -0.519030784157029,\n",
       "   -0.5251015848286718,\n",
       "   -0.5184291606867386,\n",
       "   -0.5187571076081949,\n",
       "   -0.5186623128238557,\n",
       "   -0.5189080557011732,\n",
       "   -0.514514720163467,\n",
       "   -0.505794251965803,\n",
       "   -0.5035279872421379,\n",
       "   -0.49645482305124,\n",
       "   -0.5015125415745116,\n",
       "   -0.49849569195815513,\n",
       "   -0.5070606341737172,\n",
       "   -0.5082477973813784,\n",
       "   -0.5090660896997037,\n",
       "   -0.5039200920506524,\n",
       "   -0.507918632497963,\n",
       "   -0.5106385722468147,\n",
       "   -0.5068812754140368,\n",
       "   -0.5098292854266626,\n",
       "   -0.5106374152841151,\n",
       "   -0.5126311658915037,\n",
       "   -0.5085879843326848,\n",
       "   -0.5012098666207407,\n",
       "   -0.5053651061105717,\n",
       "   -0.5027098038155988,\n",
       "   -0.5062919666736969,\n",
       "   -0.5082565040918032,\n",
       "   -0.5096575339695351,\n",
       "   -0.5158605067001216,\n",
       "   -0.5117571218655976,\n",
       "   -0.5091487021790202,\n",
       "   -0.5062478413814493,\n",
       "   -0.5060185283509431,\n",
       "   -0.49754733481554503,\n",
       "   -0.4960674218830958,\n",
       "   -0.5014447388942,\n",
       "   -0.5023016336479411,\n",
       "   -0.4986981716510299,\n",
       "   -0.5036181161979509,\n",
       "   -0.49976669666205975,\n",
       "   -0.5074626744124764,\n",
       "   -0.5020666966536678,\n",
       "   -0.5078054891508204,\n",
       "   -0.5122838735371078,\n",
       "   -0.5150934146224802,\n",
       "   -0.5135507205208978,\n",
       "   -0.5162186778410214,\n",
       "   -0.5210116992399443,\n",
       "   -0.5163597444527239,\n",
       "   -0.5188427297530422,\n",
       "   -0.5224828974296762,\n",
       "   -0.5245132991493768,\n",
       "   -0.5281317305533626,\n",
       "   -0.5329186988168393,\n",
       "   -0.5336918336174301,\n",
       "   -0.5304105344915075,\n",
       "   -0.529120322189804,\n",
       "   -0.5262152486902603,\n",
       "   -0.5312436912071865,\n",
       "   -0.5312312208569416,\n",
       "   -0.5309652428993683,\n",
       "   -0.5348634082594086,\n",
       "   -0.536105733668788,\n",
       "   -0.5314645636990238,\n",
       "   -0.5355890348922523,\n",
       "   -0.530236053781842,\n",
       "   -0.5365392708544678,\n",
       "   -0.531620728675827,\n",
       "   -0.5341325859231334,\n",
       "   -0.5342332359602338,\n",
       "   -0.5273538936481097,\n",
       "   -0.5278424939675241,\n",
       "   -0.5302976205292849,\n",
       "   -0.5383777613761407,\n",
       "   -0.5396559812674848,\n",
       "   -0.5366119743641988,\n",
       "   -0.5329394045990754,\n",
       "   -0.5317321276250284,\n",
       "   -0.5329577952165916,\n",
       "   -0.5360752619583748,\n",
       "   -0.537423619436528,\n",
       "   -0.5299206002122749,\n",
       "   -0.5371204677092787,\n",
       "   -0.5338375196435765,\n",
       "   -0.5334450941239004,\n",
       "   -0.5399887321434174,\n",
       "   -0.5383727014855779,\n",
       "   -0.5380225545848618,\n",
       "   -0.5329068500980381,\n",
       "   -0.5330535451325338,\n",
       "   -0.5290211488568122,\n",
       "   -0.5282526188467442,\n",
       "   -0.5336248945178241,\n",
       "   -0.5349422424297292,\n",
       "   -0.5420692729318417,\n",
       "   -0.5391539117588766,\n",
       "   -0.5410359510649181,\n",
       "   -0.5381447539688446,\n",
       "   -0.5327784984622717,\n",
       "   -0.533187585361214,\n",
       "   -0.5419924080332832,\n",
       "   -0.5470091414948406,\n",
       "   -0.5469436112731476,\n",
       "   -0.5411117579589177,\n",
       "   -0.5283692968833267,\n",
       "   -0.5284669583325039,\n",
       "   -0.5288551501250329,\n",
       "   -0.53483453436569,\n",
       "   -0.5412906444671093,\n",
       "   -0.5458878712042967,\n",
       "   -0.5498486686078938,\n",
       "   -0.5513722912077541,\n",
       "   -0.5536079973163068,\n",
       "   -0.5577655689613226,\n",
       "   -0.5545093137915649,\n",
       "   -0.5500973305567642,\n",
       "   -0.549059109907091,\n",
       "   -0.5546341668956047,\n",
       "   -0.5544802321434588,\n",
       "   -0.548100985706967,\n",
       "   -0.5539928491053425,\n",
       "   -0.5548951508889346,\n",
       "   -0.560030118882666,\n",
       "   -0.5605168844289028,\n",
       "   -0.5693414184458447,\n",
       "   -0.564245285675889,\n",
       "   -0.56005381720393,\n",
       "   -0.5583968058071402,\n",
       "   -0.5640853049107908,\n",
       "   -0.5588620342954607,\n",
       "   -0.554769592541781,\n",
       "   -0.5517183888262291,\n",
       "   -0.5466225019597333,\n",
       "   -0.5543931218594544,\n",
       "   -0.5548160213821112,\n",
       "   -0.5539166843310692,\n",
       "   -0.5605922243678987,\n",
       "   -0.554754803499846,\n",
       "   -0.5524711384145162,\n",
       "   -0.5513058812860512,\n",
       "   -0.5456712409998195,\n",
       "   -0.5462192540584925,\n",
       "   -0.5462862484697837,\n",
       "   -0.5442740078983364,\n",
       "   -0.5426765103405358,\n",
       "   -0.5451488904117019,\n",
       "   -0.5483922074149941,\n",
       "   -0.5536749399115986,\n",
       "   -0.5553768919222855,\n",
       "   -0.5547657020236159,\n",
       "   -0.560089478643794,\n",
       "   -0.5497035721562151,\n",
       "   -0.5476937942737905,\n",
       "   -0.5461056049745547,\n",
       "   -0.535048878693019,\n",
       "   -0.5380246811185658,\n",
       "   -0.5390618278849335,\n",
       "   -0.537969500712228,\n",
       "   -0.5439852614669749,\n",
       "   -0.5474576046164792,\n",
       "   -0.5468525808156712,\n",
       "   -0.5490275265873386,\n",
       "   -0.5466176761416233,\n",
       "   -0.5451590028803837,\n",
       "   -0.546859421470046,\n",
       "   -0.5485646782123665,\n",
       "   -0.5576154111048248,\n",
       "   -0.5540287536272366,\n",
       "   -0.5471582229267435,\n",
       "   -0.5433417886801928,\n",
       "   -0.5377098223437224,\n",
       "   -0.5419550139527467,\n",
       "   -0.547502924498634,\n",
       "   -0.5421710088827967,\n",
       "   -0.5480924308010675,\n",
       "   -0.5520889133279425,\n",
       "   -0.551454504505615,\n",
       "   -0.5582229594776156,\n",
       "   -0.5658056140155707,\n",
       "   -0.5659982529117582,\n",
       "   -0.5630481824448725,\n",
       "   -0.5609109756035529,\n",
       "   -0.5569323414530072,\n",
       "   -0.5582013600322169,\n",
       "   -0.5512583211172046,\n",
       "   -0.537186710159191,\n",
       "   -0.5359939903849371,\n",
       "   -0.5273644027185311,\n",
       "   -0.5356386618851403,\n",
       "   -0.5360076336301534,\n",
       "   -0.5372873039295263,\n",
       "   -0.5378730421361018,\n",
       "   -0.5414669934828402,\n",
       "   -0.5449659038807002,\n",
       "   -0.5493242346882965,\n",
       "   -0.5531386271703274,\n",
       "   -0.5553836502269103,\n",
       "   -0.5528081614742466,\n",
       "   -0.5519865497043694,\n",
       "   -0.5520163172868431,\n",
       "   -0.5508749699386806,\n",
       "   -0.5520713616698454,\n",
       "   -0.5508714089097504,\n",
       "   -0.5398222497204841,\n",
       "   -0.5394695670906895,\n",
       "   -0.5414584621660794,\n",
       "   -0.5368172699642983,\n",
       "   -0.5338897380174452,\n",
       "   -0.5335253871426384,\n",
       "   -0.5364875660687622,\n",
       "   -0.5398133797126734,\n",
       "   -0.5401070062102513,\n",
       "   -0.5403629677038975,\n",
       "   -0.5446182003500473,\n",
       "   -0.5430612707943903,\n",
       "   -0.5537689240004462,\n",
       "   -0.5550491724153821,\n",
       "   -0.5540307014945327,\n",
       "   -0.5565523800982313,\n",
       "   -0.5544960968787509,\n",
       "   -0.5588193379190678,\n",
       "   -0.5594013999021537,\n",
       "   -0.5544089651973263,\n",
       "   -0.5543381520164778,\n",
       "   -0.5562088526456666,\n",
       "   -0.5548824157973542,\n",
       "   -0.5534131800920403,\n",
       "   -0.5601451889651818,\n",
       "   -0.5570897555298875,\n",
       "   -0.5494171328819077,\n",
       "   -0.5414765280608667,\n",
       "   -0.5358539785067611,\n",
       "   -0.533464689750222,\n",
       "   -0.5310963376479056,\n",
       "   -0.5353161618134648,\n",
       "   -0.5388567925347125,\n",
       "   -0.5443579689862625,\n",
       "   -0.5502218324878145,\n",
       "   -0.5569121250833631,\n",
       "   -0.5522736291015182,\n",
       "   -0.5509355842377659,\n",
       "   -0.5567378256199369,\n",
       "   -0.5589675214593055,\n",
       "   -0.5588619825810588,\n",
       "   -0.5547165985892182,\n",
       "   -0.5650317880568998,\n",
       "   -0.5609737507813215,\n",
       "   -0.5649487357168665,\n",
       "   -0.5624166272489204,\n",
       "   -0.5693689921730476,\n",
       "   -0.5726645128333574,\n",
       "   -0.5696169371844333,\n",
       "   -0.5648328582234399,\n",
       "   -0.5678719233898737,\n",
       "   -0.5750437339548293,\n",
       "   -0.5720120953130006,\n",
       "   -0.5623706500075585,\n",
       "   -0.5482497990970205,\n",
       "   -0.5444933380045105,\n",
       "   -0.5487185832347823,\n",
       "   -0.5425796883772467,\n",
       "   -0.5355099692552858,\n",
       "   -0.531512587657238,\n",
       "   -0.5315481792741514,\n",
       "   -0.5325481819841522,\n",
       "   -0.5425168152204232,\n",
       "   -0.5507442202330162,\n",
       "   -0.5592601319965149,\n",
       "   -0.558585146944452,\n",
       "   -0.5625393769811677,\n",
       "   -0.5595955160227448,\n",
       "   -0.5525109914807642,\n",
       "   -0.544104476784697,\n",
       "   -0.5467044684736808,\n",
       "   -0.5438291751052098,\n",
       "   -0.5397112451191666,\n",
       "   -0.5400425223252028,\n",
       "   -0.5408419158708018,\n",
       "   -0.546785140472666,\n",
       "   -0.5454057210798957,\n",
       "   -0.5488315767067573,\n",
       "   -0.5560273085826704,\n",
       "   -0.556795520600911,\n",
       "   -0.5653308302496305,\n",
       "   -0.5628017320210519],\n",
       "  'test_r2s': [-127.46212295152243,\n",
       "   -97.83260081115932,\n",
       "   -169.6381079826788,\n",
       "   -201.13840043601815,\n",
       "   -227.250421222171,\n",
       "   -195.09456525670714,\n",
       "   -160.20074412204627,\n",
       "   -129.05813593822603,\n",
       "   -157.1258895549159,\n",
       "   -273.43168603489437,\n",
       "   -557.3018141910844,\n",
       "   -818.7950417970892,\n",
       "   -1201.9983829668226,\n",
       "   -1990.3483928344083,\n",
       "   -4006.94188258356,\n",
       "   -7495.399443619849,\n",
       "   -8318.943056308319,\n",
       "   -11014.882510644622,\n",
       "   -33565.65123448152,\n",
       "   -5851551.563461029,\n",
       "   -3.5019708673351735,\n",
       "   -3.4528853709981924,\n",
       "   -3.3976264182805584,\n",
       "   -3.3968787211633247,\n",
       "   -3.4079655755963305,\n",
       "   -3.4424173604669757,\n",
       "   -3.4095206817635866,\n",
       "   -3.4583493211026024,\n",
       "   -3.467469047083215,\n",
       "   -3.4838467169042175,\n",
       "   -3.5306582571717016,\n",
       "   -3.633435202342426,\n",
       "   -3.673889363023317,\n",
       "   -827190.9318259251,\n",
       "   -69271.75831974606,\n",
       "   -28306.73953968501,\n",
       "   -19690.885792992423,\n",
       "   -6681.161994742467,\n",
       "   -4093.2198614502195,\n",
       "   -2283.9063780855563,\n",
       "   -1457.598029325275,\n",
       "   -1174.6874318741511,\n",
       "   -890.7489318667481,\n",
       "   -732.4401557440372,\n",
       "   -550.6596266095689,\n",
       "   -478.4049989535488,\n",
       "   -413.49051588453415,\n",
       "   -365.16443324191755,\n",
       "   -328.8937562645254,\n",
       "   -380.9298275438322,\n",
       "   -408.53514944765004,\n",
       "   -354.9100213657732,\n",
       "   -305.5493113944258,\n",
       "   -294.8546147142963,\n",
       "   -288.37031253908697,\n",
       "   -258.6933855134384,\n",
       "   -234.31113626586395,\n",
       "   -225.22088341432195,\n",
       "   -214.2050206291307,\n",
       "   -213.16047891999088,\n",
       "   -182.50703570754,\n",
       "   -171.83152600097807,\n",
       "   -171.31844566614333,\n",
       "   -170.1665541305025,\n",
       "   -171.44995401785295,\n",
       "   -151.0411141744394,\n",
       "   -144.13028885971528,\n",
       "   -143.88313098074312,\n",
       "   -149.47899280420208,\n",
       "   -150.6423266603586,\n",
       "   -136.4816510664745,\n",
       "   -156.16389392930176,\n",
       "   -151.59476862649996,\n",
       "   -152.272963148445,\n",
       "   -71.67750087252041,\n",
       "   -74.73253887404287,\n",
       "   -59.93848794695956,\n",
       "   -63.45255086892723,\n",
       "   -80.41089758295423,\n",
       "   -44.21950476847484,\n",
       "   -51.85816969020891,\n",
       "   -46.59772091751358,\n",
       "   -50.23358633320162,\n",
       "   -46.03693730898768,\n",
       "   -52.50272755757109,\n",
       "   -66.80888198897891,\n",
       "   -80.74981154816113,\n",
       "   -78.64075003732052,\n",
       "   -67.04104195311098,\n",
       "   -45.52252729048486,\n",
       "   -57.624483233031235,\n",
       "   -65.55763896505579,\n",
       "   -52.01866283798089,\n",
       "   -57.60367382351092,\n",
       "   -58.803509409479915,\n",
       "   -64.24032317719629,\n",
       "   -57.85214113386574,\n",
       "   -74.3801597886629,\n",
       "   -61.869913313875294,\n",
       "   -69.97745861113279,\n",
       "   -72.61826835485162,\n",
       "   -75.14927755043746,\n",
       "   -81.07571378651276,\n",
       "   -89.9819659499977,\n",
       "   -80.66886503578866,\n",
       "   -49.532050258344,\n",
       "   -65.44877725875509,\n",
       "   -59.31901987325365,\n",
       "   -62.001885846996295,\n",
       "   -73.71489872039025,\n",
       "   -59.50846545322768,\n",
       "   -56.27651873721253,\n",
       "   -53.31196603482234,\n",
       "   -54.79864226019883,\n",
       "   -57.50676804829779,\n",
       "   -67.91616648850781,\n",
       "   -82.45897008107653,\n",
       "   -71.69026050160765,\n",
       "   -80.03799608661667,\n",
       "   -58.91546873724892,\n",
       "   -67.28709166415403,\n",
       "   -58.36586627771814,\n",
       "   -72.08331992660686,\n",
       "   -94.43519329894309,\n",
       "   -95.37035740939274,\n",
       "   -79.27711287747094,\n",
       "   -92.14923399906796,\n",
       "   -70.11326153392928,\n",
       "   -86.09544824686816,\n",
       "   -110.0114192508943,\n",
       "   -104.91992579904091,\n",
       "   -119.80223844477659,\n",
       "   -98.11066358399034,\n",
       "   -79.40514338776437,\n",
       "   -87.0586114499312,\n",
       "   -101.67109678985523,\n",
       "   -129.2814002764701,\n",
       "   -132.71530473879943,\n",
       "   -128.37574389631388,\n",
       "   -135.08212501174427,\n",
       "   -142.643075778216,\n",
       "   -129.41157006432434,\n",
       "   -140.9203415321159,\n",
       "   -141.8625549204699,\n",
       "   -137.517742085521,\n",
       "   -124.62340479217889,\n",
       "   -127.71271503706761,\n",
       "   -128.62803058020154,\n",
       "   -130.63905112022633,\n",
       "   -121.60060091515521,\n",
       "   -111.06693764824169,\n",
       "   -108.229203668949,\n",
       "   -103.59841847425858,\n",
       "   -102.96244108321443,\n",
       "   -105.3876100223489,\n",
       "   -106.09957380492504,\n",
       "   -106.84067542520286,\n",
       "   -103.27494718101326,\n",
       "   -101.5702860510698,\n",
       "   -102.73083678639837,\n",
       "   -99.77583870552361,\n",
       "   -96.78319805447177,\n",
       "   -98.65548666151622,\n",
       "   -100.75780259685943,\n",
       "   -102.54566675729117,\n",
       "   -100.3005411895606,\n",
       "   -96.24212505904347,\n",
       "   -101.09779934915274,\n",
       "   -108.55759399085348,\n",
       "   -110.12775459120907,\n",
       "   -107.74634053737702,\n",
       "   -101.46460315912263,\n",
       "   -100.26623253979199,\n",
       "   -92.40716354498406,\n",
       "   -102.24266262506012,\n",
       "   -105.78201771018423,\n",
       "   -98.57507641706054,\n",
       "   -97.68267489997142,\n",
       "   -97.65368276691132,\n",
       "   -99.08093560863963,\n",
       "   -99.03576362785532,\n",
       "   -96.85170349997006,\n",
       "   -95.83310409389628,\n",
       "   -90.77860206415855,\n",
       "   -95.06942021567102,\n",
       "   -93.08417818732038,\n",
       "   -93.760339034521,\n",
       "   -91.17637001028241,\n",
       "   -88.15674874029439,\n",
       "   -83.57757307411941,\n",
       "   -81.6699436255575,\n",
       "   -85.80930224135,\n",
       "   -89.31136242511619,\n",
       "   -81.59894992262844,\n",
       "   -79.5300502913171,\n",
       "   -69.77688115924143,\n",
       "   -73.07015588399808,\n",
       "   -76.3576338874158,\n",
       "   -77.09163692575292,\n",
       "   -75.74929041104596,\n",
       "   -78.79438970511866,\n",
       "   -81.2640757934614,\n",
       "   -83.83122723157888,\n",
       "   -83.23270003156168,\n",
       "   -77.998846076863,\n",
       "   -77.67636816370019,\n",
       "   -64.4310487362751,\n",
       "   -73.05498208362789,\n",
       "   -68.67566339740543,\n",
       "   -39.96539804446235,\n",
       "   -43.81539599108332,\n",
       "   -52.71184494737189,\n",
       "   -57.75517023106421,\n",
       "   -65.12007282566158,\n",
       "   -67.8032121578606,\n",
       "   -47.12601970057256,\n",
       "   -42.26461626892951,\n",
       "   -45.859636874968984,\n",
       "   -48.29386414968372,\n",
       "   -51.18774433147471,\n",
       "   -50.52651183493715,\n",
       "   -49.00390467468763,\n",
       "   -38.087718713532276,\n",
       "   -44.49306518040233,\n",
       "   -50.42403627831007,\n",
       "   -58.03695412073965,\n",
       "   -48.48951535457424,\n",
       "   -51.54719863849027,\n",
       "   -46.273486296708015,\n",
       "   -54.27689483622534,\n",
       "   -45.16827556197741,\n",
       "   -43.57436196051299,\n",
       "   -37.44420899531706,\n",
       "   -46.52418024460226,\n",
       "   -53.77339467908053,\n",
       "   -46.265587341806125,\n",
       "   -55.11542934700526,\n",
       "   -42.22012898878685,\n",
       "   -47.563582190674516,\n",
       "   -51.45299084032792,\n",
       "   -47.17219547617615,\n",
       "   -54.04587578622632,\n",
       "   -50.91910964799793,\n",
       "   -43.17886592197083,\n",
       "   -46.68920147127954,\n",
       "   -60.64814972329279,\n",
       "   -67.18725409395462,\n",
       "   -65.61478515562928,\n",
       "   -74.01945945667383,\n",
       "   -66.82143936733515,\n",
       "   -72.99308475240753,\n",
       "   -74.57948093121512,\n",
       "   -75.3521618302096,\n",
       "   -78.6251934428821,\n",
       "   -66.89582370310038,\n",
       "   -62.356939423793065,\n",
       "   -67.28116059794517,\n",
       "   -66.34275842619,\n",
       "   -59.12948357685962,\n",
       "   -58.888177754437955,\n",
       "   -52.03496826908032,\n",
       "   -54.71531007576725,\n",
       "   -59.605980667715706,\n",
       "   -65.42431080002771,\n",
       "   -76.50745216924349,\n",
       "   -73.5656599998836,\n",
       "   -71.65053513341219,\n",
       "   -39.16365385907817,\n",
       "   -39.084424784947615,\n",
       "   -31.869662954524063,\n",
       "   -38.04361954564813,\n",
       "   -24.614632498080397,\n",
       "   -32.12558596060122,\n",
       "   -35.8868788851673,\n",
       "   -34.827624003342535,\n",
       "   -36.96055488271983,\n",
       "   -39.24560363117542,\n",
       "   -39.694911680788735,\n",
       "   -49.74921195117301,\n",
       "   -48.463935372899826,\n",
       "   -52.6280222612216,\n",
       "   -54.36720003772326,\n",
       "   -70.5313496897146,\n",
       "   -71.96217937483199,\n",
       "   -66.20717837909332,\n",
       "   -72.4640255340582,\n",
       "   -69.64866286145603,\n",
       "   -69.34520853917873,\n",
       "   -74.74690325452706,\n",
       "   -76.64762886204056,\n",
       "   -53.402496729032,\n",
       "   -59.48820569013554,\n",
       "   -57.96183945449626,\n",
       "   -36.7564581454102,\n",
       "   -41.32059765130984,\n",
       "   -49.030633914222584,\n",
       "   -71.80724357490294,\n",
       "   -79.12868435922815,\n",
       "   -80.28928497030451,\n",
       "   -83.5938578628488,\n",
       "   -84.3052061413845,\n",
       "   -81.75707192189239,\n",
       "   -79.22852771905457,\n",
       "   -77.57797398687438,\n",
       "   -79.01806963867509,\n",
       "   -75.65159136966636,\n",
       "   -77.36359855543327,\n",
       "   -76.40811052957656,\n",
       "   -78.65942167847753,\n",
       "   -73.53311381219827,\n",
       "   -73.60844349354394,\n",
       "   -76.990199909412,\n",
       "   -80.16911546204665,\n",
       "   -77.26943673165685,\n",
       "   -76.55148079241154,\n",
       "   -77.87426498940468,\n",
       "   -75.12990980015542,\n",
       "   -77.11756517859877,\n",
       "   -80.15815485540703,\n",
       "   -82.37461994796074,\n",
       "   -82.76240372777033,\n",
       "   -78.94853919732061,\n",
       "   -77.25756605711544,\n",
       "   -77.17232698412126,\n",
       "   -76.75872513329497,\n",
       "   -77.97686252172426,\n",
       "   -76.81015087820288,\n",
       "   -78.28264410485914,\n",
       "   -76.96014226803436,\n",
       "   -78.52568052090606,\n",
       "   -81.1957127256574,\n",
       "   -82.59854712587035,\n",
       "   -82.18643936182521,\n",
       "   -79.14007982676183,\n",
       "   -78.16737479925146,\n",
       "   -73.42454389631708,\n",
       "   -75.8508682448943,\n",
       "   -77.51379760361607,\n",
       "   -74.93633505394145,\n",
       "   -73.47202861161678,\n",
       "   -75.71380786603885,\n",
       "   -78.70852775911767,\n",
       "   -78.06778774773869,\n",
       "   -74.07869430079658,\n",
       "   -73.95256878526394,\n",
       "   -75.99013698551931,\n",
       "   -73.22415893484705,\n",
       "   -73.27740071100163,\n",
       "   -75.78033524106272,\n",
       "   -78.95450429924533,\n",
       "   -85.0663182589087,\n",
       "   -87.69096363354673,\n",
       "   -87.18760782511075,\n",
       "   -83.8107129615323,\n",
       "   -85.88369012160454,\n",
       "   -82.99963034944379,\n",
       "   -83.28169970141556,\n",
       "   -85.37755329768228,\n",
       "   -83.05920151070603,\n",
       "   -80.33549839874006,\n",
       "   -81.30293436389431,\n",
       "   -79.1779811887169,\n",
       "   -76.30715494332262,\n",
       "   -77.37734814300022,\n",
       "   -77.09998534236152,\n",
       "   -74.25197244311339,\n",
       "   -74.88759423136324,\n",
       "   -73.68225863761273,\n",
       "   -73.84226891364713,\n",
       "   -74.68387686704848,\n",
       "   -74.92677542316366,\n",
       "   -75.20528296115761,\n",
       "   -75.60219123848992,\n",
       "   -77.17887349230685,\n",
       "   -80.41079232791458,\n",
       "   -83.46427166968913,\n",
       "   -80.51868752558688,\n",
       "   -84.90292631232421,\n",
       "   -83.82064239853267,\n",
       "   -81.01921816826673,\n",
       "   -74.94434920530969,\n",
       "   -71.05909472391349,\n",
       "   -72.04848674432391,\n",
       "   -72.15158638048524,\n",
       "   -56.84988031417275,\n",
       "   -52.164764638889885,\n",
       "   -40.07109138094382,\n",
       "   -53.66289153910788,\n",
       "   -46.56483736613104,\n",
       "   -52.20594942933748,\n",
       "   -47.70497960188983,\n",
       "   -57.716028910624686,\n",
       "   -40.71588884590054,\n",
       "   -47.97122836991661,\n",
       "   -48.5871881192304,\n",
       "   -41.786570412151754,\n",
       "   -50.57563402122377,\n",
       "   -33.44222632086286,\n",
       "   -33.714141379363994,\n",
       "   -39.682713915394395,\n",
       "   -51.12886213037225,\n",
       "   -63.47355172616067,\n",
       "   -75.10550853170614,\n",
       "   -76.20419760634168,\n",
       "   -78.87446348279465,\n",
       "   -78.80127430078431,\n",
       "   -74.95974883817868,\n",
       "   -71.39934692852012,\n",
       "   -71.69129935988481,\n",
       "   -67.98060984213308,\n",
       "   -69.76286276097633,\n",
       "   -67.86476137348983,\n",
       "   -65.80062999822256,\n",
       "   -63.82359192735125,\n",
       "   -62.04749644445174,\n",
       "   -41.69488366150715,\n",
       "   -46.97697445873595,\n",
       "   -46.201959699347285,\n",
       "   -49.14003353390312,\n",
       "   -51.837067859327725,\n",
       "   -55.61429035630961,\n",
       "   -55.577125630932095,\n",
       "   -56.72285287737815,\n",
       "   -58.7879193413407,\n",
       "   -47.85795009247089,\n",
       "   -38.49276881814879,\n",
       "   -39.95597993805599,\n",
       "   -40.977416400668545,\n",
       "   -53.587412042085724,\n",
       "   -59.70014688302271,\n",
       "   -59.034149366938315,\n",
       "   -48.58314691641067,\n",
       "   -46.48888421424507,\n",
       "   -46.14406166615459,\n",
       "   -52.802321247182924,\n",
       "   -57.54119190370944,\n",
       "   -66.64322101489212,\n",
       "   -68.27554706746815,\n",
       "   -73.37156288863696,\n",
       "   -67.43552198461705,\n",
       "   -52.26863720608201,\n",
       "   -53.34805952940628,\n",
       "   -60.45755894932561,\n",
       "   -66.0163571269389,\n",
       "   -69.61297653928885,\n",
       "   -72.05714497261958,\n",
       "   -67.3072186264039,\n",
       "   -50.35743038064278,\n",
       "   -58.943434171486224,\n",
       "   -54.71373888679755,\n",
       "   -52.205546491890516,\n",
       "   -59.86206769619301,\n",
       "   -51.66819727628376,\n",
       "   -57.136007324301254,\n",
       "   -56.59276415848224,\n",
       "   -54.464931399080186,\n",
       "   -55.91408208277906,\n",
       "   -62.22860131467141,\n",
       "   -51.2847093442546,\n",
       "   -56.80995450148747,\n",
       "   -67.36807268174977,\n",
       "   -59.44314070429751,\n",
       "   -64.25909796124735,\n",
       "   -51.572312257194945,\n",
       "   -63.519876231273045,\n",
       "   -50.51717115077121,\n",
       "   -54.772012859485635,\n",
       "   -62.008400975289796,\n",
       "   -52.57341727337885,\n",
       "   -42.88259686134893,\n",
       "   -42.20479183301541,\n",
       "   -40.72462201517462,\n",
       "   -49.15440093845645,\n",
       "   -41.996724337663935,\n",
       "   -50.05913690940041,\n",
       "   -53.57691548512044,\n",
       "   -58.61304688191546,\n",
       "   -55.372814655174345,\n",
       "   -56.43007761505484,\n",
       "   -55.038890080226594,\n",
       "   -57.62759321245975,\n",
       "   -58.80540991998165,\n",
       "   -60.9923858935551,\n",
       "   -57.678065758570966,\n",
       "   -57.33159112343627,\n",
       "   -58.985960316505164,\n",
       "   -56.300853876089135,\n",
       "   -48.29576160760434,\n",
       "   -40.27527100214536,\n",
       "   -44.29010678974338,\n",
       "   -42.245934755218954,\n",
       "   -43.244695924986,\n",
       "   -44.05185820631133,\n",
       "   -41.64473521735399,\n",
       "   -49.02707910626515,\n",
       "   -56.12217967704138,\n",
       "   -56.239756278676346,\n",
       "   -49.8067031648482,\n",
       "   -53.20264473586021,\n",
       "   -64.30996248150078],\n",
       "  'train_pce_loss': [0.42396557331085205,\n",
       "   0.3995397388935089,\n",
       "   0.3984648883342743,\n",
       "   0.3932550251483917,\n",
       "   0.4047253131866455,\n",
       "   0.4052375555038452,\n",
       "   0.36328357458114624,\n",
       "   0.42419999837875366,\n",
       "   0.40545475482940674,\n",
       "   0.4563128352165222,\n",
       "   0.44515183568000793,\n",
       "   0.44954296946525574,\n",
       "   0.4026409387588501,\n",
       "   0.41359996795654297,\n",
       "   0.3842707574367523,\n",
       "   0.4161240756511688,\n",
       "   0.38963836431503296,\n",
       "   0.3937655985355377,\n",
       "   0.4118325114250183,\n",
       "   0.37856218218803406,\n",
       "   0.4178410768508911,\n",
       "   0.4022851586341858,\n",
       "   0.37486469745635986,\n",
       "   0.42594632506370544,\n",
       "   0.4123450815677643,\n",
       "   0.38323935866355896,\n",
       "   0.43281084299087524,\n",
       "   0.39166340231895447,\n",
       "   0.4063630998134613,\n",
       "   0.36956432461738586,\n",
       "   0.4171088635921478,\n",
       "   0.41592198610305786,\n",
       "   0.4055401086807251,\n",
       "   0.3879226744174957,\n",
       "   0.39377859234809875,\n",
       "   0.37109941244125366,\n",
       "   0.40713396668434143,\n",
       "   0.45072075724601746,\n",
       "   0.3415881395339966,\n",
       "   0.3959510028362274,\n",
       "   0.3574149012565613,\n",
       "   0.40203747153282166,\n",
       "   0.3857400417327881,\n",
       "   0.38177862763404846,\n",
       "   0.40348362922668457,\n",
       "   0.3704601228237152,\n",
       "   0.38845810294151306,\n",
       "   0.3879503607749939,\n",
       "   0.3481409251689911,\n",
       "   0.38987302780151367,\n",
       "   0.38212162256240845,\n",
       "   0.3635236322879791,\n",
       "   0.40701940655708313,\n",
       "   0.37325721979141235,\n",
       "   0.38129058480262756,\n",
       "   0.4139852225780487,\n",
       "   0.38038623332977295,\n",
       "   0.3794873356819153,\n",
       "   0.43018800020217896,\n",
       "   0.37412670254707336,\n",
       "   0.407097727060318,\n",
       "   0.4401609003543854,\n",
       "   0.3977324664592743,\n",
       "   0.36668694019317627,\n",
       "   0.39999091625213623,\n",
       "   0.3795969784259796,\n",
       "   0.3687934875488281,\n",
       "   0.3949294984340668,\n",
       "   0.3847290575504303,\n",
       "   0.37943384051322937,\n",
       "   0.3886646032333374,\n",
       "   0.4255424737930298,\n",
       "   0.4104563891887665,\n",
       "   0.4086226522922516,\n",
       "   0.37830066680908203,\n",
       "   0.4182347357273102,\n",
       "   0.4229786992073059,\n",
       "   0.4597040116786957,\n",
       "   0.39867985248565674,\n",
       "   0.40022048354148865,\n",
       "   0.38282057642936707,\n",
       "   0.3973548412322998,\n",
       "   0.3812422752380371,\n",
       "   0.3665085434913635,\n",
       "   0.3823622167110443,\n",
       "   0.41689854860305786,\n",
       "   0.36617088317871094,\n",
       "   0.4161313474178314,\n",
       "   0.3621317446231842,\n",
       "   0.39485323429107666,\n",
       "   0.3853791058063507,\n",
       "   0.3767429292201996,\n",
       "   0.401739239692688,\n",
       "   0.37576860189437866,\n",
       "   0.3829892873764038,\n",
       "   0.41091030836105347,\n",
       "   0.37797194719314575,\n",
       "   0.3851071298122406,\n",
       "   0.36738646030426025,\n",
       "   0.39622899889945984,\n",
       "   0.37929821014404297,\n",
       "   0.41786158084869385,\n",
       "   0.4139617681503296,\n",
       "   0.3911518156528473,\n",
       "   0.3570449948310852,\n",
       "   0.3627564013004303,\n",
       "   0.37178295850753784,\n",
       "   0.37295207381248474,\n",
       "   0.38300392031669617,\n",
       "   0.364332914352417,\n",
       "   0.37793785333633423,\n",
       "   0.40712839365005493,\n",
       "   0.40251657366752625,\n",
       "   0.34405913949012756,\n",
       "   0.37780752778053284,\n",
       "   0.4161953032016754,\n",
       "   0.3665313720703125,\n",
       "   0.3799365162849426,\n",
       "   0.3761560022830963,\n",
       "   0.4097084701061249,\n",
       "   0.38117700815200806,\n",
       "   0.41140303015708923,\n",
       "   0.3548968732357025,\n",
       "   0.36291149258613586,\n",
       "   0.3978077471256256,\n",
       "   0.38189294934272766,\n",
       "   0.36203595995903015,\n",
       "   0.3858773708343506,\n",
       "   0.35646313428878784,\n",
       "   0.37580007314682007,\n",
       "   0.35818856954574585,\n",
       "   0.36072278022766113,\n",
       "   0.3813770115375519,\n",
       "   0.3545225262641907,\n",
       "   0.3630729913711548,\n",
       "   0.3953050971031189,\n",
       "   0.37714633345603943,\n",
       "   0.40142473578453064,\n",
       "   0.40432775020599365,\n",
       "   0.3888273537158966,\n",
       "   0.382459819316864,\n",
       "   0.3801860511302948,\n",
       "   0.37849536538124084,\n",
       "   0.3974982798099518,\n",
       "   0.37207818031311035,\n",
       "   0.3892010748386383,\n",
       "   0.37181341648101807,\n",
       "   0.38969242572784424,\n",
       "   0.3646502196788788,\n",
       "   0.3647162914276123,\n",
       "   0.3926132619380951,\n",
       "   0.3733069896697998,\n",
       "   0.378348171710968,\n",
       "   0.3649003505706787,\n",
       "   0.3588774800300598,\n",
       "   0.37896496057510376,\n",
       "   0.40752920508384705,\n",
       "   0.4057747721672058,\n",
       "   0.34695494174957275,\n",
       "   0.3452433943748474,\n",
       "   0.4100009500980377,\n",
       "   0.36600756645202637,\n",
       "   0.3924834728240967,\n",
       "   0.4048103988170624,\n",
       "   0.35053738951683044,\n",
       "   0.37147751450538635,\n",
       "   0.4028013050556183,\n",
       "   0.34915441274642944,\n",
       "   0.37444788217544556,\n",
       "   0.37390023469924927,\n",
       "   0.34956109523773193,\n",
       "   0.3696598410606384,\n",
       "   0.3563140332698822,\n",
       "   0.38141897320747375,\n",
       "   0.41141313314437866,\n",
       "   0.3669257164001465,\n",
       "   0.3833990693092346,\n",
       "   0.36583420634269714,\n",
       "   0.3639223575592041,\n",
       "   0.4218524694442749,\n",
       "   0.37472349405288696,\n",
       "   0.36245080828666687,\n",
       "   0.370426744222641,\n",
       "   0.3772113025188446,\n",
       "   0.3731945753097534,\n",
       "   0.380918025970459,\n",
       "   0.3640185594558716,\n",
       "   0.3853720724582672,\n",
       "   0.39903178811073303,\n",
       "   0.36924007534980774,\n",
       "   0.3828856647014618,\n",
       "   0.3644741475582123,\n",
       "   0.37607285380363464,\n",
       "   0.35872799158096313,\n",
       "   0.3695172667503357,\n",
       "   0.3595072031021118,\n",
       "   0.38236525654792786,\n",
       "   0.3434984087944031,\n",
       "   0.3905433118343353,\n",
       "   0.3677297532558441,\n",
       "   0.3785998523235321,\n",
       "   0.39340776205062866,\n",
       "   0.38725540041923523,\n",
       "   0.35530543327331543,\n",
       "   0.3849480152130127,\n",
       "   0.36826756596565247,\n",
       "   0.37829092144966125,\n",
       "   0.3943820297718048,\n",
       "   0.36074623465538025,\n",
       "   0.3718252182006836,\n",
       "   0.3820301294326782,\n",
       "   0.4011509120464325,\n",
       "   0.3623453378677368,\n",
       "   0.3636038601398468,\n",
       "   0.39305347204208374,\n",
       "   0.3420350253582001,\n",
       "   0.40038928389549255,\n",
       "   0.3692995607852936,\n",
       "   0.36157894134521484,\n",
       "   0.340057373046875,\n",
       "   0.3815566599369049,\n",
       "   0.3832171857357025,\n",
       "   0.35566943883895874,\n",
       "   0.3832325339317322,\n",
       "   0.39263883233070374,\n",
       "   0.3541509807109833,\n",
       "   0.35678547620773315,\n",
       "   0.42194604873657227,\n",
       "   0.35678762197494507,\n",
       "   0.41687390208244324,\n",
       "   0.3558094799518585,\n",
       "   0.3520194888114929,\n",
       "   0.38052916526794434,\n",
       "   0.36730027198791504,\n",
       "   0.376921683549881,\n",
       "   0.37144261598587036,\n",
       "   0.36070260405540466,\n",
       "   0.35887637734413147,\n",
       "   0.3873275816440582,\n",
       "   0.3829174339771271,\n",
       "   0.3682499825954437,\n",
       "   0.35744425654411316,\n",
       "   0.35356444120407104,\n",
       "   0.3828546106815338,\n",
       "   0.33871930837631226,\n",
       "   0.3463265895843506,\n",
       "   0.3564701974391937,\n",
       "   0.40741944313049316,\n",
       "   0.33829236030578613,\n",
       "   0.3937956988811493,\n",
       "   0.36419206857681274,\n",
       "   0.3834766149520874,\n",
       "   0.32472869753837585,\n",
       "   0.3679920434951782,\n",
       "   0.35096654295921326,\n",
       "   0.39038845896720886,\n",
       "   0.3647857904434204,\n",
       "   0.35292908549308777,\n",
       "   0.36498451232910156,\n",
       "   0.36571910977363586,\n",
       "   0.36057397723197937,\n",
       "   0.3682299554347992,\n",
       "   0.3894916772842407,\n",
       "   0.33790433406829834,\n",
       "   0.3432835042476654,\n",
       "   0.3658074140548706,\n",
       "   0.32864654064178467,\n",
       "   0.39773833751678467,\n",
       "   0.38397160172462463,\n",
       "   0.391292005777359,\n",
       "   0.3467060625553131,\n",
       "   0.3697049915790558,\n",
       "   0.3962554931640625,\n",
       "   0.37153151631355286,\n",
       "   0.3842998445034027,\n",
       "   0.3745945692062378,\n",
       "   0.38023412227630615,\n",
       "   0.36492258310317993,\n",
       "   0.36206984519958496,\n",
       "   0.33930617570877075,\n",
       "   0.35231709480285645,\n",
       "   0.33936142921447754,\n",
       "   0.35355597734451294,\n",
       "   0.3748667240142822,\n",
       "   0.3461797833442688,\n",
       "   0.37582916021347046,\n",
       "   0.365867555141449,\n",
       "   0.3567899465560913,\n",
       "   0.3438887596130371,\n",
       "   0.3556702733039856,\n",
       "   0.3850380778312683,\n",
       "   0.36679601669311523,\n",
       "   0.3656656742095947,\n",
       "   0.3546625077724457,\n",
       "   0.3433675169944763,\n",
       "   0.3517528772354126,\n",
       "   0.3707530200481415,\n",
       "   0.3790545165538788,\n",
       "   0.3650493323802948,\n",
       "   0.3805188536643982,\n",
       "   0.3699747622013092,\n",
       "   0.3677572011947632,\n",
       "   0.3528006970882416,\n",
       "   0.3669087886810303,\n",
       "   0.36785802245140076,\n",
       "   0.3718423545360565,\n",
       "   0.36072149872779846,\n",
       "   0.37598252296447754,\n",
       "   0.3501303791999817,\n",
       "   0.35920247435569763,\n",
       "   0.3544284999370575,\n",
       "   0.34653422236442566,\n",
       "   0.3605804741382599,\n",
       "   0.34671905636787415,\n",
       "   0.34330132603645325,\n",
       "   0.3761184811592102,\n",
       "   0.42121925950050354,\n",
       "   0.36166518926620483,\n",
       "   0.39019230008125305,\n",
       "   0.3804941773414612,\n",
       "   0.35301637649536133,\n",
       "   0.36186665296554565,\n",
       "   0.37331539392471313,\n",
       "   0.3627091646194458,\n",
       "   0.33472388982772827,\n",
       "   0.3681241273880005,\n",
       "   0.34917429089546204,\n",
       "   0.3861580789089203,\n",
       "   0.34888315200805664,\n",
       "   0.3761349022388458,\n",
       "   0.360420823097229,\n",
       "   0.3853175938129425,\n",
       "   0.3368874788284302,\n",
       "   0.3467874526977539,\n",
       "   0.36575639247894287,\n",
       "   0.38086196780204773,\n",
       "   0.33678609132766724,\n",
       "   0.34106364846229553,\n",
       "   0.33485499024391174,\n",
       "   0.3686712384223938,\n",
       "   0.3667845129966736,\n",
       "   0.3713451325893402,\n",
       "   0.3791149854660034,\n",
       "   0.3683309257030487,\n",
       "   0.3591453731060028,\n",
       "   0.3494853973388672,\n",
       "   0.38042348623275757,\n",
       "   0.34952300786972046,\n",
       "   0.3579849302768707,\n",
       "   0.3463878929615021,\n",
       "   0.385598748922348,\n",
       "   0.3714245557785034,\n",
       "   0.35900136828422546,\n",
       "   0.3499651253223419,\n",
       "   0.3609238266944885,\n",
       "   0.36168766021728516,\n",
       "   0.37188151478767395,\n",
       "   0.3844197392463684,\n",
       "   0.3536439836025238,\n",
       "   0.37609758973121643,\n",
       "   0.343930721282959,\n",
       "   0.3529563844203949,\n",
       "   0.37433508038520813,\n",
       "   0.3621369004249573,\n",
       "   0.3700332045555115,\n",
       "   0.3357914090156555,\n",
       "   0.3680419921875,\n",
       "   0.3451363742351532,\n",
       "   0.3472825288772583,\n",
       "   0.3691018521785736,\n",
       "   0.37666621804237366,\n",
       "   0.35497552156448364,\n",
       "   0.3794354796409607,\n",
       "   0.3177553117275238,\n",
       "   0.417169988155365,\n",
       "   0.355080246925354,\n",
       "   0.3402235209941864,\n",
       "   0.35608410835266113,\n",
       "   0.3697948753833771,\n",
       "   0.3413267731666565,\n",
       "   0.338437557220459,\n",
       "   0.37206897139549255,\n",
       "   0.35920244455337524,\n",
       "   0.34126386046409607,\n",
       "   0.34576189517974854,\n",
       "   0.33741822838783264,\n",
       "   0.3580488860607147,\n",
       "   0.3190840482711792,\n",
       "   0.3364274501800537,\n",
       "   0.3682340979576111,\n",
       "   0.3343145549297333,\n",
       "   0.37531962990760803,\n",
       "   0.3755221366882324,\n",
       "   0.36055681109428406,\n",
       "   0.3411310315132141,\n",
       "   0.3548938035964966,\n",
       "   0.36974143981933594,\n",
       "   0.3702884614467621,\n",
       "   0.36942631006240845,\n",
       "   0.37187328934669495,\n",
       "   0.33421939611434937,\n",
       "   0.34190866351127625,\n",
       "   0.3568968176841736,\n",
       "   0.36352047324180603,\n",
       "   0.3441360294818878,\n",
       "   0.3556184470653534,\n",
       "   0.36385253071784973,\n",
       "   0.3391675055027008,\n",
       "   0.34967881441116333,\n",
       "   0.3329698145389557,\n",
       "   0.34396418929100037,\n",
       "   0.3334190547466278,\n",
       "   0.3470902442932129,\n",
       "   0.3435405492782593,\n",
       "   0.3650727868080139,\n",
       "   0.35481569170951843,\n",
       "   0.3375326693058014,\n",
       "   0.34438610076904297,\n",
       "   0.3693057596683502,\n",
       "   0.33437013626098633,\n",
       "   0.35269060730934143,\n",
       "   0.3566879630088806,\n",
       "   0.36830687522888184,\n",
       "   0.3353089392185211,\n",
       "   0.3445318043231964,\n",
       "   0.34102219343185425,\n",
       "   0.35849547386169434,\n",
       "   0.35661593079566956,\n",
       "   0.3544195592403412,\n",
       "   0.3514511287212372,\n",
       "   0.3489933907985687,\n",
       "   0.3591620922088623,\n",
       "   0.3741859197616577,\n",
       "   0.3573744595050812,\n",
       "   0.33915650844573975,\n",
       "   0.35005983710289,\n",
       "   0.35013115406036377,\n",
       "   0.352804034948349,\n",
       "   0.3500101864337921,\n",
       "   0.34708625078201294,\n",
       "   0.35975778102874756,\n",
       "   0.363444447517395,\n",
       "   0.3499296009540558,\n",
       "   0.34577950835227966,\n",
       "   0.34789082407951355,\n",
       "   0.3789592683315277,\n",
       "   0.37306326627731323,\n",
       "   0.36436378955841064,\n",
       "   0.40851718187332153,\n",
       "   0.3768655061721802,\n",
       "   0.36058545112609863,\n",
       "   0.35074496269226074,\n",
       "   0.3667260408401489,\n",
       "   0.3805826008319855,\n",
       "   0.3673987090587616,\n",
       "   0.3652639389038086,\n",
       "   0.3510812520980835,\n",
       "   0.3707522749900818,\n",
       "   0.36464929580688477,\n",
       "   0.34234312176704407,\n",
       "   0.36098623275756836,\n",
       "   0.34635838866233826,\n",
       "   0.3713068962097168,\n",
       "   0.36422720551490784,\n",
       "   0.3297220766544342,\n",
       "   0.340463250875473,\n",
       "   0.31922873854637146,\n",
       "   0.3475448191165924,\n",
       "   0.35219162702560425,\n",
       "   0.3465366065502167,\n",
       "   0.3277778625488281,\n",
       "   0.3232768177986145,\n",
       "   0.3499854803085327,\n",
       "   0.363521546125412,\n",
       "   0.34791770577430725,\n",
       "   0.3360198140144348,\n",
       "   0.35943523049354553,\n",
       "   0.3312298655509949,\n",
       "   0.3782072067260742,\n",
       "   0.3520306944847107,\n",
       "   0.34418433904647827,\n",
       "   0.3456222116947174,\n",
       "   0.32082536816596985,\n",
       "   0.38128384947776794,\n",
       "   0.3337612450122833,\n",
       "   0.3536604344844818,\n",
       "   0.35443115234375,\n",
       "   0.3583768904209137,\n",
       "   0.3696244955062866,\n",
       "   0.36502376198768616,\n",
       "   0.38655900955200195,\n",
       "   0.3626658618450165,\n",
       "   0.3613343834877014,\n",
       "   0.38010868430137634,\n",
       "   0.34790390729904175,\n",
       "   0.33084625005722046,\n",
       "   0.33302900195121765,\n",
       "   0.3442319929599762,\n",
       "   0.3603871166706085,\n",
       "   0.33258143067359924],\n",
       "  'train_voc_loss': [0.7376149892807007,\n",
       "   0.48107582330703735,\n",
       "   0.5156322121620178,\n",
       "   0.48903074860572815,\n",
       "   0.4530983865261078,\n",
       "   0.5136794447898865,\n",
       "   0.47724518179893494,\n",
       "   0.5034388899803162,\n",
       "   0.45048660039901733,\n",
       "   0.47145822644233704,\n",
       "   0.523630678653717,\n",
       "   0.48777392506599426,\n",
       "   0.527496337890625,\n",
       "   0.5246258974075317,\n",
       "   0.7955952286720276,\n",
       "   0.5821055173873901,\n",
       "   0.5340085029602051,\n",
       "   0.49143531918525696,\n",
       "   0.5198731422424316,\n",
       "   0.4740036725997925,\n",
       "   0.5208858847618103,\n",
       "   0.47725000977516174,\n",
       "   0.49449101090431213,\n",
       "   0.5152839422225952,\n",
       "   0.5270420908927917,\n",
       "   0.4830842912197113,\n",
       "   0.5191446542739868,\n",
       "   0.5240129232406616,\n",
       "   0.44999319314956665,\n",
       "   0.5308578610420227,\n",
       "   0.48062387108802795,\n",
       "   0.43060556054115295,\n",
       "   0.49778318405151367,\n",
       "   0.4422750473022461,\n",
       "   0.5084591507911682,\n",
       "   0.502018392086029,\n",
       "   0.48925143480300903,\n",
       "   0.4801074266433716,\n",
       "   0.44003093242645264,\n",
       "   0.45814749598503113,\n",
       "   0.48128899931907654,\n",
       "   0.4901594817638397,\n",
       "   0.4333590567111969,\n",
       "   0.4600909948348999,\n",
       "   0.43813854455947876,\n",
       "   0.425857812166214,\n",
       "   0.4176337718963623,\n",
       "   0.48418596386909485,\n",
       "   0.42912033200263977,\n",
       "   0.7025091052055359,\n",
       "   0.42891234159469604,\n",
       "   0.6153202056884766,\n",
       "   0.48289772868156433,\n",
       "   0.4537164866924286,\n",
       "   0.46999895572662354,\n",
       "   0.4004848301410675,\n",
       "   0.40099504590034485,\n",
       "   0.4479537308216095,\n",
       "   0.44262176752090454,\n",
       "   0.4284544587135315,\n",
       "   0.45320236682891846,\n",
       "   0.44752153754234314,\n",
       "   0.4714113473892212,\n",
       "   0.417412668466568,\n",
       "   0.42345306277275085,\n",
       "   0.4594384729862213,\n",
       "   0.4494306743144989,\n",
       "   0.43838897347450256,\n",
       "   0.4528445601463318,\n",
       "   0.4404498040676117,\n",
       "   0.4526730179786682,\n",
       "   0.5054532885551453,\n",
       "   0.4628102779388428,\n",
       "   0.4459017217159271,\n",
       "   0.45215946435928345,\n",
       "   0.4889608919620514,\n",
       "   0.43539080023765564,\n",
       "   0.4778759181499481,\n",
       "   0.38713154196739197,\n",
       "   0.4417596459388733,\n",
       "   0.47806018590927124,\n",
       "   0.45717358589172363,\n",
       "   0.46508756279945374,\n",
       "   0.4498899281024933,\n",
       "   0.4254019558429718,\n",
       "   0.4578023850917816,\n",
       "   0.4535408616065979,\n",
       "   0.4109398424625397,\n",
       "   0.4463208019733429,\n",
       "   0.43568092584609985,\n",
       "   0.44995182752609253,\n",
       "   0.4772076904773712,\n",
       "   0.44387128949165344,\n",
       "   0.46413454413414,\n",
       "   0.45938625931739807,\n",
       "   0.4418105483055115,\n",
       "   0.40516427159309387,\n",
       "   0.42501452565193176,\n",
       "   0.4556427299976349,\n",
       "   0.4535693824291229,\n",
       "   0.41575658321380615,\n",
       "   0.4252870976924896,\n",
       "   0.420398473739624,\n",
       "   0.4436917006969452,\n",
       "   0.4407285153865814,\n",
       "   0.43375134468078613,\n",
       "   0.46977314352989197,\n",
       "   0.41925573348999023,\n",
       "   0.42315417528152466,\n",
       "   0.4468834400177002,\n",
       "   0.43866366147994995,\n",
       "   0.424104779958725,\n",
       "   0.45066511631011963,\n",
       "   0.46612676978111267,\n",
       "   0.4367477297782898,\n",
       "   0.4458886981010437,\n",
       "   0.44321903586387634,\n",
       "   0.42229941487312317,\n",
       "   0.43374767899513245,\n",
       "   0.4268220365047455,\n",
       "   0.4437973201274872,\n",
       "   0.4518168270587921,\n",
       "   0.43340370059013367,\n",
       "   0.43178829550743103,\n",
       "   0.46027979254722595,\n",
       "   0.4412572979927063,\n",
       "   0.39839479327201843,\n",
       "   0.4339449107646942,\n",
       "   0.44314226508140564,\n",
       "   0.43834540247917175,\n",
       "   0.44190487265586853,\n",
       "   0.4493262767791748,\n",
       "   0.4252922832965851,\n",
       "   0.49327731132507324,\n",
       "   0.4256227910518646,\n",
       "   0.44957637786865234,\n",
       "   0.4084734618663788,\n",
       "   0.4323471486568451,\n",
       "   0.4088045060634613,\n",
       "   0.4181573688983917,\n",
       "   0.4462152421474457,\n",
       "   0.46101051568984985,\n",
       "   0.43856081366539,\n",
       "   0.40922367572784424,\n",
       "   0.46020787954330444,\n",
       "   0.43215474486351013,\n",
       "   0.45034489035606384,\n",
       "   0.4170547127723694,\n",
       "   0.4235939383506775,\n",
       "   0.43569421768188477,\n",
       "   0.4183306396007538,\n",
       "   0.44910186529159546,\n",
       "   0.42666658759117126,\n",
       "   0.4426560401916504,\n",
       "   0.4250282645225525,\n",
       "   0.40379342436790466,\n",
       "   0.42613181471824646,\n",
       "   0.4294029176235199,\n",
       "   0.44217321276664734,\n",
       "   0.4379326105117798,\n",
       "   0.38976815342903137,\n",
       "   0.42075908184051514,\n",
       "   0.42827314138412476,\n",
       "   0.43005940318107605,\n",
       "   0.45408347249031067,\n",
       "   0.39642247557640076,\n",
       "   0.46849143505096436,\n",
       "   0.43422722816467285,\n",
       "   0.40235739946365356,\n",
       "   0.4449268877506256,\n",
       "   0.4412934184074402,\n",
       "   0.4284389317035675,\n",
       "   0.4219433665275574,\n",
       "   0.42041322588920593,\n",
       "   0.40684348344802856,\n",
       "   0.43215253949165344,\n",
       "   0.41000795364379883,\n",
       "   0.440581738948822,\n",
       "   0.46433404088020325,\n",
       "   0.4424445331096649,\n",
       "   0.410741925239563,\n",
       "   0.4315730631351471,\n",
       "   0.43766364455223083,\n",
       "   0.4525085389614105,\n",
       "   0.4386589825153351,\n",
       "   0.4445978105068207,\n",
       "   0.4387800395488739,\n",
       "   0.4374333918094635,\n",
       "   0.4146110713481903,\n",
       "   0.47273463010787964,\n",
       "   0.4157105088233948,\n",
       "   0.42275291681289673,\n",
       "   0.4017362594604492,\n",
       "   0.42609745264053345,\n",
       "   0.40798839926719666,\n",
       "   0.4558665454387665,\n",
       "   0.4260351359844208,\n",
       "   0.42349180579185486,\n",
       "   0.41320547461509705,\n",
       "   0.4168785810470581,\n",
       "   0.45111241936683655,\n",
       "   0.42888012528419495,\n",
       "   0.4144172668457031,\n",
       "   0.3934015929698944,\n",
       "   0.43610432744026184,\n",
       "   0.42457374930381775,\n",
       "   0.4482690691947937,\n",
       "   0.41855672001838684,\n",
       "   0.4089181423187256,\n",
       "   0.3909419775009155,\n",
       "   0.423782080411911,\n",
       "   0.44549182057380676,\n",
       "   0.40098071098327637,\n",
       "   0.3875904083251953,\n",
       "   0.4266335070133209,\n",
       "   0.42321354150772095,\n",
       "   0.4156157672405243,\n",
       "   0.42001011967658997,\n",
       "   0.4187544584274292,\n",
       "   0.436715304851532,\n",
       "   0.4090787172317505,\n",
       "   0.4202328622341156,\n",
       "   0.4268955886363983,\n",
       "   0.4669416844844818,\n",
       "   0.4021390974521637,\n",
       "   0.4490530788898468,\n",
       "   0.45154863595962524,\n",
       "   0.4084441363811493,\n",
       "   0.41933849453926086,\n",
       "   0.41762372851371765,\n",
       "   0.4146513044834137,\n",
       "   0.4159177243709564,\n",
       "   0.4437083899974823,\n",
       "   0.4228588342666626,\n",
       "   0.4259316921234131,\n",
       "   0.416195809841156,\n",
       "   0.40704524517059326,\n",
       "   0.41935116052627563,\n",
       "   0.38172078132629395,\n",
       "   0.40576547384262085,\n",
       "   0.44608238339424133,\n",
       "   0.4463525116443634,\n",
       "   0.3685392439365387,\n",
       "   0.38927167654037476,\n",
       "   0.4136222302913666,\n",
       "   0.45793312788009644,\n",
       "   0.42118725180625916,\n",
       "   0.4409954249858856,\n",
       "   0.42286255955696106,\n",
       "   0.4125056564807892,\n",
       "   0.43021050095558167,\n",
       "   0.42721590399742126,\n",
       "   0.4152804911136627,\n",
       "   0.40436092019081116,\n",
       "   0.41492000222206116,\n",
       "   0.4348120391368866,\n",
       "   0.4101203382015228,\n",
       "   0.46261465549468994,\n",
       "   0.40366774797439575,\n",
       "   0.41983547806739807,\n",
       "   0.42695915699005127,\n",
       "   0.39654815196990967,\n",
       "   0.41859322786331177,\n",
       "   0.43489280343055725,\n",
       "   0.4170951843261719,\n",
       "   0.43582046031951904,\n",
       "   0.4303012192249298,\n",
       "   0.4297524094581604,\n",
       "   0.4158686101436615,\n",
       "   0.4004761278629303,\n",
       "   0.4366982579231262,\n",
       "   0.4186614453792572,\n",
       "   0.42377617955207825,\n",
       "   0.41802722215652466,\n",
       "   0.41507840156555176,\n",
       "   0.4202761948108673,\n",
       "   0.44625750184059143,\n",
       "   0.4143480956554413,\n",
       "   0.41088151931762695,\n",
       "   0.4040304124355316,\n",
       "   0.39434945583343506,\n",
       "   0.45895954966545105,\n",
       "   0.41869261860847473,\n",
       "   0.39609643816947937,\n",
       "   0.37576237320899963,\n",
       "   0.3987683653831482,\n",
       "   0.42722904682159424,\n",
       "   0.4196473956108093,\n",
       "   0.4110066890716553,\n",
       "   0.40818169713020325,\n",
       "   0.3791414201259613,\n",
       "   0.4101155996322632,\n",
       "   0.4137527644634247,\n",
       "   0.3960948586463928,\n",
       "   0.4150828719139099,\n",
       "   0.4154353141784668,\n",
       "   0.40888506174087524,\n",
       "   0.44236108660697937,\n",
       "   0.41676560044288635,\n",
       "   0.3991129696369171,\n",
       "   0.4065075218677521,\n",
       "   0.412252277135849,\n",
       "   0.39699679613113403,\n",
       "   0.4126746654510498,\n",
       "   0.42684975266456604,\n",
       "   0.4212362766265869,\n",
       "   0.38163134455680847,\n",
       "   0.4021824896335602,\n",
       "   0.4296111762523651,\n",
       "   0.39131787419319153,\n",
       "   0.36284929513931274,\n",
       "   0.4190096855163574,\n",
       "   0.3948896825313568,\n",
       "   0.38544782996177673,\n",
       "   0.4225253164768219,\n",
       "   0.401631623506546,\n",
       "   0.4120756983757019,\n",
       "   0.3750392496585846,\n",
       "   0.3589094281196594,\n",
       "   0.3829869031906128,\n",
       "   0.4106109142303467,\n",
       "   0.4294029474258423,\n",
       "   0.4263814389705658,\n",
       "   0.41941016912460327,\n",
       "   0.4109482169151306,\n",
       "   0.3855423629283905,\n",
       "   0.4382808804512024,\n",
       "   0.3972024619579315,\n",
       "   0.4229068458080292,\n",
       "   0.3744491636753082,\n",
       "   0.396984338760376,\n",
       "   0.41470232605934143,\n",
       "   0.39676886796951294,\n",
       "   0.40530091524124146,\n",
       "   0.40224578976631165,\n",
       "   0.4132736921310425,\n",
       "   0.38949769735336304,\n",
       "   0.42073437571525574,\n",
       "   0.3908442258834839,\n",
       "   0.3987845480442047,\n",
       "   0.38148653507232666,\n",
       "   0.4136236011981964,\n",
       "   0.4311811625957489,\n",
       "   0.43616095185279846,\n",
       "   0.4017954468727112,\n",
       "   0.4337749779224396,\n",
       "   0.3805498778820038,\n",
       "   0.39231589436531067,\n",
       "   0.40282902121543884,\n",
       "   0.40176114439964294,\n",
       "   0.4047074019908905,\n",
       "   0.4155246913433075,\n",
       "   0.40557634830474854,\n",
       "   0.41202348470687866,\n",
       "   0.41474008560180664,\n",
       "   0.411113977432251,\n",
       "   0.39355170726776123,\n",
       "   0.4054613709449768,\n",
       "   0.4058006703853607,\n",
       "   0.4002821743488312,\n",
       "   0.4316304624080658,\n",
       "   0.3898748457431793,\n",
       "   0.39903730154037476,\n",
       "   0.3982667028903961,\n",
       "   0.3836668133735657,\n",
       "   0.44939178228378296,\n",
       "   0.4108399748802185,\n",
       "   0.39138156175613403,\n",
       "   0.39703798294067383,\n",
       "   0.4474320709705353,\n",
       "   0.3882969319820404,\n",
       "   0.3855426609516144,\n",
       "   0.405544251203537,\n",
       "   0.39450591802597046,\n",
       "   0.40222886204719543,\n",
       "   0.39847075939178467,\n",
       "   0.3944026529788971,\n",
       "   0.38740667700767517,\n",
       "   0.4032439887523651,\n",
       "   0.39524924755096436,\n",
       "   0.41380760073661804,\n",
       "   0.4038088619709015,\n",
       "   0.42571917176246643,\n",
       "   0.3746432363986969,\n",
       "   0.38309112191200256,\n",
       "   0.3990756869316101,\n",
       "   0.36906853318214417,\n",
       "   0.374783992767334,\n",
       "   0.41281023621559143,\n",
       "   0.40570691227912903,\n",
       "   0.3863261342048645,\n",
       "   0.3922926187515259,\n",
       "   0.3796955347061157,\n",
       "   0.43983373045921326,\n",
       "   0.40788352489471436,\n",
       "   0.39314496517181396,\n",
       "   0.42041587829589844,\n",
       "   0.3919695317745209,\n",
       "   0.38293591141700745,\n",
       "   0.38118481636047363,\n",
       "   0.40428289771080017,\n",
       "   0.39833420515060425,\n",
       "   0.40802744030952454,\n",
       "   0.3707698583602905,\n",
       "   0.3904714286327362,\n",
       "   0.4160885512828827,\n",
       "   0.38607534766197205,\n",
       "   0.38349616527557373,\n",
       "   0.39812174439430237,\n",
       "   0.3945242464542389,\n",
       "   0.40674781799316406,\n",
       "   0.4080658555030823,\n",
       "   0.39083948731422424,\n",
       "   0.3893434703350067,\n",
       "   0.4142599105834961,\n",
       "   0.3700706958770752,\n",
       "   0.39672619104385376,\n",
       "   0.4060782492160797,\n",
       "   0.3590979278087616,\n",
       "   0.3893609941005707,\n",
       "   0.3961169123649597,\n",
       "   0.3971087634563446,\n",
       "   0.3885568380355835,\n",
       "   0.3783687949180603,\n",
       "   0.4165746867656708,\n",
       "   0.41563984751701355,\n",
       "   0.3972022831439972,\n",
       "   0.39653509855270386,\n",
       "   0.37539854645729065,\n",
       "   0.38727524876594543,\n",
       "   0.4223393201828003,\n",
       "   0.4162410795688629,\n",
       "   0.37952256202697754,\n",
       "   0.3682425320148468,\n",
       "   0.3668971061706543,\n",
       "   0.4232839047908783,\n",
       "   0.39582502841949463,\n",
       "   0.38455456495285034,\n",
       "   0.3667740523815155,\n",
       "   0.4061754047870636,\n",
       "   0.40259674191474915,\n",
       "   0.39958247542381287,\n",
       "   0.3814535439014435,\n",
       "   0.427846759557724,\n",
       "   0.3914315402507782,\n",
       "   0.40430939197540283,\n",
       "   0.4162515699863434,\n",
       "   0.38434964418411255,\n",
       "   0.40915119647979736,\n",
       "   0.3719881474971771,\n",
       "   0.4062202572822571,\n",
       "   0.4069151282310486,\n",
       "   0.4026300013065338,\n",
       "   0.38243618607521057,\n",
       "   0.3878227174282074,\n",
       "   0.39331045746803284,\n",
       "   0.38867509365081787,\n",
       "   0.4004501402378082,\n",
       "   0.4096018970012665,\n",
       "   0.39027640223503113,\n",
       "   0.3955061733722687,\n",
       "   0.3983551859855652,\n",
       "   0.4102863669395447,\n",
       "   0.3987608850002289,\n",
       "   0.37652650475502014,\n",
       "   0.3897216022014618,\n",
       "   0.36487576365470886,\n",
       "   0.41395482420921326,\n",
       "   0.3862553834915161,\n",
       "   0.39013171195983887,\n",
       "   0.3727916181087494,\n",
       "   0.39413413405418396,\n",
       "   0.3896918296813965,\n",
       "   0.38684043288230896,\n",
       "   0.40930676460266113,\n",
       "   0.4143679738044739,\n",
       "   0.3866859972476959,\n",
       "   0.3718874752521515,\n",
       "   0.43087103962898254,\n",
       "   0.3927013576030731,\n",
       "   0.36810699105262756,\n",
       "   0.3657384514808655,\n",
       "   0.3925498127937317,\n",
       "   0.35657790303230286,\n",
       "   0.413431316614151,\n",
       "   0.37028899788856506,\n",
       "   0.38988548517227173,\n",
       "   0.34760019183158875,\n",
       "   0.3862454891204834,\n",
       "   0.3656902611255646,\n",
       "   0.4028887152671814,\n",
       "   0.35669851303100586,\n",
       "   0.40646201372146606,\n",
       "   0.3874770700931549,\n",
       "   0.3695725202560425,\n",
       "   0.4206683933734894,\n",
       "   0.38128218054771423,\n",
       "   0.37722912430763245,\n",
       "   0.4004296660423279,\n",
       "   0.3689742684364319],\n",
       "  'train_jsc_loss': [0.38835620880126953,\n",
       "   0.36444833874702454,\n",
       "   0.44087088108062744,\n",
       "   0.35873332619667053,\n",
       "   0.3708944618701935,\n",
       "   0.35590440034866333,\n",
       "   0.4006749093532562,\n",
       "   0.44716736674308777,\n",
       "   0.3769019842147827,\n",
       "   0.373605340719223,\n",
       "   0.35955730080604553,\n",
       "   0.3544688820838928,\n",
       "   0.3691224455833435,\n",
       "   0.40910857915878296,\n",
       "   0.3977174162864685,\n",
       "   0.41495591402053833,\n",
       "   0.3797188103199005,\n",
       "   0.34454143047332764,\n",
       "   0.39441579580307007,\n",
       "   0.36705583333969116,\n",
       "   0.33243507146835327,\n",
       "   0.33525073528289795,\n",
       "   0.3702525794506073,\n",
       "   0.3716258406639099,\n",
       "   0.3827323615550995,\n",
       "   0.3685673475265503,\n",
       "   0.37565550208091736,\n",
       "   0.389972060918808,\n",
       "   0.35662204027175903,\n",
       "   0.3500337600708008,\n",
       "   0.37098565697669983,\n",
       "   0.365983784198761,\n",
       "   0.40222105383872986,\n",
       "   0.3761725127696991,\n",
       "   0.40181657671928406,\n",
       "   0.4383681118488312,\n",
       "   0.3470655679702759,\n",
       "   0.3880634605884552,\n",
       "   0.3915357291698456,\n",
       "   0.3568871021270752,\n",
       "   0.3018600344657898,\n",
       "   0.38042184710502625,\n",
       "   0.3396473228931427,\n",
       "   0.4216017723083496,\n",
       "   0.37296009063720703,\n",
       "   0.35501378774642944,\n",
       "   0.37147605419158936,\n",
       "   0.327761709690094,\n",
       "   0.37406298518180847,\n",
       "   0.3482981324195862,\n",
       "   0.36254721879959106,\n",
       "   0.32827624678611755,\n",
       "   0.3292873203754425,\n",
       "   0.33130723237991333,\n",
       "   0.37049540877342224,\n",
       "   0.382303386926651,\n",
       "   0.36070573329925537,\n",
       "   0.36956587433815,\n",
       "   0.39691784977912903,\n",
       "   0.35914525389671326,\n",
       "   0.31471195816993713,\n",
       "   0.37569281458854675,\n",
       "   0.31131696701049805,\n",
       "   0.39592278003692627,\n",
       "   0.35519173741340637,\n",
       "   0.31855902075767517,\n",
       "   0.3558327257633209,\n",
       "   0.3624996244907379,\n",
       "   0.33675897121429443,\n",
       "   0.34220361709594727,\n",
       "   0.30331918597221375,\n",
       "   0.3504384458065033,\n",
       "   0.34329360723495483,\n",
       "   0.37393319606781006,\n",
       "   0.3226030170917511,\n",
       "   0.3761163055896759,\n",
       "   0.3043147027492523,\n",
       "   0.35598450899124146,\n",
       "   0.3974438011646271,\n",
       "   0.35431230068206787,\n",
       "   0.37122592329978943,\n",
       "   0.3308365046977997,\n",
       "   0.31938639283180237,\n",
       "   0.33376938104629517,\n",
       "   0.33478328585624695,\n",
       "   0.3337704837322235,\n",
       "   0.40225932002067566,\n",
       "   0.33291614055633545,\n",
       "   0.3564239740371704,\n",
       "   0.32642418146133423,\n",
       "   0.3529423475265503,\n",
       "   0.39364996552467346,\n",
       "   0.3237353563308716,\n",
       "   0.3652331233024597,\n",
       "   0.33522483706474304,\n",
       "   0.3731633424758911,\n",
       "   0.3521607220172882,\n",
       "   0.3415009081363678,\n",
       "   0.35730233788490295,\n",
       "   0.366224467754364,\n",
       "   0.32038116455078125,\n",
       "   0.3466483950614929,\n",
       "   0.3375738561153412,\n",
       "   0.3742377758026123,\n",
       "   0.3027210235595703,\n",
       "   0.31778255105018616,\n",
       "   0.3845192492008209,\n",
       "   0.35085371136665344,\n",
       "   0.3596540093421936,\n",
       "   0.3530036509037018,\n",
       "   0.3275522291660309,\n",
       "   0.35433274507522583,\n",
       "   0.3482619822025299,\n",
       "   0.33462217450141907,\n",
       "   0.34861063957214355,\n",
       "   0.3428153693675995,\n",
       "   0.34591159224510193,\n",
       "   0.3251821994781494,\n",
       "   0.339277982711792,\n",
       "   0.33112412691116333,\n",
       "   0.32038167119026184,\n",
       "   0.3525541424751282,\n",
       "   0.3143717050552368,\n",
       "   0.3655274212360382,\n",
       "   0.3879741430282593,\n",
       "   0.3082716464996338,\n",
       "   0.4044455885887146,\n",
       "   0.36979031562805176,\n",
       "   0.3141833543777466,\n",
       "   0.34387290477752686,\n",
       "   0.35209012031555176,\n",
       "   0.3746272921562195,\n",
       "   0.3270268142223358,\n",
       "   0.34686803817749023,\n",
       "   0.3809088468551636,\n",
       "   0.3542514443397522,\n",
       "   0.34618136286735535,\n",
       "   0.3271138072013855,\n",
       "   0.36057090759277344,\n",
       "   0.36146923899650574,\n",
       "   0.3784946799278259,\n",
       "   0.3291639983654022,\n",
       "   0.34865856170654297,\n",
       "   0.36251363158226013,\n",
       "   0.3541814982891083,\n",
       "   0.2874371111392975,\n",
       "   0.3100481331348419,\n",
       "   0.33827197551727295,\n",
       "   0.34970518946647644,\n",
       "   0.3598574101924896,\n",
       "   0.3445315659046173,\n",
       "   0.3099559247493744,\n",
       "   0.3061261475086212,\n",
       "   0.3276498019695282,\n",
       "   0.3516589403152466,\n",
       "   0.3354247212409973,\n",
       "   0.3623606562614441,\n",
       "   0.30809926986694336,\n",
       "   0.377476304769516,\n",
       "   0.35705360770225525,\n",
       "   0.3208461403846741,\n",
       "   0.3648926913738251,\n",
       "   0.36375826597213745,\n",
       "   0.3611953556537628,\n",
       "   0.33689433336257935,\n",
       "   0.32875239849090576,\n",
       "   0.3386692404747009,\n",
       "   0.3142807185649872,\n",
       "   0.3175932765007019,\n",
       "   0.3327053487300873,\n",
       "   0.3545917272567749,\n",
       "   0.36763978004455566,\n",
       "   0.33890366554260254,\n",
       "   0.35780495405197144,\n",
       "   0.30993714928627014,\n",
       "   0.3456895351409912,\n",
       "   0.30045613646507263,\n",
       "   0.3423781991004944,\n",
       "   0.29747864603996277,\n",
       "   0.31887662410736084,\n",
       "   0.3565751314163208,\n",
       "   0.34507426619529724,\n",
       "   0.35029739141464233,\n",
       "   0.32264503836631775,\n",
       "   0.3205980360507965,\n",
       "   0.3786821663379669,\n",
       "   0.3364415764808655,\n",
       "   0.3274329602718353,\n",
       "   0.3733368515968323,\n",
       "   0.32579392194747925,\n",
       "   0.30620378255844116,\n",
       "   0.3083026111125946,\n",
       "   0.3419315814971924,\n",
       "   0.33549773693084717,\n",
       "   0.35274600982666016,\n",
       "   0.30744561553001404,\n",
       "   0.3380514085292816,\n",
       "   0.30598658323287964,\n",
       "   0.3607866168022156,\n",
       "   0.3458360433578491,\n",
       "   0.3392854928970337,\n",
       "   0.33631566166877747,\n",
       "   0.30456072092056274,\n",
       "   0.3182094097137451,\n",
       "   0.27862709760665894,\n",
       "   0.35872694849967957,\n",
       "   0.28081730008125305,\n",
       "   0.2937104105949402,\n",
       "   0.3548053205013275,\n",
       "   0.303026020526886,\n",
       "   0.33317646384239197,\n",
       "   0.32295969128608704,\n",
       "   0.29782751202583313,\n",
       "   0.31856852769851685,\n",
       "   0.33087942004203796,\n",
       "   0.2962239384651184,\n",
       "   0.30250588059425354,\n",
       "   0.3396294116973877,\n",
       "   0.31396692991256714,\n",
       "   0.33316200971603394,\n",
       "   0.35640138387680054,\n",
       "   0.34429535269737244,\n",
       "   0.3094601631164551,\n",
       "   0.2924326956272125,\n",
       "   0.3328222334384918,\n",
       "   0.31524035334587097,\n",
       "   0.2786385118961334,\n",
       "   0.3473570942878723,\n",
       "   0.3413207232952118,\n",
       "   0.3007556200027466,\n",
       "   0.30967599153518677,\n",
       "   0.33252769708633423,\n",
       "   0.3240118622779846,\n",
       "   0.34696507453918457,\n",
       "   0.3798832595348358,\n",
       "   0.28308865427970886,\n",
       "   0.30824998021125793,\n",
       "   0.3002695143222809,\n",
       "   0.2779672145843506,\n",
       "   0.3467998206615448,\n",
       "   0.32936179637908936,\n",
       "   0.30648747086524963,\n",
       "   0.3249850571155548,\n",
       "   0.31134268641471863,\n",
       "   0.2911202311515808,\n",
       "   0.318737268447876,\n",
       "   0.3356131315231323,\n",
       "   0.2957572340965271,\n",
       "   0.3392593562602997,\n",
       "   0.35778293013572693,\n",
       "   0.3458358645439148,\n",
       "   0.3205001652240753,\n",
       "   0.3320046067237854,\n",
       "   0.29897767305374146,\n",
       "   0.3335934579372406,\n",
       "   0.36062556505203247,\n",
       "   0.3319053053855896,\n",
       "   0.33038949966430664,\n",
       "   0.290010541677475,\n",
       "   0.28659337759017944,\n",
       "   0.29952895641326904,\n",
       "   0.3087119162082672,\n",
       "   0.2879057824611664,\n",
       "   0.335858017206192,\n",
       "   0.3478941023349762,\n",
       "   0.3055702745914459,\n",
       "   0.2953023910522461,\n",
       "   0.36034125089645386,\n",
       "   0.30937328934669495,\n",
       "   0.30291637778282166,\n",
       "   0.33384987711906433,\n",
       "   0.2746499180793762,\n",
       "   0.33749037981033325,\n",
       "   0.326019287109375,\n",
       "   0.3467441499233246,\n",
       "   0.33071190118789673,\n",
       "   0.3108741343021393,\n",
       "   0.33505216240882874,\n",
       "   0.30669891834259033,\n",
       "   0.2970471978187561,\n",
       "   0.29624009132385254,\n",
       "   0.2841145396232605,\n",
       "   0.3075660169124603,\n",
       "   0.28587406873703003,\n",
       "   0.3107333779335022,\n",
       "   0.3364558815956116,\n",
       "   0.3086773455142975,\n",
       "   0.3065017759799957,\n",
       "   0.29017433524131775,\n",
       "   0.3385758101940155,\n",
       "   0.3042580783367157,\n",
       "   0.32760947942733765,\n",
       "   0.2755352258682251,\n",
       "   0.3058427572250366,\n",
       "   0.28257396817207336,\n",
       "   0.3170872628688812,\n",
       "   0.3347416818141937,\n",
       "   0.3439925014972687,\n",
       "   0.3024882972240448,\n",
       "   0.30741530656814575,\n",
       "   0.3244313597679138,\n",
       "   0.3347529470920563,\n",
       "   0.3287699520587921,\n",
       "   0.3091183304786682,\n",
       "   0.32640910148620605,\n",
       "   0.300564706325531,\n",
       "   0.2905580997467041,\n",
       "   0.3143291175365448,\n",
       "   0.34599435329437256,\n",
       "   0.31585079431533813,\n",
       "   0.3021775484085083,\n",
       "   0.2864803373813629,\n",
       "   0.3170594573020935,\n",
       "   0.3362155854701996,\n",
       "   0.27490660548210144,\n",
       "   0.3230496048927307,\n",
       "   0.2648608386516571,\n",
       "   0.31178542971611023,\n",
       "   0.30089884996414185,\n",
       "   0.3124033212661743,\n",
       "   0.2964511811733246,\n",
       "   0.29271167516708374,\n",
       "   0.29959186911582947,\n",
       "   0.3313997685909271,\n",
       "   0.29192569851875305,\n",
       "   0.30120521783828735,\n",
       "   0.3199808895587921,\n",
       "   0.2921839952468872,\n",
       "   0.30038103461265564,\n",
       "   0.27904781699180603,\n",
       "   0.29302048683166504,\n",
       "   0.30309468507766724,\n",
       "   0.2836993634700775,\n",
       "   0.25656524300575256,\n",
       "   0.2399383932352066,\n",
       "   0.3325023651123047,\n",
       "   0.2986329197883606,\n",
       "   0.2933027446269989,\n",
       "   0.27467215061187744,\n",
       "   0.32935571670532227,\n",
       "   0.3001495599746704,\n",
       "   0.3072851896286011,\n",
       "   0.27430132031440735,\n",
       "   0.27119049429893494,\n",
       "   0.2833825647830963,\n",
       "   0.29255160689353943,\n",
       "   0.29873016476631165,\n",
       "   0.2805805802345276,\n",
       "   0.30620959401130676,\n",
       "   0.30507543683052063,\n",
       "   0.31120383739471436,\n",
       "   0.29101642966270447,\n",
       "   0.300292044878006,\n",
       "   0.28454163670539856,\n",
       "   0.2830806076526642,\n",
       "   0.29477012157440186,\n",
       "   0.27880406379699707,\n",
       "   0.30379998683929443,\n",
       "   0.27578219771385193,\n",
       "   0.297648161649704,\n",
       "   0.29394418001174927,\n",
       "   0.3207865059375763,\n",
       "   0.27279800176620483,\n",
       "   0.2886973023414612,\n",
       "   0.30785971879959106,\n",
       "   0.34076499938964844,\n",
       "   0.29715999960899353,\n",
       "   0.27052390575408936,\n",
       "   0.2820624113082886,\n",
       "   0.283942312002182,\n",
       "   0.30221956968307495,\n",
       "   0.29420405626296997,\n",
       "   0.2946557402610779,\n",
       "   0.2616930902004242,\n",
       "   0.32894185185432434,\n",
       "   0.3049093186855316,\n",
       "   0.2799583077430725,\n",
       "   0.28013479709625244,\n",
       "   0.29324233531951904,\n",
       "   0.2871600389480591,\n",
       "   0.28532934188842773,\n",
       "   0.3303905427455902,\n",
       "   0.2981463074684143,\n",
       "   0.2787371277809143,\n",
       "   0.3092607259750366,\n",
       "   0.2895132005214691,\n",
       "   0.28118181228637695,\n",
       "   0.3004132807254791,\n",
       "   0.26223617792129517,\n",
       "   0.28954434394836426,\n",
       "   0.2764877378940582,\n",
       "   0.27515172958374023,\n",
       "   0.28951066732406616,\n",
       "   0.31825685501098633,\n",
       "   0.2933974862098694,\n",
       "   0.27352800965309143,\n",
       "   0.2867024838924408,\n",
       "   0.27607548236846924,\n",
       "   0.3103123605251312,\n",
       "   0.2845044732093811,\n",
       "   0.28026139736175537,\n",
       "   0.2847856283187866,\n",
       "   0.28480130434036255,\n",
       "   0.30589261651039124,\n",
       "   0.2951957583427429,\n",
       "   0.27577874064445496,\n",
       "   0.29196691513061523,\n",
       "   0.2936957776546478,\n",
       "   0.2807994782924652,\n",
       "   0.30668699741363525,\n",
       "   0.29011067748069763,\n",
       "   0.2667573392391205,\n",
       "   0.3048064410686493,\n",
       "   0.3074759542942047,\n",
       "   0.30365878343582153,\n",
       "   0.32993021607398987,\n",
       "   0.2687908709049225,\n",
       "   0.2849279046058655,\n",
       "   0.2721582353115082,\n",
       "   0.26908251643180847,\n",
       "   0.2855842709541321,\n",
       "   0.28948816657066345,\n",
       "   0.2853563725948334,\n",
       "   0.3132309317588806,\n",
       "   0.27425965666770935,\n",
       "   0.29045912623405457,\n",
       "   0.269146591424942,\n",
       "   0.3035857081413269,\n",
       "   0.2916506230831146,\n",
       "   0.2750087082386017,\n",
       "   0.26892662048339844,\n",
       "   0.279110312461853,\n",
       "   0.27024775743484497,\n",
       "   0.29417550563812256,\n",
       "   0.28032976388931274,\n",
       "   0.26226896047592163,\n",
       "   0.3189152181148529,\n",
       "   0.29918891191482544,\n",
       "   0.27888694405555725,\n",
       "   0.28259554505348206,\n",
       "   0.2992030084133148,\n",
       "   0.27565500140190125,\n",
       "   0.27027860283851624,\n",
       "   0.29907315969467163,\n",
       "   0.2872248888015747,\n",
       "   0.2656899690628052,\n",
       "   0.3116095960140228,\n",
       "   0.27262750267982483,\n",
       "   0.2600979506969452,\n",
       "   0.29076114296913147,\n",
       "   0.29387199878692627,\n",
       "   0.23227240145206451,\n",
       "   0.31583932042121887,\n",
       "   0.2834887206554413,\n",
       "   0.3331845998764038,\n",
       "   0.2804906666278839,\n",
       "   0.2900262176990509,\n",
       "   0.2987479269504547,\n",
       "   0.3065653145313263,\n",
       "   0.2658105492591858,\n",
       "   0.26932480931282043,\n",
       "   0.28534621000289917,\n",
       "   0.2504449784755707,\n",
       "   0.2822473645210266,\n",
       "   0.2748020887374878,\n",
       "   0.25707417726516724,\n",
       "   0.26881226897239685,\n",
       "   0.2819976508617401,\n",
       "   0.2425292432308197,\n",
       "   0.27550023794174194,\n",
       "   0.25832775235176086,\n",
       "   0.2656446099281311,\n",
       "   0.3071426749229431,\n",
       "   0.26968491077423096,\n",
       "   0.2966439723968506,\n",
       "   0.3083692491054535,\n",
       "   0.26623907685279846,\n",
       "   0.2638012170791626,\n",
       "   0.3021309971809387,\n",
       "   0.24426500499248505,\n",
       "   0.25726714730262756,\n",
       "   0.27457353472709656,\n",
       "   0.26126426458358765,\n",
       "   0.26234376430511475,\n",
       "   0.28577756881713867,\n",
       "   0.23906099796295166,\n",
       "   0.2628144323825836,\n",
       "   0.2704308032989502,\n",
       "   0.2712642252445221,\n",
       "   0.2997268736362457,\n",
       "   0.252274751663208,\n",
       "   0.2906252145767212,\n",
       "   0.2484527826309204,\n",
       "   0.2775878608226776,\n",
       "   0.2573053538799286,\n",
       "   0.26842305064201355,\n",
       "   0.2744240164756775,\n",
       "   0.31037935614585876,\n",
       "   0.2993786334991455,\n",
       "   0.2661575675010681],\n",
       "  'train_ff_loss': [0.5127567052841187,\n",
       "   0.4826759696006775,\n",
       "   0.5744696259498596,\n",
       "   0.6024547219276428,\n",
       "   0.5075996518135071,\n",
       "   0.5542365312576294,\n",
       "   0.5597234964370728,\n",
       "   0.528224527835846,\n",
       "   0.5303234457969666,\n",
       "   0.5887975692749023,\n",
       "   0.5079367756843567,\n",
       "   0.4832605719566345,\n",
       "   0.5168001055717468,\n",
       "   0.4788689911365509,\n",
       "   0.47476059198379517,\n",
       "   0.39088207483291626,\n",
       "   0.4502856135368347,\n",
       "   0.4897298216819763,\n",
       "   0.4285392165184021,\n",
       "   0.46415799856185913,\n",
       "   0.34487947821617126,\n",
       "   0.47809478640556335,\n",
       "   0.4294188618659973,\n",
       "   0.4039124846458435,\n",
       "   0.4758101999759674,\n",
       "   0.4262027442455292,\n",
       "   0.34720176458358765,\n",
       "   0.4537285566329956,\n",
       "   0.4289897084236145,\n",
       "   0.4244517982006073,\n",
       "   0.40800952911376953,\n",
       "   0.3984609544277191,\n",
       "   0.45533478260040283,\n",
       "   0.4275013208389282,\n",
       "   0.36632615327835083,\n",
       "   0.4040546715259552,\n",
       "   0.39815250039100647,\n",
       "   0.39505136013031006,\n",
       "   0.4247245192527771,\n",
       "   0.41993844509124756,\n",
       "   0.4431213140487671,\n",
       "   0.404238224029541,\n",
       "   0.39023375511169434,\n",
       "   0.4137099087238312,\n",
       "   0.39520391821861267,\n",
       "   0.3742988705635071,\n",
       "   0.3930334150791168,\n",
       "   0.38980767130851746,\n",
       "   0.40861544013023376,\n",
       "   0.36052045226097107,\n",
       "   0.36922207474708557,\n",
       "   0.39587900042533875,\n",
       "   0.3950192630290985,\n",
       "   0.3921671211719513,\n",
       "   0.3603964149951935,\n",
       "   0.37269824743270874,\n",
       "   0.3891947865486145,\n",
       "   0.37290164828300476,\n",
       "   0.359529048204422,\n",
       "   0.3767874538898468,\n",
       "   0.3707387149333954,\n",
       "   0.4575954079627991,\n",
       "   0.371247261762619,\n",
       "   0.3874419033527374,\n",
       "   0.392514705657959,\n",
       "   0.3488660752773285,\n",
       "   0.41148626804351807,\n",
       "   0.35335591435432434,\n",
       "   0.4099908173084259,\n",
       "   0.40232419967651367,\n",
       "   0.34345927834510803,\n",
       "   0.37259092926979065,\n",
       "   0.4011996388435364,\n",
       "   0.3855319619178772,\n",
       "   0.35109132528305054,\n",
       "   0.39604872465133667,\n",
       "   0.3606906831264496,\n",
       "   0.3364273011684418,\n",
       "   0.33950603008270264,\n",
       "   0.3636794686317444,\n",
       "   0.40370243787765503,\n",
       "   0.39289024472236633,\n",
       "   0.33341145515441895,\n",
       "   0.402677446603775,\n",
       "   0.3140754699707031,\n",
       "   0.3486483693122864,\n",
       "   0.37149399518966675,\n",
       "   0.36268383264541626,\n",
       "   0.42681288719177246,\n",
       "   0.3290349245071411,\n",
       "   0.3725450932979584,\n",
       "   0.36650946736335754,\n",
       "   0.35820770263671875,\n",
       "   0.34337493777275085,\n",
       "   0.35492199659347534,\n",
       "   0.35149085521698,\n",
       "   0.3313644826412201,\n",
       "   0.38096219301223755,\n",
       "   0.32766246795654297,\n",
       "   0.33140912652015686,\n",
       "   0.37880057096481323,\n",
       "   0.3127264678478241,\n",
       "   0.33449968695640564,\n",
       "   0.33496400713920593,\n",
       "   0.38028329610824585,\n",
       "   0.3685239851474762,\n",
       "   0.32102110981941223,\n",
       "   0.3212674558162689,\n",
       "   0.38971078395843506,\n",
       "   0.38957661390304565,\n",
       "   0.3909037411212921,\n",
       "   0.33651968836784363,\n",
       "   0.3542822599411011,\n",
       "   0.3326500952243805,\n",
       "   0.3241100311279297,\n",
       "   0.3830607533454895,\n",
       "   0.3537158668041229,\n",
       "   0.3002272844314575,\n",
       "   0.36763277649879456,\n",
       "   0.3478289544582367,\n",
       "   0.3726966679096222,\n",
       "   0.34022730588912964,\n",
       "   0.36060822010040283,\n",
       "   0.3742395043373108,\n",
       "   0.33703261613845825,\n",
       "   0.36782917380332947,\n",
       "   0.3487255871295929,\n",
       "   0.3762689232826233,\n",
       "   0.3711751103401184,\n",
       "   0.3858696520328522,\n",
       "   0.3507654666900635,\n",
       "   0.3390592932701111,\n",
       "   0.3550397455692291,\n",
       "   0.31788891553878784,\n",
       "   0.30601537227630615,\n",
       "   0.3388570547103882,\n",
       "   0.3688512146472931,\n",
       "   0.3303338885307312,\n",
       "   0.40334299206733704,\n",
       "   0.29562512040138245,\n",
       "   0.3932831883430481,\n",
       "   0.33694589138031006,\n",
       "   0.3639272451400757,\n",
       "   0.3663347363471985,\n",
       "   0.3671964704990387,\n",
       "   0.3474885821342468,\n",
       "   0.318409264087677,\n",
       "   0.32494646310806274,\n",
       "   0.3373720943927765,\n",
       "   0.3588012158870697,\n",
       "   0.35517844557762146,\n",
       "   0.3250446617603302,\n",
       "   0.36235225200653076,\n",
       "   0.3570733666419983,\n",
       "   0.26937922835350037,\n",
       "   0.3122931718826294,\n",
       "   0.32765936851501465,\n",
       "   0.3196706175804138,\n",
       "   0.33446627855300903,\n",
       "   0.3768158555030823,\n",
       "   0.34352293610572815,\n",
       "   0.29643556475639343,\n",
       "   0.3596469461917877,\n",
       "   0.31669023633003235,\n",
       "   0.37798213958740234,\n",
       "   0.35081350803375244,\n",
       "   0.33331072330474854,\n",
       "   0.3408460319042206,\n",
       "   0.3443649113178253,\n",
       "   0.31120309233665466,\n",
       "   0.33093157410621643,\n",
       "   0.3141545355319977,\n",
       "   0.33471542596817017,\n",
       "   0.3299540877342224,\n",
       "   0.3365294933319092,\n",
       "   0.316784530878067,\n",
       "   0.3598453104496002,\n",
       "   0.3453393876552582,\n",
       "   0.3409915864467621,\n",
       "   0.3316369354724884,\n",
       "   0.3618667423725128,\n",
       "   0.3192461133003235,\n",
       "   0.338069885969162,\n",
       "   0.34449198842048645,\n",
       "   0.2905307710170746,\n",
       "   0.3224731683731079,\n",
       "   0.32277315855026245,\n",
       "   0.3465374708175659,\n",
       "   0.29850661754608154,\n",
       "   0.38498803973197937,\n",
       "   0.3061846196651459,\n",
       "   0.3186637759208679,\n",
       "   0.38377946615219116,\n",
       "   0.37323129177093506,\n",
       "   0.3350236713886261,\n",
       "   0.3097623586654663,\n",
       "   0.30720099806785583,\n",
       "   0.3639706075191498,\n",
       "   0.31358784437179565,\n",
       "   0.3414745330810547,\n",
       "   0.30251941084861755,\n",
       "   0.29329222440719604,\n",
       "   0.3245401680469513,\n",
       "   0.31364205479621887,\n",
       "   0.3639497756958008,\n",
       "   0.36995160579681396,\n",
       "   0.318748414516449,\n",
       "   0.3650502860546112,\n",
       "   0.37687888741493225,\n",
       "   0.3319835662841797,\n",
       "   0.33379778265953064,\n",
       "   0.33663347363471985,\n",
       "   0.3245328962802887,\n",
       "   0.31399399042129517,\n",
       "   0.31760722398757935,\n",
       "   0.33756014704704285,\n",
       "   0.2919981777667999,\n",
       "   0.2896539866924286,\n",
       "   0.3204210698604584,\n",
       "   0.3073222041130066,\n",
       "   0.3142014145851135,\n",
       "   0.30147233605384827,\n",
       "   0.27728328108787537,\n",
       "   0.3536449074745178,\n",
       "   0.3529088795185089,\n",
       "   0.32290637493133545,\n",
       "   0.325337290763855,\n",
       "   0.32987669110298157,\n",
       "   0.29071512818336487,\n",
       "   0.2930542528629303,\n",
       "   0.3020317554473877,\n",
       "   0.31752121448516846,\n",
       "   0.2825011610984802,\n",
       "   0.3085569739341736,\n",
       "   0.33656734228134155,\n",
       "   0.36138248443603516,\n",
       "   0.3111620247364044,\n",
       "   0.2885119318962097,\n",
       "   0.29082149267196655,\n",
       "   0.3523930013179779,\n",
       "   0.3030833303928375,\n",
       "   0.3585488796234131,\n",
       "   0.2967172861099243,\n",
       "   0.29832902550697327,\n",
       "   0.2788785994052887,\n",
       "   0.3343561291694641,\n",
       "   0.3034801185131073,\n",
       "   0.3194388747215271,\n",
       "   0.3036956489086151,\n",
       "   0.2886262834072113,\n",
       "   0.2956800162792206,\n",
       "   0.36230146884918213,\n",
       "   0.3453035354614258,\n",
       "   0.2775653898715973,\n",
       "   0.3369179964065552,\n",
       "   0.31902259588241577,\n",
       "   0.3321686089038849,\n",
       "   0.3291327953338623,\n",
       "   0.2914328873157501,\n",
       "   0.3012843430042267,\n",
       "   0.2966689467430115,\n",
       "   0.29991698265075684,\n",
       "   0.3013528883457184,\n",
       "   0.2984040677547455,\n",
       "   0.262997567653656,\n",
       "   0.3194989562034607,\n",
       "   0.3192663788795471,\n",
       "   0.2941482365131378,\n",
       "   0.3065871000289917,\n",
       "   0.30404770374298096,\n",
       "   0.30540817975997925,\n",
       "   0.2806047797203064,\n",
       "   0.2803800702095032,\n",
       "   0.3385355770587921,\n",
       "   0.32191234827041626,\n",
       "   0.2951432764530182,\n",
       "   0.2936812937259674,\n",
       "   0.2995452582836151,\n",
       "   0.2848231792449951,\n",
       "   0.29759934544563293,\n",
       "   0.2947254180908203,\n",
       "   0.30981650948524475,\n",
       "   0.3052699863910675,\n",
       "   0.2783137857913971,\n",
       "   0.25260335206985474,\n",
       "   0.2823871374130249,\n",
       "   0.30046942830085754,\n",
       "   0.2887648642063141,\n",
       "   0.30222606658935547,\n",
       "   0.29549771547317505,\n",
       "   0.27542442083358765,\n",
       "   0.27368494868278503,\n",
       "   0.2819709777832031,\n",
       "   0.304419606924057,\n",
       "   0.30520790815353394,\n",
       "   0.3302045464515686,\n",
       "   0.2788678705692291,\n",
       "   0.28844526410102844,\n",
       "   0.31512880325317383,\n",
       "   0.3015676736831665,\n",
       "   0.3140619397163391,\n",
       "   0.27938368916511536,\n",
       "   0.28390422463417053,\n",
       "   0.3120157718658447,\n",
       "   0.2953168451786041,\n",
       "   0.2808060646057129,\n",
       "   0.3162834346294403,\n",
       "   0.27985677123069763,\n",
       "   0.2991828918457031,\n",
       "   0.2811332643032074,\n",
       "   0.28271371126174927,\n",
       "   0.26168936491012573,\n",
       "   0.27904030680656433,\n",
       "   0.3019619286060333,\n",
       "   0.2896968424320221,\n",
       "   0.28805938363075256,\n",
       "   0.27682045102119446,\n",
       "   0.28600889444351196,\n",
       "   0.282097190618515,\n",
       "   0.30201977491378784,\n",
       "   0.2789897322654724,\n",
       "   0.3197686970233917,\n",
       "   0.2909370958805084,\n",
       "   0.28127434849739075,\n",
       "   0.2538573741912842,\n",
       "   0.2868112027645111,\n",
       "   0.2722311317920685,\n",
       "   0.29023289680480957,\n",
       "   0.2655540704727173,\n",
       "   0.30072903633117676,\n",
       "   0.26575252413749695,\n",
       "   0.27681583166122437,\n",
       "   0.291151225566864,\n",
       "   0.28294914960861206,\n",
       "   0.26517534255981445,\n",
       "   0.25573602318763733,\n",
       "   0.30886030197143555,\n",
       "   0.27728140354156494,\n",
       "   0.2592397928237915,\n",
       "   0.2669823467731476,\n",
       "   0.2881307005882263,\n",
       "   0.30152034759521484,\n",
       "   0.3053205907344818,\n",
       "   0.2866722047328949,\n",
       "   0.2616671323776245,\n",
       "   0.3255220651626587,\n",
       "   0.2617131769657135,\n",
       "   0.3067062497138977,\n",
       "   0.2642897367477417,\n",
       "   0.24882540106773376,\n",
       "   0.2851122319698334,\n",
       "   0.27694132924079895,\n",
       "   0.2763221859931946,\n",
       "   0.2693597078323364,\n",
       "   0.29108870029449463,\n",
       "   0.3041539192199707,\n",
       "   0.325094610452652,\n",
       "   0.294994592666626,\n",
       "   0.2555346190929413,\n",
       "   0.25303009152412415,\n",
       "   0.2657097578048706,\n",
       "   0.2589637339115143,\n",
       "   0.23727674782276154,\n",
       "   0.25887539982795715,\n",
       "   0.2621057331562042,\n",
       "   0.266281396150589,\n",
       "   0.2643897831439972,\n",
       "   0.2709261476993561,\n",
       "   0.2831896245479584,\n",
       "   0.2643504738807678,\n",
       "   0.27720212936401367,\n",
       "   0.3072736859321594,\n",
       "   0.2511664927005768,\n",
       "   0.2753171920776367,\n",
       "   0.2534038722515106,\n",
       "   0.28292736411094666,\n",
       "   0.28498536348342896,\n",
       "   0.27571266889572144,\n",
       "   0.2786744236946106,\n",
       "   0.2790850102901459,\n",
       "   0.244204580783844,\n",
       "   0.26235416531562805,\n",
       "   0.2721737027168274,\n",
       "   0.25572091341018677,\n",
       "   0.24947410821914673,\n",
       "   0.2674073278903961,\n",
       "   0.27125510573387146,\n",
       "   0.2399383783340454,\n",
       "   0.3023773729801178,\n",
       "   0.24163182079792023,\n",
       "   0.2672188878059387,\n",
       "   0.26387879252433777,\n",
       "   0.2371876984834671,\n",
       "   0.2583848237991333,\n",
       "   0.23591314256191254,\n",
       "   0.2693125307559967,\n",
       "   0.2589568495750427,\n",
       "   0.26967766880989075,\n",
       "   0.2643641531467438,\n",
       "   0.3093450367450714,\n",
       "   0.24787963926792145,\n",
       "   0.2542615830898285,\n",
       "   0.2548835277557373,\n",
       "   0.2409955859184265,\n",
       "   0.265275776386261,\n",
       "   0.27738410234451294,\n",
       "   0.2601139545440674,\n",
       "   0.27180007100105286,\n",
       "   0.25625285506248474,\n",
       "   0.25795066356658936,\n",
       "   0.275926411151886,\n",
       "   0.27967530488967896,\n",
       "   0.266930490732193,\n",
       "   0.2291000932455063,\n",
       "   0.260804146528244,\n",
       "   0.24797870218753815,\n",
       "   0.2618130147457123,\n",
       "   0.25718221068382263,\n",
       "   0.26863032579421997,\n",
       "   0.2691621780395508,\n",
       "   0.24302087724208832,\n",
       "   0.27770689129829407,\n",
       "   0.2812858819961548,\n",
       "   0.26799413561820984,\n",
       "   0.2970498502254486,\n",
       "   0.23248109221458435,\n",
       "   0.25483864545822144,\n",
       "   0.28138870000839233,\n",
       "   0.25112003087997437,\n",
       "   0.2359292060136795,\n",
       "   0.25963330268859863,\n",
       "   0.24597422778606415,\n",
       "   0.26618775725364685,\n",
       "   0.26142963767051697,\n",
       "   0.2789715528488159,\n",
       "   0.28269854187965393,\n",
       "   0.25840455293655396,\n",
       "   0.253130167722702,\n",
       "   0.2809318006038666,\n",
       "   0.24185363948345184,\n",
       "   0.2619929313659668,\n",
       "   0.23445148766040802,\n",
       "   0.2290400117635727,\n",
       "   0.2609800100326538,\n",
       "   0.2614191472530365,\n",
       "   0.2502787709236145,\n",
       "   0.26524168252944946,\n",
       "   0.23034372925758362,\n",
       "   0.2581625282764435,\n",
       "   0.2616901993751526,\n",
       "   0.2730611264705658,\n",
       "   0.2547672688961029,\n",
       "   0.2325768917798996,\n",
       "   0.24842314422130585,\n",
       "   0.22191879153251648,\n",
       "   0.24268388748168945,\n",
       "   0.22905699908733368,\n",
       "   0.26406461000442505,\n",
       "   0.25248023867607117,\n",
       "   0.24805624783039093,\n",
       "   0.24023696780204773,\n",
       "   0.22634176909923553,\n",
       "   0.253013014793396,\n",
       "   0.22900408506393433,\n",
       "   0.21960394084453583,\n",
       "   0.2585383951663971,\n",
       "   0.24883081018924713,\n",
       "   0.2670198976993561,\n",
       "   0.24681589007377625,\n",
       "   0.24060340225696564,\n",
       "   0.2724843919277191,\n",
       "   0.23263540863990784,\n",
       "   0.259244441986084,\n",
       "   0.2463528960943222,\n",
       "   0.24193985760211945,\n",
       "   0.2307380586862564,\n",
       "   0.254469633102417,\n",
       "   0.2630908489227295,\n",
       "   0.22790873050689697,\n",
       "   0.2404565066099167,\n",
       "   0.2565138041973114,\n",
       "   0.27150416374206543,\n",
       "   0.23054635524749756,\n",
       "   0.23236627876758575,\n",
       "   0.24832235276699066,\n",
       "   0.22122381627559662,\n",
       "   0.22922804951667786,\n",
       "   0.2555849254131317,\n",
       "   0.25692909955978394,\n",
       "   0.24885207414627075,\n",
       "   0.2398594319820404,\n",
       "   0.1936611533164978,\n",
       "   0.25044146180152893,\n",
       "   0.24143464863300323,\n",
       "   0.2603412866592407,\n",
       "   0.24519841372966766,\n",
       "   0.2432873249053955,\n",
       "   0.2583836615085602,\n",
       "   0.23373447358608246,\n",
       "   0.23210246860980988]},\n",
       " 1: {'lr': 1e-05,\n",
       "  'best_loss_epoch': 496,\n",
       "  'best_acc_epoch': 496,\n",
       "  'best_r2_epoch': 0,\n",
       "  'pce_loss': [0.30337268114089966,\n",
       "   0.30337268114089966,\n",
       "   0.30337268114089966,\n",
       "   0.30337268114089966,\n",
       "   0.30337268114089966,\n",
       "   0.30337268114089966,\n",
       "   0.30337268114089966,\n",
       "   0.30073773860931396,\n",
       "   0.2969779670238495,\n",
       "   0.2942279577255249,\n",
       "   0.2885490953922272,\n",
       "   0.28421103954315186,\n",
       "   0.2755717635154724,\n",
       "   0.26187413930892944,\n",
       "   0.23952658474445343,\n",
       "   0.21550293266773224,\n",
       "   0.19533999264240265,\n",
       "   0.1785670518875122,\n",
       "   0.1410277932882309,\n",
       "   0.12374291568994522,\n",
       "   0.1142515018582344,\n",
       "   0.10424942523241043,\n",
       "   0.09075690060853958,\n",
       "   0.08513600379228592,\n",
       "   0.07753565162420273,\n",
       "   0.06976078450679779,\n",
       "   0.06223711371421814,\n",
       "   0.05780774727463722,\n",
       "   0.06123734265565872,\n",
       "   0.059062253683805466,\n",
       "   0.0578470416367054,\n",
       "   0.06272503733634949,\n",
       "   0.06260032206773758,\n",
       "   0.057377979159355164,\n",
       "   0.05497635900974274,\n",
       "   0.0520472452044487,\n",
       "   0.057577766478061676,\n",
       "   0.056923482567071915,\n",
       "   0.056466031819581985,\n",
       "   0.05289536714553833,\n",
       "   0.050022125244140625,\n",
       "   0.05600649118423462,\n",
       "   0.0627983883023262,\n",
       "   0.06918457895517349,\n",
       "   0.07125788927078247,\n",
       "   0.07478667795658112,\n",
       "   0.08569459617137909,\n",
       "   0.08575815707445145,\n",
       "   0.08265932649374008,\n",
       "   0.08705568313598633,\n",
       "   0.09727171063423157,\n",
       "   0.10836121439933777,\n",
       "   0.11095082014799118,\n",
       "   0.11887600272893906,\n",
       "   0.11833962053060532,\n",
       "   0.11965475976467133,\n",
       "   0.11740811169147491,\n",
       "   0.12353120744228363,\n",
       "   0.12922939658164978,\n",
       "   0.13391900062561035,\n",
       "   0.13215403258800507,\n",
       "   0.1282287836074829,\n",
       "   0.12342175841331482,\n",
       "   0.13319677114486694,\n",
       "   0.13037587702274323,\n",
       "   0.12485627084970474,\n",
       "   0.12970605492591858,\n",
       "   0.12940672039985657,\n",
       "   0.12228024005889893,\n",
       "   0.11852801591157913,\n",
       "   0.11120768636465073,\n",
       "   0.11468403786420822,\n",
       "   0.10931219160556793,\n",
       "   0.10239198803901672,\n",
       "   0.1016312837600708,\n",
       "   0.10817904770374298,\n",
       "   0.11443009972572327,\n",
       "   0.12200215458869934,\n",
       "   0.11941763758659363,\n",
       "   0.11735087633132935,\n",
       "   0.11758093535900116,\n",
       "   0.1130392774939537,\n",
       "   0.1026497483253479,\n",
       "   0.0943879634141922,\n",
       "   0.09594845771789551,\n",
       "   0.09769472479820251,\n",
       "   0.09317044913768768,\n",
       "   0.09207980334758759,\n",
       "   0.09879255294799805,\n",
       "   0.09468264877796173,\n",
       "   0.10007075220346451,\n",
       "   0.09198164939880371,\n",
       "   0.09602786600589752,\n",
       "   0.09534693509340286,\n",
       "   0.09036444127559662,\n",
       "   0.08139707148075104,\n",
       "   0.07172476500272751,\n",
       "   0.07033032178878784,\n",
       "   0.0725347027182579,\n",
       "   0.06708327680826187,\n",
       "   0.06116953864693642,\n",
       "   0.06222233548760414,\n",
       "   0.07078136503696442,\n",
       "   0.07953639328479767,\n",
       "   0.08004498481750488,\n",
       "   0.08870159089565277,\n",
       "   0.0949907898902893,\n",
       "   0.09514149278402328,\n",
       "   0.09522341191768646,\n",
       "   0.09705590456724167,\n",
       "   0.09019172191619873,\n",
       "   0.09255550801753998,\n",
       "   0.09849980473518372,\n",
       "   0.09320862591266632,\n",
       "   0.09694303572177887,\n",
       "   0.09501691907644272,\n",
       "   0.09676490724086761,\n",
       "   0.09458889067173004,\n",
       "   0.10025842487812042,\n",
       "   0.10181412100791931,\n",
       "   0.09995762258768082,\n",
       "   0.10270904004573822,\n",
       "   0.09741568565368652,\n",
       "   0.09887866675853729,\n",
       "   0.0967879667878151,\n",
       "   0.09171886742115021,\n",
       "   0.09017946571111679,\n",
       "   0.08580669015645981,\n",
       "   0.09007332473993301,\n",
       "   0.0883461982011795,\n",
       "   0.08467305451631546,\n",
       "   0.08416862040758133,\n",
       "   0.08351420611143112,\n",
       "   0.08200488984584808,\n",
       "   0.07334685325622559,\n",
       "   0.07860368490219116,\n",
       "   0.08207879960536957,\n",
       "   0.07991111278533936,\n",
       "   0.07949820905923843,\n",
       "   0.07516482472419739,\n",
       "   0.07847386598587036,\n",
       "   0.07378925383090973,\n",
       "   0.06571105867624283,\n",
       "   0.06921960413455963,\n",
       "   0.06905501335859299,\n",
       "   0.06869291514158249,\n",
       "   0.07078054547309875,\n",
       "   0.07310998439788818,\n",
       "   0.0782424584031105,\n",
       "   0.07942133396863937,\n",
       "   0.08244163542985916,\n",
       "   0.08436603099107742,\n",
       "   0.08263383060693741,\n",
       "   0.07893108576536179,\n",
       "   0.07790245115756989,\n",
       "   0.07844875752925873,\n",
       "   0.07354506105184555,\n",
       "   0.07025711238384247,\n",
       "   0.07512211799621582,\n",
       "   0.07117000967264175,\n",
       "   0.0734398365020752,\n",
       "   0.07455858588218689,\n",
       "   0.08018166571855545,\n",
       "   0.08147884160280228,\n",
       "   0.0821124017238617,\n",
       "   0.08654385060071945,\n",
       "   0.08958106487989426,\n",
       "   0.08628237247467041,\n",
       "   0.08717497438192368,\n",
       "   0.0961652621626854,\n",
       "   0.09562328457832336,\n",
       "   0.10322999209165573,\n",
       "   0.11179636418819427,\n",
       "   0.11239992082118988,\n",
       "   0.11012627184391022,\n",
       "   0.11922983825206757,\n",
       "   0.12135159224271774,\n",
       "   0.1261410415172577,\n",
       "   0.1318298876285553,\n",
       "   0.12893618643283844,\n",
       "   0.13120698928833008,\n",
       "   0.13888829946517944,\n",
       "   0.14157840609550476,\n",
       "   0.14073754847049713,\n",
       "   0.1422736793756485,\n",
       "   0.1477445811033249,\n",
       "   0.146587535738945,\n",
       "   0.14989228546619415,\n",
       "   0.14678876101970673,\n",
       "   0.14850056171417236,\n",
       "   0.14366434514522552,\n",
       "   0.14328135550022125,\n",
       "   0.13656413555145264,\n",
       "   0.1338486671447754,\n",
       "   0.12295553088188171,\n",
       "   0.1208978146314621,\n",
       "   0.114444300532341,\n",
       "   0.12547335028648376,\n",
       "   0.12724879384040833,\n",
       "   0.12093960493803024,\n",
       "   0.12247931957244873,\n",
       "   0.126368910074234,\n",
       "   0.12216497212648392,\n",
       "   0.12388536334037781,\n",
       "   0.13273799419403076,\n",
       "   0.13634218275547028,\n",
       "   0.13290734589099884,\n",
       "   0.1324126422405243,\n",
       "   0.1354488581418991,\n",
       "   0.13762526214122772,\n",
       "   0.13672277331352234,\n",
       "   0.1349029392004013,\n",
       "   0.13798058032989502,\n",
       "   0.1340540498495102,\n",
       "   0.13871683180332184,\n",
       "   0.13973116874694824,\n",
       "   0.13872908055782318,\n",
       "   0.14279870688915253,\n",
       "   0.14663724601268768,\n",
       "   0.14015044271945953,\n",
       "   0.14768463373184204,\n",
       "   0.14502005279064178,\n",
       "   0.14740487933158875,\n",
       "   0.14302662014961243,\n",
       "   0.14908871054649353,\n",
       "   0.1516447812318802,\n",
       "   0.1532672643661499,\n",
       "   0.15053237974643707,\n",
       "   0.1517191082239151,\n",
       "   0.1537502557039261,\n",
       "   0.14733679592609406,\n",
       "   0.14448532462120056,\n",
       "   0.1443203240633011,\n",
       "   0.1420905888080597,\n",
       "   0.1375560313463211,\n",
       "   0.14169535040855408,\n",
       "   0.1396012157201767,\n",
       "   0.14935792982578278,\n",
       "   0.14097872376441956,\n",
       "   0.14247697591781616,\n",
       "   0.1486525982618332,\n",
       "   0.15140463411808014,\n",
       "   0.14542606472969055,\n",
       "   0.14290285110473633,\n",
       "   0.1403278261423111,\n",
       "   0.13496249914169312,\n",
       "   0.12726673483848572,\n",
       "   0.12534750998020172,\n",
       "   0.11676972359418869,\n",
       "   0.10988372564315796,\n",
       "   0.11287042498588562,\n",
       "   0.10539701581001282,\n",
       "   0.10238061845302582,\n",
       "   0.10857792943716049,\n",
       "   0.10567255318164825,\n",
       "   0.10143255442380905,\n",
       "   0.10490482300519943,\n",
       "   0.09980060905218124,\n",
       "   0.09718850255012512,\n",
       "   0.09690114110708237,\n",
       "   0.09816808998584747,\n",
       "   0.09979631751775742,\n",
       "   0.10261910408735275,\n",
       "   0.1017879769206047,\n",
       "   0.09967781603336334,\n",
       "   0.09904937446117401,\n",
       "   0.09487805515527725,\n",
       "   0.09940473735332489,\n",
       "   0.09414452314376831,\n",
       "   0.0905001163482666,\n",
       "   0.0855269506573677,\n",
       "   0.0876196026802063,\n",
       "   0.0841180756688118,\n",
       "   0.08635365962982178,\n",
       "   0.0885033831000328,\n",
       "   0.0889592245221138,\n",
       "   0.08483979105949402,\n",
       "   0.08569209277629852,\n",
       "   0.09134580194950104,\n",
       "   0.08743897080421448,\n",
       "   0.08178289979696274,\n",
       "   0.0807114765048027,\n",
       "   0.08441869169473648,\n",
       "   0.0800623968243599,\n",
       "   0.07741086930036545,\n",
       "   0.07995936274528503,\n",
       "   0.0847209021449089,\n",
       "   0.0849461480975151,\n",
       "   0.08397739380598068,\n",
       "   0.08526943624019623,\n",
       "   0.0853605717420578,\n",
       "   0.09002602845430374,\n",
       "   0.09129934012889862,\n",
       "   0.09207833558320999,\n",
       "   0.08932044357061386,\n",
       "   0.08744042366743088,\n",
       "   0.0832940936088562,\n",
       "   0.0797867700457573,\n",
       "   0.08153745532035828,\n",
       "   0.08530671149492264,\n",
       "   0.08448893576860428,\n",
       "   0.08467761427164078,\n",
       "   0.08989804238080978,\n",
       "   0.09257510304450989,\n",
       "   0.0939367339015007,\n",
       "   0.09357413649559021,\n",
       "   0.09288031607866287,\n",
       "   0.09986674785614014,\n",
       "   0.10726390033960342,\n",
       "   0.1101406142115593,\n",
       "   0.1143951416015625,\n",
       "   0.11433606594800949,\n",
       "   0.11352340131998062,\n",
       "   0.11172427982091904,\n",
       "   0.10612636059522629,\n",
       "   0.10517927259206772,\n",
       "   0.10133392363786697,\n",
       "   0.10069409757852554,\n",
       "   0.1014752984046936,\n",
       "   0.09860624372959137,\n",
       "   0.09985434263944626,\n",
       "   0.09919285774230957,\n",
       "   0.0975629910826683,\n",
       "   0.09329766780138016,\n",
       "   0.08880738914012909,\n",
       "   0.085611492395401,\n",
       "   0.08468060940504074,\n",
       "   0.09020256251096725,\n",
       "   0.08562149107456207,\n",
       "   0.08708090335130692,\n",
       "   0.0859910175204277,\n",
       "   0.08137015253305435,\n",
       "   0.08155814558267593,\n",
       "   0.08423895388841629,\n",
       "   0.0829734280705452,\n",
       "   0.08045971393585205,\n",
       "   0.07707354426383972,\n",
       "   0.07612279057502747,\n",
       "   0.07615484297275543,\n",
       "   0.07241754978895187,\n",
       "   0.06963704526424408,\n",
       "   0.06885483115911484,\n",
       "   0.07089098542928696,\n",
       "   0.07314615696668625,\n",
       "   0.07455244660377502,\n",
       "   0.07601051032543182,\n",
       "   0.07849515229463577,\n",
       "   0.0801578164100647,\n",
       "   0.07786078006029129,\n",
       "   0.07418576627969742,\n",
       "   0.0746893361210823,\n",
       "   0.08117596060037613,\n",
       "   0.08172020316123962,\n",
       "   0.07989367097616196,\n",
       "   0.08049898594617844,\n",
       "   0.08480772376060486,\n",
       "   0.08263400197029114,\n",
       "   0.0832221657037735,\n",
       "   0.08152242004871368,\n",
       "   0.08186458051204681,\n",
       "   0.08258236944675446,\n",
       "   0.08473443984985352,\n",
       "   0.08206602931022644,\n",
       "   0.07888595759868622,\n",
       "   0.0786949023604393,\n",
       "   0.08344905078411102,\n",
       "   0.08373736590147018,\n",
       "   0.08426812291145325,\n",
       "   0.0833541750907898,\n",
       "   0.08275263011455536,\n",
       "   0.08252576738595963,\n",
       "   0.0819939449429512,\n",
       "   0.08487195521593094,\n",
       "   0.07792888581752777,\n",
       "   0.07950962334871292,\n",
       "   0.07956881821155548,\n",
       "   0.08153180032968521,\n",
       "   0.07826840877532959,\n",
       "   0.07693508267402649,\n",
       "   0.07765698432922363,\n",
       "   0.08301175385713577,\n",
       "   0.0790669247508049,\n",
       "   0.0763033926486969,\n",
       "   0.07341231405735016,\n",
       "   0.07076840102672577,\n",
       "   0.07110375910997391,\n",
       "   0.0759907141327858,\n",
       "   0.0798882469534874,\n",
       "   0.08638320118188858,\n",
       "   0.0857497975230217,\n",
       "   0.08390774577856064,\n",
       "   0.0842236876487732,\n",
       "   0.08368929475545883,\n",
       "   0.08928309381008148,\n",
       "   0.09371155500411987,\n",
       "   0.09382911771535873,\n",
       "   0.0922451838850975,\n",
       "   0.09618516266345978,\n",
       "   0.09402459114789963,\n",
       "   0.09068608283996582,\n",
       "   0.08319837599992752,\n",
       "   0.08266296237707138,\n",
       "   0.08521999418735504,\n",
       "   0.08602507412433624,\n",
       "   0.08438365161418915,\n",
       "   0.08162970840930939,\n",
       "   0.08429303020238876,\n",
       "   0.0845334529876709,\n",
       "   0.08866805583238602,\n",
       "   0.08769678324460983,\n",
       "   0.08850964903831482,\n",
       "   0.08703294396400452,\n",
       "   0.08560798317193985,\n",
       "   0.08733930438756943,\n",
       "   0.08696984499692917,\n",
       "   0.08872997760772705,\n",
       "   0.08520276844501495,\n",
       "   0.07888313382863998,\n",
       "   0.07344865053892136,\n",
       "   0.06707030534744263,\n",
       "   0.0616973452270031,\n",
       "   0.05908719077706337,\n",
       "   0.053376246243715286,\n",
       "   0.05210632085800171,\n",
       "   0.05221430957317352,\n",
       "   0.04917222633957863,\n",
       "   0.05139695852994919,\n",
       "   0.05113988742232323,\n",
       "   0.052274107933044434,\n",
       "   0.054210156202316284,\n",
       "   0.055824775248765945,\n",
       "   0.051566459238529205,\n",
       "   0.04988989233970642,\n",
       "   0.049746740609407425,\n",
       "   0.05263158679008484,\n",
       "   0.05306190252304077,\n",
       "   0.05299611762166023,\n",
       "   0.05228506028652191,\n",
       "   0.05414639040827751,\n",
       "   0.05607227981090546,\n",
       "   0.05546816065907478,\n",
       "   0.054706793278455734,\n",
       "   0.056535590440034866,\n",
       "   0.058309685438871384,\n",
       "   0.059720203280448914,\n",
       "   0.05821140483021736,\n",
       "   0.05671486258506775,\n",
       "   0.05313562974333763,\n",
       "   0.057020403444767,\n",
       "   0.06014294922351837,\n",
       "   0.06426980346441269,\n",
       "   0.06412028521299362,\n",
       "   0.06341301649808884,\n",
       "   0.0675283819437027,\n",
       "   0.06731174141168594,\n",
       "   0.0662323608994484,\n",
       "   0.06505908071994781,\n",
       "   0.0656624436378479,\n",
       "   0.06421270221471786,\n",
       "   0.06642179191112518,\n",
       "   0.06483519822359085,\n",
       "   0.06507741659879684,\n",
       "   0.06302254647016525,\n",
       "   0.06094149500131607,\n",
       "   0.059412308037281036,\n",
       "   0.05961080268025398,\n",
       "   0.05611376464366913,\n",
       "   0.054917726665735245,\n",
       "   0.054102443158626556,\n",
       "   0.05697278305888176,\n",
       "   0.057383935898542404,\n",
       "   0.058314815163612366,\n",
       "   0.05785631760954857,\n",
       "   0.05924225598573685,\n",
       "   0.06155452877283096,\n",
       "   0.06152231991291046,\n",
       "   0.05632266774773598,\n",
       "   0.05551602691411972,\n",
       "   0.052612632513046265,\n",
       "   0.05161423981189728,\n",
       "   0.05625726655125618,\n",
       "   0.052587032318115234,\n",
       "   0.05282163619995117,\n",
       "   0.05379531905055046,\n",
       "   0.051129937171936035,\n",
       "   0.05268682539463043,\n",
       "   0.05503356456756592,\n",
       "   0.05562753602862358,\n",
       "   0.04975171387195587,\n",
       "   0.052498169243335724,\n",
       "   0.04991339147090912,\n",
       "   0.04943928122520447,\n",
       "   0.04561645910143852,\n",
       "   0.04732687398791313,\n",
       "   0.04565052688121796,\n",
       "   0.044590581208467484,\n",
       "   0.04530533030629158,\n",
       "   0.04616282507777214,\n",
       "   0.04772074893116951,\n",
       "   0.050708379596471786],\n",
       "  'voc_loss': [0.6126338243484497,\n",
       "   0.6126338243484497,\n",
       "   0.6126338243484497,\n",
       "   0.6041919589042664,\n",
       "   0.5691892504692078,\n",
       "   0.4336017370223999,\n",
       "   0.26819342374801636,\n",
       "   0.14251501858234406,\n",
       "   0.0742083415389061,\n",
       "   0.036413293331861496,\n",
       "   0.02035483345389366,\n",
       "   0.01708943024277687,\n",
       "   0.021738244220614433,\n",
       "   0.029545290395617485,\n",
       "   0.04068349301815033,\n",
       "   0.040392305701971054,\n",
       "   0.04154924303293228,\n",
       "   0.04609934613108635,\n",
       "   0.04970800504088402,\n",
       "   0.04610571637749672,\n",
       "   0.04906560108065605,\n",
       "   0.04631958529353142,\n",
       "   0.03681536391377449,\n",
       "   0.029286744073033333,\n",
       "   0.02224399335682392,\n",
       "   0.016852857545018196,\n",
       "   0.013815823011100292,\n",
       "   0.010753687471151352,\n",
       "   0.007970239967107773,\n",
       "   0.005072476342320442,\n",
       "   0.004794571083039045,\n",
       "   0.004415248986333609,\n",
       "   0.0038679486606270075,\n",
       "   0.0032846741378307343,\n",
       "   0.0031827325001358986,\n",
       "   0.0027622978668659925,\n",
       "   0.002713983878493309,\n",
       "   0.0027376050129532814,\n",
       "   0.0028761138673871756,\n",
       "   0.003176844445988536,\n",
       "   0.003500755876302719,\n",
       "   0.004026924725621939,\n",
       "   0.004506837576627731,\n",
       "   0.0043331654742360115,\n",
       "   0.004026318434625864,\n",
       "   0.00370866060256958,\n",
       "   0.0032737054862082005,\n",
       "   0.003437980078160763,\n",
       "   0.003112243255600333,\n",
       "   0.00342574343085289,\n",
       "   0.003548775566741824,\n",
       "   0.00350272748619318,\n",
       "   0.0028759536799043417,\n",
       "   0.0027028974145650864,\n",
       "   0.0024757732171565294,\n",
       "   0.002456597052514553,\n",
       "   0.0024133767001330853,\n",
       "   0.002354950411245227,\n",
       "   0.0024903803132474422,\n",
       "   0.0025146023835986853,\n",
       "   0.00244127050973475,\n",
       "   0.0023462979588657618,\n",
       "   0.002382954815402627,\n",
       "   0.002383376006036997,\n",
       "   0.0024553597904741764,\n",
       "   0.0024693713057786226,\n",
       "   0.0025530015118420124,\n",
       "   0.0025407965295016766,\n",
       "   0.00252983788959682,\n",
       "   0.0024694495368748903,\n",
       "   0.0024832759518176317,\n",
       "   0.00260781473480165,\n",
       "   0.00270232860930264,\n",
       "   0.002630576491355896,\n",
       "   0.0025480370968580246,\n",
       "   0.0025998959317803383,\n",
       "   0.002616284182295203,\n",
       "   0.0028189760632812977,\n",
       "   0.002764229429885745,\n",
       "   0.0028491003904491663,\n",
       "   0.0028609526343643665,\n",
       "   0.0030319595243781805,\n",
       "   0.0031583011150360107,\n",
       "   0.0031430108938366175,\n",
       "   0.003215791890397668,\n",
       "   0.0033796261996030807,\n",
       "   0.0035290918312966824,\n",
       "   0.0035987719893455505,\n",
       "   0.003355753840878606,\n",
       "   0.0035051601007580757,\n",
       "   0.003580911550670862,\n",
       "   0.003474293975159526,\n",
       "   0.0033837417140603065,\n",
       "   0.003473073709756136,\n",
       "   0.003429852891713381,\n",
       "   0.0033776573836803436,\n",
       "   0.0032958032097667456,\n",
       "   0.0032290127128362656,\n",
       "   0.0032386290840804577,\n",
       "   0.0032517400104552507,\n",
       "   0.003450732911005616,\n",
       "   0.003663359908387065,\n",
       "   0.0037469258531928062,\n",
       "   0.0034277141094207764,\n",
       "   0.0034805212635546923,\n",
       "   0.0035190246999263763,\n",
       "   0.0034908365923911333,\n",
       "   0.003946180921047926,\n",
       "   0.0037277699448168278,\n",
       "   0.0036445357836782932,\n",
       "   0.0034678978845477104,\n",
       "   0.0032388020772486925,\n",
       "   0.0032262394670397043,\n",
       "   0.003161687171086669,\n",
       "   0.003560021286830306,\n",
       "   0.0036840965040028095,\n",
       "   0.003591579617932439,\n",
       "   0.0034071707632392645,\n",
       "   0.0033784648403525352,\n",
       "   0.003361394163221121,\n",
       "   0.0031087968964129686,\n",
       "   0.002892320742830634,\n",
       "   0.0028631079476326704,\n",
       "   0.0027990310918539762,\n",
       "   0.002613859251141548,\n",
       "   0.002655334072187543,\n",
       "   0.0025186866987496614,\n",
       "   0.0025618476793169975,\n",
       "   0.0025311149656772614,\n",
       "   0.002578049199655652,\n",
       "   0.0025350688956677914,\n",
       "   0.002566914539784193,\n",
       "   0.0025820841547101736,\n",
       "   0.0027550063095986843,\n",
       "   0.002695071743801236,\n",
       "   0.002733849221840501,\n",
       "   0.002710810862481594,\n",
       "   0.00287448288872838,\n",
       "   0.0026787901297211647,\n",
       "   0.002463986398652196,\n",
       "   0.0024152190890163183,\n",
       "   0.002426214050501585,\n",
       "   0.002437351504340768,\n",
       "   0.0023663353640586138,\n",
       "   0.0025912935379892588,\n",
       "   0.0024532012175768614,\n",
       "   0.0023225178010761738,\n",
       "   0.0023203673772513866,\n",
       "   0.0021701217629015446,\n",
       "   0.002189180115237832,\n",
       "   0.002180528361350298,\n",
       "   0.0021632383577525616,\n",
       "   0.002271653153002262,\n",
       "   0.0023369556292891502,\n",
       "   0.0024344208650290966,\n",
       "   0.002493941690772772,\n",
       "   0.002487370977178216,\n",
       "   0.0024830843321979046,\n",
       "   0.0024829902686178684,\n",
       "   0.0025006632786244154,\n",
       "   0.002471262589097023,\n",
       "   0.0024335640482604504,\n",
       "   0.00244610826484859,\n",
       "   0.0024765809066593647,\n",
       "   0.002493807813152671,\n",
       "   0.00246020732447505,\n",
       "   0.0024213334545493126,\n",
       "   0.0024258519988507032,\n",
       "   0.0024016881361603737,\n",
       "   0.002409333363175392,\n",
       "   0.0024322590325027704,\n",
       "   0.002466217614710331,\n",
       "   0.0024314397014677525,\n",
       "   0.0024007989559322596,\n",
       "   0.00245295581407845,\n",
       "   0.0024243127554655075,\n",
       "   0.0023684680927544832,\n",
       "   0.002340257167816162,\n",
       "   0.002371756127104163,\n",
       "   0.002350226044654846,\n",
       "   0.0024250519927591085,\n",
       "   0.0024254608433693647,\n",
       "   0.0025185176637023687,\n",
       "   0.002355442848056555,\n",
       "   0.0023612056393176317,\n",
       "   0.0023534074425697327,\n",
       "   0.002402277896180749,\n",
       "   0.002373217139393091,\n",
       "   0.0023154662922024727,\n",
       "   0.0023097197990864515,\n",
       "   0.0023221869487315416,\n",
       "   0.0022416459396481514,\n",
       "   0.00223012687638402,\n",
       "   0.0022412731777876616,\n",
       "   0.0022333774250000715,\n",
       "   0.0021920413710176945,\n",
       "   0.0021737213246524334,\n",
       "   0.0021376933436840773,\n",
       "   0.002146628452464938,\n",
       "   0.002179608913138509,\n",
       "   0.0021679347846657038,\n",
       "   0.00217974791303277,\n",
       "   0.002164758276194334,\n",
       "   0.0023169838823378086,\n",
       "   0.002528415061533451,\n",
       "   0.0027477103285491467,\n",
       "   0.002608006354421377,\n",
       "   0.0024324196856468916,\n",
       "   0.0024214284494519234,\n",
       "   0.002432187320664525,\n",
       "   0.002500050701200962,\n",
       "   0.002760306466370821,\n",
       "   0.0029771176632493734,\n",
       "   0.00250189914368093,\n",
       "   0.0022303226869553328,\n",
       "   0.002293406752869487,\n",
       "   0.002288384363055229,\n",
       "   0.002265280345454812,\n",
       "   0.0023455496411770582,\n",
       "   0.0023183166049420834,\n",
       "   0.0025915999431163073,\n",
       "   0.0024825655855238438,\n",
       "   0.002357231453061104,\n",
       "   0.0024687722325325012,\n",
       "   0.0024938166607171297,\n",
       "   0.002399421064183116,\n",
       "   0.002547586103901267,\n",
       "   0.00255890772677958,\n",
       "   0.0025913133285939693,\n",
       "   0.0026895334012806416,\n",
       "   0.0026994820218533278,\n",
       "   0.002772832289338112,\n",
       "   0.002509023528546095,\n",
       "   0.0025009268429130316,\n",
       "   0.002551876939833164,\n",
       "   0.002373600145801902,\n",
       "   0.0023562111891806126,\n",
       "   0.002353624440729618,\n",
       "   0.002503501484170556,\n",
       "   0.0024935926776379347,\n",
       "   0.0026395777240395546,\n",
       "   0.0024921807926148176,\n",
       "   0.0024139387533068657,\n",
       "   0.0025831907987594604,\n",
       "   0.002558137523010373,\n",
       "   0.002544500632211566,\n",
       "   0.00254772137850523,\n",
       "   0.002502261660993099,\n",
       "   0.002489814767614007,\n",
       "   0.0025514308363199234,\n",
       "   0.002541043097153306,\n",
       "   0.002487477380782366,\n",
       "   0.0026058282237499952,\n",
       "   0.0027436851523816586,\n",
       "   0.00259065511636436,\n",
       "   0.0025607736315578222,\n",
       "   0.0025085494853556156,\n",
       "   0.0023158977273851633,\n",
       "   0.0024096518754959106,\n",
       "   0.002428859705105424,\n",
       "   0.0024361039977520704,\n",
       "   0.002551954472437501,\n",
       "   0.002610762370750308,\n",
       "   0.002494924468919635,\n",
       "   0.002539965556934476,\n",
       "   0.002704818267375231,\n",
       "   0.002732112305238843,\n",
       "   0.0025470806285738945,\n",
       "   0.0025097522884607315,\n",
       "   0.002563758520409465,\n",
       "   0.002444109646603465,\n",
       "   0.002614733297377825,\n",
       "   0.002560989698395133,\n",
       "   0.0023735850118100643,\n",
       "   0.002430515829473734,\n",
       "   0.002535342238843441,\n",
       "   0.002703920705243945,\n",
       "   0.002774139167740941,\n",
       "   0.0026725265197455883,\n",
       "   0.0027933542151004076,\n",
       "   0.002313141943886876,\n",
       "   0.0023001220542937517,\n",
       "   0.0023946191649883986,\n",
       "   0.0025233468040823936,\n",
       "   0.0024404136929661036,\n",
       "   0.0024983002804219723,\n",
       "   0.002823818475008011,\n",
       "   0.0027316948398947716,\n",
       "   0.00253985240124166,\n",
       "   0.0024873476941138506,\n",
       "   0.0023652829695492983,\n",
       "   0.002559360582381487,\n",
       "   0.0026858244091272354,\n",
       "   0.002534317784011364,\n",
       "   0.002505072858184576,\n",
       "   0.002445819554850459,\n",
       "   0.0022918125614523888,\n",
       "   0.002211344661191106,\n",
       "   0.002124387538060546,\n",
       "   0.0021559621673077345,\n",
       "   0.0021222627256065607,\n",
       "   0.0021737096831202507,\n",
       "   0.002165368990972638,\n",
       "   0.002129953820258379,\n",
       "   0.002151347231119871,\n",
       "   0.002118778182193637,\n",
       "   0.0019654007628560066,\n",
       "   0.0019766688346862793,\n",
       "   0.002027255715802312,\n",
       "   0.0020233318209648132,\n",
       "   0.002017346676439047,\n",
       "   0.0020166784524917603,\n",
       "   0.0020355279557406902,\n",
       "   0.002038098406046629,\n",
       "   0.0020586068276315928,\n",
       "   0.0020755394361913204,\n",
       "   0.002103521255776286,\n",
       "   0.0020969717297703028,\n",
       "   0.002096773125231266,\n",
       "   0.0021279326174408197,\n",
       "   0.002228738274425268,\n",
       "   0.002259113360196352,\n",
       "   0.0022394973784685135,\n",
       "   0.0022705369628965855,\n",
       "   0.0022479896433651447,\n",
       "   0.0022499109618365765,\n",
       "   0.002260033041238785,\n",
       "   0.0022642756812274456,\n",
       "   0.002293206984177232,\n",
       "   0.002324803499504924,\n",
       "   0.002352947136387229,\n",
       "   0.0023578025866299868,\n",
       "   0.0023452085442841053,\n",
       "   0.0023464083205908537,\n",
       "   0.002364084590226412,\n",
       "   0.0024247663095593452,\n",
       "   0.002407458843663335,\n",
       "   0.0024337084032595158,\n",
       "   0.002450754167512059,\n",
       "   0.002439134754240513,\n",
       "   0.0024873483926057816,\n",
       "   0.0025164377875626087,\n",
       "   0.0024264734238386154,\n",
       "   0.00244655623100698,\n",
       "   0.0024670644197613,\n",
       "   0.0025034041609615088,\n",
       "   0.002525374758988619,\n",
       "   0.0025436640717089176,\n",
       "   0.0025677341036498547,\n",
       "   0.002567051909863949,\n",
       "   0.0025906390510499477,\n",
       "   0.0025818608701229095,\n",
       "   0.0026364969089627266,\n",
       "   0.0026621574070304632,\n",
       "   0.002614272991195321,\n",
       "   0.0027106180787086487,\n",
       "   0.0026285455096513033,\n",
       "   0.0025513058062642813,\n",
       "   0.0025747206527739763,\n",
       "   0.002582276239991188,\n",
       "   0.00246263830922544,\n",
       "   0.0024297297932207584,\n",
       "   0.0024246820248663425,\n",
       "   0.0024518496356904507,\n",
       "   0.0023685044143348932,\n",
       "   0.0023395658936351538,\n",
       "   0.0023271068930625916,\n",
       "   0.00230078911408782,\n",
       "   0.0022546604741364717,\n",
       "   0.002259257948026061,\n",
       "   0.002248369622975588,\n",
       "   0.00224102009087801,\n",
       "   0.0022629043087363243,\n",
       "   0.0023160295095294714,\n",
       "   0.0022887454833835363,\n",
       "   0.002320970641449094,\n",
       "   0.00234590214677155,\n",
       "   0.002301995176821947,\n",
       "   0.002323040273040533,\n",
       "   0.002302886452525854,\n",
       "   0.0022635492496192455,\n",
       "   0.002273114398121834,\n",
       "   0.002290458185598254,\n",
       "   0.002311937278136611,\n",
       "   0.002245119074359536,\n",
       "   0.002251783385872841,\n",
       "   0.0022688540630042553,\n",
       "   0.0022611694876104593,\n",
       "   0.002281959168612957,\n",
       "   0.002386098960414529,\n",
       "   0.0023133093491196632,\n",
       "   0.002300238236784935,\n",
       "   0.0022936046589165926,\n",
       "   0.0023029267322272062,\n",
       "   0.0023069989401847124,\n",
       "   0.0023149342741817236,\n",
       "   0.002332623116672039,\n",
       "   0.002322942018508911,\n",
       "   0.0024267928674817085,\n",
       "   0.002428851556032896,\n",
       "   0.0023792774882167578,\n",
       "   0.0023232821840792894,\n",
       "   0.002288113348186016,\n",
       "   0.0022791174706071615,\n",
       "   0.002229496603831649,\n",
       "   0.002260817214846611,\n",
       "   0.002230031182989478,\n",
       "   0.002232703845947981,\n",
       "   0.002179159317165613,\n",
       "   0.002167919185012579,\n",
       "   0.002140806056559086,\n",
       "   0.0020966543816030025,\n",
       "   0.002082832856103778,\n",
       "   0.0021291086450219154,\n",
       "   0.0021053412929177284,\n",
       "   0.00221023871563375,\n",
       "   0.0023303101770579815,\n",
       "   0.002558232517912984,\n",
       "   0.0024951330851763487,\n",
       "   0.002486898098140955,\n",
       "   0.0023724325001239777,\n",
       "   0.0024226922541856766,\n",
       "   0.0023965302389115095,\n",
       "   0.002503640716895461,\n",
       "   0.00271359970793128,\n",
       "   0.002488834084942937,\n",
       "   0.0024628997780382633,\n",
       "   0.002730584004893899,\n",
       "   0.002637803088873625,\n",
       "   0.002307544695213437,\n",
       "   0.0024433673825114965,\n",
       "   0.0027342424727976322,\n",
       "   0.0029566395096480846,\n",
       "   0.0031882766634225845,\n",
       "   0.0025274783838540316,\n",
       "   0.002559641608968377,\n",
       "   0.0026770413387566805,\n",
       "   0.0027272053994238377,\n",
       "   0.002528774319216609,\n",
       "   0.0024402113631367683,\n",
       "   0.002559094689786434,\n",
       "   0.0026249245274811983,\n",
       "   0.00263808760792017,\n",
       "   0.002603712258860469,\n",
       "   0.0025888518430292606,\n",
       "   0.0025799463037401438,\n",
       "   0.0025493705179542303,\n",
       "   0.0026647504419088364,\n",
       "   0.0026764292269945145,\n",
       "   0.0027046981267631054,\n",
       "   0.002404040889814496,\n",
       "   0.002462360542267561,\n",
       "   0.002566908486187458,\n",
       "   0.002726129489019513,\n",
       "   0.0027006028685718775,\n",
       "   0.0027742807287722826,\n",
       "   0.002809159690514207,\n",
       "   0.0028468791861087084,\n",
       "   0.0029394873417913914,\n",
       "   0.0028777839615941048,\n",
       "   0.0025498075410723686,\n",
       "   0.0025305640883743763,\n",
       "   0.002278883708640933,\n",
       "   0.002357877790927887,\n",
       "   0.002331553725525737,\n",
       "   0.0026702594477683306,\n",
       "   0.002768005011603236,\n",
       "   0.002721663098782301,\n",
       "   0.002716038841754198,\n",
       "   0.0027132490649819374,\n",
       "   0.0026001669466495514,\n",
       "   0.0025443220511078835,\n",
       "   0.0025164703838527203,\n",
       "   0.0024970148224383593,\n",
       "   0.002444210695102811,\n",
       "   0.0022717879619449377,\n",
       "   0.002202377887442708,\n",
       "   0.00214988156221807,\n",
       "   0.002245865063741803,\n",
       "   0.0021815660875290632,\n",
       "   0.0021254997700452805,\n",
       "   0.002072142669931054,\n",
       "   0.0020335952285677195,\n",
       "   0.0020029055885970592,\n",
       "   0.0019997309427708387,\n",
       "   0.001984036061912775,\n",
       "   0.0019435281865298748,\n",
       "   0.0019066946115344763,\n",
       "   0.001852574059739709,\n",
       "   0.0018957850988954306,\n",
       "   0.0018653696170076728,\n",
       "   0.0018346073338761926,\n",
       "   0.001814554212614894,\n",
       "   0.0017967668827623129,\n",
       "   0.001782042090781033,\n",
       "   0.001727801631204784,\n",
       "   0.0017311485717073083,\n",
       "   0.0017351177521049976,\n",
       "   0.0017421102384105325,\n",
       "   0.0017270019743591547],\n",
       "  'jsc_loss': [0.5213232636451721,\n",
       "   0.5213232636451721,\n",
       "   0.5213232636451721,\n",
       "   0.5213232636451721,\n",
       "   0.5213232636451721,\n",
       "   0.5213232636451721,\n",
       "   0.5213232636451721,\n",
       "   0.5209238529205322,\n",
       "   0.5203303694725037,\n",
       "   0.5197666883468628,\n",
       "   0.5182556509971619,\n",
       "   0.5151522159576416,\n",
       "   0.498213529586792,\n",
       "   0.49012699723243713,\n",
       "   0.491630494594574,\n",
       "   0.4840502142906189,\n",
       "   0.474727988243103,\n",
       "   0.46663805842399597,\n",
       "   0.46216389536857605,\n",
       "   0.44881680607795715,\n",
       "   0.44055694341659546,\n",
       "   0.42929935455322266,\n",
       "   0.43151918053627014,\n",
       "   0.4284873604774475,\n",
       "   0.415836900472641,\n",
       "   0.41307440400123596,\n",
       "   0.41707685589790344,\n",
       "   0.415746808052063,\n",
       "   0.4210922420024872,\n",
       "   0.4228556752204895,\n",
       "   0.4095194339752197,\n",
       "   0.40103837847709656,\n",
       "   0.39795297384262085,\n",
       "   0.3922336995601654,\n",
       "   0.38143718242645264,\n",
       "   0.3627924621105194,\n",
       "   0.362261563539505,\n",
       "   0.36634883284568787,\n",
       "   0.37194812297821045,\n",
       "   0.3706718683242798,\n",
       "   0.35628852248191833,\n",
       "   0.3456589877605438,\n",
       "   0.3446395695209503,\n",
       "   0.3404928743839264,\n",
       "   0.33556854724884033,\n",
       "   0.3450085520744324,\n",
       "   0.3463466167449951,\n",
       "   0.34387150406837463,\n",
       "   0.34287089109420776,\n",
       "   0.34401601552963257,\n",
       "   0.3352183401584625,\n",
       "   0.31817489862442017,\n",
       "   0.3178490698337555,\n",
       "   0.31297606229782104,\n",
       "   0.312953382730484,\n",
       "   0.3082208037376404,\n",
       "   0.3002626895904541,\n",
       "   0.2847883105278015,\n",
       "   0.28230762481689453,\n",
       "   0.2707654535770416,\n",
       "   0.2723408639431,\n",
       "   0.2786071002483368,\n",
       "   0.27721452713012695,\n",
       "   0.28415584564208984,\n",
       "   0.27428725361824036,\n",
       "   0.26738399267196655,\n",
       "   0.2594674825668335,\n",
       "   0.2590300142765045,\n",
       "   0.26268166303634644,\n",
       "   0.25741204619407654,\n",
       "   0.2561872601509094,\n",
       "   0.24673739075660706,\n",
       "   0.24397888779640198,\n",
       "   0.24475790560245514,\n",
       "   0.24900232255458832,\n",
       "   0.2439465969800949,\n",
       "   0.23472924530506134,\n",
       "   0.23681002855300903,\n",
       "   0.24112403392791748,\n",
       "   0.24027490615844727,\n",
       "   0.2342808097600937,\n",
       "   0.23260809481143951,\n",
       "   0.23028935492038727,\n",
       "   0.22674085199832916,\n",
       "   0.21580225229263306,\n",
       "   0.2068648338317871,\n",
       "   0.19974282383918762,\n",
       "   0.19761797785758972,\n",
       "   0.19000662863254547,\n",
       "   0.1831924319267273,\n",
       "   0.17785465717315674,\n",
       "   0.1788637340068817,\n",
       "   0.17448197305202484,\n",
       "   0.17322774231433868,\n",
       "   0.17684294283390045,\n",
       "   0.16953356564044952,\n",
       "   0.16303953528404236,\n",
       "   0.16046024858951569,\n",
       "   0.16130410134792328,\n",
       "   0.15691539645195007,\n",
       "   0.15411582589149475,\n",
       "   0.1492975652217865,\n",
       "   0.14590108394622803,\n",
       "   0.14057090878486633,\n",
       "   0.13645847141742706,\n",
       "   0.13614904880523682,\n",
       "   0.14093637466430664,\n",
       "   0.14058944582939148,\n",
       "   0.13876047730445862,\n",
       "   0.13745439052581787,\n",
       "   0.13500763475894928,\n",
       "   0.12968234717845917,\n",
       "   0.12765154242515564,\n",
       "   0.12521865963935852,\n",
       "   0.11876434087753296,\n",
       "   0.11762412637472153,\n",
       "   0.11785833537578583,\n",
       "   0.11893291026353836,\n",
       "   0.1167975515127182,\n",
       "   0.11588578671216965,\n",
       "   0.11365607380867004,\n",
       "   0.11506609618663788,\n",
       "   0.11504096537828445,\n",
       "   0.1116618812084198,\n",
       "   0.11155329644680023,\n",
       "   0.11355320364236832,\n",
       "   0.11164466291666031,\n",
       "   0.110518679022789,\n",
       "   0.10913117974996567,\n",
       "   0.10844641923904419,\n",
       "   0.10822612047195435,\n",
       "   0.10556309670209885,\n",
       "   0.10417257994413376,\n",
       "   0.1034029945731163,\n",
       "   0.1011819839477539,\n",
       "   0.09962639957666397,\n",
       "   0.09612333029508591,\n",
       "   0.09492240846157074,\n",
       "   0.09491308033466339,\n",
       "   0.09399902075529099,\n",
       "   0.09280581027269363,\n",
       "   0.09115834534168243,\n",
       "   0.09111496061086655,\n",
       "   0.09090640395879745,\n",
       "   0.08929000794887543,\n",
       "   0.08846310526132584,\n",
       "   0.08768506348133087,\n",
       "   0.08754699677228928,\n",
       "   0.08616364002227783,\n",
       "   0.08506707847118378,\n",
       "   0.0845150426030159,\n",
       "   0.08480006456375122,\n",
       "   0.08408888429403305,\n",
       "   0.0814804807305336,\n",
       "   0.08068195730447769,\n",
       "   0.08066696673631668,\n",
       "   0.07938080281019211,\n",
       "   0.08013026416301727,\n",
       "   0.08005411177873611,\n",
       "   0.08007010072469711,\n",
       "   0.07999004423618317,\n",
       "   0.08056142926216125,\n",
       "   0.07981368899345398,\n",
       "   0.08001932501792908,\n",
       "   0.07914374768733978,\n",
       "   0.07920211553573608,\n",
       "   0.08010183274745941,\n",
       "   0.08033782243728638,\n",
       "   0.08048427104949951,\n",
       "   0.08039294928312302,\n",
       "   0.07984386384487152,\n",
       "   0.08069293200969696,\n",
       "   0.0809011161327362,\n",
       "   0.08134273439645767,\n",
       "   0.08108934760093689,\n",
       "   0.0800137147307396,\n",
       "   0.08020332455635071,\n",
       "   0.0797044187784195,\n",
       "   0.07996559888124466,\n",
       "   0.08183424174785614,\n",
       "   0.0812213122844696,\n",
       "   0.08149828016757965,\n",
       "   0.08209309726953506,\n",
       "   0.08249960094690323,\n",
       "   0.08303280174732208,\n",
       "   0.08266506344079971,\n",
       "   0.08292362093925476,\n",
       "   0.08255946636199951,\n",
       "   0.08275502175092697,\n",
       "   0.08320155739784241,\n",
       "   0.08371690660715103,\n",
       "   0.0838751271367073,\n",
       "   0.08435661345720291,\n",
       "   0.08446452021598816,\n",
       "   0.08442717045545578,\n",
       "   0.08445306867361069,\n",
       "   0.08452772349119186,\n",
       "   0.08390284329652786,\n",
       "   0.0839105024933815,\n",
       "   0.08389786630868912,\n",
       "   0.08344607800245285,\n",
       "   0.08335898816585541,\n",
       "   0.08355922996997833,\n",
       "   0.08427847176790237,\n",
       "   0.08348865061998367,\n",
       "   0.08250834792852402,\n",
       "   0.08182206004858017,\n",
       "   0.08135329931974411,\n",
       "   0.08177848905324936,\n",
       "   0.08093088865280151,\n",
       "   0.08079954981803894,\n",
       "   0.0800192803144455,\n",
       "   0.07886669039726257,\n",
       "   0.07973530888557434,\n",
       "   0.0799001008272171,\n",
       "   0.07938873767852783,\n",
       "   0.07986780256032944,\n",
       "   0.0801016166806221,\n",
       "   0.07962722331285477,\n",
       "   0.07960104942321777,\n",
       "   0.07906530052423477,\n",
       "   0.07931853085756302,\n",
       "   0.07875242084264755,\n",
       "   0.07868627458810806,\n",
       "   0.07882846146821976,\n",
       "   0.0789262056350708,\n",
       "   0.07827118784189224,\n",
       "   0.07872859388589859,\n",
       "   0.07857435196638107,\n",
       "   0.0789552628993988,\n",
       "   0.07932231575250626,\n",
       "   0.07981039583683014,\n",
       "   0.07971619069576263,\n",
       "   0.0792117640376091,\n",
       "   0.07887682318687439,\n",
       "   0.07846258580684662,\n",
       "   0.07900616526603699,\n",
       "   0.07952216267585754,\n",
       "   0.07963746041059494,\n",
       "   0.0798652321100235,\n",
       "   0.07983888685703278,\n",
       "   0.07986292243003845,\n",
       "   0.08008407801389694,\n",
       "   0.08002204447984695,\n",
       "   0.08020494133234024,\n",
       "   0.08013509213924408,\n",
       "   0.07983466982841492,\n",
       "   0.07943593710660934,\n",
       "   0.07873933017253876,\n",
       "   0.07840313017368317,\n",
       "   0.07835428416728973,\n",
       "   0.07756432890892029,\n",
       "   0.07731044292449951,\n",
       "   0.076633982360363,\n",
       "   0.07555888593196869,\n",
       "   0.0758897066116333,\n",
       "   0.07615085691213608,\n",
       "   0.07602494955062866,\n",
       "   0.07594195008277893,\n",
       "   0.07572933286428452,\n",
       "   0.07548630982637405,\n",
       "   0.07533574104309082,\n",
       "   0.07503840327262878,\n",
       "   0.07538724690675735,\n",
       "   0.07537969201803207,\n",
       "   0.07543852180242538,\n",
       "   0.07589676976203918,\n",
       "   0.07600989937782288,\n",
       "   0.07598549127578735,\n",
       "   0.07617434859275818,\n",
       "   0.07591743022203445,\n",
       "   0.07552362233400345,\n",
       "   0.07546696811914444,\n",
       "   0.07558814436197281,\n",
       "   0.07553538680076599,\n",
       "   0.0755045935511589,\n",
       "   0.07593774795532227,\n",
       "   0.07572631537914276,\n",
       "   0.07556942850351334,\n",
       "   0.07551790028810501,\n",
       "   0.07583615183830261,\n",
       "   0.07594159245491028,\n",
       "   0.0759035274386406,\n",
       "   0.07608527690172195,\n",
       "   0.07591121643781662,\n",
       "   0.07567723095417023,\n",
       "   0.07587283849716187,\n",
       "   0.07553678750991821,\n",
       "   0.07498185336589813,\n",
       "   0.0751776397228241,\n",
       "   0.07510927319526672,\n",
       "   0.07521269470453262,\n",
       "   0.07539498805999756,\n",
       "   0.07550586014986038,\n",
       "   0.0754229798913002,\n",
       "   0.07523900270462036,\n",
       "   0.07496978342533112,\n",
       "   0.07493823021650314,\n",
       "   0.07484163343906403,\n",
       "   0.07475851476192474,\n",
       "   0.07460063695907593,\n",
       "   0.07420257478952408,\n",
       "   0.07406821846961975,\n",
       "   0.07413782179355621,\n",
       "   0.07363222539424896,\n",
       "   0.07340006530284882,\n",
       "   0.07331832498311996,\n",
       "   0.07332753390073776,\n",
       "   0.07373332232236862,\n",
       "   0.07375922054052353,\n",
       "   0.07356236129999161,\n",
       "   0.07345620542764664,\n",
       "   0.07335390150547028,\n",
       "   0.0731024295091629,\n",
       "   0.07354779541492462,\n",
       "   0.07374846190214157,\n",
       "   0.07361401617527008,\n",
       "   0.07343515753746033,\n",
       "   0.07298484444618225,\n",
       "   0.07294299453496933,\n",
       "   0.07310286164283752,\n",
       "   0.07296697050333023,\n",
       "   0.07294639199972153,\n",
       "   0.073111891746521,\n",
       "   0.07330138236284256,\n",
       "   0.07328733056783676,\n",
       "   0.07285714894533157,\n",
       "   0.07307648658752441,\n",
       "   0.07301292568445206,\n",
       "   0.0730004832148552,\n",
       "   0.07293341308832169,\n",
       "   0.07280414551496506,\n",
       "   0.07284506410360336,\n",
       "   0.07259072363376617,\n",
       "   0.07254635542631149,\n",
       "   0.07250656187534332,\n",
       "   0.07242238521575928,\n",
       "   0.07247459888458252,\n",
       "   0.07244873046875,\n",
       "   0.0722280815243721,\n",
       "   0.07196760922670364,\n",
       "   0.07163669914007187,\n",
       "   0.07194050401449203,\n",
       "   0.07191215455532074,\n",
       "   0.07197924703359604,\n",
       "   0.07185199111700058,\n",
       "   0.07204194366931915,\n",
       "   0.0721517950296402,\n",
       "   0.07234372198581696,\n",
       "   0.0719529539346695,\n",
       "   0.07201047241687775,\n",
       "   0.07231220602989197,\n",
       "   0.07247810810804367,\n",
       "   0.07236568629741669,\n",
       "   0.07215170562267303,\n",
       "   0.07240551710128784,\n",
       "   0.07249857485294342,\n",
       "   0.07284504920244217,\n",
       "   0.07282598316669464,\n",
       "   0.07322444021701813,\n",
       "   0.07284782081842422,\n",
       "   0.07314535975456238,\n",
       "   0.0729057788848877,\n",
       "   0.0728459358215332,\n",
       "   0.07268126308917999,\n",
       "   0.07280400395393372,\n",
       "   0.07248751074075699,\n",
       "   0.07274142652750015,\n",
       "   0.07283958047628403,\n",
       "   0.07241062819957733,\n",
       "   0.0725206732749939,\n",
       "   0.0727900043129921,\n",
       "   0.07249891012907028,\n",
       "   0.07256081700325012,\n",
       "   0.07244730740785599,\n",
       "   0.07247018069028854,\n",
       "   0.07238199561834335,\n",
       "   0.07233437150716782,\n",
       "   0.07232671231031418,\n",
       "   0.07217976450920105,\n",
       "   0.0721883475780487,\n",
       "   0.07202833890914917,\n",
       "   0.07205402851104736,\n",
       "   0.07192309200763702,\n",
       "   0.07188715040683746,\n",
       "   0.07197549939155579,\n",
       "   0.0721345990896225,\n",
       "   0.07196670770645142,\n",
       "   0.07181351631879807,\n",
       "   0.07180264592170715,\n",
       "   0.07162845134735107,\n",
       "   0.07183549553155899,\n",
       "   0.07189052551984787,\n",
       "   0.07194437086582184,\n",
       "   0.07217667996883392,\n",
       "   0.07198721915483475,\n",
       "   0.0720282793045044,\n",
       "   0.0717330127954483,\n",
       "   0.07214513421058655,\n",
       "   0.0720224529504776,\n",
       "   0.07223378121852875,\n",
       "   0.0719795972108841,\n",
       "   0.07204625010490417,\n",
       "   0.07186982035636902,\n",
       "   0.07203523069620132,\n",
       "   0.07195582240819931,\n",
       "   0.07192143052816391,\n",
       "   0.07193799316883087,\n",
       "   0.0722309872508049,\n",
       "   0.07236570119857788,\n",
       "   0.07229268550872803,\n",
       "   0.0721515342593193,\n",
       "   0.07220060378313065,\n",
       "   0.07201116532087326,\n",
       "   0.07221576571464539,\n",
       "   0.0722767785191536,\n",
       "   0.07210633158683777,\n",
       "   0.07195451855659485,\n",
       "   0.07152651995420456,\n",
       "   0.07125673443078995,\n",
       "   0.07101649045944214,\n",
       "   0.07091811299324036,\n",
       "   0.07122913748025894,\n",
       "   0.07097158581018448,\n",
       "   0.07100097090005875,\n",
       "   0.0709361657500267,\n",
       "   0.07076125591993332,\n",
       "   0.07071695476770401,\n",
       "   0.07084210962057114,\n",
       "   0.0711517184972763,\n",
       "   0.0714019238948822,\n",
       "   0.07159403711557388,\n",
       "   0.07162122428417206,\n",
       "   0.07188202440738678,\n",
       "   0.072026826441288,\n",
       "   0.07205033302307129,\n",
       "   0.07227663695812225,\n",
       "   0.07261279970407486,\n",
       "   0.07303141057491302,\n",
       "   0.07325747609138489,\n",
       "   0.07337521016597748,\n",
       "   0.07339178770780563,\n",
       "   0.07320362329483032,\n",
       "   0.07299045473337173,\n",
       "   0.07286202162504196,\n",
       "   0.07277808338403702,\n",
       "   0.07260201126337051,\n",
       "   0.07224868983030319,\n",
       "   0.07235131412744522,\n",
       "   0.0722527876496315,\n",
       "   0.07230538874864578,\n",
       "   0.07217711955308914,\n",
       "   0.07221820205450058,\n",
       "   0.07234830409288406,\n",
       "   0.07242154330015182,\n",
       "   0.07273168116807938,\n",
       "   0.07269573956727982,\n",
       "   0.07311490178108215,\n",
       "   0.07305654138326645,\n",
       "   0.07291878014802933,\n",
       "   0.0730447918176651,\n",
       "   0.07311682403087616,\n",
       "   0.0731814056634903,\n",
       "   0.07319072633981705,\n",
       "   0.07289740443229675,\n",
       "   0.07287558168172836,\n",
       "   0.07269548624753952,\n",
       "   0.07292410731315613,\n",
       "   0.072789266705513,\n",
       "   0.07280885428190231,\n",
       "   0.07274259626865387,\n",
       "   0.07265405356884003,\n",
       "   0.07247328758239746,\n",
       "   0.07216149568557739,\n",
       "   0.07212426513433456,\n",
       "   0.07222437113523483,\n",
       "   0.07256679981946945,\n",
       "   0.07263486087322235,\n",
       "   0.07259977608919144,\n",
       "   0.07271795719861984,\n",
       "   0.07252635806798935,\n",
       "   0.07273372262716293,\n",
       "   0.07299946993589401,\n",
       "   0.07288628071546555,\n",
       "   0.07284754514694214,\n",
       "   0.0730174109339714,\n",
       "   0.07333242893218994,\n",
       "   0.07316303998231888,\n",
       "   0.0734160989522934,\n",
       "   0.07358235120773315,\n",
       "   0.07339440286159515,\n",
       "   0.07329776883125305,\n",
       "   0.07311463356018066,\n",
       "   0.07273312658071518,\n",
       "   0.0728415846824646,\n",
       "   0.07294318079948425,\n",
       "   0.07312239706516266,\n",
       "   0.07324685901403427,\n",
       "   0.07336865365505219,\n",
       "   0.07307615131139755],\n",
       "  'ff_loss': [0.38128378987312317,\n",
       "   0.38128378987312317,\n",
       "   0.3802894353866577,\n",
       "   0.37849172949790955,\n",
       "   0.37731754779815674,\n",
       "   0.37347158789634705,\n",
       "   0.3668084144592285,\n",
       "   0.36195188760757446,\n",
       "   0.35660234093666077,\n",
       "   0.3326185941696167,\n",
       "   0.3022843599319458,\n",
       "   0.2796589136123657,\n",
       "   0.26455965638160706,\n",
       "   0.24958866834640503,\n",
       "   0.23673060536384583,\n",
       "   0.22911858558654785,\n",
       "   0.2259339839220047,\n",
       "   0.22722209990024567,\n",
       "   0.218251571059227,\n",
       "   0.21690136194229126,\n",
       "   0.21222172677516937,\n",
       "   0.2068634182214737,\n",
       "   0.20254993438720703,\n",
       "   0.20001813769340515,\n",
       "   0.20154590904712677,\n",
       "   0.19840280711650848,\n",
       "   0.19946317374706268,\n",
       "   0.1995183676481247,\n",
       "   0.20049196481704712,\n",
       "   0.1973094642162323,\n",
       "   0.1972036212682724,\n",
       "   0.1942920833826065,\n",
       "   0.1928689032793045,\n",
       "   0.18989145755767822,\n",
       "   0.19087953865528107,\n",
       "   0.18936315178871155,\n",
       "   0.18814106285572052,\n",
       "   0.1875702291727066,\n",
       "   0.1856192648410797,\n",
       "   0.18574316799640656,\n",
       "   0.18588297069072723,\n",
       "   0.18785986304283142,\n",
       "   0.1889621913433075,\n",
       "   0.18737243115901947,\n",
       "   0.18800057470798492,\n",
       "   0.18531431257724762,\n",
       "   0.1864260733127594,\n",
       "   0.19312509894371033,\n",
       "   0.1936674267053604,\n",
       "   0.19795767962932587,\n",
       "   0.2028631716966629,\n",
       "   0.2075432687997818,\n",
       "   0.21170379221439362,\n",
       "   0.21680963039398193,\n",
       "   0.21889090538024902,\n",
       "   0.2207544595003128,\n",
       "   0.21962769329547882,\n",
       "   0.22544795274734497,\n",
       "   0.2234930843114853,\n",
       "   0.22073093056678772,\n",
       "   0.2227468341588974,\n",
       "   0.22271236777305603,\n",
       "   0.22937090694904327,\n",
       "   0.23152802884578705,\n",
       "   0.22968760132789612,\n",
       "   0.2265537977218628,\n",
       "   0.2239975929260254,\n",
       "   0.2245124727487564,\n",
       "   0.22657082974910736,\n",
       "   0.2278987169265747,\n",
       "   0.22754520177841187,\n",
       "   0.23105604946613312,\n",
       "   0.22684527933597565,\n",
       "   0.22167888283729553,\n",
       "   0.22315199673175812,\n",
       "   0.21997733414173126,\n",
       "   0.219505175948143,\n",
       "   0.21565614640712738,\n",
       "   0.21316929161548615,\n",
       "   0.2131701558828354,\n",
       "   0.20913797616958618,\n",
       "   0.21069574356079102,\n",
       "   0.21238534152507782,\n",
       "   0.2121865451335907,\n",
       "   0.20884071290493011,\n",
       "   0.20872992277145386,\n",
       "   0.2047271877527237,\n",
       "   0.20700141787528992,\n",
       "   0.209788978099823,\n",
       "   0.20880652964115143,\n",
       "   0.20554183423519135,\n",
       "   0.20882242918014526,\n",
       "   0.21229393780231476,\n",
       "   0.20924006402492523,\n",
       "   0.21127508580684662,\n",
       "   0.21479187905788422,\n",
       "   0.214300736784935,\n",
       "   0.21557532250881195,\n",
       "   0.21312469244003296,\n",
       "   0.21153052151203156,\n",
       "   0.2100541889667511,\n",
       "   0.20565901696681976,\n",
       "   0.20821289718151093,\n",
       "   0.20979617536067963,\n",
       "   0.20929966866970062,\n",
       "   0.21203871071338654,\n",
       "   0.2100430577993393,\n",
       "   0.20913726091384888,\n",
       "   0.21085631847381592,\n",
       "   0.21372811496257782,\n",
       "   0.21658776700496674,\n",
       "   0.2173781394958496,\n",
       "   0.21822109818458557,\n",
       "   0.21584083139896393,\n",
       "   0.21699942648410797,\n",
       "   0.217741921544075,\n",
       "   0.21726007759571075,\n",
       "   0.2207532674074173,\n",
       "   0.22039131820201874,\n",
       "   0.21922563016414642,\n",
       "   0.21748700737953186,\n",
       "   0.21415305137634277,\n",
       "   0.21683073043823242,\n",
       "   0.21293994784355164,\n",
       "   0.21413776278495789,\n",
       "   0.21027842164039612,\n",
       "   0.2073545902967453,\n",
       "   0.20205602049827576,\n",
       "   0.19771866500377655,\n",
       "   0.19298036396503448,\n",
       "   0.18895578384399414,\n",
       "   0.18581993877887726,\n",
       "   0.18148842453956604,\n",
       "   0.17453843355178833,\n",
       "   0.17597337067127228,\n",
       "   0.1744338721036911,\n",
       "   0.17226308584213257,\n",
       "   0.17280437052249908,\n",
       "   0.1740296334028244,\n",
       "   0.16594329476356506,\n",
       "   0.1650262176990509,\n",
       "   0.1635555773973465,\n",
       "   0.16353856027126312,\n",
       "   0.16446024179458618,\n",
       "   0.16404227912425995,\n",
       "   0.16417337954044342,\n",
       "   0.1604992002248764,\n",
       "   0.15882974863052368,\n",
       "   0.15869563817977905,\n",
       "   0.15949958562850952,\n",
       "   0.1578473448753357,\n",
       "   0.1608528345823288,\n",
       "   0.17314213514328003,\n",
       "   0.1817546933889389,\n",
       "   0.18720489740371704,\n",
       "   0.19199378788471222,\n",
       "   0.18909093737602234,\n",
       "   0.18696479499340057,\n",
       "   0.18826468288898468,\n",
       "   0.1878349930047989,\n",
       "   0.18464377522468567,\n",
       "   0.18574120104312897,\n",
       "   0.18917930126190186,\n",
       "   0.19307167828083038,\n",
       "   0.1926589161157608,\n",
       "   0.18868955969810486,\n",
       "   0.19577285647392273,\n",
       "   0.19599135220050812,\n",
       "   0.19260533154010773,\n",
       "   0.19209139049053192,\n",
       "   0.19122403860092163,\n",
       "   0.18776284158229828,\n",
       "   0.18634004890918732,\n",
       "   0.18299415707588196,\n",
       "   0.18326489627361298,\n",
       "   0.18203622102737427,\n",
       "   0.18123792111873627,\n",
       "   0.17687514424324036,\n",
       "   0.17361703515052795,\n",
       "   0.17768898606300354,\n",
       "   0.17671702802181244,\n",
       "   0.17870283126831055,\n",
       "   0.178270161151886,\n",
       "   0.17523132264614105,\n",
       "   0.16949118673801422,\n",
       "   0.17011849582195282,\n",
       "   0.1703748106956482,\n",
       "   0.16900235414505005,\n",
       "   0.17517052590847015,\n",
       "   0.1711185723543167,\n",
       "   0.1772145926952362,\n",
       "   0.18371187150478363,\n",
       "   0.17925792932510376,\n",
       "   0.17706653475761414,\n",
       "   0.17649896442890167,\n",
       "   0.1721584051847458,\n",
       "   0.16635313630104065,\n",
       "   0.16298948228359222,\n",
       "   0.16713854670524597,\n",
       "   0.170490100979805,\n",
       "   0.1644393652677536,\n",
       "   0.15985643863677979,\n",
       "   0.1601051688194275,\n",
       "   0.15707162022590637,\n",
       "   0.16172648966312408,\n",
       "   0.1582760065793991,\n",
       "   0.15872561931610107,\n",
       "   0.1577865183353424,\n",
       "   0.155210480093956,\n",
       "   0.15041188895702362,\n",
       "   0.14617937803268433,\n",
       "   0.1438993215560913,\n",
       "   0.1452811062335968,\n",
       "   0.14853571355342865,\n",
       "   0.1460043489933014,\n",
       "   0.14959505200386047,\n",
       "   0.14757293462753296,\n",
       "   0.14413784444332123,\n",
       "   0.1449863761663437,\n",
       "   0.14088040590286255,\n",
       "   0.14007428288459778,\n",
       "   0.13819964230060577,\n",
       "   0.13670393824577332,\n",
       "   0.13182368874549866,\n",
       "   0.13202033936977386,\n",
       "   0.13224594295024872,\n",
       "   0.1283164769411087,\n",
       "   0.12734082341194153,\n",
       "   0.12605708837509155,\n",
       "   0.12353650480508804,\n",
       "   0.11868180334568024,\n",
       "   0.11665606498718262,\n",
       "   0.11958298832178116,\n",
       "   0.11930786818265915,\n",
       "   0.12148752808570862,\n",
       "   0.12448807060718536,\n",
       "   0.11940010637044907,\n",
       "   0.12183927744626999,\n",
       "   0.12186598777770996,\n",
       "   0.12178736925125122,\n",
       "   0.11981022357940674,\n",
       "   0.116855189204216,\n",
       "   0.121865414083004,\n",
       "   0.12422488629817963,\n",
       "   0.12075556069612503,\n",
       "   0.12675142288208008,\n",
       "   0.12745311856269836,\n",
       "   0.13026641309261322,\n",
       "   0.13018149137496948,\n",
       "   0.12546679377555847,\n",
       "   0.11715587228536606,\n",
       "   0.11913257837295532,\n",
       "   0.11606360226869583,\n",
       "   0.11177031695842743,\n",
       "   0.11137767881155014,\n",
       "   0.11342982947826385,\n",
       "   0.11098670959472656,\n",
       "   0.11312607675790787,\n",
       "   0.11377693712711334,\n",
       "   0.11353679746389389,\n",
       "   0.11435224115848541,\n",
       "   0.1168774738907814,\n",
       "   0.1147787943482399,\n",
       "   0.11416447162628174,\n",
       "   0.11169275641441345,\n",
       "   0.11109203100204468,\n",
       "   0.11275868862867355,\n",
       "   0.11316873133182526,\n",
       "   0.10944105684757233,\n",
       "   0.10997018218040466,\n",
       "   0.10446766763925552,\n",
       "   0.1072738990187645,\n",
       "   0.10318655520677567,\n",
       "   0.10151145607233047,\n",
       "   0.10048864036798477,\n",
       "   0.10335661470890045,\n",
       "   0.10240960121154785,\n",
       "   0.10330058634281158,\n",
       "   0.10557311773300171,\n",
       "   0.10669016093015671,\n",
       "   0.10686986893415451,\n",
       "   0.10706361383199692,\n",
       "   0.10945291072130203,\n",
       "   0.10811134427785873,\n",
       "   0.11299676448106766,\n",
       "   0.1114923506975174,\n",
       "   0.11148976534605026,\n",
       "   0.10852418094873428,\n",
       "   0.11016517877578735,\n",
       "   0.11356399208307266,\n",
       "   0.11265657842159271,\n",
       "   0.11120104044675827,\n",
       "   0.10948678851127625,\n",
       "   0.10453592240810394,\n",
       "   0.10676190257072449,\n",
       "   0.11092829704284668,\n",
       "   0.10971537232398987,\n",
       "   0.1087915450334549,\n",
       "   0.11331456154584885,\n",
       "   0.1117778792977333,\n",
       "   0.10673315823078156,\n",
       "   0.10996799170970917,\n",
       "   0.11176766455173492,\n",
       "   0.1126503199338913,\n",
       "   0.10660994052886963,\n",
       "   0.10200176388025284,\n",
       "   0.10218572616577148,\n",
       "   0.09989004582166672,\n",
       "   0.10018225759267807,\n",
       "   0.10151677578687668,\n",
       "   0.10182452201843262,\n",
       "   0.10227587819099426,\n",
       "   0.10194318741559982,\n",
       "   0.10253432393074036,\n",
       "   0.10282890498638153,\n",
       "   0.10052470117807388,\n",
       "   0.097413070499897,\n",
       "   0.10216956585645676,\n",
       "   0.10179846733808517,\n",
       "   0.10208438336849213,\n",
       "   0.10257623344659805,\n",
       "   0.10193100571632385,\n",
       "   0.10094708204269409,\n",
       "   0.10369648784399033,\n",
       "   0.10364913940429688,\n",
       "   0.10180637240409851,\n",
       "   0.09705901145935059,\n",
       "   0.09697242826223373,\n",
       "   0.0969661995768547,\n",
       "   0.09621834754943848,\n",
       "   0.09967774897813797,\n",
       "   0.10391103476285934,\n",
       "   0.10268774628639221,\n",
       "   0.0992652103304863,\n",
       "   0.0961974486708641,\n",
       "   0.09304594248533249,\n",
       "   0.09565972536802292,\n",
       "   0.09757553040981293,\n",
       "   0.09455951303243637,\n",
       "   0.09530578553676605,\n",
       "   0.09959293901920319,\n",
       "   0.1009640097618103,\n",
       "   0.10215894132852554,\n",
       "   0.10436123609542847,\n",
       "   0.1008397713303566,\n",
       "   0.10308528691530228,\n",
       "   0.10242345184087753,\n",
       "   0.1019907146692276,\n",
       "   0.10020003467798233,\n",
       "   0.10055136680603027,\n",
       "   0.1050155833363533,\n",
       "   0.10624920576810837,\n",
       "   0.10588765889406204,\n",
       "   0.10313444584608078,\n",
       "   0.10466307401657104,\n",
       "   0.1026405468583107,\n",
       "   0.10329261422157288,\n",
       "   0.10142970085144043,\n",
       "   0.10331292450428009,\n",
       "   0.10245056450366974,\n",
       "   0.10468271374702454,\n",
       "   0.10650370270013809,\n",
       "   0.10622316598892212,\n",
       "   0.1051705926656723,\n",
       "   0.10391642153263092,\n",
       "   0.10475900024175644,\n",
       "   0.10759803652763367,\n",
       "   0.10509788990020752,\n",
       "   0.10608670115470886,\n",
       "   0.1070345789194107,\n",
       "   0.10979612171649933,\n",
       "   0.10768474638462067,\n",
       "   0.10704299807548523,\n",
       "   0.10505180805921555,\n",
       "   0.10823306441307068,\n",
       "   0.10718988627195358,\n",
       "   0.10913854092359543,\n",
       "   0.10784841328859329,\n",
       "   0.10594773292541504,\n",
       "   0.10717649012804031,\n",
       "   0.10320422798395157,\n",
       "   0.1024780198931694,\n",
       "   0.10308799892663956,\n",
       "   0.1015796884894371,\n",
       "   0.09577473998069763,\n",
       "   0.09439089149236679,\n",
       "   0.09478974342346191,\n",
       "   0.09421048313379288,\n",
       "   0.09470154345035553,\n",
       "   0.09630834311246872,\n",
       "   0.09542246162891388,\n",
       "   0.09259984642267227,\n",
       "   0.09155555814504623,\n",
       "   0.09257370233535767,\n",
       "   0.09158092737197876,\n",
       "   0.09271460771560669,\n",
       "   0.093642458319664,\n",
       "   0.09475017338991165,\n",
       "   0.09402722120285034,\n",
       "   0.09687081724405289,\n",
       "   0.0960489809513092,\n",
       "   0.09949789941310883,\n",
       "   0.09617244452238083,\n",
       "   0.09314293414354324,\n",
       "   0.09345521777868271,\n",
       "   0.09692861139774323,\n",
       "   0.0994032546877861,\n",
       "   0.10088904947042465,\n",
       "   0.10116034001111984,\n",
       "   0.10202678292989731,\n",
       "   0.10221745818853378,\n",
       "   0.09792760014533997,\n",
       "   0.10020597279071808,\n",
       "   0.09777574241161346,\n",
       "   0.10232441127300262,\n",
       "   0.10257919132709503,\n",
       "   0.10247746109962463,\n",
       "   0.09714995324611664,\n",
       "   0.09786497801542282,\n",
       "   0.09466187655925751,\n",
       "   0.09375440329313278,\n",
       "   0.09121102094650269,\n",
       "   0.08879775553941727,\n",
       "   0.08793128281831741,\n",
       "   0.08970750868320465,\n",
       "   0.0904807597398758,\n",
       "   0.09072470664978027,\n",
       "   0.09496574848890305,\n",
       "   0.09446562826633453,\n",
       "   0.0931260734796524,\n",
       "   0.09699103236198425,\n",
       "   0.0956835150718689,\n",
       "   0.09784933179616928,\n",
       "   0.09404130280017853,\n",
       "   0.09212293475866318,\n",
       "   0.09359060227870941,\n",
       "   0.09098632633686066,\n",
       "   0.08990127593278885,\n",
       "   0.09028579294681549,\n",
       "   0.0917583480477333,\n",
       "   0.09057153761386871,\n",
       "   0.08827085793018341,\n",
       "   0.08936149626970291,\n",
       "   0.09209556132555008,\n",
       "   0.09226185828447342,\n",
       "   0.09199702739715576,\n",
       "   0.09108635783195496,\n",
       "   0.09274990111589432,\n",
       "   0.0918051078915596,\n",
       "   0.0912875309586525,\n",
       "   0.08940666913986206,\n",
       "   0.0872897133231163,\n",
       "   0.08884187787771225,\n",
       "   0.08808255195617676,\n",
       "   0.08967798948287964,\n",
       "   0.08925158530473709,\n",
       "   0.08500739187002182,\n",
       "   0.08303637057542801,\n",
       "   0.08442717045545578,\n",
       "   0.08407638221979141,\n",
       "   0.08265193551778793,\n",
       "   0.08259664475917816,\n",
       "   0.08271656930446625,\n",
       "   0.08717163652181625,\n",
       "   0.0881694108247757,\n",
       "   0.09089139848947525,\n",
       "   0.08987420052289963,\n",
       "   0.0891917273402214,\n",
       "   0.08672644197940826,\n",
       "   0.08795468509197235,\n",
       "   0.09254606068134308,\n",
       "   0.09441187232732773,\n",
       "   0.09446436166763306,\n",
       "   0.09253304451704025,\n",
       "   0.09235379099845886,\n",
       "   0.09141445904970169,\n",
       "   0.09132589399814606,\n",
       "   0.09204310923814774,\n",
       "   0.09031680971384048,\n",
       "   0.09125695377588272,\n",
       "   0.09088952839374542,\n",
       "   0.08711801469326019,\n",
       "   0.08864708989858627,\n",
       "   0.08972081542015076,\n",
       "   0.09096277505159378,\n",
       "   0.0927508994936943,\n",
       "   0.09834806621074677,\n",
       "   0.0997786894440651,\n",
       "   0.10053706914186478,\n",
       "   0.1008487194776535,\n",
       "   0.10133825242519379,\n",
       "   0.099261574447155,\n",
       "   0.09697767347097397,\n",
       "   0.09625580161809921,\n",
       "   0.09600435197353363,\n",
       "   0.09339545667171478,\n",
       "   0.0897359549999237,\n",
       "   0.09044493734836578,\n",
       "   0.09064686298370361,\n",
       "   0.09061884135007858],\n",
       "  'test_losses': [1.8186135590076447,\n",
       "   1.8186135590076447,\n",
       "   1.8176192045211792,\n",
       "   1.8073796331882477,\n",
       "   1.7712027430534363,\n",
       "   1.6317692697048187,\n",
       "   1.4596977829933167,\n",
       "   1.3261284977197647,\n",
       "   1.24811901897192,\n",
       "   1.183026533573866,\n",
       "   1.1294439397752285,\n",
       "   1.096111599355936,\n",
       "   1.060083193704486,\n",
       "   1.031135095283389,\n",
       "   1.0085711777210236,\n",
       "   0.96906403824687,\n",
       "   0.9375512078404427,\n",
       "   0.9185265563428402,\n",
       "   0.871151264756918,\n",
       "   0.8355668000876904,\n",
       "   0.8160957731306553,\n",
       "   0.7867317833006382,\n",
       "   0.7616413794457912,\n",
       "   0.7429282460361719,\n",
       "   0.7171624545007944,\n",
       "   0.6980908531695604,\n",
       "   0.6925929663702846,\n",
       "   0.6838266104459763,\n",
       "   0.6907917894423008,\n",
       "   0.6842998694628477,\n",
       "   0.6693646679632366,\n",
       "   0.6624707481823862,\n",
       "   0.6572901478502899,\n",
       "   0.6427878104150295,\n",
       "   0.6304758125916123,\n",
       "   0.6069651569705456,\n",
       "   0.6106943767517805,\n",
       "   0.6135801495984197,\n",
       "   0.6169095335062593,\n",
       "   0.6124872479122132,\n",
       "   0.5956943742930889,\n",
       "   0.5935522667132318,\n",
       "   0.6009069867432117,\n",
       "   0.6013830499723554,\n",
       "   0.5988533296622336,\n",
       "   0.6088182032108307,\n",
       "   0.6217409917153418,\n",
       "   0.6261927401646972,\n",
       "   0.6223098875489086,\n",
       "   0.6324551217257977,\n",
       "   0.6389019980560988,\n",
       "   0.6375821093097329,\n",
       "   0.6433796358760446,\n",
       "   0.6513645928353071,\n",
       "   0.6526596818584949,\n",
       "   0.6510866200551391,\n",
       "   0.6397118712775409,\n",
       "   0.6361224211286753,\n",
       "   0.637520486023277,\n",
       "   0.6279299871530384,\n",
       "   0.6296830011997372,\n",
       "   0.6318945495877415,\n",
       "   0.6323901473078877,\n",
       "   0.6512640216387808,\n",
       "   0.6368060917593539,\n",
       "   0.6212634325493127,\n",
       "   0.6157241319306195,\n",
       "   0.6154900039546192,\n",
       "   0.6140625707339495,\n",
       "   0.6063082285691053,\n",
       "   0.5974234242457896,\n",
       "   0.59508529282175,\n",
       "   0.5828386873472482,\n",
       "   0.5714593529701233,\n",
       "   0.5763336401432753,\n",
       "   0.5747028747573495,\n",
       "   0.5712808051612228,\n",
       "   0.577287305612117,\n",
       "   0.576475192559883,\n",
       "   0.5736450387630612,\n",
       "   0.5638606739230454,\n",
       "   0.5593750753905624,\n",
       "   0.548482745885849,\n",
       "   0.5364583714399487,\n",
       "   0.5238072148058563,\n",
       "   0.5166691076010466,\n",
       "   0.5011695525608957,\n",
       "   0.5002979710698128,\n",
       "   0.5019439135212451,\n",
       "   0.49018677044659853,\n",
       "   0.48704815516248345,\n",
       "   0.4831421065609902,\n",
       "   0.48618751857429743,\n",
       "   0.4812878151424229,\n",
       "   0.48191232280805707,\n",
       "   0.4691001735627651,\n",
       "   0.4523608402814716,\n",
       "   0.44959490559995174,\n",
       "   0.4502021255902946,\n",
       "   0.43878093478269875,\n",
       "   0.4287902864161879,\n",
       "   0.42084227758459747,\n",
       "   0.4286422720178962,\n",
       "   0.4333311915397644,\n",
       "   0.42928364616818726,\n",
       "   0.4404083751142025,\n",
       "   0.4494610589463264,\n",
       "   0.44881438044831157,\n",
       "   0.4485679776407778,\n",
       "   0.45188294583931565,\n",
       "   0.44525502156466246,\n",
       "   0.44285479676909745,\n",
       "   0.44759868481196463,\n",
       "   0.43742980412207544,\n",
       "   0.4362668243702501,\n",
       "   0.43406706349924207,\n",
       "   0.43547489983029664,\n",
       "   0.43768223910592496,\n",
       "   0.4408257594332099,\n",
       "   0.4402869320474565,\n",
       "   0.4342095006722957,\n",
       "   0.4348205083515495,\n",
       "   0.43215048941783607,\n",
       "   0.4262795269023627,\n",
       "   0.42509288527071476,\n",
       "   0.4182058267761022,\n",
       "   0.41169740562327206,\n",
       "   0.40094323735684156,\n",
       "   0.3994542844593525,\n",
       "   0.39235103060491383,\n",
       "   0.38439002772793174,\n",
       "   0.3781185704283416,\n",
       "   0.3717572947498411,\n",
       "   0.3627013242803514,\n",
       "   0.353197279619053,\n",
       "   0.35539780580438673,\n",
       "   0.35317602660506964,\n",
       "   0.35051237465813756,\n",
       "   0.3511197129264474,\n",
       "   0.33757112664170563,\n",
       "   0.3387211130466312,\n",
       "   0.33092939062044024,\n",
       "   0.32280193106271327,\n",
       "   0.3269525852520019,\n",
       "   0.3249785939697176,\n",
       "   0.3237826011609286,\n",
       "   0.3212873269803822,\n",
       "   0.32180709717795253,\n",
       "   0.32527185836806893,\n",
       "   0.3261771781835705,\n",
       "   0.32698455126956105,\n",
       "   0.33218216849491,\n",
       "   0.34213650319725275,\n",
       "   0.34450321551412344,\n",
       "   0.3482237267307937,\n",
       "   0.3536034538410604,\n",
       "   0.3445041722152382,\n",
       "   0.3398352558724582,\n",
       "   0.3459239029325545,\n",
       "   0.34157576668076217,\n",
       "   0.34054491855204105,\n",
       "   0.34329478023573756,\n",
       "   0.3516207642387599,\n",
       "   0.3570464258082211,\n",
       "   0.35640887334011495,\n",
       "   0.35689573315903544,\n",
       "   0.3678770875558257,\n",
       "   0.3650373991113156,\n",
       "   0.3626662651076913,\n",
       "   0.3710589352995157,\n",
       "   0.3691234460566193,\n",
       "   0.3741519832983613,\n",
       "   0.38146896893158555,\n",
       "   0.37913761124946177,\n",
       "   0.37693347153253853,\n",
       "   0.38370408676564693,\n",
       "   0.3851613060105592,\n",
       "   0.3850608617067337,\n",
       "   0.3877842777874321,\n",
       "   0.39080964028835297,\n",
       "   0.39157038158737123,\n",
       "   0.401514871744439,\n",
       "   0.4044601821806282,\n",
       "   0.40082391491159797,\n",
       "   0.39715887350030243,\n",
       "   0.40288154780864716,\n",
       "   0.4022882452700287,\n",
       "   0.4038273231126368,\n",
       "   0.4070297749713063,\n",
       "   0.40513041126541793,\n",
       "   0.4069180313963443,\n",
       "   0.41311000008136034,\n",
       "   0.4024088052101433,\n",
       "   0.39762099529616535,\n",
       "   0.38611504319123924,\n",
       "   0.37970132986083627,\n",
       "   0.36749888164922595,\n",
       "   0.37450336921028793,\n",
       "   0.38044447149150074,\n",
       "   0.37750718113966286,\n",
       "   0.3725326976273209,\n",
       "   0.37176408478990197,\n",
       "   0.3679941291920841,\n",
       "   0.36755243921652436,\n",
       "   0.38048154953867197,\n",
       "   0.37987424759194255,\n",
       "   0.37606303161010146,\n",
       "   0.3739848795812577,\n",
       "   0.3748592557385564,\n",
       "   0.3714002270717174,\n",
       "   0.36620175186544657,\n",
       "   0.36158184753730893,\n",
       "   0.36510549462400377,\n",
       "   0.3648269714321941,\n",
       "   0.36685160431079566,\n",
       "   0.37100836518220603,\n",
       "   0.3684582021087408,\n",
       "   0.36930344835855067,\n",
       "   0.3735963951330632,\n",
       "   0.36295021465048194,\n",
       "   0.3694158170837909,\n",
       "   0.3650207915343344,\n",
       "   0.3652184698730707,\n",
       "   0.35600535571575165,\n",
       "   0.3624313280452043,\n",
       "   0.3652163508813828,\n",
       "   0.3624025152530521,\n",
       "   0.35916070477105677,\n",
       "   0.3589418618939817,\n",
       "   0.3589315568096936,\n",
       "   0.3480403970461339,\n",
       "   0.34372461773455143,\n",
       "   0.346128526609391,\n",
       "   0.343111147871241,\n",
       "   0.3404722595587373,\n",
       "   0.34701960696838796,\n",
       "   0.34036369854584336,\n",
       "   0.3530729943886399,\n",
       "   0.344985673436895,\n",
       "   0.3466231699567288,\n",
       "   0.35094128642231226,\n",
       "   0.3506149265449494,\n",
       "   0.34978949557989836,\n",
       "   0.34973297268152237,\n",
       "   0.34384646569378674,\n",
       "   0.34439351479522884,\n",
       "   0.33710224460810423,\n",
       "   0.3375521218404174,\n",
       "   0.32818035990931094,\n",
       "   0.3163050804287195,\n",
       "   0.3109216245356947,\n",
       "   0.3045814004726708,\n",
       "   0.29836049186997116,\n",
       "   0.2997259139083326,\n",
       "   0.29519977304153144,\n",
       "   0.29331286414526403,\n",
       "   0.2945509389974177,\n",
       "   0.29126753308810294,\n",
       "   0.2893170416355133,\n",
       "   0.2885961311403662,\n",
       "   0.290442744968459,\n",
       "   0.29456148692406714,\n",
       "   0.29504706407897174,\n",
       "   0.29383461992256343,\n",
       "   0.28929023002274334,\n",
       "   0.2882847455330193,\n",
       "   0.28626562585122883,\n",
       "   0.2911304486915469,\n",
       "   0.2820808235555887,\n",
       "   0.2792084056418389,\n",
       "   0.26835615816526115,\n",
       "   0.27303185733035207,\n",
       "   0.26533258869312704,\n",
       "   0.2658268450759351,\n",
       "   0.2669579260982573,\n",
       "   0.2703557750210166,\n",
       "   0.2658910609316081,\n",
       "   0.2674931336659938,\n",
       "   0.27516087470576167,\n",
       "   0.2724403862375766,\n",
       "   0.26680206251330674,\n",
       "   0.26601680484600365,\n",
       "   0.2721697490196675,\n",
       "   0.266782364808023,\n",
       "   0.26875926391221583,\n",
       "   0.26962724467739463,\n",
       "   0.27490732446312904,\n",
       "   0.2717388113960624,\n",
       "   0.27166427834890783,\n",
       "   0.27649841574020684,\n",
       "   0.27549170632846653,\n",
       "   0.2789991241879761,\n",
       "   0.27886694110929966,\n",
       "   0.2746544359251857,\n",
       "   0.2740103988908231,\n",
       "   0.2760535429697484,\n",
       "   0.2702710619196296,\n",
       "   0.26572788995690644,\n",
       "   0.2718180378433317,\n",
       "   0.2739990677218884,\n",
       "   0.2679449936840683,\n",
       "   0.2710218904539943,\n",
       "   0.2778992943931371,\n",
       "   0.2814931985922158,\n",
       "   0.27633024705573916,\n",
       "   0.2710947438608855,\n",
       "   0.27034976799041033,\n",
       "   0.2750609964132309,\n",
       "   0.28320673597045243,\n",
       "   0.2874399423599243,\n",
       "   0.29179937159642577,\n",
       "   0.29208482801914215,\n",
       "   0.2908560181967914,\n",
       "   0.2893991316668689,\n",
       "   0.28456166782416403,\n",
       "   0.2815279751084745,\n",
       "   0.27446453156881034,\n",
       "   0.27839579270221293,\n",
       "   0.2783553833141923,\n",
       "   0.27576155425049365,\n",
       "   0.2777621760033071,\n",
       "   0.27634994732216,\n",
       "   0.27369596250355244,\n",
       "   0.27237658435478806,\n",
       "   0.26800590055063367,\n",
       "   0.26295510632917285,\n",
       "   0.2568568028509617,\n",
       "   0.26251575304195285,\n",
       "   0.25789382332004607,\n",
       "   0.2586245376151055,\n",
       "   0.2609551267232746,\n",
       "   0.26044313539750874,\n",
       "   0.2594361645169556,\n",
       "   0.2584412961732596,\n",
       "   0.2540813167579472,\n",
       "   0.2484369846060872,\n",
       "   0.24756311369128525,\n",
       "   0.24860662827268243,\n",
       "   0.24561384064145386,\n",
       "   0.24239055160433054,\n",
       "   0.2436849419027567,\n",
       "   0.24397197784855962,\n",
       "   0.24741690419614315,\n",
       "   0.25186610384844244,\n",
       "   0.24983852938748896,\n",
       "   0.2534511925186962,\n",
       "   0.2554859225638211,\n",
       "   0.2568439901806414,\n",
       "   0.25297227082774043,\n",
       "   0.24925713893026114,\n",
       "   0.2543060309253633,\n",
       "   0.2623192332684994,\n",
       "   0.26272246707230806,\n",
       "   0.2580559605266899,\n",
       "   0.25992803857661784,\n",
       "   0.26256440579891205,\n",
       "   0.26105373655445874,\n",
       "   0.2600482215639204,\n",
       "   0.2602360483724624,\n",
       "   0.26012186147272587,\n",
       "   0.26257554232142866,\n",
       "   0.26681323209777474,\n",
       "   0.2636196562089026,\n",
       "   0.2593543357215822,\n",
       "   0.2576610913965851,\n",
       "   0.26335162087343633,\n",
       "   0.26615002006292343,\n",
       "   0.26440822845324874,\n",
       "   0.26453511719591916,\n",
       "   0.26445709518156946,\n",
       "   0.26709093200042844,\n",
       "   0.264709715731442,\n",
       "   0.2666767677292228,\n",
       "   0.2578575403895229,\n",
       "   0.2624787406530231,\n",
       "   0.2615498558152467,\n",
       "   0.26539823901839554,\n",
       "   0.26075318874791265,\n",
       "   0.25753256818279624,\n",
       "   0.25931612541899085,\n",
       "   0.2606678786687553,\n",
       "   0.2558463979512453,\n",
       "   0.2537358782719821,\n",
       "   0.2492270318325609,\n",
       "   0.2406754104886204,\n",
       "   0.23972193337976933,\n",
       "   0.24518391070887446,\n",
       "   0.24832660728134215,\n",
       "   0.25518022011965513,\n",
       "   0.2562468855176121,\n",
       "   0.25327196810394526,\n",
       "   0.2509592678397894,\n",
       "   0.24942898307926953,\n",
       "   0.2561040937434882,\n",
       "   0.25977616128511727,\n",
       "   0.2608458788599819,\n",
       "   0.26024854462593794,\n",
       "   0.26499129086732864,\n",
       "   0.2626237394288182,\n",
       "   0.2620082045905292,\n",
       "   0.25386041565798223,\n",
       "   0.2564637411851436,\n",
       "   0.25572680216282606,\n",
       "   0.25331694609485567,\n",
       "   0.25210359669290483,\n",
       "   0.25277495943009853,\n",
       "   0.25784774660132825,\n",
       "   0.2595931994728744,\n",
       "   0.2642385424114764,\n",
       "   0.2642571865580976,\n",
       "   0.2651605987921357,\n",
       "   0.2592087327502668,\n",
       "   0.26009739260189235,\n",
       "   0.25925532076507807,\n",
       "   0.2636153632774949,\n",
       "   0.2657961861696094,\n",
       "   0.26211687130853534,\n",
       "   0.25054583814926445,\n",
       "   0.24533528159372509,\n",
       "   0.23547581443563104,\n",
       "   0.228840671479702,\n",
       "   0.2236390169709921,\n",
       "   0.215799669502303,\n",
       "   0.21351283020339906,\n",
       "   0.2156363888643682,\n",
       "   0.21307798591442406,\n",
       "   0.21534582087770104,\n",
       "   0.21955317468382418,\n",
       "   0.22021964890882373,\n",
       "   0.22079549287445843,\n",
       "   0.2266610988881439,\n",
       "   0.22157825389876962,\n",
       "   0.22231708792969584,\n",
       "   0.21885834448039532,\n",
       "   0.21930882637389004,\n",
       "   0.22126247943378985,\n",
       "   0.21893612225539982,\n",
       "   0.21752634132280946,\n",
       "   0.21999236824922264,\n",
       "   0.22352831531316042,\n",
       "   0.2219740031287074,\n",
       "   0.21899436344392598,\n",
       "   0.22173879761248827,\n",
       "   0.22599941375665367,\n",
       "   0.22743293503299356,\n",
       "   0.22556646191515028,\n",
       "   0.22295260219834745,\n",
       "   0.22079897113144398,\n",
       "   0.22385325469076633,\n",
       "   0.22638796595856547,\n",
       "   0.22838590224273503,\n",
       "   0.22604947863146663,\n",
       "   0.22704000491648912,\n",
       "   0.23068536748178303,\n",
       "   0.23211187706328928,\n",
       "   0.23098990810103714,\n",
       "   0.22557137184776366,\n",
       "   0.22466059518046677,\n",
       "   0.22463590139523149,\n",
       "   0.22629473824054003,\n",
       "   0.22308173310011625,\n",
       "   0.22332144947722554,\n",
       "   0.22119940514676273,\n",
       "   0.22366173565387726,\n",
       "   0.22281067701987922,\n",
       "   0.22604804229922593,\n",
       "   0.2214514564257115,\n",
       "   0.21975522441789508,\n",
       "   0.21633419068530202,\n",
       "   0.22044957149773836,\n",
       "   0.2252727597951889,\n",
       "   0.227925063110888,\n",
       "   0.2273104372434318,\n",
       "   0.22643381101079285,\n",
       "   0.2284767956007272,\n",
       "   0.22743293805979192,\n",
       "   0.2224177394527942,\n",
       "   0.22234387858770788,\n",
       "   0.21777508337982,\n",
       "   0.2177707168739289,\n",
       "   0.22179865278303623,\n",
       "   0.21451091230846941,\n",
       "   0.21650179126299918,\n",
       "   0.21840532077476382,\n",
       "   0.2169399883132428,\n",
       "   0.22043917188420892,\n",
       "   0.2286575878970325,\n",
       "   0.23047596006654203,\n",
       "   0.22555745602585375,\n",
       "   0.2288250250276178,\n",
       "   0.22651141637470573,\n",
       "   0.2238332318374887,\n",
       "   0.21752332034520805,\n",
       "   0.21811256906948984,\n",
       "   0.21627850562799722,\n",
       "   0.2126570203108713,\n",
       "   0.20989483094308525,\n",
       "   0.2115897391922772,\n",
       "   0.21347837580833584,\n",
       "   0.21613037423230708],\n",
       "  'pce_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.593017578125,\n",
       "   98.80357360839844,\n",
       "   98.04512786865234,\n",
       "   96.63422393798828,\n",
       "   95.47875213623047,\n",
       "   93.66071319580078,\n",
       "   91.04043579101562,\n",
       "   86.67298126220703,\n",
       "   82.05436706542969,\n",
       "   77.8738784790039,\n",
       "   73.97730255126953,\n",
       "   65.13360595703125,\n",
       "   60.76347732543945,\n",
       "   58.36055374145508,\n",
       "   55.59914779663086,\n",
       "   51.67924880981445,\n",
       "   49.99936294555664,\n",
       "   47.57164001464844,\n",
       "   44.9553337097168,\n",
       "   42.3228759765625,\n",
       "   40.68994140625,\n",
       "   42.17164993286133,\n",
       "   41.47787857055664,\n",
       "   41.0718994140625,\n",
       "   43.005672454833984,\n",
       "   43.04051208496094,\n",
       "   41.174171447753906,\n",
       "   40.33378601074219,\n",
       "   39.18430709838867,\n",
       "   41.38411331176758,\n",
       "   41.2113037109375,\n",
       "   41.03074645996094,\n",
       "   39.606048583984375,\n",
       "   38.42104721069336,\n",
       "   40.846622467041016,\n",
       "   43.39634323120117,\n",
       "   45.63332748413086,\n",
       "   46.38202667236328,\n",
       "   47.614566802978516,\n",
       "   51.131927490234375,\n",
       "   51.18329620361328,\n",
       "   50.254608154296875,\n",
       "   51.641441345214844,\n",
       "   54.719688415527344,\n",
       "   57.87140655517578,\n",
       "   58.5751838684082,\n",
       "   60.67554473876953,\n",
       "   60.51813888549805,\n",
       "   60.84745407104492,\n",
       "   60.23851013183594,\n",
       "   61.815547943115234,\n",
       "   63.23725509643555,\n",
       "   64.38526153564453,\n",
       "   63.962310791015625,\n",
       "   63.004432678222656,\n",
       "   61.80577850341797,\n",
       "   64.2953872680664,\n",
       "   63.5999870300293,\n",
       "   62.21993637084961,\n",
       "   63.436073303222656,\n",
       "   63.37478256225586,\n",
       "   61.54741668701172,\n",
       "   60.55465316772461,\n",
       "   58.60254669189453,\n",
       "   59.48657989501953,\n",
       "   57.976158142089844,\n",
       "   56.00808334350586,\n",
       "   55.76586151123047,\n",
       "   57.6241569519043,\n",
       "   59.349910736083984,\n",
       "   61.293540954589844,\n",
       "   60.57029342651367,\n",
       "   60.075157165527344,\n",
       "   60.12748336791992,\n",
       "   58.89006423950195,\n",
       "   55.992828369140625,\n",
       "   53.669822692871094,\n",
       "   54.15654754638672,\n",
       "   54.67410659790039,\n",
       "   53.37603759765625,\n",
       "   53.02895736694336,\n",
       "   54.95492172241211,\n",
       "   53.7708740234375,\n",
       "   55.36239242553711,\n",
       "   52.96999740600586,\n",
       "   54.14948272705078,\n",
       "   53.98243713378906,\n",
       "   52.50421905517578,\n",
       "   49.70573043823242,\n",
       "   46.52045822143555,\n",
       "   46.0628776550293,\n",
       "   46.8388557434082,\n",
       "   45.00222396850586,\n",
       "   42.91063690185547,\n",
       "   43.27516174316406,\n",
       "   46.290977478027344,\n",
       "   49.22450637817383,\n",
       "   49.40953826904297,\n",
       "   52.14385986328125,\n",
       "   54.04958724975586,\n",
       "   54.12794876098633,\n",
       "   54.16387939453125,\n",
       "   54.70452880859375,\n",
       "   52.682579040527344,\n",
       "   53.4222297668457,\n",
       "   55.16367721557617,\n",
       "   53.58473587036133,\n",
       "   54.6912841796875,\n",
       "   54.124977111816406,\n",
       "   54.61698913574219,\n",
       "   53.948341369628906,\n",
       "   55.597991943359375,\n",
       "   55.99189376831055,\n",
       "   55.479740142822266,\n",
       "   56.27850341796875,\n",
       "   54.747535705566406,\n",
       "   55.110530853271484,\n",
       "   54.49726104736328,\n",
       "   53.03515625,\n",
       "   52.55643081665039,\n",
       "   51.19166564941406,\n",
       "   52.474857330322266,\n",
       "   51.924156188964844,\n",
       "   50.771278381347656,\n",
       "   50.619388580322266,\n",
       "   50.29880905151367,\n",
       "   49.7965202331543,\n",
       "   46.928504943847656,\n",
       "   48.621334075927734,\n",
       "   49.7347526550293,\n",
       "   49.066383361816406,\n",
       "   48.912784576416016,\n",
       "   47.48601150512695,\n",
       "   48.60540008544922,\n",
       "   47.097007751464844,\n",
       "   44.25255584716797,\n",
       "   45.49492645263672,\n",
       "   45.41978073120117,\n",
       "   45.310142517089844,\n",
       "   46.03028869628906,\n",
       "   46.804012298583984,\n",
       "   48.55858612060547,\n",
       "   48.92781448364258,\n",
       "   49.90728759765625,\n",
       "   50.511985778808594,\n",
       "   49.96845626831055,\n",
       "   48.823150634765625,\n",
       "   48.51397705078125,\n",
       "   48.64775848388672,\n",
       "   47.00233459472656,\n",
       "   45.89714050292969,\n",
       "   47.52013397216797,\n",
       "   46.09324264526367,\n",
       "   46.84561538696289,\n",
       "   47.12160873413086,\n",
       "   48.89738082885742,\n",
       "   49.27878189086914,\n",
       "   49.46318817138672,\n",
       "   50.818084716796875,\n",
       "   51.6702995300293,\n",
       "   50.646812438964844,\n",
       "   50.9383544921875,\n",
       "   53.6968879699707,\n",
       "   53.472476959228516,\n",
       "   55.632232666015625,\n",
       "   58.045650482177734,\n",
       "   58.22256851196289,\n",
       "   57.55338668823242,\n",
       "   59.952205657958984,\n",
       "   60.492557525634766,\n",
       "   61.80665969848633,\n",
       "   63.2453727722168,\n",
       "   62.490379333496094,\n",
       "   63.083065032958984,\n",
       "   64.98157501220703,\n",
       "   65.60095977783203,\n",
       "   65.38626861572266,\n",
       "   65.78199005126953,\n",
       "   67.01707458496094,\n",
       "   66.69517517089844,\n",
       "   67.45169067382812,\n",
       "   66.74675750732422,\n",
       "   67.23793029785156,\n",
       "   66.00294494628906,\n",
       "   65.93643951416016,\n",
       "   64.32605743408203,\n",
       "   63.6115608215332,\n",
       "   60.8472900390625,\n",
       "   60.27645492553711,\n",
       "   58.595245361328125,\n",
       "   61.4727668762207,\n",
       "   61.88885498046875,\n",
       "   60.26032638549805,\n",
       "   60.655975341796875,\n",
       "   61.676998138427734,\n",
       "   60.646705627441406,\n",
       "   61.10851287841797,\n",
       "   63.34292221069336,\n",
       "   64.24986267089844,\n",
       "   63.428871154785156,\n",
       "   63.348453521728516,\n",
       "   64.09860229492188,\n",
       "   64.6417236328125,\n",
       "   64.47897338867188,\n",
       "   64.06025695800781,\n",
       "   64.8399658203125,\n",
       "   63.89573287963867,\n",
       "   65.070068359375,\n",
       "   65.32592010498047,\n",
       "   65.07888793945312,\n",
       "   66.06332397460938,\n",
       "   66.97649383544922,\n",
       "   65.44149017333984,\n",
       "   67.23792266845703,\n",
       "   66.58792114257812,\n",
       "   67.125244140625,\n",
       "   66.10453796386719,\n",
       "   67.53113555908203,\n",
       "   68.13146209716797,\n",
       "   68.456787109375,\n",
       "   67.79011535644531,\n",
       "   68.050537109375,\n",
       "   68.48882293701172,\n",
       "   66.97151947021484,\n",
       "   66.3115463256836,\n",
       "   66.28707885742188,\n",
       "   65.76956939697266,\n",
       "   64.6964340209961,\n",
       "   65.64920043945312,\n",
       "   65.1131820678711,\n",
       "   67.435546875,\n",
       "   65.46819305419922,\n",
       "   65.81595611572266,\n",
       "   67.2640151977539,\n",
       "   67.880615234375,\n",
       "   66.49396514892578,\n",
       "   65.9040298461914,\n",
       "   65.29519653320312,\n",
       "   63.99364471435547,\n",
       "   62.083412170410156,\n",
       "   61.60383224487305,\n",
       "   59.4057731628418,\n",
       "   57.56353759765625,\n",
       "   58.36197280883789,\n",
       "   56.335906982421875,\n",
       "   55.50175476074219,\n",
       "   57.23105239868164,\n",
       "   56.43379592895508,\n",
       "   55.28202819824219,\n",
       "   56.270599365234375,\n",
       "   54.839237213134766,\n",
       "   54.115116119384766,\n",
       "   54.051918029785156,\n",
       "   54.410667419433594,\n",
       "   54.879005432128906,\n",
       "   55.69029998779297,\n",
       "   55.486480712890625,\n",
       "   54.919795989990234,\n",
       "   54.73185729980469,\n",
       "   53.52260971069336,\n",
       "   54.8396110534668,\n",
       "   53.32952880859375,\n",
       "   52.24199295043945,\n",
       "   50.74509811401367,\n",
       "   51.37238311767578,\n",
       "   50.29243087768555,\n",
       "   50.96996307373047,\n",
       "   51.601810455322266,\n",
       "   51.71979522705078,\n",
       "   50.416725158691406,\n",
       "   50.68772506713867,\n",
       "   52.43415832519531,\n",
       "   51.21857452392578,\n",
       "   49.4793815612793,\n",
       "   49.15673065185547,\n",
       "   50.340248107910156,\n",
       "   48.97639083862305,\n",
       "   48.14361572265625,\n",
       "   48.97203826904297,\n",
       "   50.45269012451172,\n",
       "   50.563621520996094,\n",
       "   50.27495193481445,\n",
       "   50.664329528808594,\n",
       "   50.69123077392578,\n",
       "   52.10588455200195,\n",
       "   52.5097770690918,\n",
       "   52.708839416503906,\n",
       "   51.88926315307617,\n",
       "   51.35487365722656,\n",
       "   50.07872772216797,\n",
       "   48.98811721801758,\n",
       "   49.54740905761719,\n",
       "   50.71405029296875,\n",
       "   50.48228454589844,\n",
       "   50.534934997558594,\n",
       "   52.11781692504883,\n",
       "   52.904884338378906,\n",
       "   53.32489013671875,\n",
       "   53.2369499206543,\n",
       "   53.03030014038086,\n",
       "   55.025291442871094,\n",
       "   57.07928466796875,\n",
       "   57.871212005615234,\n",
       "   59.01616287231445,\n",
       "   58.9826774597168,\n",
       "   58.73889923095703,\n",
       "   58.27130126953125,\n",
       "   56.769012451171875,\n",
       "   56.53953552246094,\n",
       "   55.44751739501953,\n",
       "   55.257755279541016,\n",
       "   55.4727897644043,\n",
       "   54.66082763671875,\n",
       "   55.07099151611328,\n",
       "   54.84762954711914,\n",
       "   54.41083908081055,\n",
       "   53.15666961669922,\n",
       "   51.79517364501953,\n",
       "   50.83435821533203,\n",
       "   50.553916931152344,\n",
       "   52.24369812011719,\n",
       "   50.81024169921875,\n",
       "   51.22459411621094,\n",
       "   50.90380859375,\n",
       "   49.46753692626953,\n",
       "   49.51789093017578,\n",
       "   50.318443298339844,\n",
       "   49.92092514038086,\n",
       "   49.106903076171875,\n",
       "   48.02874755859375,\n",
       "   47.717952728271484,\n",
       "   47.71140670776367,\n",
       "   46.466190338134766,\n",
       "   45.49770736694336,\n",
       "   45.19613265991211,\n",
       "   45.87710189819336,\n",
       "   46.62974166870117,\n",
       "   47.10136795043945,\n",
       "   47.57839584350586,\n",
       "   48.3577880859375,\n",
       "   48.89642333984375,\n",
       "   48.12946701049805,\n",
       "   46.928531646728516,\n",
       "   47.06632614135742,\n",
       "   49.17031478881836,\n",
       "   49.34678649902344,\n",
       "   48.795387268066406,\n",
       "   48.9886474609375,\n",
       "   50.333763122558594,\n",
       "   49.65530776977539,\n",
       "   49.85310745239258,\n",
       "   49.29594802856445,\n",
       "   49.359310150146484,\n",
       "   49.55718231201172,\n",
       "   50.27484893798828,\n",
       "   49.40275573730469,\n",
       "   48.41280746459961,\n",
       "   48.341796875,\n",
       "   49.83127975463867,\n",
       "   49.89695739746094,\n",
       "   50.040863037109375,\n",
       "   49.7580451965332,\n",
       "   49.623046875,\n",
       "   49.58611297607422,\n",
       "   49.43414306640625,\n",
       "   50.34418869018555,\n",
       "   48.10760498046875,\n",
       "   48.637577056884766,\n",
       "   48.68775939941406,\n",
       "   49.33110046386719,\n",
       "   48.26730728149414,\n",
       "   47.85091018676758,\n",
       "   48.10356903076172,\n",
       "   49.82914352416992,\n",
       "   48.562862396240234,\n",
       "   47.63143539428711,\n",
       "   46.674896240234375,\n",
       "   45.75218200683594,\n",
       "   45.87656784057617,\n",
       "   47.518863677978516,\n",
       "   48.7659912109375,\n",
       "   50.83262252807617,\n",
       "   50.610206604003906,\n",
       "   50.04061508178711,\n",
       "   50.16184616088867,\n",
       "   49.99419403076172,\n",
       "   51.70167541503906,\n",
       "   53.02125930786133,\n",
       "   53.045166015625,\n",
       "   52.61830520629883,\n",
       "   53.810691833496094,\n",
       "   53.19438552856445,\n",
       "   52.22626495361328,\n",
       "   49.97561264038086,\n",
       "   49.866458892822266,\n",
       "   50.69635009765625,\n",
       "   50.9373893737793,\n",
       "   50.47100830078125,\n",
       "   49.63311004638672,\n",
       "   50.52602767944336,\n",
       "   50.555564880371094,\n",
       "   51.783782958984375,\n",
       "   51.50023651123047,\n",
       "   51.75067901611328,\n",
       "   51.29616928100586,\n",
       "   50.83125305175781,\n",
       "   51.34797668457031,\n",
       "   51.244197845458984,\n",
       "   51.762847900390625,\n",
       "   50.6993408203125,\n",
       "   48.729225158691406,\n",
       "   46.96272277832031,\n",
       "   44.78501510620117,\n",
       "   42.83516311645508,\n",
       "   41.838809967041016,\n",
       "   39.6318359375,\n",
       "   39.13274383544922,\n",
       "   39.18865203857422,\n",
       "   38.025760650634766,\n",
       "   38.990509033203125,\n",
       "   38.91347885131836,\n",
       "   39.36707305908203,\n",
       "   40.12789535522461,\n",
       "   40.803043365478516,\n",
       "   39.16612243652344,\n",
       "   38.512115478515625,\n",
       "   38.50273513793945,\n",
       "   39.70185089111328,\n",
       "   39.892189025878906,\n",
       "   39.8614387512207,\n",
       "   39.58436584472656,\n",
       "   40.33548355102539,\n",
       "   41.04334259033203,\n",
       "   40.81337356567383,\n",
       "   40.534523010253906,\n",
       "   41.21535873413086,\n",
       "   41.890560150146484,\n",
       "   42.4272346496582,\n",
       "   41.900203704833984,\n",
       "   41.368682861328125,\n",
       "   39.977970123291016,\n",
       "   41.467079162597656,\n",
       "   42.632415771484375,\n",
       "   44.1199836730957,\n",
       "   44.078277587890625,\n",
       "   43.84120559692383,\n",
       "   45.2899169921875,\n",
       "   45.223514556884766,\n",
       "   44.85347366333008,\n",
       "   44.4618034362793,\n",
       "   44.70159912109375,\n",
       "   44.12782287597656,\n",
       "   44.84254455566406,\n",
       "   44.22463607788086,\n",
       "   44.28249740600586,\n",
       "   43.55202102661133,\n",
       "   42.80424880981445,\n",
       "   42.28242874145508,\n",
       "   42.37199783325195,\n",
       "   41.06136703491211,\n",
       "   40.61560821533203,\n",
       "   40.297176361083984,\n",
       "   41.382938385009766,\n",
       "   41.52989196777344,\n",
       "   41.91543960571289,\n",
       "   41.73515319824219,\n",
       "   42.27953338623047,\n",
       "   43.12407302856445,\n",
       "   43.065364837646484,\n",
       "   41.136226654052734,\n",
       "   40.80792236328125,\n",
       "   39.68072509765625,\n",
       "   39.2716064453125,\n",
       "   41.07355880737305,\n",
       "   39.63468551635742,\n",
       "   39.7375602722168,\n",
       "   40.12960433959961,\n",
       "   39.07960510253906,\n",
       "   39.740997314453125,\n",
       "   40.67416763305664,\n",
       "   40.89738845825195,\n",
       "   38.598960876464844,\n",
       "   39.694366455078125,\n",
       "   38.64750671386719,\n",
       "   38.47045135498047,\n",
       "   36.91191101074219,\n",
       "   37.663978576660156,\n",
       "   36.97233200073242,\n",
       "   36.52779006958008,\n",
       "   36.82646179199219,\n",
       "   37.185157775878906,\n",
       "   37.84184265136719,\n",
       "   39.05561447143555],\n",
       "  'voc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.10810852050781,\n",
       "   95.22362518310547,\n",
       "   81.32511138916016,\n",
       "   61.42805862426758,\n",
       "   43.443809509277344,\n",
       "   29.833295822143555,\n",
       "   19.211816787719727,\n",
       "   14.962586402893066,\n",
       "   14.258062362670898,\n",
       "   15.484380722045898,\n",
       "   18.092464447021484,\n",
       "   22.572877883911133,\n",
       "   22.714529037475586,\n",
       "   23.353717803955078,\n",
       "   25.26613998413086,\n",
       "   26.74277687072754,\n",
       "   25.836933135986328,\n",
       "   27.042085647583008,\n",
       "   26.387939453125,\n",
       "   23.34821891784668,\n",
       "   20.62447738647461,\n",
       "   17.740510940551758,\n",
       "   15.086639404296875,\n",
       "   13.434404373168945,\n",
       "   11.513252258300781,\n",
       "   9.49150562286377,\n",
       "   7.062843322753906,\n",
       "   6.871313571929932,\n",
       "   6.571547508239746,\n",
       "   6.140998363494873,\n",
       "   5.548038482666016,\n",
       "   5.463534832000732,\n",
       "   5.021236419677734,\n",
       "   4.992785453796387,\n",
       "   5.088417053222656,\n",
       "   5.456381797790527,\n",
       "   5.957935333251953,\n",
       "   6.362461090087891,\n",
       "   6.9024434089660645,\n",
       "   7.359724521636963,\n",
       "   7.238026142120361,\n",
       "   6.999794960021973,\n",
       "   6.772452354431152,\n",
       "   6.414223670959473,\n",
       "   6.60486364364624,\n",
       "   6.309930801391602,\n",
       "   6.643216133117676,\n",
       "   6.743905544281006,\n",
       "   6.696926116943359,\n",
       "   6.1360859870910645,\n",
       "   5.957366943359375,\n",
       "   5.668439865112305,\n",
       "   5.648148059844971,\n",
       "   5.50006628036499,\n",
       "   5.4111809730529785,\n",
       "   5.710599422454834,\n",
       "   5.757628917694092,\n",
       "   5.621723651885986,\n",
       "   5.207146167755127,\n",
       "   5.3386311531066895,\n",
       "   5.165896415710449,\n",
       "   5.285913467407227,\n",
       "   5.133314609527588,\n",
       "   5.141606330871582,\n",
       "   5.046926021575928,\n",
       "   5.109879493713379,\n",
       "   5.25911808013916,\n",
       "   5.465014457702637,\n",
       "   5.814783096313477,\n",
       "   6.0042805671691895,\n",
       "   5.8988118171691895,\n",
       "   5.645599365234375,\n",
       "   5.74378776550293,\n",
       "   5.666783809661865,\n",
       "   6.082457542419434,\n",
       "   5.928184986114502,\n",
       "   6.004772186279297,\n",
       "   5.928916931152344,\n",
       "   6.250359058380127,\n",
       "   6.463336944580078,\n",
       "   6.429443359375,\n",
       "   6.54774284362793,\n",
       "   6.739950180053711,\n",
       "   6.922729969024658,\n",
       "   6.992962837219238,\n",
       "   6.643479347229004,\n",
       "   6.803363800048828,\n",
       "   6.886044979095459,\n",
       "   6.722312927246094,\n",
       "   6.552661418914795,\n",
       "   6.729937553405762,\n",
       "   6.681100845336914,\n",
       "   6.556455135345459,\n",
       "   6.295512676239014,\n",
       "   5.964085102081299,\n",
       "   5.796631813049316,\n",
       "   5.685065746307373,\n",
       "   5.520218849182129,\n",
       "   5.454285144805908,\n",
       "   5.469971656799316,\n",
       "   5.526272773742676,\n",
       "   5.495024681091309,\n",
       "   5.481142044067383,\n",
       "   5.482917308807373,\n",
       "   5.535180568695068,\n",
       "   5.393547058105469,\n",
       "   5.336018085479736,\n",
       "   5.24510383605957,\n",
       "   5.229123592376709,\n",
       "   5.112959861755371,\n",
       "   5.108956813812256,\n",
       "   5.268588542938232,\n",
       "   5.354217052459717,\n",
       "   5.291571140289307,\n",
       "   5.147136211395264,\n",
       "   5.127538204193115,\n",
       "   5.135881423950195,\n",
       "   4.944460868835449,\n",
       "   4.83804178237915,\n",
       "   4.822089195251465,\n",
       "   4.7814040184021,\n",
       "   4.805774688720703,\n",
       "   4.799842834472656,\n",
       "   4.999750137329102,\n",
       "   5.410690784454346,\n",
       "   5.146890640258789,\n",
       "   4.8645853996276855,\n",
       "   5.0823974609375,\n",
       "   4.8475022315979,\n",
       "   4.859438419342041,\n",
       "   4.725356578826904,\n",
       "   4.743608474731445,\n",
       "   4.708400249481201,\n",
       "   4.721141338348389,\n",
       "   4.753009796142578,\n",
       "   4.663568496704102,\n",
       "   4.768433570861816,\n",
       "   4.8441691398620605,\n",
       "   4.813908576965332,\n",
       "   4.7156291007995605,\n",
       "   4.687937259674072,\n",
       "   4.548623561859131,\n",
       "   4.500339508056641,\n",
       "   4.526695251464844,\n",
       "   4.456513404846191,\n",
       "   4.6774373054504395,\n",
       "   4.495483875274658,\n",
       "   4.570225238800049,\n",
       "   4.569659233093262,\n",
       "   4.717175006866455,\n",
       "   4.889928817749023,\n",
       "   4.91403341293335,\n",
       "   5.082263469696045,\n",
       "   5.016746997833252,\n",
       "   4.959229469299316,\n",
       "   5.080540180206299,\n",
       "   4.903804302215576,\n",
       "   4.946922302246094,\n",
       "   4.982093334197998,\n",
       "   4.98874568939209,\n",
       "   5.0095415115356445,\n",
       "   4.935860633850098,\n",
       "   5.0326080322265625,\n",
       "   5.06057596206665,\n",
       "   5.0498223304748535,\n",
       "   5.045891761779785,\n",
       "   5.007772445678711,\n",
       "   4.924449443817139,\n",
       "   4.934354305267334,\n",
       "   4.968512058258057,\n",
       "   4.927880764007568,\n",
       "   4.884965896606445,\n",
       "   4.843600749969482,\n",
       "   4.915157318115234,\n",
       "   4.971065998077393,\n",
       "   5.058913230895996,\n",
       "   4.992949962615967,\n",
       "   5.16382360458374,\n",
       "   5.160967826843262,\n",
       "   5.345693588256836,\n",
       "   4.990161895751953,\n",
       "   4.986510276794434,\n",
       "   4.96791934967041,\n",
       "   5.099143981933594,\n",
       "   5.004602432250977,\n",
       "   4.8730244636535645,\n",
       "   4.911980628967285,\n",
       "   5.0072126388549805,\n",
       "   4.800834655761719,\n",
       "   4.746776580810547,\n",
       "   4.756947994232178,\n",
       "   4.750685214996338,\n",
       "   4.709136962890625,\n",
       "   4.612947463989258,\n",
       "   4.6527581214904785,\n",
       "   4.765842437744141,\n",
       "   4.860536575317383,\n",
       "   4.816682815551758,\n",
       "   4.813363552093506,\n",
       "   4.752956867218018,\n",
       "   5.145503520965576,\n",
       "   5.523068428039551,\n",
       "   5.800198554992676,\n",
       "   5.609177112579346,\n",
       "   5.361005783081055,\n",
       "   5.338916778564453,\n",
       "   5.357780456542969,\n",
       "   5.457670211791992,\n",
       "   5.776333332061768,\n",
       "   5.998913764953613,\n",
       "   5.473182201385498,\n",
       "   5.044792175292969,\n",
       "   5.1598896980285645,\n",
       "   5.1519012451171875,\n",
       "   5.162464618682861,\n",
       "   5.276578903198242,\n",
       "   5.234945774078369,\n",
       "   5.585437774658203,\n",
       "   5.450900077819824,\n",
       "   5.281499862670898,\n",
       "   5.4325270652771,\n",
       "   5.472109317779541,\n",
       "   5.339034557342529,\n",
       "   5.548069000244141,\n",
       "   5.579561710357666,\n",
       "   5.604903221130371,\n",
       "   5.72491455078125,\n",
       "   5.749483108520508,\n",
       "   5.833040714263916,\n",
       "   5.477274417877197,\n",
       "   5.467652797698975,\n",
       "   5.542694568634033,\n",
       "   5.26121187210083,\n",
       "   5.22723913192749,\n",
       "   5.224754810333252,\n",
       "   5.480727195739746,\n",
       "   5.464837551116943,\n",
       "   5.677484512329102,\n",
       "   5.4455742835998535,\n",
       "   5.341755390167236,\n",
       "   5.594543933868408,\n",
       "   5.583980083465576,\n",
       "   5.555047035217285,\n",
       "   5.539162635803223,\n",
       "   5.478935241699219,\n",
       "   5.451492786407471,\n",
       "   5.543728351593018,\n",
       "   5.531601428985596,\n",
       "   5.449708461761475,\n",
       "   5.629497051239014,\n",
       "   5.823527812957764,\n",
       "   5.625324249267578,\n",
       "   5.571223258972168,\n",
       "   5.486293315887451,\n",
       "   5.140566349029541,\n",
       "   5.282759189605713,\n",
       "   5.314116954803467,\n",
       "   5.335125923156738,\n",
       "   5.53088903427124,\n",
       "   5.635890007019043,\n",
       "   5.460353851318359,\n",
       "   5.5042338371276855,\n",
       "   5.736794948577881,\n",
       "   5.769455432891846,\n",
       "   5.497429370880127,\n",
       "   5.4315948486328125,\n",
       "   5.506618022918701,\n",
       "   5.324697494506836,\n",
       "   5.589134216308594,\n",
       "   5.5144362449646,\n",
       "   5.252410888671875,\n",
       "   5.329976558685303,\n",
       "   5.484805107116699,\n",
       "   5.729251861572266,\n",
       "   5.815251350402832,\n",
       "   5.68912410736084,\n",
       "   5.826185703277588,\n",
       "   5.219424724578857,\n",
       "   5.175054550170898,\n",
       "   5.317506790161133,\n",
       "   5.503075122833252,\n",
       "   5.37993860244751,\n",
       "   5.491783142089844,\n",
       "   5.858072757720947,\n",
       "   5.7681050300598145,\n",
       "   5.544447422027588,\n",
       "   5.479538917541504,\n",
       "   5.308081150054932,\n",
       "   5.5899176597595215,\n",
       "   5.739997863769531,\n",
       "   5.558012008666992,\n",
       "   5.501489162445068,\n",
       "   5.389662265777588,\n",
       "   5.2025065422058105,\n",
       "   5.050432205200195,\n",
       "   4.851517677307129,\n",
       "   4.883408069610596,\n",
       "   4.817549705505371,\n",
       "   4.965793609619141,\n",
       "   4.959723949432373,\n",
       "   4.8510823249816895,\n",
       "   4.94041109085083,\n",
       "   4.901289939880371,\n",
       "   4.530864715576172,\n",
       "   4.486265659332275,\n",
       "   4.413630962371826,\n",
       "   4.4525580406188965,\n",
       "   4.438861846923828,\n",
       "   4.52254056930542,\n",
       "   4.5228681564331055,\n",
       "   4.567541599273682,\n",
       "   4.598325252532959,\n",
       "   4.671305179595947,\n",
       "   4.686030864715576,\n",
       "   4.673007488250732,\n",
       "   4.680586814880371,\n",
       "   4.789710998535156,\n",
       "   4.996465682983398,\n",
       "   5.056183815002441,\n",
       "   4.9916276931762695,\n",
       "   5.025532245635986,\n",
       "   4.954874515533447,\n",
       "   4.914180278778076,\n",
       "   4.914316654205322,\n",
       "   4.902719497680664,\n",
       "   4.95905876159668,\n",
       "   5.000382423400879,\n",
       "   5.044771194458008,\n",
       "   5.055813789367676,\n",
       "   5.044642448425293,\n",
       "   5.056140422821045,\n",
       "   5.150743007659912,\n",
       "   5.302420616149902,\n",
       "   5.241825580596924,\n",
       "   5.286771774291992,\n",
       "   5.320734024047852,\n",
       "   5.228837013244629,\n",
       "   5.329686164855957,\n",
       "   5.368253707885742,\n",
       "   5.055196762084961,\n",
       "   5.010464668273926,\n",
       "   4.997656345367432,\n",
       "   5.023204326629639,\n",
       "   4.972387790679932,\n",
       "   4.980445384979248,\n",
       "   5.0031609535217285,\n",
       "   5.038563251495361,\n",
       "   5.198531627655029,\n",
       "   5.081099987030029,\n",
       "   4.973191261291504,\n",
       "   4.946857452392578,\n",
       "   4.947193145751953,\n",
       "   4.962508678436279,\n",
       "   4.943576812744141,\n",
       "   4.948689937591553,\n",
       "   4.898036956787109,\n",
       "   4.8712029457092285,\n",
       "   4.874781608581543,\n",
       "   4.868020057678223,\n",
       "   4.819045066833496,\n",
       "   4.7656707763671875,\n",
       "   4.873046875,\n",
       "   4.759115219116211,\n",
       "   4.723513603210449,\n",
       "   4.745052814483643,\n",
       "   4.754557132720947,\n",
       "   4.810347557067871,\n",
       "   4.818716049194336,\n",
       "   4.766829490661621,\n",
       "   4.877173900604248,\n",
       "   5.101499080657959,\n",
       "   4.993535995483398,\n",
       "   5.108303070068359,\n",
       "   5.151706218719482,\n",
       "   5.02433967590332,\n",
       "   5.082778453826904,\n",
       "   5.046098709106445,\n",
       "   4.934180736541748,\n",
       "   4.968599796295166,\n",
       "   4.9905219078063965,\n",
       "   5.019938945770264,\n",
       "   4.7928080558776855,\n",
       "   4.691226959228516,\n",
       "   4.632604122161865,\n",
       "   4.709810256958008,\n",
       "   4.819469451904297,\n",
       "   5.162269592285156,\n",
       "   4.915863990783691,\n",
       "   4.86206579208374,\n",
       "   4.853253364562988,\n",
       "   4.7802581787109375,\n",
       "   4.830117702484131,\n",
       "   4.857295513153076,\n",
       "   4.761458873748779,\n",
       "   4.957160949707031,\n",
       "   5.266978740692139,\n",
       "   5.282008171081543,\n",
       "   5.191401958465576,\n",
       "   5.017241477966309,\n",
       "   4.945773601531982,\n",
       "   4.967690944671631,\n",
       "   4.79784631729126,\n",
       "   4.9977545738220215,\n",
       "   4.943404197692871,\n",
       "   4.999277114868164,\n",
       "   4.88276481628418,\n",
       "   4.896159648895264,\n",
       "   4.776266574859619,\n",
       "   4.583752155303955,\n",
       "   4.602362155914307,\n",
       "   4.881335258483887,\n",
       "   4.8259711265563965,\n",
       "   5.105131149291992,\n",
       "   5.326685905456543,\n",
       "   5.667866230010986,\n",
       "   5.585352897644043,\n",
       "   5.556180477142334,\n",
       "   5.367474555969238,\n",
       "   5.454254150390625,\n",
       "   5.4059648513793945,\n",
       "   5.560997009277344,\n",
       "   5.815958023071289,\n",
       "   5.541935920715332,\n",
       "   5.49758243560791,\n",
       "   5.814399719238281,\n",
       "   5.712155818939209,\n",
       "   5.287280082702637,\n",
       "   5.472914695739746,\n",
       "   5.793616771697998,\n",
       "   6.022011756896973,\n",
       "   6.238054275512695,\n",
       "   5.56412935256958,\n",
       "   5.606926918029785,\n",
       "   5.756096363067627,\n",
       "   5.801326274871826,\n",
       "   5.538825035095215,\n",
       "   5.389513969421387,\n",
       "   5.581786632537842,\n",
       "   5.657146453857422,\n",
       "   5.672547817230225,\n",
       "   5.614140033721924,\n",
       "   5.597677230834961,\n",
       "   5.5924553871154785,\n",
       "   5.570474147796631,\n",
       "   5.725954532623291,\n",
       "   5.749056339263916,\n",
       "   5.7896881103515625,\n",
       "   5.357265472412109,\n",
       "   5.434380054473877,\n",
       "   5.599615097045898,\n",
       "   5.800651550292969,\n",
       "   5.786818504333496,\n",
       "   5.875790119171143,\n",
       "   5.923820495605469,\n",
       "   5.966732501983643,\n",
       "   6.07939338684082,\n",
       "   6.02481746673584,\n",
       "   5.613426685333252,\n",
       "   5.593571186065674,\n",
       "   5.098732948303223,\n",
       "   5.240874767303467,\n",
       "   5.170574188232422,\n",
       "   5.740467548370361,\n",
       "   5.88829231262207,\n",
       "   5.825067520141602,\n",
       "   5.820196151733398,\n",
       "   5.815539360046387,\n",
       "   5.6709089279174805,\n",
       "   5.57262659072876,\n",
       "   5.5365824699401855,\n",
       "   5.522744178771973,\n",
       "   5.434872150421143,\n",
       "   5.134850025177002,\n",
       "   4.993379592895508,\n",
       "   4.811839580535889,\n",
       "   5.099175930023193,\n",
       "   4.931189060211182,\n",
       "   4.763050556182861,\n",
       "   4.59840726852417,\n",
       "   4.422830581665039,\n",
       "   4.271913528442383,\n",
       "   4.2375311851501465,\n",
       "   4.21270751953125,\n",
       "   4.187635898590088,\n",
       "   4.158349514007568,\n",
       "   4.116547107696533,\n",
       "   4.143028736114502,\n",
       "   4.113759994506836,\n",
       "   4.089664459228516,\n",
       "   4.086404323577881,\n",
       "   4.0793657302856445,\n",
       "   4.07050895690918,\n",
       "   4.02482795715332,\n",
       "   4.025006294250488,\n",
       "   4.0250420570373535,\n",
       "   4.0435471534729,\n",
       "   4.037726879119873],\n",
       "  'jsc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.9741439819336,\n",
       "   99.93478393554688,\n",
       "   99.8962631225586,\n",
       "   99.79946899414062,\n",
       "   99.58987426757812,\n",
       "   97.96209716796875,\n",
       "   97.1470947265625,\n",
       "   97.23344421386719,\n",
       "   96.3702392578125,\n",
       "   95.23208618164062,\n",
       "   94.21966552734375,\n",
       "   93.6417007446289,\n",
       "   92.02835083007812,\n",
       "   91.09517669677734,\n",
       "   89.8126220703125,\n",
       "   90.05935668945312,\n",
       "   89.72344207763672,\n",
       "   88.28407287597656,\n",
       "   87.95135498046875,\n",
       "   88.4302978515625,\n",
       "   88.26992797851562,\n",
       "   88.87136840820312,\n",
       "   89.0590591430664,\n",
       "   87.42671966552734,\n",
       "   86.44449615478516,\n",
       "   86.09627532958984,\n",
       "   85.39836883544922,\n",
       "   84.08512115478516,\n",
       "   81.74076080322266,\n",
       "   81.6522445678711,\n",
       "   82.11217498779297,\n",
       "   82.78203582763672,\n",
       "   82.59718322753906,\n",
       "   80.77568054199219,\n",
       "   79.36146545410156,\n",
       "   79.2438735961914,\n",
       "   78.68064880371094,\n",
       "   78.02727508544922,\n",
       "   79.27345275878906,\n",
       "   79.45844268798828,\n",
       "   79.14100646972656,\n",
       "   79.02291870117188,\n",
       "   79.17530059814453,\n",
       "   78.0190658569336,\n",
       "   75.68402099609375,\n",
       "   75.6257553100586,\n",
       "   74.92019653320312,\n",
       "   74.91127014160156,\n",
       "   74.27607727050781,\n",
       "   73.13035583496094,\n",
       "   70.87873840332031,\n",
       "   70.51538848876953,\n",
       "   68.7463607788086,\n",
       "   68.96955108642578,\n",
       "   69.91325378417969,\n",
       "   69.6548080444336,\n",
       "   70.64642333984375,\n",
       "   69.13227081298828,\n",
       "   68.0438232421875,\n",
       "   66.81482696533203,\n",
       "   66.76913452148438,\n",
       "   67.28541564941406,\n",
       "   66.44437408447266,\n",
       "   66.25971984863281,\n",
       "   64.73355102539062,\n",
       "   64.2333755493164,\n",
       "   64.26142883300781,\n",
       "   64.99299621582031,\n",
       "   64.09333038330078,\n",
       "   62.53341293334961,\n",
       "   62.808048248291016,\n",
       "   63.48553466796875,\n",
       "   63.362548828125,\n",
       "   62.36898422241211,\n",
       "   62.059104919433594,\n",
       "   61.60710525512695,\n",
       "   60.906673431396484,\n",
       "   58.82341766357422,\n",
       "   57.184452056884766,\n",
       "   55.8418083190918,\n",
       "   55.39377212524414,\n",
       "   53.80899429321289,\n",
       "   52.44052505493164,\n",
       "   51.32943344116211,\n",
       "   51.53916549682617,\n",
       "   50.49482727050781,\n",
       "   50.22986602783203,\n",
       "   51.046539306640625,\n",
       "   49.45317077636719,\n",
       "   48.00135040283203,\n",
       "   47.436378479003906,\n",
       "   47.67023468017578,\n",
       "   46.690311431884766,\n",
       "   46.1884651184082,\n",
       "   45.055145263671875,\n",
       "   44.24415588378906,\n",
       "   42.985748291015625,\n",
       "   42.01170349121094,\n",
       "   41.97103500366211,\n",
       "   43.1747932434082,\n",
       "   43.11577606201172,\n",
       "   42.576744079589844,\n",
       "   42.19673538208008,\n",
       "   41.63806915283203,\n",
       "   40.36664962768555,\n",
       "   39.84696960449219,\n",
       "   39.28057861328125,\n",
       "   37.62853240966797,\n",
       "   37.321807861328125,\n",
       "   37.35214614868164,\n",
       "   37.73102569580078,\n",
       "   37.107574462890625,\n",
       "   36.8482551574707,\n",
       "   36.26747512817383,\n",
       "   36.734771728515625,\n",
       "   36.6840934753418,\n",
       "   35.687782287597656,\n",
       "   35.739994049072266,\n",
       "   36.31208038330078,\n",
       "   35.625099182128906,\n",
       "   35.35767364501953,\n",
       "   35.02641296386719,\n",
       "   34.82210922241211,\n",
       "   34.73525619506836,\n",
       "   33.922306060791016,\n",
       "   33.576114654541016,\n",
       "   33.26980209350586,\n",
       "   32.64867401123047,\n",
       "   32.16032409667969,\n",
       "   30.989765167236328,\n",
       "   30.63249969482422,\n",
       "   30.72086524963379,\n",
       "   30.417240142822266,\n",
       "   29.988929748535156,\n",
       "   29.416297912597656,\n",
       "   29.41865348815918,\n",
       "   29.414363861083984,\n",
       "   28.84431266784668,\n",
       "   28.545745849609375,\n",
       "   28.247922897338867,\n",
       "   28.15970230102539,\n",
       "   27.584985733032227,\n",
       "   27.05105209350586,\n",
       "   26.64935874938965,\n",
       "   26.771196365356445,\n",
       "   26.664379119873047,\n",
       "   25.598764419555664,\n",
       "   25.26915740966797,\n",
       "   25.242036819458008,\n",
       "   24.540115356445312,\n",
       "   24.867515563964844,\n",
       "   24.863481521606445,\n",
       "   24.84235191345215,\n",
       "   24.708141326904297,\n",
       "   24.958066940307617,\n",
       "   24.71613883972168,\n",
       "   24.8089599609375,\n",
       "   24.380586624145508,\n",
       "   24.415332794189453,\n",
       "   24.79458236694336,\n",
       "   24.933000564575195,\n",
       "   25.015262603759766,\n",
       "   24.862531661987305,\n",
       "   24.70453453063965,\n",
       "   25.0760498046875,\n",
       "   25.20638084411621,\n",
       "   25.438934326171875,\n",
       "   25.277814865112305,\n",
       "   24.815378189086914,\n",
       "   24.848352432250977,\n",
       "   24.5916748046875,\n",
       "   24.7049503326416,\n",
       "   25.695451736450195,\n",
       "   25.377098083496094,\n",
       "   25.47007179260254,\n",
       "   25.652009963989258,\n",
       "   25.92202377319336,\n",
       "   26.211383819580078,\n",
       "   26.05814552307129,\n",
       "   26.22217559814453,\n",
       "   26.04730987548828,\n",
       "   26.061853408813477,\n",
       "   26.30229377746582,\n",
       "   26.568466186523438,\n",
       "   26.583576202392578,\n",
       "   26.856136322021484,\n",
       "   26.891347885131836,\n",
       "   26.914648056030273,\n",
       "   26.912763595581055,\n",
       "   26.948389053344727,\n",
       "   26.626136779785156,\n",
       "   26.60577392578125,\n",
       "   26.589506149291992,\n",
       "   26.322895050048828,\n",
       "   26.305452346801758,\n",
       "   26.391902923583984,\n",
       "   26.761335372924805,\n",
       "   26.3273868560791,\n",
       "   25.786230087280273,\n",
       "   25.385013580322266,\n",
       "   25.111818313598633,\n",
       "   25.33121109008789,\n",
       "   24.850160598754883,\n",
       "   24.772409439086914,\n",
       "   24.26911735534668,\n",
       "   23.73458480834961,\n",
       "   24.13332748413086,\n",
       "   24.201560974121094,\n",
       "   23.951139450073242,\n",
       "   24.158119201660156,\n",
       "   24.321598052978516,\n",
       "   24.09973907470703,\n",
       "   24.130374908447266,\n",
       "   23.871946334838867,\n",
       "   24.025636672973633,\n",
       "   23.6829833984375,\n",
       "   23.583608627319336,\n",
       "   23.643985748291016,\n",
       "   23.665813446044922,\n",
       "   23.353313446044922,\n",
       "   23.541423797607422,\n",
       "   23.491004943847656,\n",
       "   23.654823303222656,\n",
       "   23.82244110107422,\n",
       "   24.0513916015625,\n",
       "   23.970460891723633,\n",
       "   23.748321533203125,\n",
       "   23.63404083251953,\n",
       "   23.4185791015625,\n",
       "   23.70241355895996,\n",
       "   23.940561294555664,\n",
       "   24.00623321533203,\n",
       "   24.13475799560547,\n",
       "   24.119657516479492,\n",
       "   24.122194290161133,\n",
       "   24.23917579650879,\n",
       "   24.21319007873535,\n",
       "   24.333637237548828,\n",
       "   24.221553802490234,\n",
       "   24.048015594482422,\n",
       "   23.823720932006836,\n",
       "   23.499685287475586,\n",
       "   23.35032844543457,\n",
       "   23.323198318481445,\n",
       "   22.93263053894043,\n",
       "   22.853567123413086,\n",
       "   22.613733291625977,\n",
       "   22.297563552856445,\n",
       "   22.384262084960938,\n",
       "   22.423473358154297,\n",
       "   22.383710861206055,\n",
       "   22.33847999572754,\n",
       "   22.290124893188477,\n",
       "   22.21269989013672,\n",
       "   22.12775230407715,\n",
       "   22.033315658569336,\n",
       "   22.152917861938477,\n",
       "   22.155303955078125,\n",
       "   22.173507690429688,\n",
       "   22.274824142456055,\n",
       "   22.294858932495117,\n",
       "   22.298376083374023,\n",
       "   22.377199172973633,\n",
       "   22.29779624938965,\n",
       "   22.161861419677734,\n",
       "   22.157930374145508,\n",
       "   22.224830627441406,\n",
       "   22.209081649780273,\n",
       "   22.162717819213867,\n",
       "   22.299800872802734,\n",
       "   22.19819450378418,\n",
       "   22.1287899017334,\n",
       "   22.116355895996094,\n",
       "   22.23202896118164,\n",
       "   22.286287307739258,\n",
       "   22.278213500976562,\n",
       "   22.336523056030273,\n",
       "   22.237770080566406,\n",
       "   22.16185760498047,\n",
       "   22.24948501586914,\n",
       "   22.095752716064453,\n",
       "   21.903926849365234,\n",
       "   21.956470489501953,\n",
       "   21.914287567138672,\n",
       "   21.95039939880371,\n",
       "   22.004274368286133,\n",
       "   21.990875244140625,\n",
       "   21.93265151977539,\n",
       "   21.8418025970459,\n",
       "   21.762449264526367,\n",
       "   21.754222869873047,\n",
       "   21.725507736206055,\n",
       "   21.721019744873047,\n",
       "   21.6434383392334,\n",
       "   21.519895553588867,\n",
       "   21.48518180847168,\n",
       "   21.495594024658203,\n",
       "   21.28396987915039,\n",
       "   21.19261360168457,\n",
       "   21.158344268798828,\n",
       "   21.162105560302734,\n",
       "   21.297142028808594,\n",
       "   21.291654586791992,\n",
       "   21.20604133605957,\n",
       "   21.14190101623535,\n",
       "   21.06886863708496,\n",
       "   20.964859008789062,\n",
       "   21.150653839111328,\n",
       "   21.181535720825195,\n",
       "   21.124401092529297,\n",
       "   21.053016662597656,\n",
       "   20.8992919921875,\n",
       "   20.90213966369629,\n",
       "   20.960805892944336,\n",
       "   20.87925148010254,\n",
       "   20.881771087646484,\n",
       "   20.93988037109375,\n",
       "   20.979045867919922,\n",
       "   20.93887710571289,\n",
       "   20.78989028930664,\n",
       "   20.858734130859375,\n",
       "   20.828510284423828,\n",
       "   20.795753479003906,\n",
       "   20.77728843688965,\n",
       "   20.707317352294922,\n",
       "   20.71157455444336,\n",
       "   20.623653411865234,\n",
       "   20.619165420532227,\n",
       "   20.604366302490234,\n",
       "   20.58702278137207,\n",
       "   20.625913619995117,\n",
       "   20.61688232421875,\n",
       "   20.566917419433594,\n",
       "   20.485523223876953,\n",
       "   20.395139694213867,\n",
       "   20.480676651000977,\n",
       "   20.41857147216797,\n",
       "   20.448375701904297,\n",
       "   20.449867248535156,\n",
       "   20.48933219909668,\n",
       "   20.479875564575195,\n",
       "   20.48802947998047,\n",
       "   20.36961555480957,\n",
       "   20.382043838500977,\n",
       "   20.396053314208984,\n",
       "   20.42512321472168,\n",
       "   20.394880294799805,\n",
       "   20.305395126342773,\n",
       "   20.291194915771484,\n",
       "   20.318315505981445,\n",
       "   20.408361434936523,\n",
       "   20.418508529663086,\n",
       "   20.516456604003906,\n",
       "   20.459524154663086,\n",
       "   20.557024002075195,\n",
       "   20.477487564086914,\n",
       "   20.498003005981445,\n",
       "   20.436092376708984,\n",
       "   20.545398712158203,\n",
       "   20.49213409423828,\n",
       "   20.52945327758789,\n",
       "   20.52869987487793,\n",
       "   20.400423049926758,\n",
       "   20.43123435974121,\n",
       "   20.48952865600586,\n",
       "   20.398456573486328,\n",
       "   20.427888870239258,\n",
       "   20.38712501525879,\n",
       "   20.345151901245117,\n",
       "   20.269329071044922,\n",
       "   20.25887107849121,\n",
       "   20.269229888916016,\n",
       "   20.271814346313477,\n",
       "   20.31409454345703,\n",
       "   20.32853126525879,\n",
       "   20.32065200805664,\n",
       "   20.306529998779297,\n",
       "   20.296621322631836,\n",
       "   20.255739212036133,\n",
       "   20.26210594177246,\n",
       "   20.25799560546875,\n",
       "   20.235816955566406,\n",
       "   20.28253746032715,\n",
       "   20.294097900390625,\n",
       "   20.338457107543945,\n",
       "   20.319700241088867,\n",
       "   20.299598693847656,\n",
       "   20.34015464782715,\n",
       "   20.28432846069336,\n",
       "   20.329984664916992,\n",
       "   20.318571090698242,\n",
       "   20.325542449951172,\n",
       "   20.286632537841797,\n",
       "   20.344881057739258,\n",
       "   20.290943145751953,\n",
       "   20.331546783447266,\n",
       "   20.31179428100586,\n",
       "   20.324125289916992,\n",
       "   20.303001403808594,\n",
       "   20.2761173248291,\n",
       "   20.269731521606445,\n",
       "   20.335805892944336,\n",
       "   20.337474822998047,\n",
       "   20.311080932617188,\n",
       "   20.274120330810547,\n",
       "   20.30763053894043,\n",
       "   20.34453582763672,\n",
       "   20.354246139526367,\n",
       "   20.390878677368164,\n",
       "   20.423120498657227,\n",
       "   20.45497703552246,\n",
       "   20.477197647094727,\n",
       "   20.426433563232422,\n",
       "   20.4399471282959,\n",
       "   20.453197479248047,\n",
       "   20.403804779052734,\n",
       "   20.350738525390625,\n",
       "   20.303651809692383,\n",
       "   20.215614318847656,\n",
       "   20.181650161743164,\n",
       "   20.227970123291016,\n",
       "   20.229171752929688,\n",
       "   20.2696590423584,\n",
       "   20.215734481811523,\n",
       "   20.253387451171875,\n",
       "   20.363080978393555,\n",
       "   20.39604949951172,\n",
       "   20.34748649597168,\n",
       "   20.304250717163086,\n",
       "   20.344335556030273,\n",
       "   20.41059684753418,\n",
       "   20.438785552978516,\n",
       "   20.40209197998047,\n",
       "   20.392032623291016,\n",
       "   20.387008666992188,\n",
       "   20.418563842773438,\n",
       "   20.436786651611328,\n",
       "   20.37668800354004,\n",
       "   20.3814640045166,\n",
       "   20.381359100341797,\n",
       "   20.38457489013672,\n",
       "   20.330875396728516,\n",
       "   20.278791427612305,\n",
       "   20.330984115600586,\n",
       "   20.332433700561523,\n",
       "   20.238021850585938,\n",
       "   20.26268768310547,\n",
       "   20.165424346923828,\n",
       "   20.182170867919922,\n",
       "   20.194290161132812,\n",
       "   20.232881546020508,\n",
       "   20.235872268676758,\n",
       "   20.25855255126953,\n",
       "   20.292360305786133,\n",
       "   20.26902961730957,\n",
       "   20.27210807800293,\n",
       "   20.2265567779541,\n",
       "   20.12693214416504,\n",
       "   20.147661209106445,\n",
       "   20.12144660949707,\n",
       "   20.134397506713867,\n",
       "   20.10250473022461,\n",
       "   20.05605125427246,\n",
       "   20.141738891601562,\n",
       "   20.182397842407227,\n",
       "   20.22420883178711,\n",
       "   20.134220123291016,\n",
       "   20.042007446289062,\n",
       "   20.158668518066406,\n",
       "   20.23050880432129,\n",
       "   20.27128791809082,\n",
       "   20.24766731262207,\n",
       "   20.292394638061523,\n",
       "   20.288835525512695,\n",
       "   20.26076316833496,\n",
       "   20.246583938598633,\n",
       "   20.21249771118164,\n",
       "   20.175283432006836,\n",
       "   20.123037338256836,\n",
       "   20.214479446411133,\n",
       "   20.210962295532227,\n",
       "   20.19824981689453,\n",
       "   20.16803550720215,\n",
       "   20.170536041259766,\n",
       "   20.215600967407227,\n",
       "   20.230201721191406,\n",
       "   20.188581466674805,\n",
       "   20.204397201538086,\n",
       "   20.31241798400879,\n",
       "   20.350357055664062,\n",
       "   20.40607452392578,\n",
       "   20.40547752380371,\n",
       "   20.348413467407227],\n",
       "  'ff_acc': [100.0,\n",
       "   100.0,\n",
       "   99.81840515136719,\n",
       "   99.38652801513672,\n",
       "   98.95853424072266,\n",
       "   97.71183013916016,\n",
       "   96.12283325195312,\n",
       "   95.00473022460938,\n",
       "   93.84471893310547,\n",
       "   90.55099487304688,\n",
       "   86.3533935546875,\n",
       "   82.91101837158203,\n",
       "   80.64086151123047,\n",
       "   78.17166900634766,\n",
       "   75.97039031982422,\n",
       "   74.62641906738281,\n",
       "   74.01013946533203,\n",
       "   74.1618881225586,\n",
       "   72.20088195800781,\n",
       "   71.76177215576172,\n",
       "   70.72808074951172,\n",
       "   69.52877044677734,\n",
       "   68.56534576416016,\n",
       "   68.02226257324219,\n",
       "   68.4730224609375,\n",
       "   68.03438568115234,\n",
       "   68.41891479492188,\n",
       "   68.54104614257812,\n",
       "   68.91902923583984,\n",
       "   68.35039520263672,\n",
       "   68.38601684570312,\n",
       "   67.91425323486328,\n",
       "   67.787841796875,\n",
       "   67.23754119873047,\n",
       "   67.56492614746094,\n",
       "   67.38005065917969,\n",
       "   67.18047332763672,\n",
       "   67.19622802734375,\n",
       "   66.84201049804688,\n",
       "   66.89701080322266,\n",
       "   66.98104858398438,\n",
       "   67.45883178710938,\n",
       "   67.78994750976562,\n",
       "   67.42375183105469,\n",
       "   67.6196060180664,\n",
       "   67.07184600830078,\n",
       "   67.38349914550781,\n",
       "   68.94445037841797,\n",
       "   69.1351318359375,\n",
       "   70.16603088378906,\n",
       "   71.26272583007812,\n",
       "   72.2385025024414,\n",
       "   73.10916900634766,\n",
       "   74.17221069335938,\n",
       "   74.5993881225586,\n",
       "   74.99982452392578,\n",
       "   74.8647232055664,\n",
       "   76.01038360595703,\n",
       "   75.65483093261719,\n",
       "   75.21452331542969,\n",
       "   75.63687133789062,\n",
       "   75.71725463867188,\n",
       "   77.01935577392578,\n",
       "   77.45724487304688,\n",
       "   77.10921478271484,\n",
       "   76.60060119628906,\n",
       "   76.1234359741211,\n",
       "   76.21526336669922,\n",
       "   76.65901184082031,\n",
       "   76.87098693847656,\n",
       "   76.84831237792969,\n",
       "   77.47319030761719,\n",
       "   76.72947692871094,\n",
       "   75.81800842285156,\n",
       "   76.1032943725586,\n",
       "   75.52458953857422,\n",
       "   75.4088134765625,\n",
       "   74.73975372314453,\n",
       "   74.27246856689453,\n",
       "   74.27206420898438,\n",
       "   73.55903625488281,\n",
       "   73.88601684570312,\n",
       "   74.20860290527344,\n",
       "   74.2602767944336,\n",
       "   73.65289306640625,\n",
       "   73.61217498779297,\n",
       "   72.86996459960938,\n",
       "   73.31221771240234,\n",
       "   73.82737731933594,\n",
       "   73.64987182617188,\n",
       "   73.07312774658203,\n",
       "   73.71675872802734,\n",
       "   74.40336608886719,\n",
       "   73.85810852050781,\n",
       "   74.26449584960938,\n",
       "   74.95683288574219,\n",
       "   74.88201904296875,\n",
       "   75.15571594238281,\n",
       "   74.73432159423828,\n",
       "   74.49817657470703,\n",
       "   74.2578125,\n",
       "   73.43018341064453,\n",
       "   73.91236877441406,\n",
       "   74.20369720458984,\n",
       "   74.1310806274414,\n",
       "   74.66048431396484,\n",
       "   74.31658172607422,\n",
       "   74.12664031982422,\n",
       "   74.4426040649414,\n",
       "   74.97772216796875,\n",
       "   75.49758911132812,\n",
       "   75.62919616699219,\n",
       "   75.77828216552734,\n",
       "   75.36074829101562,\n",
       "   75.58412170410156,\n",
       "   75.70938110351562,\n",
       "   75.61532592773438,\n",
       "   76.22679138183594,\n",
       "   76.14665985107422,\n",
       "   75.92774200439453,\n",
       "   75.62715911865234,\n",
       "   75.04597473144531,\n",
       "   75.52315521240234,\n",
       "   74.84921264648438,\n",
       "   75.06233215332031,\n",
       "   74.38118743896484,\n",
       "   73.85442352294922,\n",
       "   72.88348388671875,\n",
       "   72.06539154052734,\n",
       "   71.19515228271484,\n",
       "   70.43769073486328,\n",
       "   69.82333374023438,\n",
       "   69.00263977050781,\n",
       "   67.63440704345703,\n",
       "   67.92304992675781,\n",
       "   67.60813903808594,\n",
       "   67.18141174316406,\n",
       "   67.27527618408203,\n",
       "   67.50507354736328,\n",
       "   65.87335968017578,\n",
       "   65.68096160888672,\n",
       "   65.38302612304688,\n",
       "   65.38753509521484,\n",
       "   65.5765380859375,\n",
       "   65.49502563476562,\n",
       "   65.524658203125,\n",
       "   64.75212860107422,\n",
       "   64.41202545166016,\n",
       "   64.40526580810547,\n",
       "   64.54979705810547,\n",
       "   64.2106704711914,\n",
       "   64.84090423583984,\n",
       "   67.33333587646484,\n",
       "   69.03495025634766,\n",
       "   70.1015625,\n",
       "   71.02891540527344,\n",
       "   70.4996109008789,\n",
       "   70.10369110107422,\n",
       "   70.3513412475586,\n",
       "   70.24617004394531,\n",
       "   69.62078857421875,\n",
       "   69.8775863647461,\n",
       "   70.52310943603516,\n",
       "   71.25379180908203,\n",
       "   71.13043212890625,\n",
       "   70.38208770751953,\n",
       "   71.73167419433594,\n",
       "   71.80160522460938,\n",
       "   71.14895629882812,\n",
       "   71.0868911743164,\n",
       "   70.92049407958984,\n",
       "   70.19378662109375,\n",
       "   69.93853759765625,\n",
       "   69.27839660644531,\n",
       "   69.3114242553711,\n",
       "   69.0635986328125,\n",
       "   68.90409851074219,\n",
       "   68.1123046875,\n",
       "   67.43440246582031,\n",
       "   68.21057891845703,\n",
       "   68.07361602783203,\n",
       "   68.40718078613281,\n",
       "   68.35249328613281,\n",
       "   67.77572631835938,\n",
       "   66.6243667602539,\n",
       "   66.71466827392578,\n",
       "   66.74505615234375,\n",
       "   66.47137451171875,\n",
       "   67.69309997558594,\n",
       "   66.89641571044922,\n",
       "   68.09473419189453,\n",
       "   69.34333801269531,\n",
       "   68.4779052734375,\n",
       "   68.04883575439453,\n",
       "   67.9476547241211,\n",
       "   67.07868957519531,\n",
       "   65.91506958007812,\n",
       "   65.23558807373047,\n",
       "   66.09745788574219,\n",
       "   66.76236724853516,\n",
       "   65.5431137084961,\n",
       "   64.60014343261719,\n",
       "   64.64137268066406,\n",
       "   63.99916458129883,\n",
       "   64.96340942382812,\n",
       "   64.2283706665039,\n",
       "   64.31208801269531,\n",
       "   64.09871673583984,\n",
       "   63.58269500732422,\n",
       "   62.5500602722168,\n",
       "   61.64691925048828,\n",
       "   61.1504020690918,\n",
       "   61.454856872558594,\n",
       "   62.15584945678711,\n",
       "   61.623348236083984,\n",
       "   62.395469665527344,\n",
       "   61.955833435058594,\n",
       "   61.20511245727539,\n",
       "   61.3971061706543,\n",
       "   60.50959777832031,\n",
       "   60.32512664794922,\n",
       "   59.91650390625,\n",
       "   59.59895706176758,\n",
       "   58.482093811035156,\n",
       "   58.5207405090332,\n",
       "   58.563785552978516,\n",
       "   57.66011428833008,\n",
       "   57.4380989074707,\n",
       "   57.12894058227539,\n",
       "   56.53770446777344,\n",
       "   55.368751525878906,\n",
       "   54.86381149291992,\n",
       "   55.58788299560547,\n",
       "   55.5195198059082,\n",
       "   56.05986022949219,\n",
       "   56.761192321777344,\n",
       "   55.54265213012695,\n",
       "   56.129432678222656,\n",
       "   56.142677307128906,\n",
       "   56.128482818603516,\n",
       "   55.642311096191406,\n",
       "   54.90969467163086,\n",
       "   56.14289093017578,\n",
       "   56.716007232666016,\n",
       "   55.87990188598633,\n",
       "   57.31576156616211,\n",
       "   57.49142837524414,\n",
       "   58.14872360229492,\n",
       "   58.14321517944336,\n",
       "   57.03392791748047,\n",
       "   55.02144241333008,\n",
       "   55.50385665893555,\n",
       "   54.75288772583008,\n",
       "   53.66080856323242,\n",
       "   53.5611457824707,\n",
       "   54.07965850830078,\n",
       "   53.461585998535156,\n",
       "   54.000885009765625,\n",
       "   54.17461395263672,\n",
       "   54.109745025634766,\n",
       "   54.316104888916016,\n",
       "   54.93877029418945,\n",
       "   54.40861129760742,\n",
       "   54.252750396728516,\n",
       "   53.61671447753906,\n",
       "   53.4524040222168,\n",
       "   53.86720275878906,\n",
       "   53.9747428894043,\n",
       "   53.024017333984375,\n",
       "   53.17414474487305,\n",
       "   51.73588943481445,\n",
       "   52.47127914428711,\n",
       "   51.39110565185547,\n",
       "   50.939361572265625,\n",
       "   50.671142578125,\n",
       "   51.439369201660156,\n",
       "   51.19361877441406,\n",
       "   51.43421936035156,\n",
       "   52.04590606689453,\n",
       "   52.33454895019531,\n",
       "   52.372371673583984,\n",
       "   52.417625427246094,\n",
       "   53.03498077392578,\n",
       "   52.68581008911133,\n",
       "   53.94429016113281,\n",
       "   53.55648422241211,\n",
       "   53.556800842285156,\n",
       "   52.7914924621582,\n",
       "   53.2142219543457,\n",
       "   54.0705680847168,\n",
       "   53.835445404052734,\n",
       "   53.45464324951172,\n",
       "   53.022743225097656,\n",
       "   51.70687484741211,\n",
       "   52.28346633911133,\n",
       "   53.372467041015625,\n",
       "   53.07076644897461,\n",
       "   52.82777404785156,\n",
       "   53.98539733886719,\n",
       "   53.58449172973633,\n",
       "   52.26396179199219,\n",
       "   53.10102081298828,\n",
       "   53.5531120300293,\n",
       "   53.770912170410156,\n",
       "   52.19976806640625,\n",
       "   50.96168518066406,\n",
       "   50.99003982543945,\n",
       "   50.332027435302734,\n",
       "   50.39390182495117,\n",
       "   50.74113464355469,\n",
       "   50.81281661987305,\n",
       "   50.934452056884766,\n",
       "   50.83111572265625,\n",
       "   50.97111892700195,\n",
       "   51.01715850830078,\n",
       "   50.37208938598633,\n",
       "   49.495750427246094,\n",
       "   50.810646057128906,\n",
       "   50.697391510009766,\n",
       "   50.761497497558594,\n",
       "   50.8865966796875,\n",
       "   50.704280853271484,\n",
       "   50.4259033203125,\n",
       "   51.14985656738281,\n",
       "   51.13557052612305,\n",
       "   50.6314582824707,\n",
       "   49.29227828979492,\n",
       "   49.2540283203125,\n",
       "   49.224491119384766,\n",
       "   49.002288818359375,\n",
       "   49.97099685668945,\n",
       "   51.12571716308594,\n",
       "   50.785179138183594,\n",
       "   49.81546401977539,\n",
       "   48.934085845947266,\n",
       "   48.01956558227539,\n",
       "   48.7650032043457,\n",
       "   49.31782913208008,\n",
       "   48.43688201904297,\n",
       "   48.649269104003906,\n",
       "   49.87450408935547,\n",
       "   50.2441291809082,\n",
       "   50.56598663330078,\n",
       "   51.1683235168457,\n",
       "   50.195587158203125,\n",
       "   50.81687545776367,\n",
       "   50.649322509765625,\n",
       "   50.54875183105469,\n",
       "   50.05854797363281,\n",
       "   50.15328598022461,\n",
       "   51.37810516357422,\n",
       "   51.7037467956543,\n",
       "   51.61528778076172,\n",
       "   50.88376998901367,\n",
       "   51.2892951965332,\n",
       "   50.7156982421875,\n",
       "   50.89183044433594,\n",
       "   50.367591857910156,\n",
       "   50.8814697265625,\n",
       "   50.64500427246094,\n",
       "   51.24718475341797,\n",
       "   51.741600036621094,\n",
       "   51.671897888183594,\n",
       "   51.38916778564453,\n",
       "   51.0495719909668,\n",
       "   51.263004302978516,\n",
       "   52.0157470703125,\n",
       "   51.34758377075195,\n",
       "   51.630615234375,\n",
       "   51.88446044921875,\n",
       "   52.619361877441406,\n",
       "   52.06732177734375,\n",
       "   51.88776397705078,\n",
       "   51.363746643066406,\n",
       "   52.198177337646484,\n",
       "   51.91678237915039,\n",
       "   52.4442138671875,\n",
       "   52.08961868286133,\n",
       "   51.55724334716797,\n",
       "   51.874900817871094,\n",
       "   50.78532028198242,\n",
       "   50.58300018310547,\n",
       "   50.742794036865234,\n",
       "   50.33580017089844,\n",
       "   48.67597961425781,\n",
       "   48.27676010131836,\n",
       "   48.359901428222656,\n",
       "   48.16616439819336,\n",
       "   48.296199798583984,\n",
       "   48.74292755126953,\n",
       "   48.4752082824707,\n",
       "   47.627315521240234,\n",
       "   47.30403518676758,\n",
       "   47.60893249511719,\n",
       "   47.306068420410156,\n",
       "   47.6227912902832,\n",
       "   47.870792388916016,\n",
       "   48.18111801147461,\n",
       "   47.966739654541016,\n",
       "   48.774574279785156,\n",
       "   48.525123596191406,\n",
       "   49.51374435424805,\n",
       "   48.53397750854492,\n",
       "   47.62240982055664,\n",
       "   47.69043731689453,\n",
       "   48.698822021484375,\n",
       "   49.37265396118164,\n",
       "   49.78080749511719,\n",
       "   49.856422424316406,\n",
       "   50.08766555786133,\n",
       "   50.11578369140625,\n",
       "   48.90492630004883,\n",
       "   49.53612518310547,\n",
       "   48.847328186035156,\n",
       "   50.143558502197266,\n",
       "   50.19719696044922,\n",
       "   50.15236282348633,\n",
       "   48.622291564941406,\n",
       "   48.81910705566406,\n",
       "   47.845699310302734,\n",
       "   47.526390075683594,\n",
       "   46.71944808959961,\n",
       "   45.96078872680664,\n",
       "   45.64973068237305,\n",
       "   46.14386749267578,\n",
       "   46.347225189208984,\n",
       "   46.39564514160156,\n",
       "   47.59537124633789,\n",
       "   47.422306060791016,\n",
       "   46.97600555419922,\n",
       "   48.06504821777344,\n",
       "   47.65916061401367,\n",
       "   48.22982406616211,\n",
       "   47.0744514465332,\n",
       "   46.44342803955078,\n",
       "   46.811092376708984,\n",
       "   46.0093994140625,\n",
       "   45.58908462524414,\n",
       "   45.62691879272461,\n",
       "   46.004207611083984,\n",
       "   45.564083099365234,\n",
       "   44.74396514892578,\n",
       "   45.001827239990234,\n",
       "   45.79017639160156,\n",
       "   45.792236328125,\n",
       "   45.65959548950195,\n",
       "   45.319915771484375,\n",
       "   45.77619934082031,\n",
       "   45.43771743774414,\n",
       "   45.27960968017578,\n",
       "   44.68593215942383,\n",
       "   44.01118087768555,\n",
       "   44.529361724853516,\n",
       "   44.29755401611328,\n",
       "   44.79572677612305,\n",
       "   44.72555923461914,\n",
       "   43.429229736328125,\n",
       "   42.761802673339844,\n",
       "   43.21049880981445,\n",
       "   43.06299591064453,\n",
       "   42.54828643798828,\n",
       "   42.4890251159668,\n",
       "   42.51485061645508,\n",
       "   43.94428253173828,\n",
       "   44.21786117553711,\n",
       "   45.05707550048828,\n",
       "   44.76338577270508,\n",
       "   44.603389739990234,\n",
       "   43.87596893310547,\n",
       "   44.341529846191406,\n",
       "   45.771236419677734,\n",
       "   46.361358642578125,\n",
       "   46.3984260559082,\n",
       "   45.8213996887207,\n",
       "   45.73790740966797,\n",
       "   45.43019104003906,\n",
       "   45.34043502807617,\n",
       "   45.524513244628906,\n",
       "   44.9810791015625,\n",
       "   45.21735382080078,\n",
       "   45.06690979003906,\n",
       "   43.84467315673828,\n",
       "   44.25616455078125,\n",
       "   44.59288024902344,\n",
       "   44.95307922363281,\n",
       "   45.45310974121094,\n",
       "   47.117103576660156,\n",
       "   47.522483825683594,\n",
       "   47.74536895751953,\n",
       "   47.84891891479492,\n",
       "   47.9744987487793,\n",
       "   47.33407974243164,\n",
       "   46.66289520263672,\n",
       "   46.41215896606445,\n",
       "   46.34366989135742,\n",
       "   45.51503372192383,\n",
       "   44.398719787597656,\n",
       "   44.62298583984375,\n",
       "   44.68561935424805,\n",
       "   44.66484832763672],\n",
       "  'test_accs': [400.0,\n",
       "   400.0,\n",
       "   399.8184051513672,\n",
       "   398.49463653564453,\n",
       "   394.1821594238281,\n",
       "   379.0369415283203,\n",
       "   357.5508918762207,\n",
       "   338.0157012939453,\n",
       "   322.41637229919434,\n",
       "   307.70420265197754,\n",
       "   297.7496728897095,\n",
       "   292.2377071380615,\n",
       "   287.7480525970459,\n",
       "   284.45166397094727,\n",
       "   282.44969367980957,\n",
       "   275.7655544281006,\n",
       "   270.46982192993164,\n",
       "   267.62499618530273,\n",
       "   257.7189655303955,\n",
       "   250.39053344726562,\n",
       "   247.22589683532715,\n",
       "   241.3284797668457,\n",
       "   233.6521701812744,\n",
       "   228.36954498291016,\n",
       "   222.06924629211426,\n",
       "   216.02771377563477,\n",
       "   212.60649299621582,\n",
       "   209.01416778564453,\n",
       "   209.45355319976807,\n",
       "   205.95017623901367,\n",
       "   203.7559494972229,\n",
       "   203.93596935272217,\n",
       "   203.06562757492065,\n",
       "   199.3581199645996,\n",
       "   197.447368144989,\n",
       "   193.32635498046875,\n",
       "   195.20961666107178,\n",
       "   195.60812377929688,\n",
       "   196.11117458343506,\n",
       "   195.05817794799805,\n",
       "   192.5402374267578,\n",
       "   194.56936311721802,\n",
       "   197.78988885879517,\n",
       "   198.97575426101685,\n",
       "   199.02870273590088,\n",
       "   200.7323179244995,\n",
       "   204.38809299468994,\n",
       "   205.87361669540405,\n",
       "   204.72258949279785,\n",
       "   207.6259889602661,\n",
       "   210.74538564682007,\n",
       "   212.4908561706543,\n",
       "   213.44619417190552,\n",
       "   215.7253189086914,\n",
       "   215.6972370147705,\n",
       "   215.7715039253235,\n",
       "   213.73365545272827,\n",
       "   214.11585092544556,\n",
       "   215.1180739402771,\n",
       "   214.1037745475769,\n",
       "   214.19045686721802,\n",
       "   213.84208726882935,\n",
       "   213.81857347488403,\n",
       "   217.56495189666748,\n",
       "   215.12738609313965,\n",
       "   211.99767541885376,\n",
       "   211.51594257354736,\n",
       "   211.40610647201538,\n",
       "   210.60172367095947,\n",
       "   209.129132270813,\n",
       "   207.17559337615967,\n",
       "   207.50810432434082,\n",
       "   204.94329118728638,\n",
       "   201.98633241653442,\n",
       "   202.50775146484375,\n",
       "   202.98586463928223,\n",
       "   202.95892095565796,\n",
       "   204.92380046844482,\n",
       "   204.25648164749146,\n",
       "   203.71454238891602,\n",
       "   201.9844207763672,\n",
       "   201.0855450630188,\n",
       "   198.2718734741211,\n",
       "   195.26621627807617,\n",
       "   193.18060111999512,\n",
       "   192.21068382263184,\n",
       "   189.01054048538208,\n",
       "   188.72791004180908,\n",
       "   189.23477268218994,\n",
       "   186.66463470458984,\n",
       "   186.6509985923767,\n",
       "   184.94823455810547,\n",
       "   185.60033750534058,\n",
       "   184.80034923553467,\n",
       "   184.4963550567627,\n",
       "   180.67218923568726,\n",
       "   175.69934034347534,\n",
       "   174.61905717849731,\n",
       "   175.04004383087158,\n",
       "   171.87577772140503,\n",
       "   168.8771333694458,\n",
       "   167.21477556228638,\n",
       "   169.91747379302979,\n",
       "   171.94022464752197,\n",
       "   171.04734706878662,\n",
       "   174.2565212249756,\n",
       "   177.02387952804565,\n",
       "   176.90554571151733,\n",
       "   176.57677459716797,\n",
       "   177.21500444412231,\n",
       "   175.06334114074707,\n",
       "   174.64719915390015,\n",
       "   175.90188884735107,\n",
       "   173.33501958847046,\n",
       "   173.17252683639526,\n",
       "   172.51038312911987,\n",
       "   172.8760323524475,\n",
       "   173.0532946586609,\n",
       "   173.97976446151733,\n",
       "   173.90377235412598,\n",
       "   172.3188352584839,\n",
       "   172.89729166030884,\n",
       "   171.776873588562,\n",
       "   170.42892980575562,\n",
       "   170.10536193847656,\n",
       "   168.52826690673828,\n",
       "   167.03570365905762,\n",
       "   164.8435139656067,\n",
       "   164.7135524749756,\n",
       "   162.80600309371948,\n",
       "   161.0266227722168,\n",
       "   159.21253061294556,\n",
       "   157.73700189590454,\n",
       "   155.4260859489441,\n",
       "   152.24383735656738,\n",
       "   153.09819746017456,\n",
       "   152.62707090377808,\n",
       "   151.72716903686523,\n",
       "   151.8022918701172,\n",
       "   148.54504489898682,\n",
       "   149.11946058273315,\n",
       "   146.7102403640747,\n",
       "   143.77437353134155,\n",
       "   145.17376565933228,\n",
       "   144.3077425956726,\n",
       "   143.88088607788086,\n",
       "   143.557035446167,\n",
       "   143.83225345611572,\n",
       "   145.2262749671936,\n",
       "   145.02414751052856,\n",
       "   145.33754205703735,\n",
       "   146.69374561309814,\n",
       "   148.6833462715149,\n",
       "   148.34679412841797,\n",
       "   148.79873037338257,\n",
       "   150.0009741783142,\n",
       "   147.05880784988403,\n",
       "   145.82757663726807,\n",
       "   147.8154969215393,\n",
       "   146.0855689048767,\n",
       "   146.12146759033203,\n",
       "   146.93935537338257,\n",
       "   149.12537479400635,\n",
       "   150.35107517242432,\n",
       "   149.91006755828857,\n",
       "   150.64811325073242,\n",
       "   153.25713205337524,\n",
       "   152.43124055862427,\n",
       "   152.14846515655518,\n",
       "   154.65408325195312,\n",
       "   154.02195501327515,\n",
       "   155.8364233970642,\n",
       "   158.15908098220825,\n",
       "   157.86778020858765,\n",
       "   157.02759170532227,\n",
       "   158.67478322982788,\n",
       "   159.16016578674316,\n",
       "   159.48170518875122,\n",
       "   160.4436388015747,\n",
       "   161.3893599510193,\n",
       "   161.69760274887085,\n",
       "   164.01979541778564,\n",
       "   164.95115661621094,\n",
       "   164.07418060302734,\n",
       "   163.60425090789795,\n",
       "   164.75780773162842,\n",
       "   164.7615509033203,\n",
       "   164.97497749328613,\n",
       "   165.3747353553772,\n",
       "   165.3486204147339,\n",
       "   165.673357963562,\n",
       "   166.66418838500977,\n",
       "   164.40687561035156,\n",
       "   163.30869245529175,\n",
       "   160.4602780342102,\n",
       "   158.9770450592041,\n",
       "   156.07165145874023,\n",
       "   157.9872498512268,\n",
       "   159.35792922973633,\n",
       "   158.47273635864258,\n",
       "   157.33866691589355,\n",
       "   157.39595746994019,\n",
       "   156.43293809890747,\n",
       "   157.01451635360718,\n",
       "   160.15678691864014,\n",
       "   160.0646619796753,\n",
       "   158.73514986038208,\n",
       "   157.91999435424805,\n",
       "   158.35142517089844,\n",
       "   157.39972496032715,\n",
       "   156.35597229003906,\n",
       "   155.25610971450806,\n",
       "   156.02832126617432,\n",
       "   155.65809202194214,\n",
       "   155.93976974487305,\n",
       "   156.83241891860962,\n",
       "   156.34474182128906,\n",
       "   156.75249910354614,\n",
       "   157.7499179840088,\n",
       "   155.3164086341858,\n",
       "   157.02043342590332,\n",
       "   155.98096179962158,\n",
       "   155.68868446350098,\n",
       "   153.60276746749878,\n",
       "   155.1679711341858,\n",
       "   155.70009565353394,\n",
       "   155.01828384399414,\n",
       "   154.3491997718811,\n",
       "   154.27538585662842,\n",
       "   154.40626525878906,\n",
       "   151.91219520568848,\n",
       "   151.05979013442993,\n",
       "   151.32269716262817,\n",
       "   150.50506353378296,\n",
       "   149.93302965164185,\n",
       "   151.0901837348938,\n",
       "   149.5854868888855,\n",
       "   152.73029565811157,\n",
       "   151.0978307723999,\n",
       "   151.54403448104858,\n",
       "   152.7034683227539,\n",
       "   152.35807847976685,\n",
       "   152.2177872657776,\n",
       "   152.42777109146118,\n",
       "   151.09271574020386,\n",
       "   151.0860071182251,\n",
       "   149.16201877593994,\n",
       "   149.05521202087402,\n",
       "   146.5001664161682,\n",
       "   143.4915223121643,\n",
       "   142.238214969635,\n",
       "   140.22210264205933,\n",
       "   138.73770666122437,\n",
       "   139.3291220664978,\n",
       "   137.9178295135498,\n",
       "   137.31717205047607,\n",
       "   137.64195203781128,\n",
       "   136.364399433136,\n",
       "   135.91096925735474,\n",
       "   135.76590490341187,\n",
       "   136.27459812164307,\n",
       "   137.47641706466675,\n",
       "   137.76811695098877,\n",
       "   137.35250282287598,\n",
       "   136.1960482597351,\n",
       "   136.09456396102905,\n",
       "   135.43409204483032,\n",
       "   136.60664224624634,\n",
       "   134.08351707458496,\n",
       "   133.29995489120483,\n",
       "   130.1034812927246,\n",
       "   131.59465789794922,\n",
       "   129.35590314865112,\n",
       "   129.38656616210938,\n",
       "   129.81201124191284,\n",
       "   130.8066873550415,\n",
       "   129.63939666748047,\n",
       "   130.13539028167725,\n",
       "   132.29797840118408,\n",
       "   131.49566507339478,\n",
       "   129.30320692062378,\n",
       "   129.03569793701172,\n",
       "   130.97094917297363,\n",
       "   129.5017991065979,\n",
       "   129.70561456680298,\n",
       "   130.1821632385254,\n",
       "   132.11704874038696,\n",
       "   131.21897172927856,\n",
       "   130.93754816055298,\n",
       "   132.17090702056885,\n",
       "   131.74904489517212,\n",
       "   133.1008448600769,\n",
       "   133.27679252624512,\n",
       "   131.96460151672363,\n",
       "   131.60687017440796,\n",
       "   131.95880556106567,\n",
       "   130.11444997787476,\n",
       "   128.62054634094238,\n",
       "   130.10983180999756,\n",
       "   130.90296983718872,\n",
       "   129.2072343826294,\n",
       "   130.12164497375488,\n",
       "   132.11583471298218,\n",
       "   133.02247285842896,\n",
       "   131.74903917312622,\n",
       "   130.2925386428833,\n",
       "   129.7095489501953,\n",
       "   131.00569009780884,\n",
       "   133.18395948410034,\n",
       "   134.3565592765808,\n",
       "   135.4738826751709,\n",
       "   135.58157110214233,\n",
       "   135.16175174713135,\n",
       "   134.77482080459595,\n",
       "   133.53515005111694,\n",
       "   132.7644658088684,\n",
       "   130.7536997795105,\n",
       "   131.7944254875183,\n",
       "   131.75006008148193,\n",
       "   131.1141757965088,\n",
       "   131.91485977172852,\n",
       "   131.4873456954956,\n",
       "   130.7101411819458,\n",
       "   130.27193880081177,\n",
       "   128.86466455459595,\n",
       "   127.3188738822937,\n",
       "   125.55040216445923,\n",
       "   127.25918006896973,\n",
       "   125.82230186462402,\n",
       "   126.0230188369751,\n",
       "   126.69686508178711,\n",
       "   126.35638523101807,\n",
       "   126.05928707122803,\n",
       "   125.81370115280151,\n",
       "   124.62491941452026,\n",
       "   123.0332555770874,\n",
       "   122.62259912490845,\n",
       "   122.94846725463867,\n",
       "   122.08590507507324,\n",
       "   120.9112138748169,\n",
       "   121.18742084503174,\n",
       "   121.20365524291992,\n",
       "   121.97896194458008,\n",
       "   123.22710132598877,\n",
       "   122.7429871559143,\n",
       "   123.86834287643433,\n",
       "   124.46883058547974,\n",
       "   124.90549612045288,\n",
       "   123.67920541763306,\n",
       "   122.48999643325806,\n",
       "   124.02500677108765,\n",
       "   126.35121488571167,\n",
       "   126.36038875579834,\n",
       "   125.02089500427246,\n",
       "   125.53053092956543,\n",
       "   126.30316495895386,\n",
       "   125.80903053283691,\n",
       "   125.57775068283081,\n",
       "   125.49396324157715,\n",
       "   125.39197397232056,\n",
       "   126.13867282867432,\n",
       "   127.4414930343628,\n",
       "   126.37118625640869,\n",
       "   125.06564903259277,\n",
       "   124.70050811767578,\n",
       "   126.3987979888916,\n",
       "   127.12835216522217,\n",
       "   126.66295289993286,\n",
       "   126.67191743850708,\n",
       "   126.71827793121338,\n",
       "   127.45542526245117,\n",
       "   126.75782299041748,\n",
       "   127.5075831413269,\n",
       "   125.00073957443237,\n",
       "   126.21641540527344,\n",
       "   126.05799674987793,\n",
       "   127.19634962081909,\n",
       "   125.64013671875,\n",
       "   124.76016187667847,\n",
       "   125.29638290405273,\n",
       "   125.86273908615112,\n",
       "   124.44299364089966,\n",
       "   123.68540334701538,\n",
       "   122.33716535568237,\n",
       "   119.51759099960327,\n",
       "   119.10029411315918,\n",
       "   120.7734751701355,\n",
       "   121.89996147155762,\n",
       "   124.18410873413086,\n",
       "   124.79794120788574,\n",
       "   123.72578525543213,\n",
       "   122.98968458175659,\n",
       "   122.47118282318115,\n",
       "   124.39046478271484,\n",
       "   125.49760007858276,\n",
       "   125.80958127975464,\n",
       "   125.58054113388062,\n",
       "   127.26754188537598,\n",
       "   126.75364637374878,\n",
       "   126.56947994232178,\n",
       "   124.0370192527771,\n",
       "   124.68838787078857,\n",
       "   124.50764799118042,\n",
       "   123.83928442001343,\n",
       "   123.28341722488403,\n",
       "   123.63268804550171,\n",
       "   125.11820316314697,\n",
       "   125.60538101196289,\n",
       "   126.8587760925293,\n",
       "   126.82153654098511,\n",
       "   126.95381021499634,\n",
       "   125.05896806716919,\n",
       "   125.27737092971802,\n",
       "   125.42117595672607,\n",
       "   126.56797361373901,\n",
       "   127.4560546875,\n",
       "   126.6015100479126,\n",
       "   123.47435998916626,\n",
       "   121.84438037872314,\n",
       "   118.61332845687866,\n",
       "   116.16897487640381,\n",
       "   114.4657096862793,\n",
       "   111.40239429473877,\n",
       "   110.69421005249023,\n",
       "   111.45212936401367,\n",
       "   110.13053607940674,\n",
       "   111.06538677215576,\n",
       "   112.55121994018555,\n",
       "   112.73070669174194,\n",
       "   112.66084003448486,\n",
       "   114.55674076080322,\n",
       "   112.87228727340698,\n",
       "   113.12703227996826,\n",
       "   112.21129035949707,\n",
       "   112.05689477920532,\n",
       "   112.61445903778076,\n",
       "   111.9712700843811,\n",
       "   111.38537359237671,\n",
       "   111.94001293182373,\n",
       "   112.83915615081787,\n",
       "   112.35127592086792,\n",
       "   111.3226432800293,\n",
       "   112.30829763412476,\n",
       "   113.7316632270813,\n",
       "   114.1938362121582,\n",
       "   113.53371858596802,\n",
       "   112.64043188095093,\n",
       "   111.86469888687134,\n",
       "   112.98472833633423,\n",
       "   113.98050498962402,\n",
       "   114.49416542053223,\n",
       "   113.85627222061157,\n",
       "   114.20820426940918,\n",
       "   115.65081024169922,\n",
       "   115.97148418426514,\n",
       "   115.63699388504028,\n",
       "   114.0091438293457,\n",
       "   113.66301584243774,\n",
       "   113.6535873413086,\n",
       "   114.18891048431396,\n",
       "   112.67870950698853,\n",
       "   112.6341233253479,\n",
       "   111.43771266937256,\n",
       "   112.2159628868103,\n",
       "   111.79779624938965,\n",
       "   113.31720209121704,\n",
       "   111.83449172973633,\n",
       "   111.17846298217773,\n",
       "   110.09584617614746,\n",
       "   111.59605884552002,\n",
       "   113.11377620697021,\n",
       "   114.031822681427,\n",
       "   113.89437055587769,\n",
       "   113.75789737701416,\n",
       "   114.33886003494263,\n",
       "   113.78907442092896,\n",
       "   111.7005500793457,\n",
       "   111.41556310653687,\n",
       "   110.00864744186401,\n",
       "   109.71254396438599,\n",
       "   111.19235467910767,\n",
       "   108.33852910995483,\n",
       "   108.66313934326172,\n",
       "   109.20689582824707,\n",
       "   108.44549894332886,\n",
       "   109.52985191345215,\n",
       "   112.19338655471802,\n",
       "   112.78918409347534,\n",
       "   110.65912675857544,\n",
       "   111.8543496131897,\n",
       "   110.90630149841309,\n",
       "   110.10979652404785,\n",
       "   107.8914122581482,\n",
       "   108.34408473968506,\n",
       "   107.59090805053711,\n",
       "   106.38006973266602,\n",
       "   105.6005449295044,\n",
       "   106.23926019668579,\n",
       "   106.97648668289185,\n",
       "   108.10660314559937],\n",
       "  'pce_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -1256.9942650492217,\n",
       "   -245.15146148777094,\n",
       "   -99.41497555467987,\n",
       "   -51.46364087700984,\n",
       "   -35.02855491261108,\n",
       "   -26.42852376845403,\n",
       "   -18.54819760775506,\n",
       "   -13.502562219123126,\n",
       "   -10.80463559822323,\n",
       "   -8.574270419546467,\n",
       "   -7.10456039480227,\n",
       "   -5.735254564780303,\n",
       "   -5.185928323653397,\n",
       "   -5.089644022791837,\n",
       "   -4.984705703803514,\n",
       "   -4.621303819559189,\n",
       "   -4.563779576468845,\n",
       "   -4.418365459019299,\n",
       "   -4.3244984957121595,\n",
       "   -4.0804522664507665,\n",
       "   -4.039704051751813,\n",
       "   -4.435713424227273,\n",
       "   -4.4741358270736615,\n",
       "   -4.444311134000093,\n",
       "   -4.89394903039578,\n",
       "   -5.132812383838449,\n",
       "   -5.079498210013265,\n",
       "   -5.239685139968942,\n",
       "   -5.223101061702344,\n",
       "   -5.940898848557482,\n",
       "   -6.157352100480457,\n",
       "   -6.523707605498228,\n",
       "   -6.800874917216215,\n",
       "   -6.9516502813771055,\n",
       "   -7.653320540719561,\n",
       "   -8.488050081844309,\n",
       "   -9.383014917398146,\n",
       "   -10.121049667101405,\n",
       "   -10.809560170754697,\n",
       "   -12.002363494791991,\n",
       "   -12.331122890280923,\n",
       "   -12.62658078031003,\n",
       "   -13.142664017972358,\n",
       "   -13.95649677079821,\n",
       "   -14.457861463036604,\n",
       "   -14.973441352618506,\n",
       "   -14.874635969831079,\n",
       "   -14.819380710847184,\n",
       "   -14.681225690116772,\n",
       "   -14.588725178094165,\n",
       "   -14.339162806225726,\n",
       "   -14.249036170679434,\n",
       "   -14.460746367166367,\n",
       "   -14.464611828821678,\n",
       "   -14.61992436945698,\n",
       "   -14.882151125167848,\n",
       "   -15.27287120037727,\n",
       "   -15.142671100834583,\n",
       "   -15.00907611195414,\n",
       "   -14.946292972411888,\n",
       "   -14.923286861353938,\n",
       "   -14.65544223898517,\n",
       "   -14.30779505067846,\n",
       "   -14.349999512410687,\n",
       "   -13.822740230755,\n",
       "   -13.050698528579234,\n",
       "   -12.439520289138468,\n",
       "   -11.956058099650226,\n",
       "   -12.263637003113194,\n",
       "   -12.610941601951287,\n",
       "   -12.26705774494477,\n",
       "   -11.79893958418246,\n",
       "   -11.597708047875885,\n",
       "   -11.130992502374456,\n",
       "   -10.595027916843666,\n",
       "   -9.92188415733508,\n",
       "   -9.770254950670576,\n",
       "   -9.727330317252,\n",
       "   -9.673080517228025,\n",
       "   -9.612339620433826,\n",
       "   -9.256855563276355,\n",
       "   -9.20094497415529,\n",
       "   -9.223930922404634,\n",
       "   -9.614418739766625,\n",
       "   -9.454557346392157,\n",
       "   -9.806193970311606,\n",
       "   -10.278075015577132,\n",
       "   -10.218030942672145,\n",
       "   -10.152722379695815,\n",
       "   -9.706864556996615,\n",
       "   -10.104537164744555,\n",
       "   -10.505087687003243,\n",
       "   -10.460493087964663,\n",
       "   -10.864510725401628,\n",
       "   -10.659172498747004,\n",
       "   -11.17102403348667,\n",
       "   -11.75894026877132,\n",
       "   -12.048124874880541,\n",
       "   -12.628081799448573,\n",
       "   -13.159608385343489,\n",
       "   -13.708101701990984,\n",
       "   -13.924767677267731,\n",
       "   -14.139080937720287,\n",
       "   -13.996319711001155,\n",
       "   -14.644876741820617,\n",
       "   -14.754298365819071,\n",
       "   -14.221207122429494,\n",
       "   -14.431898056690208,\n",
       "   -14.438572906207977,\n",
       "   -14.211398067590672,\n",
       "   -13.806371604244477,\n",
       "   -13.909853335178337,\n",
       "   -13.613849791153624,\n",
       "   -13.555023871322874,\n",
       "   -14.048858351154637,\n",
       "   -13.750334045024598,\n",
       "   -13.12319956874342,\n",
       "   -12.97959309116315,\n",
       "   -13.292499239227071,\n",
       "   -12.656403778139817,\n",
       "   -11.899668141402586,\n",
       "   -11.691193116393567,\n",
       "   -11.206008170557674,\n",
       "   -11.112180351688155,\n",
       "   -11.361033804037563,\n",
       "   -10.603901119663856,\n",
       "   -10.985547851074257,\n",
       "   -10.083898512773969,\n",
       "   -10.025885629489213,\n",
       "   -10.033399478151042,\n",
       "   -9.959761336754555,\n",
       "   -9.818584699738114,\n",
       "   -9.765593619348982,\n",
       "   -10.413071580359174,\n",
       "   -10.535062990743038,\n",
       "   -9.821937722958552,\n",
       "   -10.017518889157767,\n",
       "   -10.31852960087924,\n",
       "   -10.559414270883526,\n",
       "   -10.70745275942303,\n",
       "   -10.72847964143414,\n",
       "   -11.491602953459246,\n",
       "   -11.573728317667593,\n",
       "   -11.650920410177033,\n",
       "   -11.833434461180612,\n",
       "   -11.953018230486773,\n",
       "   -12.275093542655027,\n",
       "   -12.573283181750371,\n",
       "   -12.137610439268698,\n",
       "   -11.645350802738689,\n",
       "   -11.485012144588737,\n",
       "   -11.432165875073117,\n",
       "   -10.713333617824427,\n",
       "   -10.930252336575258,\n",
       "   -10.242732199148797,\n",
       "   -10.099360943018018,\n",
       "   -10.043154281107896,\n",
       "   -10.025774589268286,\n",
       "   -9.960099082940394,\n",
       "   -9.670149865601074,\n",
       "   -9.448460898901372,\n",
       "   -9.623115703667082,\n",
       "   -10.34784868740085,\n",
       "   -9.870483117697901,\n",
       "   -9.941662521991152,\n",
       "   -10.307262013126044,\n",
       "   -10.46565753432271,\n",
       "   -10.162503722267685,\n",
       "   -10.14212168075556,\n",
       "   -10.04205109158371,\n",
       "   -10.481748046252765,\n",
       "   -10.614410533977425,\n",
       "   -10.300743330918593,\n",
       "   -10.44301640436012,\n",
       "   -10.588631368328574,\n",
       "   -10.558980824236848,\n",
       "   -10.408261312919157,\n",
       "   -10.69846432067592,\n",
       "   -10.401413202386339,\n",
       "   -10.147058447634535,\n",
       "   -10.163339965304468,\n",
       "   -10.25459653104828,\n",
       "   -10.611578587027552,\n",
       "   -10.089395380979502,\n",
       "   -10.207466787801634,\n",
       "   -10.01565063313866,\n",
       "   -9.739863738104058,\n",
       "   -9.464024945242167,\n",
       "   -9.21960345834702,\n",
       "   -8.990159394269657,\n",
       "   -9.14794371290301,\n",
       "   -8.906108685998174,\n",
       "   -8.8559372932913,\n",
       "   -8.8157576294149,\n",
       "   -9.166491897116847,\n",
       "   -9.245856016059937,\n",
       "   -9.388172346922072,\n",
       "   -9.630053701000177,\n",
       "   -9.77967750352422,\n",
       "   -9.796466430433112,\n",
       "   -9.924353793880755,\n",
       "   -9.894621301132982,\n",
       "   -9.937107811133748,\n",
       "   -10.160961486400309,\n",
       "   -10.271146574437944,\n",
       "   -10.438965107176147,\n",
       "   -10.458670125518363,\n",
       "   -10.640608809412203,\n",
       "   -10.686242400241122,\n",
       "   -10.547128430279793,\n",
       "   -10.59548657089247,\n",
       "   -10.722494497977253,\n",
       "   -10.664619730175927,\n",
       "   -10.843974348365837,\n",
       "   -10.608084336537576,\n",
       "   -10.421424591443634,\n",
       "   -10.500325135350392,\n",
       "   -10.551061200706624,\n",
       "   -10.571789210273026,\n",
       "   -10.205553628776835,\n",
       "   -9.942887234185166,\n",
       "   -9.73825636050364,\n",
       "   -9.566741402625619,\n",
       "   -9.478231796136093,\n",
       "   -9.459836818126547,\n",
       "   -9.496813329015733,\n",
       "   -9.636975466707103,\n",
       "   -9.70575139914452,\n",
       "   -9.513937209863778,\n",
       "   -9.312586799286239,\n",
       "   -9.710588778966233,\n",
       "   -9.656928621259853,\n",
       "   -9.703447442896485,\n",
       "   -9.738603021186613,\n",
       "   -9.523266969505713,\n",
       "   -9.484980830538255,\n",
       "   -9.518003813238458,\n",
       "   -9.44881304188331,\n",
       "   -9.150099391949835,\n",
       "   -9.02722750738216,\n",
       "   -9.07710423089413,\n",
       "   -8.958080465140423,\n",
       "   -8.999076203489453,\n",
       "   -9.01935797149842,\n",
       "   -8.900991365064861,\n",
       "   -9.018330212768781,\n",
       "   -9.289723952849279,\n",
       "   -9.154874453624581,\n",
       "   -9.135182531570152,\n",
       "   -9.404506169070896,\n",
       "   -9.382448362047063,\n",
       "   -9.351868289441263,\n",
       "   -9.421861040974836,\n",
       "   -9.501051518772892,\n",
       "   -9.36171124763773,\n",
       "   -9.404701874906813,\n",
       "   -8.948127474998584,\n",
       "   -8.80630907088925,\n",
       "   -8.655935605299469,\n",
       "   -8.71324311618083,\n",
       "   -8.937564978341564,\n",
       "   -8.721744397646919,\n",
       "   -8.499280472569716,\n",
       "   -8.397589224864543,\n",
       "   -8.31592655666615,\n",
       "   -8.165526390507413,\n",
       "   -8.260593847903408,\n",
       "   -8.210411200925954,\n",
       "   -8.196760920500461,\n",
       "   -8.073973223824567,\n",
       "   -8.001460090132564,\n",
       "   -8.352575755376577,\n",
       "   -8.16653451435824,\n",
       "   -7.93553477456166,\n",
       "   -7.950627617289298,\n",
       "   -8.262745196102633,\n",
       "   -8.211195284222384,\n",
       "   -8.337849824728085,\n",
       "   -8.585867557109662,\n",
       "   -9.054609611275927,\n",
       "   -9.080822281073761,\n",
       "   -8.774673201302324,\n",
       "   -8.638859090532195,\n",
       "   -8.613078828250611,\n",
       "   -8.881176004932158,\n",
       "   -9.010280028403914,\n",
       "   -8.755104418371054,\n",
       "   -8.844582837854961,\n",
       "   -9.173224995664121,\n",
       "   -9.357726651041155,\n",
       "   -9.597977326513444,\n",
       "   -9.615898754516172,\n",
       "   -9.702872185335668,\n",
       "   -9.805258719611574,\n",
       "   -9.84612361017052,\n",
       "   -9.751672196957145,\n",
       "   -9.562485372902463,\n",
       "   -9.369261756563223,\n",
       "   -9.287379354177546,\n",
       "   -9.450978239929656,\n",
       "   -9.603742134517573,\n",
       "   -9.680071405003446,\n",
       "   -10.031198023798602,\n",
       "   -10.146728511564298,\n",
       "   -10.192288059021648,\n",
       "   -10.198416342074884,\n",
       "   -10.208346076870617,\n",
       "   -10.230653183682671,\n",
       "   -10.39902987614388,\n",
       "   -10.052925063821329,\n",
       "   -10.017355871530276,\n",
       "   -10.06798295834915,\n",
       "   -10.056544530964771,\n",
       "   -10.659015351493522,\n",
       "   -10.329207276645631,\n",
       "   -10.491950088824382,\n",
       "   -10.352135689348609,\n",
       "   -9.86068346099443,\n",
       "   -9.860308195163498,\n",
       "   -9.958178915109253,\n",
       "   -10.171308203904918,\n",
       "   -9.715525960951897,\n",
       "   -9.422080538002707,\n",
       "   -9.394525237037993,\n",
       "   -9.52759254649805,\n",
       "   -9.439923917134424,\n",
       "   -9.279810821062297,\n",
       "   -9.397869828433443,\n",
       "   -9.066125279951324,\n",
       "   -9.095909781766855,\n",
       "   -8.686424617826448,\n",
       "   -8.88041452720983,\n",
       "   -8.640951962222173,\n",
       "   -8.672810628365944,\n",
       "   -8.606618951249928,\n",
       "   -8.46181828404406,\n",
       "   -8.89317496250163,\n",
       "   -8.99656211031811,\n",
       "   -9.082650363044246,\n",
       "   -8.998168413033572,\n",
       "   -9.109349512937682,\n",
       "   -8.867125947049466,\n",
       "   -8.628244678359238,\n",
       "   -8.45473834725482,\n",
       "   -8.61107551621034,\n",
       "   -8.59725908454756,\n",
       "   -8.640652876449701,\n",
       "   -8.95877470306418,\n",
       "   -8.972826383124827,\n",
       "   -8.930697162852493,\n",
       "   -8.932058795019799,\n",
       "   -8.787353452771844,\n",
       "   -8.549695707033012,\n",
       "   -8.33403572706018,\n",
       "   -8.678597397626083,\n",
       "   -8.523030807286649,\n",
       "   -8.53676474008215,\n",
       "   -8.28547939828633,\n",
       "   -8.39762684730218,\n",
       "   -8.4677107889583,\n",
       "   -8.315893753366982,\n",
       "   -8.43761896404044,\n",
       "   -8.382561368332079,\n",
       "   -8.514803337364675,\n",
       "   -8.411715842519238,\n",
       "   -8.459388089779914,\n",
       "   -8.160774347572177,\n",
       "   -8.278535323171063,\n",
       "   -8.338516838587733,\n",
       "   -8.290143844593684,\n",
       "   -8.097546431356534,\n",
       "   -7.981393906099251,\n",
       "   -8.357540451452143,\n",
       "   -8.689974659742541,\n",
       "   -8.59022442326747,\n",
       "   -8.285757317693836,\n",
       "   -8.196777183786153,\n",
       "   -8.298431745496888,\n",
       "   -8.097589392244593,\n",
       "   -8.070409782958437,\n",
       "   -8.051876504637594,\n",
       "   -8.280891311038166,\n",
       "   -8.18314216087172,\n",
       "   -8.198241652049914,\n",
       "   -8.201824831129267,\n",
       "   -8.257526041131877,\n",
       "   -8.644518267971945,\n",
       "   -8.742760571996085,\n",
       "   -8.676552064712856,\n",
       "   -8.841457956846957,\n",
       "   -8.99848620002841,\n",
       "   -8.887678295527111,\n",
       "   -8.957664289787852,\n",
       "   -9.257670693160266,\n",
       "   -9.575099184998107,\n",
       "   -10.1991537420327,\n",
       "   -9.934918967012274,\n",
       "   -10.073568718938182,\n",
       "   -10.162801350839494,\n",
       "   -10.735196166427682,\n",
       "   -10.414730257158348,\n",
       "   -10.062164220125368,\n",
       "   -10.161980240590273,\n",
       "   -10.19969018460936,\n",
       "   -9.980533034384028,\n",
       "   -9.82520393527364,\n",
       "   -9.71448594368296,\n",
       "   -9.680704707491476,\n",
       "   -9.567981793949036,\n",
       "   -9.687663902098327,\n",
       "   -9.575345870849873,\n",
       "   -9.601831639347894,\n",
       "   -9.452510856630209,\n",
       "   -8.988158664779522,\n",
       "   -8.690328222653836,\n",
       "   -8.404977160550537,\n",
       "   -8.203733803955,\n",
       "   -8.36431320936648,\n",
       "   -8.556878273309692,\n",
       "   -8.961078332812539,\n",
       "   -8.926594926971699,\n",
       "   -8.392636162678171,\n",
       "   -8.712833010087055,\n",
       "   -8.992309135251721,\n",
       "   -8.902065788405089,\n",
       "   -8.71132717092662,\n",
       "   -8.956820099375655,\n",
       "   -9.14505333536926,\n",
       "   -9.198558990034659,\n",
       "   -9.35253412638707,\n",
       "   -9.466404963622605,\n",
       "   -9.70541960566886,\n",
       "   -9.311025643984975,\n",
       "   -9.15127616215929,\n",
       "   -9.219097153138627,\n",
       "   -8.878239401500368,\n",
       "   -9.107575989099777,\n",
       "   -9.28045228462875,\n",
       "   -9.509012715173483,\n",
       "   -9.751296578312822,\n",
       "   -9.534075017605877,\n",
       "   -9.37638244425564,\n",
       "   -9.452090501723978,\n",
       "   -9.580034389021254,\n",
       "   -9.602531771798274,\n",
       "   -9.757250315980869,\n",
       "   -9.7488264406928,\n",
       "   -9.895825435295531,\n",
       "   -9.928173629070788,\n",
       "   -10.002554401332677,\n",
       "   -10.413222024010766,\n",
       "   -9.750179024900925,\n",
       "   -9.306679810019975,\n",
       "   -9.058163018262528,\n",
       "   -8.833075928022014,\n",
       "   -8.923336039815782,\n",
       "   -8.849232699978744,\n",
       "   -9.066877351797253,\n",
       "   -9.078945429615176,\n",
       "   -9.04818021566308,\n",
       "   -9.322071069356497,\n",
       "   -9.14465467382313,\n",
       "   -9.02810408138329,\n",
       "   -8.967249036874477,\n",
       "   -9.386271228118272,\n",
       "   -9.296104742138398,\n",
       "   -9.594388470550754,\n",
       "   -9.568684326526654,\n",
       "   -9.052624621526691,\n",
       "   -9.133689198904474,\n",
       "   -8.78987157979318,\n",
       "   -8.809449555096547,\n",
       "   -8.993320045204594,\n",
       "   -9.226234403634141,\n",
       "   -8.957952470572678,\n",
       "   -9.196285767763488,\n",
       "   -9.310728843633663,\n",
       "   -9.314927627780971,\n",
       "   -9.752529851687166,\n",
       "   -10.03804759955543,\n",
       "   -9.834753488046877,\n",
       "   -10.008309244353253,\n",
       "   -9.818175437299823,\n",
       "   -9.530499267696728,\n",
       "   -9.772590764340295,\n",
       "   -10.01069318339806,\n",
       "   -10.311571836400425,\n",
       "   -10.425937114232916,\n",
       "   -10.359135335114019,\n",
       "   -10.193566766414918,\n",
       "   -10.241795222101958,\n",
       "   -10.371441018646548,\n",
       "   -10.384846066102181],\n",
       "  'voc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -587.9678474937068,\n",
       "   -78.82965300558315,\n",
       "   -22.200222119306638,\n",
       "   -8.901077537328746,\n",
       "   -5.198066264423792,\n",
       "   -2.793928174066954,\n",
       "   -1.1909300891665708,\n",
       "   -0.46925967363398136,\n",
       "   -0.4558648784455368,\n",
       "   -1.1263695431389853,\n",
       "   -2.2752922884985614,\n",
       "   -4.20702266491407,\n",
       "   -4.852504650238649,\n",
       "   -5.747363300052661,\n",
       "   -7.701300353681809,\n",
       "   -10.212030993241113,\n",
       "   -11.208978893060536,\n",
       "   -15.12095429330352,\n",
       "   -17.77834344334687,\n",
       "   -16.86501632989019,\n",
       "   -15.94692209452348,\n",
       "   -15.467658367949785,\n",
       "   -13.595825140550241,\n",
       "   -13.065718185733024,\n",
       "   -11.435983972687794,\n",
       "   -8.554989695295486,\n",
       "   -5.048928371105227,\n",
       "   -5.151831678463026,\n",
       "   -4.650388489389639,\n",
       "   -4.158639532507698,\n",
       "   -3.3617042790265117,\n",
       "   -3.1823921074122676,\n",
       "   -2.478768588094321,\n",
       "   -2.2054643667668596,\n",
       "   -2.0521131400959365,\n",
       "   -2.0140928528050464,\n",
       "   -2.141514161918625,\n",
       "   -2.3013566764153546,\n",
       "   -2.509063656809857,\n",
       "   -2.9179172622791856,\n",
       "   -2.755328408061656,\n",
       "   -2.4137426363134438,\n",
       "   -2.08542895999183,\n",
       "   -1.6771745061090932,\n",
       "   -1.7700247367643347,\n",
       "   -1.5370680353160355,\n",
       "   -1.768401748145684,\n",
       "   -1.9798844302169205,\n",
       "   -2.029152373147661,\n",
       "   -1.4256975110219519,\n",
       "   -1.2639126882724017,\n",
       "   -0.9968534458580494,\n",
       "   -0.9123646491779673,\n",
       "   -0.7764506487994254,\n",
       "   -0.7126822758051523,\n",
       "   -0.7763607650433062,\n",
       "   -0.7667709243941812,\n",
       "   -0.6570785746961736,\n",
       "   -0.4981641800583685,\n",
       "   -0.5124286539064413,\n",
       "   -0.4615262729685461,\n",
       "   -0.4530429281185855,\n",
       "   -0.4350312353546666,\n",
       "   -0.40148750722065807,\n",
       "   -0.4034065481055591,\n",
       "   -0.3355576392731692,\n",
       "   -0.2976577620526617,\n",
       "   -0.3398787342440157,\n",
       "   -0.39861635778593163,\n",
       "   -0.46909098542435856,\n",
       "   -0.4365752927668187,\n",
       "   -0.32611688181728415,\n",
       "   -0.32339716253126594,\n",
       "   -0.2624584020011118,\n",
       "   -0.3389507956460398,\n",
       "   -0.26477213514707665,\n",
       "   -0.23289059151456248,\n",
       "   -0.1913329617980053,\n",
       "   -0.23237041488327148,\n",
       "   -0.30311372181731744,\n",
       "   -0.28075284427473846,\n",
       "   -0.3306138110630865,\n",
       "   -0.3492820718651741,\n",
       "   -0.3919907018363207,\n",
       "   -0.40568799448350146,\n",
       "   -0.2425092686557333,\n",
       "   -0.2488455257415716,\n",
       "   -0.26558962148459675,\n",
       "   -0.19057023362851933,\n",
       "   -0.14679048375072545,\n",
       "   -0.19766092740077745,\n",
       "   -0.19511496102295833,\n",
       "   -0.14381863225783809,\n",
       "   -0.07486771054372898,\n",
       "   -0.041955750087279364,\n",
       "   -0.027425696758789364,\n",
       "   -0.0229068140084403,\n",
       "   -0.08122686963326986,\n",
       "   -0.1184915359691403,\n",
       "   -0.1325426038798918,\n",
       "   -0.05505988754555613,\n",
       "   -0.08388287006410633,\n",
       "   -0.09671993164869885,\n",
       "   -0.08108152982261263,\n",
       "   -0.2060360095048801,\n",
       "   -0.16265796882640915,\n",
       "   -0.13174642110093582,\n",
       "   -0.08479660881112494,\n",
       "   -0.0369196966229246,\n",
       "   -0.06176839144089019,\n",
       "   -0.04872938154794193,\n",
       "   -0.1613965552260388,\n",
       "   -0.1600423366615682,\n",
       "   -0.14604332086988459,\n",
       "   -0.11330514880640807,\n",
       "   -0.10080642942076667,\n",
       "   -0.12291103748904253,\n",
       "   -0.07609599178347315,\n",
       "   -0.027506733543384287,\n",
       "   -0.028372759641464018,\n",
       "   -0.018514698779308558,\n",
       "   -0.005003509183017396,\n",
       "   -0.018004029666514043,\n",
       "   -0.002488001052659339,\n",
       "   -0.029613708042051545,\n",
       "   0.012065693866387561,\n",
       "   0.022292560122933458,\n",
       "   0.01275186393144434,\n",
       "   0.010789993916727747,\n",
       "   0.032025168178333674,\n",
       "   -0.01462026262156746,\n",
       "   0.00816067255798314,\n",
       "   -0.01713363366730669,\n",
       "   -0.01784033035693877,\n",
       "   -0.07421441442462906,\n",
       "   -0.04204218253754477,\n",
       "   0.00977112722832496,\n",
       "   0.020162131717547882,\n",
       "   0.02355128616048674,\n",
       "   0.03943725010240762,\n",
       "   0.044678147034953763,\n",
       "   -0.00818698473940671,\n",
       "   0.029835927808753993,\n",
       "   0.05238647235747851,\n",
       "   0.04574446220005768,\n",
       "   0.0506836567028206,\n",
       "   0.0598620832454162,\n",
       "   0.05103805089537705,\n",
       "   0.04894891293412906,\n",
       "   0.05002524042031253,\n",
       "   0.050483188321166494,\n",
       "   0.06543440964644742,\n",
       "   0.07420120717391054,\n",
       "   0.07660071287089387,\n",
       "   0.08299466098296726,\n",
       "   0.07730941881405362,\n",
       "   0.0995378839915938,\n",
       "   0.10690394909341128,\n",
       "   0.1027427652207269,\n",
       "   0.09903715594797058,\n",
       "   0.10796135626299297,\n",
       "   0.10447921055030462,\n",
       "   0.10609429201247189,\n",
       "   0.08598836601833348,\n",
       "   0.09033626165897257,\n",
       "   0.07055651979163824,\n",
       "   0.08523978336418914,\n",
       "   0.09499590978217987,\n",
       "   0.10543849794636584,\n",
       "   0.1135429365916859,\n",
       "   0.10343582441073884,\n",
       "   0.11444734822495695,\n",
       "   0.10592225216943107,\n",
       "   0.11546912259512931,\n",
       "   0.0873366595112155,\n",
       "   0.07080448702788511,\n",
       "   0.09343343822612915,\n",
       "   0.0547725553106545,\n",
       "   0.05587024364041526,\n",
       "   0.01776725262026624,\n",
       "   0.09150377400098686,\n",
       "   0.09142718551125051,\n",
       "   0.09361023982381755,\n",
       "   0.0664787476571469,\n",
       "   0.08715815132163907,\n",
       "   0.1008194775438439,\n",
       "   0.09371869065793192,\n",
       "   0.06552525036117762,\n",
       "   0.09646309278694587,\n",
       "   0.10426192916628219,\n",
       "   0.10705277370044264,\n",
       "   0.10446388164655984,\n",
       "   0.09627911108172582,\n",
       "   0.10088104874207438,\n",
       "   0.08808772576918777,\n",
       "   0.06586013279253522,\n",
       "   0.04765569890943833,\n",
       "   0.05530097071742501,\n",
       "   0.0651096913844953,\n",
       "   0.07768819562479956,\n",
       "   -0.015020869788141766,\n",
       "   -0.1568537508997201,\n",
       "   -0.27663635209176385,\n",
       "   -0.1890824340855195,\n",
       "   -0.11083130449598633,\n",
       "   -0.10525424803792749,\n",
       "   -0.11558048624930506,\n",
       "   -0.16432929033444554,\n",
       "   -0.3140463534421849,\n",
       "   -0.44099493620511354,\n",
       "   -0.18840713272571952,\n",
       "   -0.0328456919057345,\n",
       "   -0.08712597795939181,\n",
       "   -0.08106343929910209,\n",
       "   -0.12839923597530878,\n",
       "   -0.17996808409971976,\n",
       "   -0.16178692482202495,\n",
       "   -0.27649265788996313,\n",
       "   -0.21205558202209307,\n",
       "   -0.14449917508736965,\n",
       "   -0.17734510438617335,\n",
       "   -0.17970903784734693,\n",
       "   -0.1376601033128455,\n",
       "   -0.20022922944554544,\n",
       "   -0.20697666597828812,\n",
       "   -0.1921414865666431,\n",
       "   -0.2386784761278471,\n",
       "   -0.2646702886425518,\n",
       "   -0.3069308272238982,\n",
       "   -0.13886234577432632,\n",
       "   -0.12944975779018097,\n",
       "   -0.1560880184791258,\n",
       "   -0.05202434032688341,\n",
       "   -0.04030738265411382,\n",
       "   -0.04050813960739452,\n",
       "   -0.13389528551639795,\n",
       "   -0.1277070616547713,\n",
       "   -0.21187835584976944,\n",
       "   -0.10378595928310741,\n",
       "   -0.08076359463540062,\n",
       "   -0.16500856890668691,\n",
       "   -0.1805646582098166,\n",
       "   -0.15220619103118627,\n",
       "   -0.12619747507988044,\n",
       "   -0.10943558068195913,\n",
       "   -0.09458152294641753,\n",
       "   -0.12325625353856706,\n",
       "   -0.1209230191411077,\n",
       "   -0.09139175537199629,\n",
       "   -0.15175844898770263,\n",
       "   -0.23533010444247404,\n",
       "   -0.17177717917300894,\n",
       "   -0.1440355474603825,\n",
       "   -0.10820741737948647,\n",
       "   -0.00035344173058238226,\n",
       "   -0.013293147075755574,\n",
       "   -0.019491352090643232,\n",
       "   -0.030377965745335578,\n",
       "   -0.10568013666602871,\n",
       "   -0.15894067073457196,\n",
       "   -0.09909674125122381,\n",
       "   -0.09934155438613845,\n",
       "   -0.19307392464837703,\n",
       "   -0.21189628196402888,\n",
       "   -0.10019144230792332,\n",
       "   -0.06622303795417683,\n",
       "   -0.09771682943325288,\n",
       "   -0.030756417882527254,\n",
       "   -0.13786377625869362,\n",
       "   -0.11410024613952707,\n",
       "   -0.04115993367199988,\n",
       "   -0.06087229232110847,\n",
       "   -0.12365957688419038,\n",
       "   -0.2475874107860343,\n",
       "   -0.2992236456724202,\n",
       "   -0.23906616141041992,\n",
       "   -0.3119515115740521,\n",
       "   -0.08517898958382641,\n",
       "   -0.05158119948475681,\n",
       "   -0.1023080605797917,\n",
       "   -0.18496780557710912,\n",
       "   -0.12892809654257653,\n",
       "   -0.19775915838416624,\n",
       "   -0.36082646516982875,\n",
       "   -0.32994035825991164,\n",
       "   -0.21476724570372885,\n",
       "   -0.19654434384127106,\n",
       "   -0.12056852951478092,\n",
       "   -0.26359854384284453,\n",
       "   -0.3320637059965117,\n",
       "   -0.24566040452751792,\n",
       "   -0.19046787098390316,\n",
       "   -0.11453477787060007,\n",
       "   -0.07310650364713278,\n",
       "   -0.01591578497795565,\n",
       "   0.045950728906833604,\n",
       "   0.052220633282296314,\n",
       "   0.06408118515927119,\n",
       "   0.014849412144266494,\n",
       "   0.011712409553974679,\n",
       "   0.05267614017282751,\n",
       "   0.013175455241978717,\n",
       "   0.011692104644468015,\n",
       "   0.07710899293625151,\n",
       "   0.09298179820743957,\n",
       "   0.10055264569079447,\n",
       "   0.1060028201960106,\n",
       "   0.10290253273084138,\n",
       "   0.09982043468964819,\n",
       "   0.0980725394801839,\n",
       "   0.10103373862292009,\n",
       "   0.09911223172790284,\n",
       "   0.08970461246711003,\n",
       "   0.10146167988384425,\n",
       "   0.09826430517177398,\n",
       "   0.0929575813354927,\n",
       "   0.07089902058111708,\n",
       "   0.03578097849430151,\n",
       "   0.02677864801022023,\n",
       "   0.04284958005457107,\n",
       "   0.046975368617333535,\n",
       "   0.06571078414727016,\n",
       "   0.07798251169255277,\n",
       "   0.08057067366851811,\n",
       "   0.08768752198004415,\n",
       "   0.0809560542368628,\n",
       "   0.08247118860971625,\n",
       "   0.0777690634671232,\n",
       "   0.0724647724870231,\n",
       "   0.06684771431607206,\n",
       "   0.05861389873980816,\n",
       "   0.03155602655256817,\n",
       "   -0.0007545301013140993,\n",
       "   0.01624786795610389,\n",
       "   0.009463810762737168,\n",
       "   0.0039013145630111268,\n",
       "   0.03674338835396773,\n",
       "   0.021445670960071106,\n",
       "   0.01838627382662239,\n",
       "   0.06769194973890658,\n",
       "   0.07934234840130538,\n",
       "   0.08177279528332315,\n",
       "   0.08705910533243766,\n",
       "   0.08451485111936052,\n",
       "   0.08450135974700801,\n",
       "   0.0815968363587124,\n",
       "   0.08347356155357888,\n",
       "   0.09235832191430982,\n",
       "   0.10207102051627637,\n",
       "   0.08587100005369541,\n",
       "   0.07939219540013143,\n",
       "   0.09248336788991851,\n",
       "   0.08625106180927511,\n",
       "   0.0964290113750016,\n",
       "   0.10158502981223838,\n",
       "   0.09594419334876902,\n",
       "   0.09629357328877652,\n",
       "   0.10560167416945532,\n",
       "   0.1099669871600567,\n",
       "   0.10381046690484685,\n",
       "   0.10622214302853084,\n",
       "   0.11467812796450683,\n",
       "   0.10967598672433287,\n",
       "   0.1091202313035573,\n",
       "   0.10878224502891964,\n",
       "   0.09496845731569636,\n",
       "   0.09354757644447442,\n",
       "   0.08458694489122243,\n",
       "   0.09332038824628264,\n",
       "   0.08259273840747572,\n",
       "   0.03836553014917454,\n",
       "   0.06435868491381025,\n",
       "   0.044007527967031224,\n",
       "   0.039393514556896325,\n",
       "   0.06552826486864805,\n",
       "   0.054091508300397884,\n",
       "   0.058992939039906656,\n",
       "   0.07555737162618037,\n",
       "   0.07079878517660831,\n",
       "   0.07560326331978962,\n",
       "   0.07878674712252687,\n",
       "   0.10192359125161343,\n",
       "   0.11213991275651314,\n",
       "   0.11001009432957343,\n",
       "   0.10936666635638304,\n",
       "   0.10449207918524261,\n",
       "   0.05777590003258848,\n",
       "   0.09548728076664781,\n",
       "   0.09664546432735777,\n",
       "   0.0975471448897891,\n",
       "   0.11130386729798014,\n",
       "   0.10895968492636987,\n",
       "   0.10579470361558863,\n",
       "   0.12480132979493153,\n",
       "   0.09520872830604743,\n",
       "   0.032200520507332575,\n",
       "   0.02475400255222404,\n",
       "   0.043241615589057925,\n",
       "   0.0830923968787185,\n",
       "   0.08649692550333088,\n",
       "   0.0772939884608439,\n",
       "   0.09248219001694202,\n",
       "   0.05557890882248284,\n",
       "   0.05753494137813531,\n",
       "   0.039864655676530636,\n",
       "   0.04868673087288278,\n",
       "   0.038676806321967905,\n",
       "   0.06752510043457827,\n",
       "   0.08532158077803287,\n",
       "   0.07676332111024842,\n",
       "   0.028449445593594858,\n",
       "   0.035828115887163725,\n",
       "   -0.04629286294637702,\n",
       "   -0.11641314096805133,\n",
       "   -0.2440488731544408,\n",
       "   -0.2166722758780839,\n",
       "   -0.18643254833333334,\n",
       "   -0.11426506859550245,\n",
       "   -0.15565877701660757,\n",
       "   -0.1398456837539208,\n",
       "   -0.2063478942920398,\n",
       "   -0.3231186898283407,\n",
       "   -0.21866624981286154,\n",
       "   -0.20381010244496833,\n",
       "   -0.34165645151361046,\n",
       "   -0.31160208092929853,\n",
       "   -0.1594966157460096,\n",
       "   -0.22526212256822387,\n",
       "   -0.3493232794369112,\n",
       "   -0.49607221685554204,\n",
       "   -0.6116256469354466,\n",
       "   -0.2218317067244462,\n",
       "   -0.21225098323743707,\n",
       "   -0.2720078821459533,\n",
       "   -0.25843998706277316,\n",
       "   -0.14023671853430364,\n",
       "   -0.07652912337456841,\n",
       "   -0.14499076821698687,\n",
       "   -0.16498601777365973,\n",
       "   -0.16787363555368007,\n",
       "   -0.14116638174616192,\n",
       "   -0.13965044724853648,\n",
       "   -0.13818119313302812,\n",
       "   -0.13991543743778845,\n",
       "   -0.18927225669668846,\n",
       "   -0.2030954484549734,\n",
       "   -0.21352089317612366,\n",
       "   -0.07138713624033599,\n",
       "   -0.0847892454684509,\n",
       "   -0.14515302221787163,\n",
       "   -0.21228539697341287,\n",
       "   -0.223338750050599,\n",
       "   -0.27793684774745575,\n",
       "   -0.3019045916333414,\n",
       "   -0.3056836841011268,\n",
       "   -0.34638832772631134,\n",
       "   -0.30069063380000816,\n",
       "   -0.13681208376646814,\n",
       "   -0.13175280299904757,\n",
       "   -0.0013038951378130381,\n",
       "   -0.013441511017315833,\n",
       "   0.0030408933665110327,\n",
       "   -0.132960132371174,\n",
       "   -0.1942101852268201,\n",
       "   -0.1704636085883977,\n",
       "   -0.1707926770374053,\n",
       "   -0.16924342720032115,\n",
       "   -0.1323612599588515,\n",
       "   -0.09628315168854185,\n",
       "   -0.0972145799907056,\n",
       "   -0.10851022097416396,\n",
       "   -0.08184630877800125,\n",
       "   -0.013678171203838074,\n",
       "   0.009032434969086345,\n",
       "   0.0513132519828684,\n",
       "   -0.012625941793999562,\n",
       "   0.026971683402363666,\n",
       "   0.06027258238498845,\n",
       "   0.07558715721436648,\n",
       "   0.08631015196689595,\n",
       "   0.0734511429528012,\n",
       "   0.0697591999860373,\n",
       "   0.060838901181585636,\n",
       "   0.0694916618842748,\n",
       "   0.06905302742324104,\n",
       "   0.06242619452285425,\n",
       "   0.05558211785717371,\n",
       "   0.051399445986953896,\n",
       "   0.05015898406693209,\n",
       "   0.05184600193759159,\n",
       "   0.053836248101582274,\n",
       "   0.05438196228441061,\n",
       "   0.052213870782979566,\n",
       "   0.04899612232506423,\n",
       "   0.0329392454332148,\n",
       "   0.032879469020494545,\n",
       "   0.049496313932814395],\n",
       "  'jsc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -138794.7114059863,\n",
       "   -21788.107321336414,\n",
       "   -8601.663183715633,\n",
       "   -3729.5230940448027,\n",
       "   -1879.6951221575023,\n",
       "   -840.9369910396374,\n",
       "   -552.0568025057718,\n",
       "   -660.5306582299664,\n",
       "   -355.8355512903635,\n",
       "   -248.83034814204547,\n",
       "   -196.55720421332543,\n",
       "   -174.6374928233941,\n",
       "   -134.17706766832418,\n",
       "   -128.9063736047583,\n",
       "   -127.67084021312996,\n",
       "   -132.64735220998298,\n",
       "   -136.1692468276662,\n",
       "   -133.78842644411876,\n",
       "   -133.60104298316978,\n",
       "   -136.21886475917216,\n",
       "   -135.71623199533346,\n",
       "   -135.69038054236267,\n",
       "   -139.35012538710814,\n",
       "   -121.50540659799778,\n",
       "   -118.86366412970786,\n",
       "   -121.34491901324441,\n",
       "   -116.95981258553932,\n",
       "   -108.35038389576613,\n",
       "   -95.26207219286964,\n",
       "   -91.6275668613023,\n",
       "   -86.34068422506617,\n",
       "   -86.22719796823306,\n",
       "   -83.35749242697477,\n",
       "   -77.15833166532812,\n",
       "   -71.42134446510113,\n",
       "   -71.42305741242818,\n",
       "   -68.11766515220772,\n",
       "   -64.9779828168784,\n",
       "   -65.77587918135077,\n",
       "   -65.0290198152511,\n",
       "   -64.92661636684792,\n",
       "   -62.26773581743804,\n",
       "   -61.24561243364489,\n",
       "   -56.76939036492758,\n",
       "   -50.22089620235728,\n",
       "   -47.536988119616744,\n",
       "   -44.08877757842259,\n",
       "   -43.20587242797072,\n",
       "   -41.579671018560425,\n",
       "   -37.33672273060852,\n",
       "   -33.028379795133624,\n",
       "   -32.518823891664866,\n",
       "   -29.005130912665685,\n",
       "   -28.659449598809736,\n",
       "   -29.43096188684117,\n",
       "   -28.38891213179503,\n",
       "   -27.954106115656124,\n",
       "   -25.986315183738856,\n",
       "   -24.010193198860442,\n",
       "   -22.90595945471777,\n",
       "   -22.83967853390127,\n",
       "   -22.297433289171504,\n",
       "   -21.244839286800968,\n",
       "   -21.04861056038378,\n",
       "   -19.296532679648696,\n",
       "   -18.44199325873816,\n",
       "   -17.7567629370062,\n",
       "   -18.200946385571044,\n",
       "   -16.754584596441244,\n",
       "   -15.237560360161591,\n",
       "   -14.650472339345928,\n",
       "   -14.663096745629682,\n",
       "   -14.502099346476102,\n",
       "   -14.148279824021815,\n",
       "   -13.868317322267938,\n",
       "   -13.11701336942575,\n",
       "   -12.287901735316712,\n",
       "   -10.738272926837492,\n",
       "   -10.007086425708243,\n",
       "   -9.34538940299537,\n",
       "   -8.935996966924893,\n",
       "   -8.190904768509583,\n",
       "   -7.685037606351864,\n",
       "   -7.18709133948615,\n",
       "   -7.189498592293106,\n",
       "   -6.580589806531598,\n",
       "   -6.44542907711406,\n",
       "   -6.764998521926769,\n",
       "   -6.119222749634996,\n",
       "   -5.561840301659715,\n",
       "   -5.357352479638243,\n",
       "   -5.378475224716469,\n",
       "   -5.099460526066187,\n",
       "   -5.165662966947863,\n",
       "   -4.935564029704792,\n",
       "   -4.77915313689043,\n",
       "   -4.494285545591972,\n",
       "   -4.291321449894681,\n",
       "   -4.293576300262689,\n",
       "   -4.617047546403612,\n",
       "   -4.585288321427289,\n",
       "   -4.3752815436503,\n",
       "   -4.2156813741486365,\n",
       "   -4.100026663908095,\n",
       "   -3.8342580461626623,\n",
       "   -3.694818447167595,\n",
       "   -3.6209607927460263,\n",
       "   -3.2689605141583797,\n",
       "   -3.2224690081003198,\n",
       "   -3.206430122703927,\n",
       "   -3.3492235927310263,\n",
       "   -3.1803131464571193,\n",
       "   -3.1193480026552756,\n",
       "   -2.984864544635303,\n",
       "   -3.105338648975322,\n",
       "   -3.0441756121709913,\n",
       "   -2.804996684108358,\n",
       "   -2.821814902406687,\n",
       "   -2.931555050917578,\n",
       "   -2.7264196067408104,\n",
       "   -2.6530480343015044,\n",
       "   -2.6089148711862107,\n",
       "   -2.5504167458600238,\n",
       "   -2.520229914576374,\n",
       "   -2.366190771500813,\n",
       "   -2.324907948512617,\n",
       "   -2.2553134683116225,\n",
       "   -2.177201517730918,\n",
       "   -2.1207455260196055,\n",
       "   -1.9551940478802203,\n",
       "   -1.9152033279965823,\n",
       "   -1.9416006636165344,\n",
       "   -1.8982696056410338,\n",
       "   -1.8276315978066733,\n",
       "   -1.7712006471296813,\n",
       "   -1.7652973527749465,\n",
       "   -1.7782342601227512,\n",
       "   -1.7147796394752919,\n",
       "   -1.6804144607767881,\n",
       "   -1.6339404067544052,\n",
       "   -1.6112733359650289,\n",
       "   -1.5413238257752275,\n",
       "   -1.462012705845511,\n",
       "   -1.3897044720491016,\n",
       "   -1.3975655679923533,\n",
       "   -1.4281872221268799,\n",
       "   -1.334356322469481,\n",
       "   -1.3072122247137368,\n",
       "   -1.30525695297457,\n",
       "   -1.2334989756885486,\n",
       "   -1.2630838866103478,\n",
       "   -1.2644503397860953,\n",
       "   -1.2621113796720227,\n",
       "   -1.2392362512380233,\n",
       "   -1.26400963987223,\n",
       "   -1.252248293845196,\n",
       "   -1.255690769835624,\n",
       "   -1.219316604328847,\n",
       "   -1.2246166711610464,\n",
       "   -1.2592277320985614,\n",
       "   -1.2760035629788384,\n",
       "   -1.2859760262070798,\n",
       "   -1.2601302373217327,\n",
       "   -1.253333611082772,\n",
       "   -1.285436654398374,\n",
       "   -1.298003453946598,\n",
       "   -1.323122167709752,\n",
       "   -1.3005833028854763,\n",
       "   -1.2561590698477252,\n",
       "   -1.2517142450934808,\n",
       "   -1.2238394477172236,\n",
       "   -1.2288897903919889,\n",
       "   -1.3288506643304379,\n",
       "   -1.2963439313768141,\n",
       "   -1.3021013195573414,\n",
       "   -1.315396401836272,\n",
       "   -1.343727825104323,\n",
       "   -1.3791821682358174,\n",
       "   -1.3622987910944864,\n",
       "   -1.385576988297068,\n",
       "   -1.3663238125463204,\n",
       "   -1.3612279706879518,\n",
       "   -1.38947926907995,\n",
       "   -1.4189369633576256,\n",
       "   -1.419495118116679,\n",
       "   -1.4439403316426849,\n",
       "   -1.4375682809278816,\n",
       "   -1.4407627100740261,\n",
       "   -1.4358640656625754,\n",
       "   -1.4379415755193508,\n",
       "   -1.3968464873721214,\n",
       "   -1.395643024268319,\n",
       "   -1.3844331286234248,\n",
       "   -1.3506946050698931,\n",
       "   -1.3562157508276416,\n",
       "   -1.3692556039192807,\n",
       "   -1.408040860704999,\n",
       "   -1.3573361955113383,\n",
       "   -1.2996839278661265,\n",
       "   -1.2550630477518054,\n",
       "   -1.2175750592514345,\n",
       "   -1.2392509678036316,\n",
       "   -1.1857077277678272,\n",
       "   -1.177403444213363,\n",
       "   -1.1155830055438165,\n",
       "   -1.0674960146551618,\n",
       "   -1.0993821183871493,\n",
       "   -1.1000721284915729,\n",
       "   -1.0700624128880687,\n",
       "   -1.084215665951913,\n",
       "   -1.1045094623924645,\n",
       "   -1.0818130650429625,\n",
       "   -1.0853396277597702,\n",
       "   -1.0536218515058473,\n",
       "   -1.0730688593481235,\n",
       "   -1.0351859233958733,\n",
       "   -1.0123732340351648,\n",
       "   -1.017746121182114,\n",
       "   -1.0214837725108246,\n",
       "   -0.9844823696224065,\n",
       "   -0.9980434532682918,\n",
       "   -0.9957396662790234,\n",
       "   -1.013879267297198,\n",
       "   -1.0330473495913304,\n",
       "   -1.0609231132405443,\n",
       "   -1.0501963330302386,\n",
       "   -1.0258505507695062,\n",
       "   -1.016401409107647,\n",
       "   -0.9903398737930638,\n",
       "   -1.0269343686789592,\n",
       "   -1.051307989616372,\n",
       "   -1.058336501491015,\n",
       "   -1.0693377910988224,\n",
       "   -1.066572871168428,\n",
       "   -1.0634797039239512,\n",
       "   -1.0792041351814436,\n",
       "   -1.0773580291232197,\n",
       "   -1.0912736534533756,\n",
       "   -1.0719553028020012,\n",
       "   -1.0500406808571072,\n",
       "   -1.0207578594182904,\n",
       "   -0.9866344865334895,\n",
       "   -0.9685037339867515,\n",
       "   -0.9593268401193402,\n",
       "   -0.9106784776495458,\n",
       "   -0.8907431863961222,\n",
       "   -0.8628410392881865,\n",
       "   -0.8192587607866797,\n",
       "   -0.8399269722657126,\n",
       "   -0.8421746182264425,\n",
       "   -0.8297758492408509,\n",
       "   -0.8241395306519925,\n",
       "   -0.8098222590015893,\n",
       "   -0.7923025726026331,\n",
       "   -0.7763842354849806,\n",
       "   -0.7676997773862704,\n",
       "   -0.781808829113741,\n",
       "   -0.7878297731185739,\n",
       "   -0.7872170682636144,\n",
       "   -0.7990534650088568,\n",
       "   -0.801038847546236,\n",
       "   -0.797348315967557,\n",
       "   -0.8131308805930826,\n",
       "   -0.7974095151824849,\n",
       "   -0.7687177587196814,\n",
       "   -0.7665227529332022,\n",
       "   -0.7770965609260427,\n",
       "   -0.766296499364659,\n",
       "   -0.7585323176337659,\n",
       "   -0.7793059184541569,\n",
       "   -0.764310987914318,\n",
       "   -0.7469367256333825,\n",
       "   -0.7360502429590516,\n",
       "   -0.7554851799163731,\n",
       "   -0.7700617757890442,\n",
       "   -0.7648196040955861,\n",
       "   -0.7688268829427192,\n",
       "   -0.7568405711486281,\n",
       "   -0.7471463461490111,\n",
       "   -0.7648334930425684,\n",
       "   -0.7492517358384048,\n",
       "   -0.7210502163709349,\n",
       "   -0.7273865422198007,\n",
       "   -0.7190339494980071,\n",
       "   -0.7197808548558879,\n",
       "   -0.7304957459056278,\n",
       "   -0.7347406568463755,\n",
       "   -0.7337821521642407,\n",
       "   -0.7249108083229627,\n",
       "   -0.714736987269065,\n",
       "   -0.710046558295518,\n",
       "   -0.7034386620543267,\n",
       "   -0.7025685219968401,\n",
       "   -0.6865799303296625,\n",
       "   -0.6611995130835033,\n",
       "   -0.6567356449545261,\n",
       "   -0.6649025331939493,\n",
       "   -0.6339364558133382,\n",
       "   -0.6281630818689306,\n",
       "   -0.6265360365154691,\n",
       "   -0.629890653065071,\n",
       "   -0.6440805744476759,\n",
       "   -0.6452410571972791,\n",
       "   -0.6321468121339813,\n",
       "   -0.6252259694608615,\n",
       "   -0.6196869281187607,\n",
       "   -0.6019615412388775,\n",
       "   -0.6318001172777872,\n",
       "   -0.6428751041760956,\n",
       "   -0.632503074417178,\n",
       "   -0.6231561515234849,\n",
       "   -0.5926099538597611,\n",
       "   -0.5925753001481024,\n",
       "   -0.601954659721794,\n",
       "   -0.581764606239068,\n",
       "   -0.589589767918411,\n",
       "   -0.6000391600827721,\n",
       "   -0.6037004770235839,\n",
       "   -0.6023907397062818,\n",
       "   -0.5830176281046853,\n",
       "   -0.6044953178361756,\n",
       "   -0.5958204499663857,\n",
       "   -0.5884368324078393,\n",
       "   -0.5858577786011618,\n",
       "   -0.5790718181100678,\n",
       "   -0.585130028772163,\n",
       "   -0.5696374860369864,\n",
       "   -0.5671828713344067,\n",
       "   -0.5598101011861039,\n",
       "   -0.5506605397659683,\n",
       "   -0.5545862346316266,\n",
       "   -0.5561845828221847,\n",
       "   -0.5428235174651035,\n",
       "   -0.5278735411673852,\n",
       "   -0.5189672736996758,\n",
       "   -0.5434860247520579,\n",
       "   -0.5294272594309852,\n",
       "   -0.5276458115334011,\n",
       "   -0.5310040506475497,\n",
       "   -0.5365928657387427,\n",
       "   -0.5315015709033388,\n",
       "   -0.53093032641808,\n",
       "   -0.5275078492740106,\n",
       "   -0.526481554759819,\n",
       "   -0.5212575338544274,\n",
       "   -0.5339758269388322,\n",
       "   -0.5373671000241402,\n",
       "   -0.5266474088073836,\n",
       "   -0.5281865595190602,\n",
       "   -0.5298968522841416,\n",
       "   -0.5502087757624354,\n",
       "   -0.548711563976285,\n",
       "   -0.5641712858009553,\n",
       "   -0.5589247406191342,\n",
       "   -0.584541874604932,\n",
       "   -0.570204139006371,\n",
       "   -0.5675040422926585,\n",
       "   -0.545684996628788,\n",
       "   -0.5750681702589588,\n",
       "   -0.5610756119953253,\n",
       "   -0.5645933611191079,\n",
       "   -0.572433898266169,\n",
       "   -0.5584474644059962,\n",
       "   -0.5651767226034707,\n",
       "   -0.5808249645188803,\n",
       "   -0.5604042785842502,\n",
       "   -0.5771313975286798,\n",
       "   -0.576403833999666,\n",
       "   -0.5684961884410595,\n",
       "   -0.5481276650931886,\n",
       "   -0.543396279690419,\n",
       "   -0.5276175804875647,\n",
       "   -0.5162197194960698,\n",
       "   -0.5088782515414054,\n",
       "   -0.4938900233413521,\n",
       "   -0.4889792776958044,\n",
       "   -0.48955597536735107,\n",
       "   -0.4889175048919807,\n",
       "   -0.4992588470357515,\n",
       "   -0.5022989504336333,\n",
       "   -0.4936483033043211,\n",
       "   -0.4964389441337258,\n",
       "   -0.48471665221183713,\n",
       "   -0.48002242014473206,\n",
       "   -0.48425257131663124,\n",
       "   -0.4912537885909303,\n",
       "   -0.49222867834937767,\n",
       "   -0.48838510104381117,\n",
       "   -0.4869161082792346,\n",
       "   -0.47292521381647723,\n",
       "   -0.46124358383068587,\n",
       "   -0.46969390519096366,\n",
       "   -0.4694429610744597,\n",
       "   -0.46623930545882986,\n",
       "   -0.47321644809374686,\n",
       "   -0.46800357956668415,\n",
       "   -0.4691071688355253,\n",
       "   -0.479990773035075,\n",
       "   -0.4727245792498691,\n",
       "   -0.4727295374344709,\n",
       "   -0.4734052456844302,\n",
       "   -0.466877369114016,\n",
       "   -0.4796928383908272,\n",
       "   -0.4867480521830432,\n",
       "   -0.4851678684130216,\n",
       "   -0.4676351181450855,\n",
       "   -0.44936533255798894,\n",
       "   -0.4498285915801341,\n",
       "   -0.4438547332865037,\n",
       "   -0.43662025888717837,\n",
       "   -0.42879874826528996,\n",
       "   -0.4147231743020454,\n",
       "   -0.41521259851094605,\n",
       "   -0.40770383663541643,\n",
       "   -0.40306719578036465,\n",
       "   -0.41233117612566295,\n",
       "   -0.41345157214595374,\n",
       "   -0.41685422027905994,\n",
       "   -0.42531635006998103,\n",
       "   -0.4231147234408583,\n",
       "   -0.4163858424264688,\n",
       "   -0.4165973479938756,\n",
       "   -0.4195431673605188,\n",
       "   -0.4293117532510031,\n",
       "   -0.428804296442852,\n",
       "   -0.4136257078786745,\n",
       "   -0.4164805759247996,\n",
       "   -0.4296707209378139,\n",
       "   -0.4430321532189001,\n",
       "   -0.44481921914245914,\n",
       "   -0.4444828174702642,\n",
       "   -0.44847893330112987,\n",
       "   -0.46300206028948376,\n",
       "   -0.4659301331575474,\n",
       "   -0.46760818344948807,\n",
       "   -0.45936586052424366,\n",
       "   -0.44471975919498474,\n",
       "   -0.4504562457892265,\n",
       "   -0.44256329947136464,\n",
       "   -0.43628263295019387,\n",
       "   -0.43271961638974465,\n",
       "   -0.43934567680999637,\n",
       "   -0.44290661347930227,\n",
       "   -0.4261361557971428,\n",
       "   -0.41807922839257583,\n",
       "   -0.4344114754453259,\n",
       "   -0.43549852583077175,\n",
       "   -0.45784457593934436,\n",
       "   -0.4531388405134025,\n",
       "   -0.4465761207662833,\n",
       "   -0.44878156022172555,\n",
       "   -0.43978136922910394,\n",
       "   -0.4345173809064684,\n",
       "   -0.4296508308867366,\n",
       "   -0.43185434433931524,\n",
       "   -0.4316933740170008,\n",
       "   -0.43957176415179244,\n",
       "   -0.44774232080148746,\n",
       "   -0.44226856830809047,\n",
       "   -0.4448184946159883,\n",
       "   -0.4427367781709164,\n",
       "   -0.44629138959585335,\n",
       "   -0.4524015273424524,\n",
       "   -0.4351307589676634,\n",
       "   -0.42002969018722713,\n",
       "   -0.41200466303900884,\n",
       "   -0.4198485556370759,\n",
       "   -0.42898422988806995,\n",
       "   -0.4111391311628907,\n",
       "   -0.405490294669844,\n",
       "   -0.3981411738833507,\n",
       "   -0.3990179211779523,\n",
       "   -0.3948945102774819,\n",
       "   -0.39182069361644056,\n",
       "   -0.39653322773842037,\n",
       "   -0.3997893929351959,\n",
       "   -0.4008594091903661,\n",
       "   -0.4041483012698821,\n",
       "   -0.40971984959436525,\n",
       "   -0.4028669171960102,\n",
       "   -0.4000118543105291,\n",
       "   -0.4042190014612217,\n",
       "   -0.41135847368275935,\n",
       "   -0.4076613378872276,\n",
       "   -0.40227215576530195,\n",
       "   -0.3971452313790229,\n",
       "   -0.4000550539104408,\n",
       "   -0.39923835755091686,\n",
       "   -0.3884866022977542,\n",
       "   -0.3869403999469958,\n",
       "   -0.38479743100408403,\n",
       "   -0.38590343732194676,\n",
       "   -0.3915106736426588],\n",
       "  'ff_r2': [0.0,\n",
       "   0.0,\n",
       "   -5461.428132458502,\n",
       "   -475.35783849424183,\n",
       "   -193.18760645216818,\n",
       "   -99.8969329773336,\n",
       "   -57.058454802762164,\n",
       "   -39.051621615232804,\n",
       "   -30.80571527653235,\n",
       "   -23.792228390809687,\n",
       "   -16.83827750028005,\n",
       "   -12.427589278626638,\n",
       "   -9.698291617693823,\n",
       "   -7.631277675647519,\n",
       "   -6.756789383780373,\n",
       "   -6.381464076732927,\n",
       "   -6.323252244492524,\n",
       "   -6.5678422906537905,\n",
       "   -6.422064876250946,\n",
       "   -6.494137929256434,\n",
       "   -6.390106325703681,\n",
       "   -6.104094519008794,\n",
       "   -5.804107306298087,\n",
       "   -5.601205039225982,\n",
       "   -5.680930288831127,\n",
       "   -5.419238563900473,\n",
       "   -5.449464098030816,\n",
       "   -5.454247685495118,\n",
       "   -5.4377577666818055,\n",
       "   -5.260738741422229,\n",
       "   -5.186106700681151,\n",
       "   -4.8928431062126565,\n",
       "   -4.673987120356584,\n",
       "   -4.436908321446916,\n",
       "   -4.367810527621416,\n",
       "   -4.221093919726698,\n",
       "   -4.101196174490343,\n",
       "   -3.9686856750265322,\n",
       "   -3.819820675925489,\n",
       "   -3.8148081962554414,\n",
       "   -3.7927902655346486,\n",
       "   -3.856756654904067,\n",
       "   -3.8301619030153597,\n",
       "   -3.7147580028273417,\n",
       "   -3.723515236044305,\n",
       "   -3.563848109590687,\n",
       "   -3.5622157313207206,\n",
       "   -3.786753406157766,\n",
       "   -3.735631060297602,\n",
       "   -3.871000235799041,\n",
       "   -4.025256507399332,\n",
       "   -4.163768160497238,\n",
       "   -4.28895680498153,\n",
       "   -4.428695649714874,\n",
       "   -4.510708352578625,\n",
       "   -4.491155652863854,\n",
       "   -4.369617913386573,\n",
       "   -4.455169334137086,\n",
       "   -4.341216546318808,\n",
       "   -4.192452768826395,\n",
       "   -4.19012744083438,\n",
       "   -4.142433471115351,\n",
       "   -4.274551907066395,\n",
       "   -4.33709923882478,\n",
       "   -4.340408065959288,\n",
       "   -4.314603389022733,\n",
       "   -4.19699002816075,\n",
       "   -4.210579597996586,\n",
       "   -4.281223878712121,\n",
       "   -4.285776560312114,\n",
       "   -4.246111313984015,\n",
       "   -4.271190141438638,\n",
       "   -4.120317394913217,\n",
       "   -3.9722350124081665,\n",
       "   -4.006605752393995,\n",
       "   -3.867128041731984,\n",
       "   -3.8511437392699595,\n",
       "   -3.726752138595744,\n",
       "   -3.6216309460556326,\n",
       "   -3.602439925790981,\n",
       "   -3.5244348533642613,\n",
       "   -3.5789297493281573,\n",
       "   -3.6186028728416506,\n",
       "   -3.5735697490603657,\n",
       "   -3.4650319320948926,\n",
       "   -3.50213599827694,\n",
       "   -3.4374888510871537,\n",
       "   -3.4848082988956444,\n",
       "   -3.5319492340477137,\n",
       "   -3.5167775393480056,\n",
       "   -3.397935112291072,\n",
       "   -3.4604273428760317,\n",
       "   -3.4977287519243037,\n",
       "   -3.4487045660368914,\n",
       "   -3.4428784659982927,\n",
       "   -3.4729661886700844,\n",
       "   -3.4563220541611726,\n",
       "   -3.4241927698665133,\n",
       "   -3.354784186114652,\n",
       "   -3.2758978159618337,\n",
       "   -3.2100981429425435,\n",
       "   -3.114152961607169,\n",
       "   -3.1662833755840367,\n",
       "   -3.2016158356010065,\n",
       "   -3.161527880902484,\n",
       "   -3.185976073923337,\n",
       "   -3.11979744464994,\n",
       "   -3.129742534045981,\n",
       "   -3.1595783748421056,\n",
       "   -3.1953786697279503,\n",
       "   -3.240787612976167,\n",
       "   -3.27906251492295,\n",
       "   -3.3009814474009893,\n",
       "   -3.2410769966591975,\n",
       "   -3.2428075142797415,\n",
       "   -3.2551025432702145,\n",
       "   -3.233879643585939,\n",
       "   -3.326080413024159,\n",
       "   -3.317256280222863,\n",
       "   -3.290299455252062,\n",
       "   -3.246050130597288,\n",
       "   -3.1584585004092185,\n",
       "   -3.1991698137268516,\n",
       "   -3.1787814721625622,\n",
       "   -3.217084814602824,\n",
       "   -3.2041974960535846,\n",
       "   -3.174637473247987,\n",
       "   -3.1363148718318374,\n",
       "   -3.091655515587818,\n",
       "   -2.998078398354551,\n",
       "   -2.950442135098348,\n",
       "   -2.9491566032659042,\n",
       "   -2.9043088031463946,\n",
       "   -2.8359040290902047,\n",
       "   -2.8469486092111636,\n",
       "   -2.8244575009775286,\n",
       "   -2.770185964158113,\n",
       "   -2.75681723749484,\n",
       "   -2.781329462688087,\n",
       "   -2.7076007553331576,\n",
       "   -2.6959418985419354,\n",
       "   -2.657120160644381,\n",
       "   -2.66386680029424,\n",
       "   -2.6848298480372152,\n",
       "   -2.693962962001657,\n",
       "   -2.6956291392303195,\n",
       "   -2.671146127382814,\n",
       "   -2.6472750929061397,\n",
       "   -2.6433880048341294,\n",
       "   -2.694909144445283,\n",
       "   -2.6730453491033135,\n",
       "   -2.720992131062192,\n",
       "   -2.912821722772185,\n",
       "   -3.003757760825124,\n",
       "   -3.0363886541414935,\n",
       "   -3.068781606219848,\n",
       "   -3.01542792309803,\n",
       "   -2.9783414379926834,\n",
       "   -2.9959619624135296,\n",
       "   -3.0328128856636694,\n",
       "   -3.0293211915445255,\n",
       "   -3.0694488014661294,\n",
       "   -3.120022384437683,\n",
       "   -3.1708715395952005,\n",
       "   -3.1661353916345876,\n",
       "   -3.0973461555242263,\n",
       "   -3.197525881431887,\n",
       "   -3.2192074788047016,\n",
       "   -3.171365212002107,\n",
       "   -3.1824891968153723,\n",
       "   -3.1701358146837855,\n",
       "   -3.1154425407809363,\n",
       "   -3.076574046703846,\n",
       "   -3.054039763548487,\n",
       "   -3.0671255828673667,\n",
       "   -3.0627405071483507,\n",
       "   -3.0459253723118938,\n",
       "   -3.0040448795500003,\n",
       "   -2.9675320839832273,\n",
       "   -3.0332493145544497,\n",
       "   -3.053879408835809,\n",
       "   -3.0680037498957473,\n",
       "   -3.086397856048791,\n",
       "   -3.073961303286792,\n",
       "   -2.971549066838997,\n",
       "   -2.9556727854616747,\n",
       "   -2.9273171646496676,\n",
       "   -2.9098636152012682,\n",
       "   -2.9944815700001812,\n",
       "   -2.9348253257813557,\n",
       "   -3.0290488292446573,\n",
       "   -3.117707154759781,\n",
       "   -3.074653067180005,\n",
       "   -3.0405699842427643,\n",
       "   -3.008778273132992,\n",
       "   -2.977580940337304,\n",
       "   -2.8931197514768074,\n",
       "   -2.831043553480874,\n",
       "   -2.8594669499659497,\n",
       "   -2.9058094934098553,\n",
       "   -2.836370831275199,\n",
       "   -2.7620394690194088,\n",
       "   -2.7557025686233767,\n",
       "   -2.727433868535984,\n",
       "   -2.8027189704661417,\n",
       "   -2.783781371720667,\n",
       "   -2.802756652572358,\n",
       "   -2.819454427628546,\n",
       "   -2.7412938680787953,\n",
       "   -2.6977673442709516,\n",
       "   -2.6094028084107066,\n",
       "   -2.55973068410997,\n",
       "   -2.571017902559538,\n",
       "   -2.631631884535621,\n",
       "   -2.578355555425371,\n",
       "   -2.629104492745987,\n",
       "   -2.604981696529277,\n",
       "   -2.5466741462116844,\n",
       "   -2.53358956860463,\n",
       "   -2.473500677443506,\n",
       "   -2.490072874759185,\n",
       "   -2.4433702546419283,\n",
       "   -2.4137889510594657,\n",
       "   -2.340878750565519,\n",
       "   -2.3478536971509523,\n",
       "   -2.369164755687612,\n",
       "   -2.292905297641736,\n",
       "   -2.273055978902987,\n",
       "   -2.2655744138717,\n",
       "   -2.207925492375522,\n",
       "   -2.117941360232236,\n",
       "   -2.102789990761771,\n",
       "   -2.163350315182986,\n",
       "   -2.1646367234882966,\n",
       "   -2.1869577244146967,\n",
       "   -2.272490296519443,\n",
       "   -2.1776595745331018,\n",
       "   -2.244154620535573,\n",
       "   -2.248687395459282,\n",
       "   -2.238806675373512,\n",
       "   -2.218507055329173,\n",
       "   -2.1885119296557236,\n",
       "   -2.245235687551893,\n",
       "   -2.2737563827163076,\n",
       "   -2.2217575968784424,\n",
       "   -2.3281957261941324,\n",
       "   -2.320178351291178,\n",
       "   -2.3512802418192362,\n",
       "   -2.3185131702242767,\n",
       "   -2.264134591945148,\n",
       "   -2.109041888293878,\n",
       "   -2.1451230308571145,\n",
       "   -2.0761311449900064,\n",
       "   -2.033738224147846,\n",
       "   -2.0192492765959456,\n",
       "   -2.0519771209801005,\n",
       "   -1.9931727833055146,\n",
       "   -2.0258311961338653,\n",
       "   -2.0249750608498536,\n",
       "   -2.0201468314075792,\n",
       "   -2.0387335803796915,\n",
       "   -2.1167401408345263,\n",
       "   -2.090313212505382,\n",
       "   -2.099603094410023,\n",
       "   -2.066424250907635,\n",
       "   -2.079861356285548,\n",
       "   -2.1049217697442653,\n",
       "   -2.097434905335531,\n",
       "   -2.038075311518034,\n",
       "   -2.002892930897013,\n",
       "   -1.9077737009336926,\n",
       "   -1.9791324024665955,\n",
       "   -1.903818349611515,\n",
       "   -1.8755010919347428,\n",
       "   -1.836136490092957,\n",
       "   -1.889093644412294,\n",
       "   -1.870804041053208,\n",
       "   -1.8884056888096428,\n",
       "   -1.900769552690877,\n",
       "   -1.9067122520920745,\n",
       "   -1.9229480186934764,\n",
       "   -1.9088681232064055,\n",
       "   -1.953001838152082,\n",
       "   -1.9268157144908793,\n",
       "   -1.994450265298262,\n",
       "   -1.954652738496442,\n",
       "   -1.9372142207250462,\n",
       "   -1.8770084071526463,\n",
       "   -1.9137955816019394,\n",
       "   -1.9830363815059755,\n",
       "   -1.9601795658649803,\n",
       "   -1.9571588965047821,\n",
       "   -1.9232677363709523,\n",
       "   -1.84929171183814,\n",
       "   -1.916209400808932,\n",
       "   -1.9862433907158334,\n",
       "   -1.9435826591139045,\n",
       "   -1.9243193512996624,\n",
       "   -2.002377536415913,\n",
       "   -1.99133223475899,\n",
       "   -1.9121786262209564,\n",
       "   -1.9675203281882125,\n",
       "   -1.9739057968764926,\n",
       "   -1.9793611504449289,\n",
       "   -1.8701302604911914,\n",
       "   -1.7841736347960193,\n",
       "   -1.7761368796959953,\n",
       "   -1.7478637796937053,\n",
       "   -1.7496360237285469,\n",
       "   -1.778589887997121,\n",
       "   -1.7684380469792882,\n",
       "   -1.7502303540154913,\n",
       "   -1.7181582161728763,\n",
       "   -1.7225228072248546,\n",
       "   -1.7315866209186939,\n",
       "   -1.6696854092876037,\n",
       "   -1.6309333540112134,\n",
       "   -1.7248773700370048,\n",
       "   -1.7229213648260444,\n",
       "   -1.7247534237929791,\n",
       "   -1.7390397573052683,\n",
       "   -1.726882356500242,\n",
       "   -1.710704043687004,\n",
       "   -1.7632362880532604,\n",
       "   -1.7492268995292841,\n",
       "   -1.6998946387181575,\n",
       "   -1.6088706577061744,\n",
       "   -1.5910809099700955,\n",
       "   -1.5957331594940611,\n",
       "   -1.5696235795060955,\n",
       "   -1.6072810704441114,\n",
       "   -1.661444699118921,\n",
       "   -1.6183551319928284,\n",
       "   -1.5704957447870735,\n",
       "   -1.530566331892131,\n",
       "   -1.4864765580556072,\n",
       "   -1.5217725652570548,\n",
       "   -1.5448643686178807,\n",
       "   -1.4778682959629323,\n",
       "   -1.5005630660645917,\n",
       "   -1.585683749513294,\n",
       "   -1.6176556059101639,\n",
       "   -1.650933222440968,\n",
       "   -1.6827881142846537,\n",
       "   -1.6599905633719794,\n",
       "   -1.7134238171325387,\n",
       "   -1.7038000033613763,\n",
       "   -1.687682317205458,\n",
       "   -1.659556774798288,\n",
       "   -1.6795691941376782,\n",
       "   -1.7806306167277244,\n",
       "   -1.8104751349647064,\n",
       "   -1.808221465262326,\n",
       "   -1.7530903165325373,\n",
       "   -1.7900426239975622,\n",
       "   -1.7807554784362662,\n",
       "   -1.7929029593033188,\n",
       "   -1.7796494905943994,\n",
       "   -1.7980958240200167,\n",
       "   -1.778423740592829,\n",
       "   -1.7972188711305113,\n",
       "   -1.8470647291091749,\n",
       "   -1.8519765532488286,\n",
       "   -1.8365724026868526,\n",
       "   -1.8029124257147777,\n",
       "   -1.8088317986301887,\n",
       "   -1.859805336212105,\n",
       "   -1.8149436558413825,\n",
       "   -1.8181643616767795,\n",
       "   -1.833891613833711,\n",
       "   -1.894789247433558,\n",
       "   -1.8490367602011832,\n",
       "   -1.8486010784484983,\n",
       "   -1.8010660987210603,\n",
       "   -1.853152931830416,\n",
       "   -1.818882864613797,\n",
       "   -1.829467008581346,\n",
       "   -1.8010335840837617,\n",
       "   -1.762199607886858,\n",
       "   -1.7771186483691936,\n",
       "   -1.7084768692411885,\n",
       "   -1.6984737201028581,\n",
       "   -1.7130652481334696,\n",
       "   -1.6768469440495188,\n",
       "   -1.5889443801499863,\n",
       "   -1.5551715383217921,\n",
       "   -1.561159240847993,\n",
       "   -1.554443367108851,\n",
       "   -1.5605735885900542,\n",
       "   -1.5993012841648087,\n",
       "   -1.5809129541706084,\n",
       "   -1.5351130427880952,\n",
       "   -1.4927475681918656,\n",
       "   -1.4984396195116827,\n",
       "   -1.4632074504637402,\n",
       "   -1.4706154842802897,\n",
       "   -1.4840716537731433,\n",
       "   -1.504718893639113,\n",
       "   -1.4737610476472511,\n",
       "   -1.5301063379022568,\n",
       "   -1.4939196273519038,\n",
       "   -1.533850143799775,\n",
       "   -1.4671850597314529,\n",
       "   -1.413181292642904,\n",
       "   -1.4156900566373078,\n",
       "   -1.455827824817705,\n",
       "   -1.5016022783027867,\n",
       "   -1.4849789017061839,\n",
       "   -1.493797014821964,\n",
       "   -1.5116489269764273,\n",
       "   -1.5060945813461442,\n",
       "   -1.4394427027131012,\n",
       "   -1.4753244838386617,\n",
       "   -1.4217277766765792,\n",
       "   -1.4887934325916454,\n",
       "   -1.482181400339635,\n",
       "   -1.483920319664402,\n",
       "   -1.3778650240758497,\n",
       "   -1.3873253843232178,\n",
       "   -1.340208772426887,\n",
       "   -1.3219605203166616,\n",
       "   -1.2816486783065106,\n",
       "   -1.24151440887168,\n",
       "   -1.2295701818363183,\n",
       "   -1.2504441021951385,\n",
       "   -1.2629543349554542,\n",
       "   -1.241265291225945,\n",
       "   -1.300909528636966,\n",
       "   -1.2824035797644138,\n",
       "   -1.2567811740170836,\n",
       "   -1.3160893932082849,\n",
       "   -1.2804884008229482,\n",
       "   -1.3150075169911641,\n",
       "   -1.2482918821286697,\n",
       "   -1.2127396265697445,\n",
       "   -1.2234113767279524,\n",
       "   -1.18108065945174,\n",
       "   -1.1673373171998027,\n",
       "   -1.1772482700167002,\n",
       "   -1.1943077974290994,\n",
       "   -1.1720635190754445,\n",
       "   -1.1122404275948723,\n",
       "   -1.1315616023610806,\n",
       "   -1.193517833624146,\n",
       "   -1.197918832210941,\n",
       "   -1.1873186559015627,\n",
       "   -1.171557783806743,\n",
       "   -1.1910239727449508,\n",
       "   -1.1741492552792985,\n",
       "   -1.1601398424012803,\n",
       "   -1.1161594911828971,\n",
       "   -1.0642640373301036,\n",
       "   -1.1002375977442829,\n",
       "   -1.0848424072726068,\n",
       "   -1.095089493041844,\n",
       "   -1.0953241820453177,\n",
       "   -1.030535764098019,\n",
       "   -0.9913113206226722,\n",
       "   -1.0191851933943679,\n",
       "   -1.0215234879014559,\n",
       "   -0.9979047032159158,\n",
       "   -0.9839878053904334,\n",
       "   -0.9827630690951823,\n",
       "   -1.055615821646112,\n",
       "   -1.092353276094007,\n",
       "   -1.1337138097538806,\n",
       "   -1.1182093336914423,\n",
       "   -1.105152653695069,\n",
       "   -1.075452347129299,\n",
       "   -1.1082533278957722,\n",
       "   -1.1881191009859466,\n",
       "   -1.2361291122305378,\n",
       "   -1.2383721522302555,\n",
       "   -1.2113707852853675,\n",
       "   -1.2086460640622976,\n",
       "   -1.1814510303228531,\n",
       "   -1.185676785468063,\n",
       "   -1.1980711445304704,\n",
       "   -1.1561284003977441,\n",
       "   -1.1676102488943103,\n",
       "   -1.1511230480440902,\n",
       "   -1.0835346638264518,\n",
       "   -1.0995445963127466,\n",
       "   -1.114694372910631,\n",
       "   -1.128095318141897,\n",
       "   -1.141851695060239,\n",
       "   -1.225414472020113,\n",
       "   -1.246471900523614,\n",
       "   -1.2558142040131677,\n",
       "   -1.2648272667391174,\n",
       "   -1.269575919378482,\n",
       "   -1.2501778232584995,\n",
       "   -1.2051217610351612,\n",
       "   -1.182359652629633,\n",
       "   -1.181466346854759,\n",
       "   -1.1362016872375658,\n",
       "   -1.0690871063620118,\n",
       "   -1.085394874114498,\n",
       "   -1.070283692742354,\n",
       "   -1.0686634323862605],\n",
       "  'test_r2s': [0.0,\n",
       "   0.0,\n",
       "   -5461.428132458502,\n",
       "   -1063.3256859879486,\n",
       "   -272.0172594577513,\n",
       "   -122.09715509664025,\n",
       "   -65.9595323400909,\n",
       "   -140095.9553589152,\n",
       "   -22066.858426274783,\n",
       "   -8726.06131775029,\n",
       "   -3798.2942720957267,\n",
       "   -1927.6071312271854,\n",
       "   -878.1901759689242,\n",
       "   -580.5115700776729,\n",
       "   -684.9970324977841,\n",
       "   -377.8741556155583,\n",
       "   -269.4752341061371,\n",
       "   -217.93090725246333,\n",
       "   -197.00684325766645,\n",
       "   -157.06611281429454,\n",
       "   -155.5070782465573,\n",
       "   -156.53798387928913,\n",
       "   -159.93777966573043,\n",
       "   -162.2811535378845,\n",
       "   -159.35538055991896,\n",
       "   -156.94060518333265,\n",
       "   -158.81449930938675,\n",
       "   -156.6461677052682,\n",
       "   -154.11884142856724,\n",
       "   -154.13392832670925,\n",
       "   -136.28765611114204,\n",
       "   -133.30084475570595,\n",
       "   -135.31035804994713,\n",
       "   -129.837923396026,\n",
       "   -121.14027167076875,\n",
       "   -107.185035762393,\n",
       "   -103.87512625111698,\n",
       "   -98.5188351406691,\n",
       "   -98.58481910246182,\n",
       "   -96.11468970236506,\n",
       "   -90.20412888865523,\n",
       "   -85.44048531753461,\n",
       "   -86.65918665956704,\n",
       "   -83.97076648049486,\n",
       "   -81.23629035633755,\n",
       "   -82.23471642168798,\n",
       "   -82.27077354747291,\n",
       "   -82.81451740005096,\n",
       "   -80.1670156933617,\n",
       "   -80.02767843556197,\n",
       "   -76.73102807334205,\n",
       "   -70.87167819903878,\n",
       "   -68.22508378823873,\n",
       "   -64.65602188624094,\n",
       "   -63.53281493725458,\n",
       "   -61.66441701071902,\n",
       "   -57.07151647088868,\n",
       "   -52.53539421130159,\n",
       "   -51.88543737370641,\n",
       "   -48.42510097305263,\n",
       "   -47.97126744316197,\n",
       "   -48.69148390747188,\n",
       "   -48.05804381793572,\n",
       "   -48.02560282782672,\n",
       "   -45.92243727865131,\n",
       "   -43.768903935191986,\n",
       "   -42.45072996251106,\n",
       "   -42.37695154135736,\n",
       "   -41.56965704614196,\n",
       "   -40.136068659844206,\n",
       "   -39.984600121022496,\n",
       "   -37.789079409628265,\n",
       "   -36.08210016765497,\n",
       "   -34.60509353131965,\n",
       "   -34.48972711943255,\n",
       "   -33.20874680381769,\n",
       "   -31.96210410338395,\n",
       "   -30.98323301853248,\n",
       "   -30.34843941101485,\n",
       "   -29.935137911657534,\n",
       "   -28.99504014155854,\n",
       "   -28.27464540332303,\n",
       "   -26.960614121419795,\n",
       "   -25.912479279322394,\n",
       "   -24.26124898724747,\n",
       "   -23.531585013078384,\n",
       "   -22.78720857635267,\n",
       "   -22.083348823580394,\n",
       "   -21.16630824536832,\n",
       "   -20.674591593846074,\n",
       "   -20.465034813028446,\n",
       "   -20.295053515189814,\n",
       "   -20.031303012518233,\n",
       "   -20.36986958612886,\n",
       "   -20.621022891620164,\n",
       "   -19.888729950258735,\n",
       "   -18.799894623361233,\n",
       "   -18.92803816433659,\n",
       "   -19.265772794593154,\n",
       "   -18.858758244001123,\n",
       "   -19.321498704925304,\n",
       "   -18.827381026028103,\n",
       "   -19.249003149841027,\n",
       "   -19.509901537509855,\n",
       "   -19.584857075741812,\n",
       "   -20.2043541052833,\n",
       "   -20.977534906219653,\n",
       "   -21.629168566969135,\n",
       "   -21.622285564586544,\n",
       "   -21.68188740269781,\n",
       "   -21.42193059669654,\n",
       "   -21.795116999529156,\n",
       "   -21.811866651828545,\n",
       "   -21.13197429338266,\n",
       "   -21.10506264035437,\n",
       "   -21.07618679424008,\n",
       "   -20.797751154750422,\n",
       "   -20.59498075880607,\n",
       "   -20.508229191279085,\n",
       "   -20.146408286550006,\n",
       "   -19.86203453833894,\n",
       "   -20.340162234082563,\n",
       "   -20.022052230563904,\n",
       "   -19.12549242379365,\n",
       "   -19.02349631735568,\n",
       "   -19.44625581586475,\n",
       "   -18.559948859181276,\n",
       "   -17.718644755577976,\n",
       "   -17.379697809301206,\n",
       "   -16.732210754649316,\n",
       "   -16.570100537431433,\n",
       "   -16.665591184887553,\n",
       "   -15.801092703144533,\n",
       "   -16.091385611097653,\n",
       "   -15.099887967158068,\n",
       "   -14.988222290153654,\n",
       "   -14.776619820546312,\n",
       "   -14.705996316670607,\n",
       "   -14.58355700858028,\n",
       "   -14.361692853094848,\n",
       "   -14.916482944990236,\n",
       "   -14.939832512356613,\n",
       "   -14.21166462592533,\n",
       "   -14.435904850282778,\n",
       "   -14.735459187095596,\n",
       "   -14.90562194308188,\n",
       "   -14.96015282120277,\n",
       "   -14.941283608105252,\n",
       "   -15.625631127365782,\n",
       "   -15.67078808471297,\n",
       "   -15.662632180434072,\n",
       "   -15.903043247301028,\n",
       "   -16.244001934965524,\n",
       "   -16.562724437628464,\n",
       "   -16.851449650959154,\n",
       "   -16.437447791289205,\n",
       "   -15.817676988654373,\n",
       "   -15.6434428082088,\n",
       "   -15.615268758458688,\n",
       "   -14.908719999168525,\n",
       "   -15.091905830264395,\n",
       "   -14.47344787526643,\n",
       "   -14.372594465352925,\n",
       "   -14.361755234275726,\n",
       "   -14.306747374681418,\n",
       "   -14.175967617613194,\n",
       "   -14.040915113113186,\n",
       "   -13.853335679025939,\n",
       "   -14.00990042208463,\n",
       "   -14.705228338173765,\n",
       "   -14.198956633682279,\n",
       "   -14.237103219224096,\n",
       "   -14.568296577184803,\n",
       "   -14.73938364117021,\n",
       "   -14.41576525979557,\n",
       "   -14.355099005582204,\n",
       "   -14.224221586393956,\n",
       "   -14.622295714008771,\n",
       "   -14.740027921324756,\n",
       "   -14.569409871577353,\n",
       "   -14.738467189262089,\n",
       "   -14.902866194141247,\n",
       "   -14.943007829501644,\n",
       "   -14.734446667309284,\n",
       "   -14.957768370239481,\n",
       "   -14.625774539118682,\n",
       "   -14.393473852924124,\n",
       "   -14.352369241730418,\n",
       "   -14.509486594192568,\n",
       "   -14.842164491230927,\n",
       "   -14.471855923220605,\n",
       "   -14.648205967891148,\n",
       "   -14.429982102795067,\n",
       "   -14.110949229574263,\n",
       "   -13.809102046802625,\n",
       "   -13.536769353265175,\n",
       "   -13.220339672523739,\n",
       "   -13.287746027986817,\n",
       "   -13.095358527439906,\n",
       "   -13.098524216415141,\n",
       "   -12.947522095042567,\n",
       "   -13.219637425579403,\n",
       "   -13.293125992977796,\n",
       "   -13.538667945951197,\n",
       "   -13.946962617877377,\n",
       "   -14.139779155202778,\n",
       "   -14.043368564842794,\n",
       "   -14.072214585256722,\n",
       "   -13.980420385053336,\n",
       "   -13.936163369421832,\n",
       "   -14.112097029358823,\n",
       "   -14.260506617533917,\n",
       "   -14.51847396059596,\n",
       "   -14.378091261166853,\n",
       "   -14.351882185234881,\n",
       "   -14.47253528383457,\n",
       "   -14.317389232060085,\n",
       "   -14.375069415471927,\n",
       "   -14.517865215724566,\n",
       "   -14.38524696020123,\n",
       "   -14.664161732520833,\n",
       "   -14.33657903254972,\n",
       "   -14.014898640986344,\n",
       "   -14.03092222433725,\n",
       "   -14.096370056887038,\n",
       "   -14.100097841784308,\n",
       "   -13.683170525486524,\n",
       "   -13.420963332334733,\n",
       "   -13.191711927221005,\n",
       "   -13.027224638426187,\n",
       "   -12.89389079460221,\n",
       "   -12.93048074935276,\n",
       "   -12.849222323003282,\n",
       "   -12.956912498755086,\n",
       "   -13.06519855114599,\n",
       "   -12.828791720503167,\n",
       "   -12.557488125152414,\n",
       "   -13.046559528725574,\n",
       "   -13.097847803726546,\n",
       "   -13.13929897102359,\n",
       "   -13.235561303533984,\n",
       "   -12.879044562368495,\n",
       "   -12.89018424790699,\n",
       "   -13.034126793984672,\n",
       "   -12.942408950424944,\n",
       "   -12.702456611977155,\n",
       "   -12.523644014610325,\n",
       "   -12.558577912813616,\n",
       "   -12.357809644844606,\n",
       "   -12.35497078295992,\n",
       "   -12.208649719052747,\n",
       "   -12.048184628943517,\n",
       "   -12.136962993142612,\n",
       "   -12.421633320727786,\n",
       "   -12.165159670180216,\n",
       "   -12.171122172276348,\n",
       "   -12.348060987982338,\n",
       "   -12.238408849152362,\n",
       "   -12.214276028018865,\n",
       "   -12.271321483474647,\n",
       "   -12.362465637500552,\n",
       "   -12.360515760623267,\n",
       "   -12.421655535533038,\n",
       "   -11.928636139773571,\n",
       "   -11.759904649301598,\n",
       "   -11.71608795449701,\n",
       "   -11.829114632897982,\n",
       "   -11.936230173531253,\n",
       "   -11.623391063086688,\n",
       "   -11.413021113493063,\n",
       "   -11.133528858863247,\n",
       "   -11.201640494111121,\n",
       "   -10.949967739191656,\n",
       "   -10.954351434436193,\n",
       "   -10.873716482704676,\n",
       "   -10.96804645943071,\n",
       "   -10.971670594117965,\n",
       "   -10.953400412528945,\n",
       "   -11.239348195111257,\n",
       "   -11.121248520983418,\n",
       "   -10.699146962755337,\n",
       "   -10.681138715769503,\n",
       "   -11.082874698930093,\n",
       "   -11.09180568723309,\n",
       "   -11.218068757717552,\n",
       "   -11.48542580013928,\n",
       "   -12.11748379021337,\n",
       "   -12.037022782324723,\n",
       "   -11.62428624497893,\n",
       "   -11.545826358099243,\n",
       "   -11.412860873128379,\n",
       "   -11.821714300135671,\n",
       "   -11.996107216677007,\n",
       "   -11.584797191583087,\n",
       "   -11.685042261812036,\n",
       "   -11.998913972573517,\n",
       "   -12.089152801071256,\n",
       "   -12.24825902108658,\n",
       "   -12.275764224079579,\n",
       "   -12.344552308809202,\n",
       "   -12.339936091002922,\n",
       "   -12.45999403929797,\n",
       "   -12.37060122923419,\n",
       "   -12.154072916368513,\n",
       "   -11.860153017625773,\n",
       "   -11.688023966198028,\n",
       "   -11.77654216320487,\n",
       "   -11.888514769068909,\n",
       "   -11.973235357488875,\n",
       "   -12.349026148796991,\n",
       "   -12.444410837946727,\n",
       "   -12.467923947808352,\n",
       "   -12.438188946886337,\n",
       "   -12.43179668671143,\n",
       "   -12.49492769015125,\n",
       "   -12.62188577714047,\n",
       "   -12.214899812365877,\n",
       "   -12.267125087918991,\n",
       "   -12.290556695699463,\n",
       "   -12.302974234324736,\n",
       "   -12.964228790026283,\n",
       "   -12.611075591374721,\n",
       "   -12.749394320375226,\n",
       "   -12.668435768867308,\n",
       "   -12.147900053400026,\n",
       "   -12.084611061895384,\n",
       "   -12.069496527251594,\n",
       "   -12.279196909731144,\n",
       "   -11.826123516175482,\n",
       "   -11.497669761306927,\n",
       "   -11.509895022616142,\n",
       "   -11.695644291240015,\n",
       "   -11.576561363583343,\n",
       "   -11.361330153146548,\n",
       "   -11.464063005107413,\n",
       "   -11.113166469294349,\n",
       "   -11.152095018833773,\n",
       "   -10.77641141031322,\n",
       "   -10.910566091431935,\n",
       "   -10.647595157397902,\n",
       "   -10.764922248086553,\n",
       "   -10.724855557033145,\n",
       "   -10.588545581498181,\n",
       "   -11.026047987815964,\n",
       "   -11.102425689940167,\n",
       "   -11.240019125491896,\n",
       "   -11.15404643101433,\n",
       "   -11.24403204129947,\n",
       "   -10.97601621190712,\n",
       "   -10.75184816021735,\n",
       "   -10.669492196828052,\n",
       "   -10.840737164513197,\n",
       "   -10.853585376695023,\n",
       "   -10.851718097606247,\n",
       "   -11.182981367979206,\n",
       "   -11.195517359270879,\n",
       "   -11.157067963064952,\n",
       "   -11.160332031564396,\n",
       "   -11.038216647419377,\n",
       "   -10.795997160138018,\n",
       "   -10.58457766464037,\n",
       "   -11.000237014180133,\n",
       "   -10.841401032637002,\n",
       "   -10.83461904203313,\n",
       "   -10.51939869266539,\n",
       "   -10.671850829466994,\n",
       "   -10.779471505862173,\n",
       "   -10.586648525298553,\n",
       "   -10.733248766667693,\n",
       "   -10.681352870127311,\n",
       "   -10.890182362510481,\n",
       "   -10.748257178993018,\n",
       "   -10.785800708405187,\n",
       "   -10.500606313672742,\n",
       "   -10.643733404087335,\n",
       "   -10.681888363675558,\n",
       "   -10.628345003711322,\n",
       "   -10.376448030262067,\n",
       "   -10.217119586173276,\n",
       "   -10.591885880277498,\n",
       "   -10.831772408898955,\n",
       "   -10.711789381535073,\n",
       "   -10.41219858020332,\n",
       "   -10.284393356080496,\n",
       "   -10.274370039287241,\n",
       "   -10.039879864845624,\n",
       "   -10.02385787991049,\n",
       "   -9.990601508694382,\n",
       "   -10.233411764576704,\n",
       "   -10.209384197215776,\n",
       "   -10.163689745598607,\n",
       "   -10.124544980906636,\n",
       "   -10.143980253024882,\n",
       "   -10.523882698535024,\n",
       "   -10.585393438577265,\n",
       "   -10.528288953656793,\n",
       "   -10.673653494641645,\n",
       "   -10.869239949192162,\n",
       "   -10.798932727857993,\n",
       "   -10.932459586212344,\n",
       "   -11.174588010381942,\n",
       "   -11.49907338001291,\n",
       "   -12.047845455827504,\n",
       "   -11.73991344002986,\n",
       "   -11.876767358593623,\n",
       "   -12.035774846084585,\n",
       "   -12.651993040786806,\n",
       "   -12.333249748872431,\n",
       "   -11.974151873188465,\n",
       "   -12.114645199635559,\n",
       "   -12.12500771770397,\n",
       "   -11.819822024732119,\n",
       "   -11.691400216147139,\n",
       "   -11.557129607323933,\n",
       "   -11.583498615776092,\n",
       "   -11.540310790521552,\n",
       "   -11.72461762161796,\n",
       "   -11.626058516345452,\n",
       "   -11.620552473851241,\n",
       "   -11.394364775901375,\n",
       "   -10.832088090327103,\n",
       "   -10.530702873757319,\n",
       "   -10.1986684293018,\n",
       "   -10.053103452229312,\n",
       "   -10.35473022166902,\n",
       "   -10.463815208147988,\n",
       "   -10.829268449924312,\n",
       "   -10.985546749548744,\n",
       "   -10.403239171365758,\n",
       "   -10.548653967210667,\n",
       "   -10.962972404279233,\n",
       "   -10.9606817651078,\n",
       "   -10.936032612652,\n",
       "   -11.233218204364572,\n",
       "   -11.009295389601265,\n",
       "   -11.077253503218948,\n",
       "   -11.250441887127224,\n",
       "   -11.336665085355445,\n",
       "   -11.471383527520995,\n",
       "   -11.044864625078125,\n",
       "   -10.93426058260927,\n",
       "   -10.96393178195665,\n",
       "   -10.637040499939374,\n",
       "   -10.88697996366507,\n",
       "   -11.068477809877454,\n",
       "   -11.277075863679439,\n",
       "   -11.499052432507547,\n",
       "   -11.347090863437261,\n",
       "   -11.192972824799908,\n",
       "   -11.268657850780684,\n",
       "   -11.193717172241628,\n",
       "   -11.169664282989405,\n",
       "   -11.43705241138835,\n",
       "   -11.481452770769591,\n",
       "   -11.672098254327318,\n",
       "   -11.754573499376962,\n",
       "   -11.78157087783032,\n",
       "   -12.15899858895629,\n",
       "   -11.555533915250708,\n",
       "   -11.063411312627908,\n",
       "   -10.622530636131648,\n",
       "   -10.38067088075081,\n",
       "   -10.33909637806578,\n",
       "   -10.357861796793964,\n",
       "   -10.603932055326235,\n",
       "   -10.787887940048321,\n",
       "   -10.805418229197333,\n",
       "   -11.040424109810878,\n",
       "   -10.837191087585687,\n",
       "   -10.758002363821836,\n",
       "   -10.72286015678694,\n",
       "   -11.13871318222458,\n",
       "   -11.043696137398367,\n",
       "   -11.334118032447362,\n",
       "   -11.288160929255021,\n",
       "   -10.658892954216272,\n",
       "   -10.715823844073295,\n",
       "   -10.334770646224133,\n",
       "   -10.377221818466243,\n",
       "   -10.528853120974023,\n",
       "   -10.708905562909683,\n",
       "   -10.362433204923184,\n",
       "   -10.609309605044535,\n",
       "   -10.752831482781858,\n",
       "   -10.777412047206711,\n",
       "   -11.243262495160186,\n",
       "   -11.596837326887279,\n",
       "   -11.41218421545778,\n",
       "   -11.605916255304788,\n",
       "   -11.438779059864526,\n",
       "   -11.156337078975483,\n",
       "   -11.374881759297164,\n",
       "   -11.561114173874653,\n",
       "   -11.840150294838917,\n",
       "   -11.952259856354182,\n",
       "   -11.83160975386636,\n",
       "   -11.600598150398861,\n",
       "   -11.679048281787328,\n",
       "   -11.794748679690354,\n",
       "   -11.795523858198287],\n",
       "  'train_pce_loss': [0.4331679046154022,\n",
       "   0.4662209749221802,\n",
       "   0.46038734912872314,\n",
       "   0.4525172710418701,\n",
       "   0.43567317724227905,\n",
       "   0.396699994802475,\n",
       "   0.407406747341156,\n",
       "   0.386232852935791,\n",
       "   0.4495891332626343,\n",
       "   0.4269222319126129,\n",
       "   0.422744482755661,\n",
       "   0.42358899116516113,\n",
       "   0.4257284700870514,\n",
       "   0.3877147436141968,\n",
       "   0.39578163623809814,\n",
       "   0.40205448865890503,\n",
       "   0.3783474564552307,\n",
       "   0.4048232436180115,\n",
       "   0.41761520504951477,\n",
       "   0.40841200947761536,\n",
       "   0.3599972128868103,\n",
       "   0.4233500063419342,\n",
       "   0.40332043170928955,\n",
       "   0.372379332780838,\n",
       "   0.4020841121673584,\n",
       "   0.3926139175891876,\n",
       "   0.38040244579315186,\n",
       "   0.402444988489151,\n",
       "   0.37031635642051697,\n",
       "   0.44882696866989136,\n",
       "   0.36521831154823303,\n",
       "   0.39577698707580566,\n",
       "   0.3618597686290741,\n",
       "   0.39979639649391174,\n",
       "   0.33480024337768555,\n",
       "   0.43483367562294006,\n",
       "   0.37880080938339233,\n",
       "   0.3960376977920532,\n",
       "   0.42690667510032654,\n",
       "   0.40430131554603577,\n",
       "   0.3837026357650757,\n",
       "   0.36316969990730286,\n",
       "   0.3808460533618927,\n",
       "   0.350699245929718,\n",
       "   0.37811776995658875,\n",
       "   0.3961638808250427,\n",
       "   0.3761950433254242,\n",
       "   0.37563779950141907,\n",
       "   0.38513055443763733,\n",
       "   0.36268487572669983,\n",
       "   0.36937645077705383,\n",
       "   0.3734963536262512,\n",
       "   0.41069144010543823,\n",
       "   0.3729546070098877,\n",
       "   0.37538477778434753,\n",
       "   0.3937724828720093,\n",
       "   0.404695987701416,\n",
       "   0.3797697126865387,\n",
       "   0.3752920627593994,\n",
       "   0.3658897578716278,\n",
       "   0.35193657875061035,\n",
       "   0.3629424571990967,\n",
       "   0.38305503129959106,\n",
       "   0.3709363043308258,\n",
       "   0.34164249897003174,\n",
       "   0.3661437928676605,\n",
       "   0.39295539259910583,\n",
       "   0.3609452545642853,\n",
       "   0.3570931851863861,\n",
       "   0.3497127890586853,\n",
       "   0.370146244764328,\n",
       "   0.37059706449508667,\n",
       "   0.30550411343574524,\n",
       "   0.3842884600162506,\n",
       "   0.43729838728904724,\n",
       "   0.4103183150291443,\n",
       "   0.34952282905578613,\n",
       "   0.31296974420547485,\n",
       "   0.34647074341773987,\n",
       "   0.3902043402194977,\n",
       "   0.40226298570632935,\n",
       "   0.3391377329826355,\n",
       "   0.3514766991138458,\n",
       "   0.33474940061569214,\n",
       "   0.378700852394104,\n",
       "   0.3497212529182434,\n",
       "   0.37948206067085266,\n",
       "   0.33324211835861206,\n",
       "   0.3671312928199768,\n",
       "   0.3300134539604187,\n",
       "   0.35191282629966736,\n",
       "   0.36313432455062866,\n",
       "   0.33375826478004456,\n",
       "   0.33213984966278076,\n",
       "   0.3548669219017029,\n",
       "   0.31156420707702637,\n",
       "   0.34431329369544983,\n",
       "   0.35714635252952576,\n",
       "   0.3502137362957001,\n",
       "   0.33960920572280884,\n",
       "   0.367265522480011,\n",
       "   0.32085898518562317,\n",
       "   0.3403186500072479,\n",
       "   0.35936304926872253,\n",
       "   0.3396449685096741,\n",
       "   0.35759443044662476,\n",
       "   0.3282833695411682,\n",
       "   0.31946611404418945,\n",
       "   0.34292444586753845,\n",
       "   0.2948705852031708,\n",
       "   0.35914313793182373,\n",
       "   0.3603324294090271,\n",
       "   0.3297983705997467,\n",
       "   0.3172760009765625,\n",
       "   0.35633301734924316,\n",
       "   0.3548949062824249,\n",
       "   0.3182027041912079,\n",
       "   0.3119926154613495,\n",
       "   0.35248705744743347,\n",
       "   0.30537599325180054,\n",
       "   0.3145594298839569,\n",
       "   0.33595457673072815,\n",
       "   0.2947646379470825,\n",
       "   0.2878037095069885,\n",
       "   0.3289840519428253,\n",
       "   0.3189990818500519,\n",
       "   0.3265957534313202,\n",
       "   0.33066827058792114,\n",
       "   0.3472042381763458,\n",
       "   0.3168077766895294,\n",
       "   0.34939220547676086,\n",
       "   0.3289233446121216,\n",
       "   0.30809086561203003,\n",
       "   0.3323536813259125,\n",
       "   0.3106776773929596,\n",
       "   0.3235033452510834,\n",
       "   0.32216599583625793,\n",
       "   0.33909374475479126,\n",
       "   0.3155227601528168,\n",
       "   0.3135022819042206,\n",
       "   0.329317182302475,\n",
       "   0.29625725746154785,\n",
       "   0.31964659690856934,\n",
       "   0.33664271235466003,\n",
       "   0.31067368388175964,\n",
       "   0.33369651436805725,\n",
       "   0.2953377962112427,\n",
       "   0.33883410692214966,\n",
       "   0.31290310621261597,\n",
       "   0.3231757879257202,\n",
       "   0.2954082190990448,\n",
       "   0.32919761538505554,\n",
       "   0.3147220015525818,\n",
       "   0.3039299547672272,\n",
       "   0.3179110586643219,\n",
       "   0.3286133110523224,\n",
       "   0.2996712923049927,\n",
       "   0.2700367271900177,\n",
       "   0.27857038378715515,\n",
       "   0.3067930042743683,\n",
       "   0.3070634603500366,\n",
       "   0.3110119700431824,\n",
       "   0.3401547372341156,\n",
       "   0.31153371930122375,\n",
       "   0.28941264748573303,\n",
       "   0.3299541175365448,\n",
       "   0.31773000955581665,\n",
       "   0.307767778635025,\n",
       "   0.3093416392803192,\n",
       "   0.33569151163101196,\n",
       "   0.33603984117507935,\n",
       "   0.31316879391670227,\n",
       "   0.30402466654777527,\n",
       "   0.33199697732925415,\n",
       "   0.299842894077301,\n",
       "   0.3089456260204315,\n",
       "   0.29034048318862915,\n",
       "   0.2920670807361603,\n",
       "   0.29303157329559326,\n",
       "   0.3336309492588043,\n",
       "   0.3495579957962036,\n",
       "   0.30062243342399597,\n",
       "   0.3185673654079437,\n",
       "   0.3176426887512207,\n",
       "   0.30438512563705444,\n",
       "   0.31445133686065674,\n",
       "   0.3251703083515167,\n",
       "   0.2836677134037018,\n",
       "   0.3350788354873657,\n",
       "   0.30617988109588623,\n",
       "   0.2963622212409973,\n",
       "   0.3053281009197235,\n",
       "   0.28230607509613037,\n",
       "   0.28152772784233093,\n",
       "   0.28164178133010864,\n",
       "   0.32880711555480957,\n",
       "   0.2894132137298584,\n",
       "   0.28740063309669495,\n",
       "   0.28223180770874023,\n",
       "   0.29737457633018494,\n",
       "   0.30110976099967957,\n",
       "   0.29743796586990356,\n",
       "   0.3289629817008972,\n",
       "   0.32590001821517944,\n",
       "   0.2922039031982422,\n",
       "   0.29699209332466125,\n",
       "   0.2781520187854767,\n",
       "   0.31053316593170166,\n",
       "   0.31199750304222107,\n",
       "   0.30880045890808105,\n",
       "   0.2862670421600342,\n",
       "   0.272236168384552,\n",
       "   0.283873587846756,\n",
       "   0.2946140766143799,\n",
       "   0.28471896052360535,\n",
       "   0.29320213198661804,\n",
       "   0.28218749165534973,\n",
       "   0.2973202168941498,\n",
       "   0.30364078283309937,\n",
       "   0.3026958703994751,\n",
       "   0.27547529339790344,\n",
       "   0.30059152841567993,\n",
       "   0.2986498475074768,\n",
       "   0.30350327491760254,\n",
       "   0.292749285697937,\n",
       "   0.3037694990634918,\n",
       "   0.31659141182899475,\n",
       "   0.31589362025260925,\n",
       "   0.2791593372821808,\n",
       "   0.3004941940307617,\n",
       "   0.2882789671421051,\n",
       "   0.2854529023170471,\n",
       "   0.27681782841682434,\n",
       "   0.2740519642829895,\n",
       "   0.28398969769477844,\n",
       "   0.3334972858428955,\n",
       "   0.3056108355522156,\n",
       "   0.3235413134098053,\n",
       "   0.3044586181640625,\n",
       "   0.3025968372821808,\n",
       "   0.31991341710090637,\n",
       "   0.2938306927680969,\n",
       "   0.3122827112674713,\n",
       "   0.28902336955070496,\n",
       "   0.24014796316623688,\n",
       "   0.2815621793270111,\n",
       "   0.28658050298690796,\n",
       "   0.29606693983078003,\n",
       "   0.2784388065338135,\n",
       "   0.27110037207603455,\n",
       "   0.2810814082622528,\n",
       "   0.28188464045524597,\n",
       "   0.2904129922389984,\n",
       "   0.28886881470680237,\n",
       "   0.2879045903682709,\n",
       "   0.3028678297996521,\n",
       "   0.2716921865940094,\n",
       "   0.29624542593955994,\n",
       "   0.25922879576683044,\n",
       "   0.275028258562088,\n",
       "   0.27302396297454834,\n",
       "   0.30222076177597046,\n",
       "   0.2561623156070709,\n",
       "   0.31217846274375916,\n",
       "   0.283258318901062,\n",
       "   0.29827970266342163,\n",
       "   0.28705793619155884,\n",
       "   0.2848954498767853,\n",
       "   0.30529725551605225,\n",
       "   0.26599445939064026,\n",
       "   0.26491859555244446,\n",
       "   0.27976781129837036,\n",
       "   0.27957919239997864,\n",
       "   0.2859254777431488,\n",
       "   0.297523558139801,\n",
       "   0.2729806900024414,\n",
       "   0.26528382301330566,\n",
       "   0.26652291417121887,\n",
       "   0.2908446788787842,\n",
       "   0.26523804664611816,\n",
       "   0.27972692251205444,\n",
       "   0.2627834975719452,\n",
       "   0.3159438967704773,\n",
       "   0.2634861171245575,\n",
       "   0.28635069727897644,\n",
       "   0.28170087933540344,\n",
       "   0.2867109179496765,\n",
       "   0.25931647419929504,\n",
       "   0.2780509889125824,\n",
       "   0.2835827171802521,\n",
       "   0.2938605546951294,\n",
       "   0.2601519227027893,\n",
       "   0.29112526774406433,\n",
       "   0.27057310938835144,\n",
       "   0.2822774350643158,\n",
       "   0.2705862820148468,\n",
       "   0.2660134434700012,\n",
       "   0.2605765759944916,\n",
       "   0.27204322814941406,\n",
       "   0.27954769134521484,\n",
       "   0.250897616147995,\n",
       "   0.25298354029655457,\n",
       "   0.25154176354408264,\n",
       "   0.27924320101737976,\n",
       "   0.2293156087398529,\n",
       "   0.28009575605392456,\n",
       "   0.26435133814811707,\n",
       "   0.27159038186073303,\n",
       "   0.2567812204360962,\n",
       "   0.26127392053604126,\n",
       "   0.24204981327056885,\n",
       "   0.2765044867992401,\n",
       "   0.2559584081172943,\n",
       "   0.2683981955051422,\n",
       "   0.26095861196517944,\n",
       "   0.26028555631637573,\n",
       "   0.2916395366191864,\n",
       "   0.24633781611919403,\n",
       "   0.25718405842781067,\n",
       "   0.26774948835372925,\n",
       "   0.24678702652454376,\n",
       "   0.2433176338672638,\n",
       "   0.26185372471809387,\n",
       "   0.28466954827308655,\n",
       "   0.24786396324634552,\n",
       "   0.2513473927974701,\n",
       "   0.2589264214038849,\n",
       "   0.28641679883003235,\n",
       "   0.25744619965553284,\n",
       "   0.26017582416534424,\n",
       "   0.23856490850448608,\n",
       "   0.2746378183364868,\n",
       "   0.2671195864677429,\n",
       "   0.2806446850299835,\n",
       "   0.2648871839046478,\n",
       "   0.24841532111167908,\n",
       "   0.2374783754348755,\n",
       "   0.2435719072818756,\n",
       "   0.28536996245384216,\n",
       "   0.2565353810787201,\n",
       "   0.265446275472641,\n",
       "   0.2659449279308319,\n",
       "   0.24856992065906525,\n",
       "   0.27957338094711304,\n",
       "   0.2767028510570526,\n",
       "   0.24820169806480408,\n",
       "   0.28869444131851196,\n",
       "   0.2627078890800476,\n",
       "   0.2827152907848358,\n",
       "   0.23544853925704956,\n",
       "   0.2685157358646393,\n",
       "   0.26961636543273926,\n",
       "   0.2555970251560211,\n",
       "   0.26370057463645935,\n",
       "   0.24604421854019165,\n",
       "   0.25502026081085205,\n",
       "   0.25479280948638916,\n",
       "   0.2733522653579712,\n",
       "   0.2444332242012024,\n",
       "   0.26181936264038086,\n",
       "   0.2486691027879715,\n",
       "   0.2623606026172638,\n",
       "   0.24232517182826996,\n",
       "   0.27614715695381165,\n",
       "   0.24739687144756317,\n",
       "   0.250215083360672,\n",
       "   0.2666875720024109,\n",
       "   0.2371724545955658,\n",
       "   0.24289511144161224,\n",
       "   0.2682959735393524,\n",
       "   0.24956777691841125,\n",
       "   0.24837501347064972,\n",
       "   0.25507456064224243,\n",
       "   0.2479427009820938,\n",
       "   0.27353641390800476,\n",
       "   0.24178895354270935,\n",
       "   0.25521454215049744,\n",
       "   0.26235806941986084,\n",
       "   0.24372144043445587,\n",
       "   0.28450092673301697,\n",
       "   0.23815380036830902,\n",
       "   0.24540989100933075,\n",
       "   0.24363262951374054,\n",
       "   0.27234652638435364,\n",
       "   0.25683096051216125,\n",
       "   0.2532978355884552,\n",
       "   0.2760605812072754,\n",
       "   0.24662120640277863,\n",
       "   0.2522510886192322,\n",
       "   0.26662778854370117,\n",
       "   0.22860614955425262,\n",
       "   0.2564331591129303,\n",
       "   0.2557853162288666,\n",
       "   0.24315370619297028,\n",
       "   0.2682643532752991,\n",
       "   0.24708329141139984,\n",
       "   0.2568408250808716,\n",
       "   0.23029311001300812,\n",
       "   0.2451222687959671,\n",
       "   0.25074487924575806,\n",
       "   0.2619806230068207,\n",
       "   0.25804394483566284,\n",
       "   0.2613571584224701,\n",
       "   0.2408660650253296,\n",
       "   0.252354234457016,\n",
       "   0.22965848445892334,\n",
       "   0.27582666277885437,\n",
       "   0.24100913107395172,\n",
       "   0.24713541567325592,\n",
       "   0.2587684392929077,\n",
       "   0.23530657589435577,\n",
       "   0.2438763976097107,\n",
       "   0.2549925148487091,\n",
       "   0.237015962600708,\n",
       "   0.2481498271226883,\n",
       "   0.27477404475212097,\n",
       "   0.2490798532962799,\n",
       "   0.2749130129814148,\n",
       "   0.24002882838249207,\n",
       "   0.256552129983902,\n",
       "   0.23505418002605438,\n",
       "   0.2659825086593628,\n",
       "   0.25886857509613037,\n",
       "   0.2384311705827713,\n",
       "   0.25501495599746704,\n",
       "   0.24737437069416046,\n",
       "   0.23965154588222504,\n",
       "   0.2577472925186157,\n",
       "   0.24135009944438934,\n",
       "   0.2594515383243561,\n",
       "   0.23519498109817505,\n",
       "   0.22375690937042236,\n",
       "   0.23921316862106323,\n",
       "   0.2503889501094818,\n",
       "   0.24375122785568237,\n",
       "   0.23613490164279938,\n",
       "   0.25047552585601807,\n",
       "   0.24329321086406708,\n",
       "   0.24460174143314362,\n",
       "   0.22933007776737213,\n",
       "   0.23461399972438812,\n",
       "   0.26195546984672546,\n",
       "   0.2509361803531647,\n",
       "   0.24815885722637177,\n",
       "   0.24710920453071594,\n",
       "   0.2261352837085724,\n",
       "   0.2476201206445694,\n",
       "   0.22847844660282135,\n",
       "   0.2469542920589447,\n",
       "   0.22625060379505157,\n",
       "   0.22574053704738617,\n",
       "   0.22580450773239136,\n",
       "   0.24634721875190735,\n",
       "   0.23905545473098755,\n",
       "   0.24301007390022278,\n",
       "   0.2327328324317932,\n",
       "   0.2492673099040985,\n",
       "   0.24185706675052643,\n",
       "   0.22820614278316498,\n",
       "   0.23217551410198212,\n",
       "   0.25221019983291626,\n",
       "   0.24753327667713165,\n",
       "   0.22822311520576477,\n",
       "   0.2626104950904846,\n",
       "   0.23385246098041534,\n",
       "   0.24512460827827454,\n",
       "   0.22601447999477386,\n",
       "   0.23454560339450836,\n",
       "   0.2378775179386139,\n",
       "   0.22201481461524963,\n",
       "   0.24677111208438873,\n",
       "   0.25507670640945435,\n",
       "   0.2296527922153473,\n",
       "   0.2416057586669922,\n",
       "   0.2361602932214737,\n",
       "   0.24117609858512878,\n",
       "   0.24157941341400146,\n",
       "   0.22839179635047913,\n",
       "   0.2315165102481842,\n",
       "   0.24846762418746948,\n",
       "   0.2176872193813324,\n",
       "   0.23598438501358032,\n",
       "   0.2373906672000885,\n",
       "   0.21998648345470428,\n",
       "   0.2316443920135498,\n",
       "   0.2227238267660141,\n",
       "   0.21692553162574768,\n",
       "   0.2537766098976135,\n",
       "   0.23585888743400574,\n",
       "   0.2439340502023697,\n",
       "   0.22285401821136475,\n",
       "   0.2364048808813095,\n",
       "   0.21621853113174438,\n",
       "   0.2392805516719818,\n",
       "   0.25091513991355896,\n",
       "   0.22959749400615692,\n",
       "   0.24516873061656952,\n",
       "   0.23609331250190735,\n",
       "   0.222734734416008,\n",
       "   0.227478489279747],\n",
       "  'train_voc_loss': [0.8852471709251404,\n",
       "   0.8271220326423645,\n",
       "   0.8245803713798523,\n",
       "   0.7943403124809265,\n",
       "   0.7677792906761169,\n",
       "   0.8887820243835449,\n",
       "   0.780066967010498,\n",
       "   0.7063632607460022,\n",
       "   0.665008544921875,\n",
       "   0.6360357999801636,\n",
       "   0.6751488447189331,\n",
       "   0.5121215581893921,\n",
       "   0.6885228157043457,\n",
       "   0.5413527488708496,\n",
       "   0.5227110981941223,\n",
       "   0.5578662157058716,\n",
       "   0.5363849401473999,\n",
       "   0.5601836442947388,\n",
       "   0.45936623215675354,\n",
       "   0.42803794145584106,\n",
       "   0.4393986463546753,\n",
       "   0.4207606613636017,\n",
       "   0.5009564161300659,\n",
       "   0.40898582339286804,\n",
       "   0.46286502480506897,\n",
       "   0.4273662865161896,\n",
       "   0.501331627368927,\n",
       "   0.39921703934669495,\n",
       "   0.38596710562705994,\n",
       "   0.5357410311698914,\n",
       "   0.45016542077064514,\n",
       "   0.4120664894580841,\n",
       "   0.3842914402484894,\n",
       "   0.35148879885673523,\n",
       "   0.4316345453262329,\n",
       "   0.4040207862854004,\n",
       "   0.39728090167045593,\n",
       "   0.3872593641281128,\n",
       "   0.37273675203323364,\n",
       "   0.44903823733329773,\n",
       "   0.3458276391029358,\n",
       "   0.3553279936313629,\n",
       "   0.3852081596851349,\n",
       "   0.311055064201355,\n",
       "   0.35879063606262207,\n",
       "   0.3681248426437378,\n",
       "   0.37807056307792664,\n",
       "   0.3328763246536255,\n",
       "   0.29987916350364685,\n",
       "   0.33794111013412476,\n",
       "   0.30490341782569885,\n",
       "   0.35956060886383057,\n",
       "   0.40138643980026245,\n",
       "   0.32437849044799805,\n",
       "   0.27924537658691406,\n",
       "   0.3331220746040344,\n",
       "   0.3294214606285095,\n",
       "   0.3164195418357849,\n",
       "   0.2936267554759979,\n",
       "   0.263437956571579,\n",
       "   0.3215175271034241,\n",
       "   0.29327499866485596,\n",
       "   0.2603541314601898,\n",
       "   0.28774988651275635,\n",
       "   0.31385183334350586,\n",
       "   0.27607905864715576,\n",
       "   0.2556484043598175,\n",
       "   0.30466246604919434,\n",
       "   0.27676013112068176,\n",
       "   0.29203173518180847,\n",
       "   0.25784167647361755,\n",
       "   0.31060317158699036,\n",
       "   0.29986733198165894,\n",
       "   0.2798643112182617,\n",
       "   0.25029176473617554,\n",
       "   0.2568930387496948,\n",
       "   0.25487762689590454,\n",
       "   0.25908780097961426,\n",
       "   0.2810716927051544,\n",
       "   0.25074636936187744,\n",
       "   0.2249090075492859,\n",
       "   0.2377542108297348,\n",
       "   0.24817690253257751,\n",
       "   0.23882581293582916,\n",
       "   0.25240442156791687,\n",
       "   0.2985932230949402,\n",
       "   0.24341191351413727,\n",
       "   0.2407360076904297,\n",
       "   0.2353491187095642,\n",
       "   0.2618057131767273,\n",
       "   0.25863879919052124,\n",
       "   0.24616332352161407,\n",
       "   0.25943541526794434,\n",
       "   0.2399107664823532,\n",
       "   0.2560954988002777,\n",
       "   0.22099685668945312,\n",
       "   0.2096491903066635,\n",
       "   0.23484092950820923,\n",
       "   0.2384093999862671,\n",
       "   0.25954756140708923,\n",
       "   0.25445395708084106,\n",
       "   0.2333923578262329,\n",
       "   0.23212608695030212,\n",
       "   0.2204921543598175,\n",
       "   0.22302840650081635,\n",
       "   0.21175384521484375,\n",
       "   0.24911531805992126,\n",
       "   0.2314399778842926,\n",
       "   0.3654968738555908,\n",
       "   0.20790712535381317,\n",
       "   0.18264047801494598,\n",
       "   0.2177809178829193,\n",
       "   0.22700349986553192,\n",
       "   0.20347462594509125,\n",
       "   0.21831099689006805,\n",
       "   0.23897378146648407,\n",
       "   0.23018161952495575,\n",
       "   0.17249926924705505,\n",
       "   0.20945702493190765,\n",
       "   0.193093940615654,\n",
       "   0.20082174241542816,\n",
       "   0.17681634426116943,\n",
       "   0.21407921612262726,\n",
       "   0.2226022332906723,\n",
       "   0.20822468400001526,\n",
       "   0.18146724998950958,\n",
       "   0.20877531170845032,\n",
       "   0.31632092595100403,\n",
       "   0.18487021327018738,\n",
       "   0.22135108709335327,\n",
       "   0.1883503645658493,\n",
       "   0.2339966744184494,\n",
       "   0.303075909614563,\n",
       "   0.2368054836988449,\n",
       "   0.19905062019824982,\n",
       "   0.19739475846290588,\n",
       "   0.18214519321918488,\n",
       "   0.21804024279117584,\n",
       "   0.21454036235809326,\n",
       "   0.3105553686618805,\n",
       "   0.2046014368534088,\n",
       "   0.22439882159233093,\n",
       "   0.18139559030532837,\n",
       "   0.17483976483345032,\n",
       "   0.20520561933517456,\n",
       "   0.17819556593894958,\n",
       "   0.16439378261566162,\n",
       "   0.16684108972549438,\n",
       "   0.18409693241119385,\n",
       "   0.18326997756958008,\n",
       "   0.19897839426994324,\n",
       "   0.19979269802570343,\n",
       "   0.2001548558473587,\n",
       "   0.17684955894947052,\n",
       "   0.172618106007576,\n",
       "   0.295582115650177,\n",
       "   0.1895545870065689,\n",
       "   0.18359661102294922,\n",
       "   0.19011366367340088,\n",
       "   0.16832005977630615,\n",
       "   0.1759393960237503,\n",
       "   0.16669432818889618,\n",
       "   0.19765225052833557,\n",
       "   0.17600521445274353,\n",
       "   0.1520645171403885,\n",
       "   0.16008272767066956,\n",
       "   0.18640506267547607,\n",
       "   0.15481208264827728,\n",
       "   0.1729368269443512,\n",
       "   0.15944506227970123,\n",
       "   0.16028429567813873,\n",
       "   0.16791942715644836,\n",
       "   0.15524299442768097,\n",
       "   0.1854954957962036,\n",
       "   0.14789599180221558,\n",
       "   0.168827623128891,\n",
       "   0.14785756170749664,\n",
       "   0.16237574815750122,\n",
       "   0.17063400149345398,\n",
       "   0.15819795429706573,\n",
       "   0.1661101132631302,\n",
       "   0.15163515508174896,\n",
       "   0.14891958236694336,\n",
       "   0.1626957356929779,\n",
       "   0.16981641948223114,\n",
       "   0.15499210357666016,\n",
       "   0.1682262420654297,\n",
       "   0.17003343999385834,\n",
       "   0.16684046387672424,\n",
       "   0.29994654655456543,\n",
       "   0.15258124470710754,\n",
       "   0.157011479139328,\n",
       "   0.17757706344127655,\n",
       "   0.1621357500553131,\n",
       "   0.13683435320854187,\n",
       "   0.13860781490802765,\n",
       "   0.15264467895030975,\n",
       "   0.16209439933300018,\n",
       "   0.15506915748119354,\n",
       "   0.15073591470718384,\n",
       "   0.15751157701015472,\n",
       "   0.16663484275341034,\n",
       "   0.16979563236236572,\n",
       "   0.14102527499198914,\n",
       "   0.17578689754009247,\n",
       "   0.1496446281671524,\n",
       "   0.15622980892658234,\n",
       "   0.15067505836486816,\n",
       "   0.16352172195911407,\n",
       "   0.1601143181324005,\n",
       "   0.15669238567352295,\n",
       "   0.14153270423412323,\n",
       "   0.15778499841690063,\n",
       "   0.15337136387825012,\n",
       "   0.17298968136310577,\n",
       "   0.1363377571105957,\n",
       "   0.15247564017772675,\n",
       "   0.1351269632577896,\n",
       "   0.1299467235803604,\n",
       "   0.13930568099021912,\n",
       "   0.14698061347007751,\n",
       "   0.14556729793548584,\n",
       "   0.2104678750038147,\n",
       "   0.14930564165115356,\n",
       "   0.16185054183006287,\n",
       "   0.15715667605400085,\n",
       "   0.16874881088733673,\n",
       "   0.14141221344470978,\n",
       "   0.1541743129491806,\n",
       "   0.13726255297660828,\n",
       "   0.1335480809211731,\n",
       "   0.14182908833026886,\n",
       "   0.1353529840707779,\n",
       "   0.14629244804382324,\n",
       "   0.12947651743888855,\n",
       "   0.1490180343389511,\n",
       "   0.13680793344974518,\n",
       "   0.1425824910402298,\n",
       "   0.14461089670658112,\n",
       "   0.16369648277759552,\n",
       "   0.13834764063358307,\n",
       "   0.13292264938354492,\n",
       "   0.13578279316425323,\n",
       "   0.13705891370773315,\n",
       "   0.13869799673557281,\n",
       "   0.13509511947631836,\n",
       "   0.16723234951496124,\n",
       "   0.14831072092056274,\n",
       "   0.15424741804599762,\n",
       "   0.1460677981376648,\n",
       "   0.1496051847934723,\n",
       "   0.13024888932704926,\n",
       "   0.14095240831375122,\n",
       "   0.13596411049365997,\n",
       "   0.12067818641662598,\n",
       "   0.13430245220661163,\n",
       "   0.1333104372024536,\n",
       "   0.12989288568496704,\n",
       "   0.13634997606277466,\n",
       "   0.13627108931541443,\n",
       "   0.13117150962352753,\n",
       "   0.13886664807796478,\n",
       "   0.1397598385810852,\n",
       "   0.1304750293493271,\n",
       "   0.12516739964485168,\n",
       "   0.14886067807674408,\n",
       "   0.13492299616336823,\n",
       "   0.13359926640987396,\n",
       "   0.1373794972896576,\n",
       "   0.11765436828136444,\n",
       "   0.12585441768169403,\n",
       "   0.126969575881958,\n",
       "   0.1294790804386139,\n",
       "   0.13108469545841217,\n",
       "   0.12037713825702667,\n",
       "   0.1256828010082245,\n",
       "   0.1326037347316742,\n",
       "   0.12895873188972473,\n",
       "   0.13983772695064545,\n",
       "   0.131320059299469,\n",
       "   0.13639280200004578,\n",
       "   0.1553158462047577,\n",
       "   0.13676971197128296,\n",
       "   0.12871137261390686,\n",
       "   0.12888844311237335,\n",
       "   0.1253814548254013,\n",
       "   0.1364590972661972,\n",
       "   0.13110525906085968,\n",
       "   0.12587231397628784,\n",
       "   0.11636736989021301,\n",
       "   0.13654282689094543,\n",
       "   0.12111862003803253,\n",
       "   0.14004960656166077,\n",
       "   0.11715564876794815,\n",
       "   0.11017818003892899,\n",
       "   0.11602835357189178,\n",
       "   0.11964831501245499,\n",
       "   0.13159003853797913,\n",
       "   0.13319280743598938,\n",
       "   0.11821085214614868,\n",
       "   0.12153351306915283,\n",
       "   0.1332104206085205,\n",
       "   0.1334228664636612,\n",
       "   0.13301536440849304,\n",
       "   0.12662562727928162,\n",
       "   0.12346647679805756,\n",
       "   0.12288226187229156,\n",
       "   0.11450793594121933,\n",
       "   0.15586961805820465,\n",
       "   0.12488263100385666,\n",
       "   0.114187091588974,\n",
       "   0.13055361807346344,\n",
       "   0.12037781625986099,\n",
       "   0.12697556614875793,\n",
       "   0.1261340230703354,\n",
       "   0.13672968745231628,\n",
       "   0.13512347638607025,\n",
       "   0.11932171881198883,\n",
       "   0.12133502215147018,\n",
       "   0.142642080783844,\n",
       "   0.19399507343769073,\n",
       "   0.1316303163766861,\n",
       "   0.14542144536972046,\n",
       "   0.1176849752664566,\n",
       "   0.12905104458332062,\n",
       "   0.11975857615470886,\n",
       "   0.13055457174777985,\n",
       "   0.11939840018749237,\n",
       "   0.12265773862600327,\n",
       "   0.12151947617530823,\n",
       "   0.12409891188144684,\n",
       "   0.11476162821054459,\n",
       "   0.12731251120567322,\n",
       "   0.11021968722343445,\n",
       "   0.111527219414711,\n",
       "   0.12551283836364746,\n",
       "   0.11392500251531601,\n",
       "   0.12150995433330536,\n",
       "   0.12041532248258591,\n",
       "   0.1275876760482788,\n",
       "   0.11901943385601044,\n",
       "   0.12146664410829544,\n",
       "   0.12077100574970245,\n",
       "   0.11164234578609467,\n",
       "   0.10527811199426651,\n",
       "   0.11267471313476562,\n",
       "   0.11314170062541962,\n",
       "   0.1216459646821022,\n",
       "   0.11289875209331512,\n",
       "   0.12452976405620575,\n",
       "   0.12484510242938995,\n",
       "   0.12021464854478836,\n",
       "   0.10924072563648224,\n",
       "   0.12017132341861725,\n",
       "   0.11309152096509933,\n",
       "   0.12870106101036072,\n",
       "   0.125963494181633,\n",
       "   0.1245192289352417,\n",
       "   0.10669854283332825,\n",
       "   0.13117395341396332,\n",
       "   0.12256782501935959,\n",
       "   0.12142720818519592,\n",
       "   0.12371505051851273,\n",
       "   0.12282544374465942,\n",
       "   0.13024970889091492,\n",
       "   0.11155921220779419,\n",
       "   0.11943653225898743,\n",
       "   0.1121974065899849,\n",
       "   0.11960116773843765,\n",
       "   0.11817266047000885,\n",
       "   0.11229773610830307,\n",
       "   0.10814385861158371,\n",
       "   0.12585963308811188,\n",
       "   0.11849784106016159,\n",
       "   0.11868192255496979,\n",
       "   0.11134973913431168,\n",
       "   0.11034585535526276,\n",
       "   0.12629888951778412,\n",
       "   0.11861627548933029,\n",
       "   0.10637674480676651,\n",
       "   0.11382089555263519,\n",
       "   0.11526859551668167,\n",
       "   0.11095677316188812,\n",
       "   0.10984031111001968,\n",
       "   0.11279185116291046,\n",
       "   0.10685776174068451,\n",
       "   0.10712815076112747,\n",
       "   0.10693324357271194,\n",
       "   0.10997430980205536,\n",
       "   0.14901509881019592,\n",
       "   0.11523867398500443,\n",
       "   0.11902496963739395,\n",
       "   0.11165575683116913,\n",
       "   0.11072646081447601,\n",
       "   0.12344782054424286,\n",
       "   0.12410145252943039,\n",
       "   0.10969943553209305,\n",
       "   0.13537386059761047,\n",
       "   0.16619832813739777,\n",
       "   0.11368456482887268,\n",
       "   0.1037275567650795,\n",
       "   0.10121437162160873,\n",
       "   0.12068233639001846,\n",
       "   0.13519179821014404,\n",
       "   0.1177389919757843,\n",
       "   0.11636645346879959,\n",
       "   0.10097947716712952,\n",
       "   0.1114969477057457,\n",
       "   0.11591549962759018,\n",
       "   0.1088419258594513,\n",
       "   0.1263856440782547,\n",
       "   0.11753197759389877,\n",
       "   0.11777825653553009,\n",
       "   0.11480201780796051,\n",
       "   0.11394041776657104,\n",
       "   0.11396601051092148,\n",
       "   0.10727937519550323,\n",
       "   0.18295583128929138,\n",
       "   0.11782529205083847,\n",
       "   0.1165996566414833,\n",
       "   0.09972486644983292,\n",
       "   0.10486841946840286,\n",
       "   0.1066461130976677,\n",
       "   0.11165761202573776,\n",
       "   0.10727805644273758,\n",
       "   0.10521098226308823,\n",
       "   0.09847082942724228,\n",
       "   0.1094292476773262,\n",
       "   0.10626181960105896,\n",
       "   0.11512906104326248,\n",
       "   0.11155182123184204,\n",
       "   0.15859608352184296,\n",
       "   0.2820052206516266,\n",
       "   0.17204423248767853,\n",
       "   0.11882540583610535,\n",
       "   0.11810722202062607,\n",
       "   0.14221206307411194,\n",
       "   0.09859617054462433,\n",
       "   0.11465659737586975,\n",
       "   0.10596413165330887,\n",
       "   0.1332518756389618,\n",
       "   0.10560345649719238,\n",
       "   0.10247951000928879,\n",
       "   0.11005336046218872,\n",
       "   0.104358971118927,\n",
       "   0.1183718666434288,\n",
       "   0.11029094457626343,\n",
       "   0.10690873116254807,\n",
       "   0.10279721766710281,\n",
       "   0.10344164818525314,\n",
       "   0.10325729846954346,\n",
       "   0.10941310971975327,\n",
       "   0.09549441933631897,\n",
       "   0.14433936774730682,\n",
       "   0.11502585560083389,\n",
       "   0.10352510958909988,\n",
       "   0.10124290734529495,\n",
       "   0.1086275577545166,\n",
       "   0.19818687438964844,\n",
       "   0.11395635455846786,\n",
       "   0.10977144539356232,\n",
       "   0.10551830381155014,\n",
       "   0.11767677962779999,\n",
       "   0.10832339525222778,\n",
       "   0.10061020404100418,\n",
       "   0.09641139954328537,\n",
       "   0.11154831945896149,\n",
       "   0.08832704275846481,\n",
       "   0.10094837099313736,\n",
       "   0.10445874184370041,\n",
       "   0.09983118623495102,\n",
       "   0.09841684252023697,\n",
       "   0.10331703722476959,\n",
       "   0.1076822429895401,\n",
       "   0.10949729382991791,\n",
       "   0.10819895565509796,\n",
       "   0.10318785905838013,\n",
       "   0.10457908362150192,\n",
       "   0.10639376938343048,\n",
       "   0.09833796322345734,\n",
       "   0.09846558421850204,\n",
       "   0.10232909768819809,\n",
       "   0.09557846188545227,\n",
       "   0.10355962812900543,\n",
       "   0.09804626554250717,\n",
       "   0.10934068262577057,\n",
       "   0.11045506596565247,\n",
       "   0.0853211060166359,\n",
       "   0.1044793426990509,\n",
       "   0.09587361663579941,\n",
       "   0.0976434051990509,\n",
       "   0.09767673909664154,\n",
       "   0.10527386516332626,\n",
       "   0.10300105065107346,\n",
       "   0.10908541828393936,\n",
       "   0.0935867577791214,\n",
       "   0.08942682296037674,\n",
       "   0.09559150040149689,\n",
       "   0.08788545429706573,\n",
       "   0.08321165293455124],\n",
       "  'train_jsc_loss': [0.5819567441940308,\n",
       "   0.5868623852729797,\n",
       "   0.5899404287338257,\n",
       "   0.5792057514190674,\n",
       "   0.5904005169868469,\n",
       "   0.5872371792793274,\n",
       "   0.5794860124588013,\n",
       "   0.5810274481773376,\n",
       "   0.5878785252571106,\n",
       "   0.5779719948768616,\n",
       "   0.5788068175315857,\n",
       "   0.5836864709854126,\n",
       "   0.5872562527656555,\n",
       "   0.5829067230224609,\n",
       "   0.5817685127258301,\n",
       "   0.5789904594421387,\n",
       "   0.5760408043861389,\n",
       "   0.5826084613800049,\n",
       "   0.5891193151473999,\n",
       "   0.5901183485984802,\n",
       "   0.589968204498291,\n",
       "   0.5849591493606567,\n",
       "   0.5841581225395203,\n",
       "   0.5774573087692261,\n",
       "   0.5881415605545044,\n",
       "   0.5822608470916748,\n",
       "   0.5814282894134521,\n",
       "   0.5903213024139404,\n",
       "   0.5854775905609131,\n",
       "   0.5807803273200989,\n",
       "   0.5834149718284607,\n",
       "   0.5826181769371033,\n",
       "   0.5774416923522949,\n",
       "   0.5812700390815735,\n",
       "   0.5810250043869019,\n",
       "   0.586417019367218,\n",
       "   0.5775912404060364,\n",
       "   0.5867566466331482,\n",
       "   0.5826572775840759,\n",
       "   0.5809648633003235,\n",
       "   0.5822405815124512,\n",
       "   0.5866733193397522,\n",
       "   0.5842334032058716,\n",
       "   0.5748794674873352,\n",
       "   0.5843276381492615,\n",
       "   0.5856478214263916,\n",
       "   0.5862632393836975,\n",
       "   0.5821021795272827,\n",
       "   0.5808765292167664,\n",
       "   0.5812950730323792,\n",
       "   0.5895308256149292,\n",
       "   0.5863438844680786,\n",
       "   0.5820895433425903,\n",
       "   0.5821195840835571,\n",
       "   0.5794873833656311,\n",
       "   0.5850294828414917,\n",
       "   0.5829059481620789,\n",
       "   0.5891379714012146,\n",
       "   0.574393093585968,\n",
       "   0.5806288719177246,\n",
       "   0.5753276944160461,\n",
       "   0.5852609872817993,\n",
       "   0.5754292011260986,\n",
       "   0.5802443027496338,\n",
       "   0.5810120701789856,\n",
       "   0.5838274359703064,\n",
       "   0.579873263835907,\n",
       "   0.5780228972434998,\n",
       "   0.5805304050445557,\n",
       "   0.5785816311836243,\n",
       "   0.5755626559257507,\n",
       "   0.5803378224372864,\n",
       "   0.5725468993186951,\n",
       "   0.5849465131759644,\n",
       "   0.573649525642395,\n",
       "   0.5842036008834839,\n",
       "   0.5805540680885315,\n",
       "   0.5848085880279541,\n",
       "   0.5820476412773132,\n",
       "   0.57762211561203,\n",
       "   0.5818824172019958,\n",
       "   0.5845884680747986,\n",
       "   0.5711230635643005,\n",
       "   0.5754037499427795,\n",
       "   0.5800323486328125,\n",
       "   0.5784991979598999,\n",
       "   0.5762629508972168,\n",
       "   0.5848684906959534,\n",
       "   0.5726311802864075,\n",
       "   0.5682409405708313,\n",
       "   0.5800087451934814,\n",
       "   0.5808457732200623,\n",
       "   0.5786791443824768,\n",
       "   0.5716405510902405,\n",
       "   0.5761082172393799,\n",
       "   0.5757861137390137,\n",
       "   0.5711933374404907,\n",
       "   0.5764700770378113,\n",
       "   0.5742480754852295,\n",
       "   0.5771372318267822,\n",
       "   0.5820947885513306,\n",
       "   0.5778243541717529,\n",
       "   0.5706641674041748,\n",
       "   0.5781780481338501,\n",
       "   0.5731621384620667,\n",
       "   0.576652467250824,\n",
       "   0.5745831727981567,\n",
       "   0.5689485669136047,\n",
       "   0.5773812532424927,\n",
       "   0.5654081106185913,\n",
       "   0.5648086071014404,\n",
       "   0.580740213394165,\n",
       "   0.5717467665672302,\n",
       "   0.5724800229072571,\n",
       "   0.5746508240699768,\n",
       "   0.5771041512489319,\n",
       "   0.5777045488357544,\n",
       "   0.5715866684913635,\n",
       "   0.5757700204849243,\n",
       "   0.5682497620582581,\n",
       "   0.573194682598114,\n",
       "   0.5650550127029419,\n",
       "   0.5765427350997925,\n",
       "   0.5642417073249817,\n",
       "   0.5747573375701904,\n",
       "   0.5663039088249207,\n",
       "   0.5686025619506836,\n",
       "   0.5720169544219971,\n",
       "   0.5698889493942261,\n",
       "   0.5667688250541687,\n",
       "   0.5708141326904297,\n",
       "   0.5597625970840454,\n",
       "   0.56605464220047,\n",
       "   0.5574852228164673,\n",
       "   0.570950984954834,\n",
       "   0.5602641701698303,\n",
       "   0.5681115388870239,\n",
       "   0.5640481114387512,\n",
       "   0.5705165266990662,\n",
       "   0.5614774227142334,\n",
       "   0.5622124075889587,\n",
       "   0.5699182748794556,\n",
       "   0.5689047574996948,\n",
       "   0.5573469996452332,\n",
       "   0.5646378397941589,\n",
       "   0.555453896522522,\n",
       "   0.5654504895210266,\n",
       "   0.566676914691925,\n",
       "   0.5665178894996643,\n",
       "   0.5553567409515381,\n",
       "   0.5529645085334778,\n",
       "   0.5454638004302979,\n",
       "   0.5604938268661499,\n",
       "   0.5536078810691833,\n",
       "   0.556210994720459,\n",
       "   0.5447061061859131,\n",
       "   0.5572062730789185,\n",
       "   0.5567665696144104,\n",
       "   0.5568108558654785,\n",
       "   0.5567002296447754,\n",
       "   0.5559029579162598,\n",
       "   0.5560106635093689,\n",
       "   0.5505874752998352,\n",
       "   0.5597279071807861,\n",
       "   0.5554316639900208,\n",
       "   0.5468446612358093,\n",
       "   0.5498934984207153,\n",
       "   0.5522995591163635,\n",
       "   0.5525208711624146,\n",
       "   0.5537309050559998,\n",
       "   0.5520947575569153,\n",
       "   0.5505807399749756,\n",
       "   0.5521243214607239,\n",
       "   0.5398743152618408,\n",
       "   0.5423725247383118,\n",
       "   0.5500171780586243,\n",
       "   0.5427617430686951,\n",
       "   0.543610692024231,\n",
       "   0.55357825756073,\n",
       "   0.5519721508026123,\n",
       "   0.5404061079025269,\n",
       "   0.5497703552246094,\n",
       "   0.538320004940033,\n",
       "   0.5443398952484131,\n",
       "   0.5444232225418091,\n",
       "   0.5414124727249146,\n",
       "   0.535759449005127,\n",
       "   0.5423631072044373,\n",
       "   0.5461456775665283,\n",
       "   0.5452191829681396,\n",
       "   0.540248453617096,\n",
       "   0.5414990782737732,\n",
       "   0.5438774824142456,\n",
       "   0.5402277112007141,\n",
       "   0.5423429012298584,\n",
       "   0.5436544418334961,\n",
       "   0.5422185659408569,\n",
       "   0.542417049407959,\n",
       "   0.5356138348579407,\n",
       "   0.5332033038139343,\n",
       "   0.5458270311355591,\n",
       "   0.5244475603103638,\n",
       "   0.5361878871917725,\n",
       "   0.5346956253051758,\n",
       "   0.5352845191955566,\n",
       "   0.5325570106506348,\n",
       "   0.532656192779541,\n",
       "   0.5322361588478088,\n",
       "   0.5302876234054565,\n",
       "   0.5359311103820801,\n",
       "   0.5406314134597778,\n",
       "   0.5302114486694336,\n",
       "   0.5331348180770874,\n",
       "   0.5175060033798218,\n",
       "   0.5204989314079285,\n",
       "   0.5344586372375488,\n",
       "   0.5274614095687866,\n",
       "   0.525789201259613,\n",
       "   0.520108699798584,\n",
       "   0.5235127806663513,\n",
       "   0.5247161984443665,\n",
       "   0.5361786484718323,\n",
       "   0.5100442171096802,\n",
       "   0.5242624282836914,\n",
       "   0.5216798186302185,\n",
       "   0.5160250067710876,\n",
       "   0.5249947309494019,\n",
       "   0.5161193013191223,\n",
       "   0.5324609875679016,\n",
       "   0.5190201997756958,\n",
       "   0.5192198753356934,\n",
       "   0.5223403573036194,\n",
       "   0.5169135332107544,\n",
       "   0.5144548416137695,\n",
       "   0.5227254033088684,\n",
       "   0.5198705196380615,\n",
       "   0.5192990303039551,\n",
       "   0.5153985619544983,\n",
       "   0.5072711110115051,\n",
       "   0.5151169896125793,\n",
       "   0.5104755759239197,\n",
       "   0.5306083559989929,\n",
       "   0.5185927152633667,\n",
       "   0.5164874792098999,\n",
       "   0.508473813533783,\n",
       "   0.5118040442466736,\n",
       "   0.5072634220123291,\n",
       "   0.5070674419403076,\n",
       "   0.5155840516090393,\n",
       "   0.5090323090553284,\n",
       "   0.5084519982337952,\n",
       "   0.5119748115539551,\n",
       "   0.516282320022583,\n",
       "   0.5047771334648132,\n",
       "   0.4912082552909851,\n",
       "   0.5008273720741272,\n",
       "   0.5105997920036316,\n",
       "   0.5023522973060608,\n",
       "   0.4926697313785553,\n",
       "   0.5141937732696533,\n",
       "   0.5004602670669556,\n",
       "   0.5084925293922424,\n",
       "   0.493649959564209,\n",
       "   0.5045672655105591,\n",
       "   0.4921708405017853,\n",
       "   0.49940407276153564,\n",
       "   0.5046908259391785,\n",
       "   0.5060948729515076,\n",
       "   0.504499077796936,\n",
       "   0.5004209876060486,\n",
       "   0.5028950572013855,\n",
       "   0.49621373414993286,\n",
       "   0.49127915501594543,\n",
       "   0.49144691228866577,\n",
       "   0.5063459277153015,\n",
       "   0.4993884563446045,\n",
       "   0.5008103251457214,\n",
       "   0.4871091842651367,\n",
       "   0.4893466532230377,\n",
       "   0.4924326241016388,\n",
       "   0.49528273940086365,\n",
       "   0.49462005496025085,\n",
       "   0.49476388096809387,\n",
       "   0.500997006893158,\n",
       "   0.4961193799972534,\n",
       "   0.4953089952468872,\n",
       "   0.49316754937171936,\n",
       "   0.49378618597984314,\n",
       "   0.49085599184036255,\n",
       "   0.4938506782054901,\n",
       "   0.48429957032203674,\n",
       "   0.49793168902397156,\n",
       "   0.4986574351787567,\n",
       "   0.48670586943626404,\n",
       "   0.4831043779850006,\n",
       "   0.48195329308509827,\n",
       "   0.4912296533584595,\n",
       "   0.4969167411327362,\n",
       "   0.4872820973396301,\n",
       "   0.4920291006565094,\n",
       "   0.4712274968624115,\n",
       "   0.48421356081962585,\n",
       "   0.48300835490226746,\n",
       "   0.49109941720962524,\n",
       "   0.47940370440483093,\n",
       "   0.4926852881908417,\n",
       "   0.4870781898498535,\n",
       "   0.4813518822193146,\n",
       "   0.47912895679473877,\n",
       "   0.46646732091903687,\n",
       "   0.4846060574054718,\n",
       "   0.4739549160003662,\n",
       "   0.4726174473762512,\n",
       "   0.47979387640953064,\n",
       "   0.47833606600761414,\n",
       "   0.4725174307823181,\n",
       "   0.4842723608016968,\n",
       "   0.4842991828918457,\n",
       "   0.48170584440231323,\n",
       "   0.48692816495895386,\n",
       "   0.4731115400791168,\n",
       "   0.47864392399787903,\n",
       "   0.47828271985054016,\n",
       "   0.4742785096168518,\n",
       "   0.4821598529815674,\n",
       "   0.4654046595096588,\n",
       "   0.4705004394054413,\n",
       "   0.48281529545783997,\n",
       "   0.47351139783859253,\n",
       "   0.4709743857383728,\n",
       "   0.48137205839157104,\n",
       "   0.4799879789352417,\n",
       "   0.4808013141155243,\n",
       "   0.469662070274353,\n",
       "   0.466509610414505,\n",
       "   0.47605863213539124,\n",
       "   0.4802134335041046,\n",
       "   0.4820901155471802,\n",
       "   0.4676077365875244,\n",
       "   0.4792149066925049,\n",
       "   0.47056424617767334,\n",
       "   0.4748893678188324,\n",
       "   0.4675692021846771,\n",
       "   0.4646044075489044,\n",
       "   0.47352027893066406,\n",
       "   0.47219809889793396,\n",
       "   0.47632545232772827,\n",
       "   0.47008687257766724,\n",
       "   0.4712115228176117,\n",
       "   0.4763827621936798,\n",
       "   0.46213963627815247,\n",
       "   0.4673161506652832,\n",
       "   0.4681835174560547,\n",
       "   0.4702180027961731,\n",
       "   0.4694460332393646,\n",
       "   0.4637502133846283,\n",
       "   0.4668652415275574,\n",
       "   0.467557817697525,\n",
       "   0.45967891812324524,\n",
       "   0.45653095841407776,\n",
       "   0.48108798265457153,\n",
       "   0.4540468454360962,\n",
       "   0.4700137972831726,\n",
       "   0.4598436951637268,\n",
       "   0.45695531368255615,\n",
       "   0.46912020444869995,\n",
       "   0.4516158401966095,\n",
       "   0.4457840621471405,\n",
       "   0.4670651853084564,\n",
       "   0.45633193850517273,\n",
       "   0.4670140743255615,\n",
       "   0.4630274772644043,\n",
       "   0.46751612424850464,\n",
       "   0.4469023048877716,\n",
       "   0.4623297452926636,\n",
       "   0.45859572291374207,\n",
       "   0.45843425393104553,\n",
       "   0.4651701748371124,\n",
       "   0.4428757131099701,\n",
       "   0.46090880036354065,\n",
       "   0.4704897105693817,\n",
       "   0.45853662490844727,\n",
       "   0.45910707116127014,\n",
       "   0.46124163269996643,\n",
       "   0.4546617567539215,\n",
       "   0.4470849633216858,\n",
       "   0.4596092104911804,\n",
       "   0.4612894058227539,\n",
       "   0.46439746022224426,\n",
       "   0.45615121722221375,\n",
       "   0.4564424753189087,\n",
       "   0.45013463497161865,\n",
       "   0.4619382619857788,\n",
       "   0.4635058343410492,\n",
       "   0.45509251952171326,\n",
       "   0.46651461720466614,\n",
       "   0.43892520666122437,\n",
       "   0.47343331575393677,\n",
       "   0.4490456283092499,\n",
       "   0.45193979144096375,\n",
       "   0.4524349272251129,\n",
       "   0.46595999598503113,\n",
       "   0.4428170621395111,\n",
       "   0.4659697413444519,\n",
       "   0.4500027298927307,\n",
       "   0.45886027812957764,\n",
       "   0.4644399583339691,\n",
       "   0.4572735130786896,\n",
       "   0.44695228338241577,\n",
       "   0.44477126002311707,\n",
       "   0.44949907064437866,\n",
       "   0.44995617866516113,\n",
       "   0.44810402393341064,\n",
       "   0.4393173158168793,\n",
       "   0.4511672258377075,\n",
       "   0.44739437103271484,\n",
       "   0.4372733235359192,\n",
       "   0.44687673449516296,\n",
       "   0.44682273268699646,\n",
       "   0.4443490505218506,\n",
       "   0.4513194262981415,\n",
       "   0.44144347310066223,\n",
       "   0.4417669475078583,\n",
       "   0.44585832953453064,\n",
       "   0.44126155972480774,\n",
       "   0.45098865032196045,\n",
       "   0.45628148317337036,\n",
       "   0.44711077213287354,\n",
       "   0.4531150758266449,\n",
       "   0.45302248001098633,\n",
       "   0.4422130882740021,\n",
       "   0.45302653312683105,\n",
       "   0.44270050525665283,\n",
       "   0.44879764318466187,\n",
       "   0.4430244266986847,\n",
       "   0.4357595145702362,\n",
       "   0.4525966942310333,\n",
       "   0.4404878616333008,\n",
       "   0.45252725481987,\n",
       "   0.4480477273464203,\n",
       "   0.44775956869125366,\n",
       "   0.44305431842803955,\n",
       "   0.45037248730659485,\n",
       "   0.4467164874076843,\n",
       "   0.44697725772857666,\n",
       "   0.4480644166469574,\n",
       "   0.441583514213562,\n",
       "   0.44848185777664185,\n",
       "   0.44761860370635986,\n",
       "   0.44896426796913147,\n",
       "   0.43075183033943176,\n",
       "   0.45283716917037964,\n",
       "   0.43545472621917725,\n",
       "   0.4531176686286926,\n",
       "   0.4457870423793793,\n",
       "   0.4436279833316803,\n",
       "   0.4400659203529358,\n",
       "   0.4371955990791321,\n",
       "   0.45955339074134827,\n",
       "   0.4382373094558716,\n",
       "   0.4432285726070404,\n",
       "   0.4351159632205963,\n",
       "   0.4444921612739563,\n",
       "   0.45147383213043213,\n",
       "   0.43980687856674194,\n",
       "   0.44651150703430176,\n",
       "   0.4502776563167572,\n",
       "   0.4226929247379303,\n",
       "   0.42656978964805603,\n",
       "   0.43831107020378113,\n",
       "   0.4387660324573517,\n",
       "   0.4488106369972229,\n",
       "   0.43212535977363586,\n",
       "   0.4381602108478546,\n",
       "   0.4369826018810272,\n",
       "   0.4384051263332367,\n",
       "   0.43828561902046204,\n",
       "   0.4403008818626404,\n",
       "   0.44201454520225525,\n",
       "   0.4367225766181946,\n",
       "   0.44179052114486694,\n",
       "   0.43796223402023315,\n",
       "   0.4515105187892914,\n",
       "   0.43393442034721375,\n",
       "   0.44397786259651184,\n",
       "   0.4318262040615082,\n",
       "   0.4246373474597931,\n",
       "   0.44574007391929626,\n",
       "   0.44067901372909546,\n",
       "   0.4428902864456177,\n",
       "   0.43533357977867126,\n",
       "   0.43122079968452454,\n",
       "   0.42795228958129883,\n",
       "   0.43539923429489136,\n",
       "   0.4380689859390259,\n",
       "   0.44773420691490173,\n",
       "   0.434419184923172,\n",
       "   0.4404456615447998,\n",
       "   0.4450972378253937,\n",
       "   0.4317552149295807],\n",
       "  'train_ff_loss': [0.4242706894874573,\n",
       "   0.43496784567832947,\n",
       "   0.4381069540977478,\n",
       "   0.4245088994503021,\n",
       "   0.4296835958957672,\n",
       "   0.44994252920150757,\n",
       "   0.431829571723938,\n",
       "   0.44177666306495667,\n",
       "   0.4158618450164795,\n",
       "   0.41596487164497375,\n",
       "   0.46923163533210754,\n",
       "   0.42189350724220276,\n",
       "   0.4287447929382324,\n",
       "   0.47439318895339966,\n",
       "   0.4234619438648224,\n",
       "   0.4374981224536896,\n",
       "   0.4229395389556885,\n",
       "   0.44125592708587646,\n",
       "   0.4358593225479126,\n",
       "   0.41820472478866577,\n",
       "   0.4236624240875244,\n",
       "   0.42242738604545593,\n",
       "   0.40076279640197754,\n",
       "   0.4084104001522064,\n",
       "   0.5057165622711182,\n",
       "   0.40821993350982666,\n",
       "   0.41354215145111084,\n",
       "   0.42031002044677734,\n",
       "   0.39162716269493103,\n",
       "   0.3851170241832733,\n",
       "   0.41025498509407043,\n",
       "   0.4227936863899231,\n",
       "   0.4309726059436798,\n",
       "   0.4207308292388916,\n",
       "   0.41813328862190247,\n",
       "   0.43523481488227844,\n",
       "   0.3975107669830322,\n",
       "   0.39905810356140137,\n",
       "   0.4270815849304199,\n",
       "   0.4127497375011444,\n",
       "   0.40044787526130676,\n",
       "   0.4259263575077057,\n",
       "   0.4044117331504822,\n",
       "   0.41156598925590515,\n",
       "   0.4329594671726227,\n",
       "   0.3863672912120819,\n",
       "   0.38804471492767334,\n",
       "   0.41650834679603577,\n",
       "   0.40475279092788696,\n",
       "   0.38590678572654724,\n",
       "   0.40534621477127075,\n",
       "   0.4192034900188446,\n",
       "   0.40400227904319763,\n",
       "   0.4166608452796936,\n",
       "   0.41549456119537354,\n",
       "   0.3990837037563324,\n",
       "   0.41960951685905457,\n",
       "   0.4132094979286194,\n",
       "   0.4311334788799286,\n",
       "   0.38948583602905273,\n",
       "   0.4084843099117279,\n",
       "   0.43409350514411926,\n",
       "   0.4191109836101532,\n",
       "   0.39525943994522095,\n",
       "   0.4031333327293396,\n",
       "   0.381510853767395,\n",
       "   0.4389820694923401,\n",
       "   0.398080050945282,\n",
       "   0.4118649661540985,\n",
       "   0.41810736060142517,\n",
       "   0.38737693428993225,\n",
       "   0.43583470582962036,\n",
       "   0.38570845127105713,\n",
       "   0.4131692945957184,\n",
       "   0.39102599024772644,\n",
       "   0.38989320397377014,\n",
       "   0.390162855386734,\n",
       "   0.43025773763656616,\n",
       "   0.3881923258304596,\n",
       "   0.42404136061668396,\n",
       "   0.3840239644050598,\n",
       "   0.3968643248081207,\n",
       "   0.44036710262298584,\n",
       "   0.4095906615257263,\n",
       "   0.37171074748039246,\n",
       "   0.4279671311378479,\n",
       "   0.4372575283050537,\n",
       "   0.4317711591720581,\n",
       "   0.4172214865684509,\n",
       "   0.36825722455978394,\n",
       "   0.3882388174533844,\n",
       "   0.39974501729011536,\n",
       "   0.40415406227111816,\n",
       "   0.40421414375305176,\n",
       "   0.3887637257575989,\n",
       "   0.43162667751312256,\n",
       "   0.4170246720314026,\n",
       "   0.3949141502380371,\n",
       "   0.4240873157978058,\n",
       "   0.4223557710647583,\n",
       "   0.4015437960624695,\n",
       "   0.3722603917121887,\n",
       "   0.3628107011318207,\n",
       "   0.41295668482780457,\n",
       "   0.38217514753341675,\n",
       "   0.40335074067115784,\n",
       "   0.4031303822994232,\n",
       "   0.39927196502685547,\n",
       "   0.40666237473487854,\n",
       "   0.38425442576408386,\n",
       "   0.38961488008499146,\n",
       "   0.41143083572387695,\n",
       "   0.41886407136917114,\n",
       "   0.39412423968315125,\n",
       "   0.4175049066543579,\n",
       "   0.35538554191589355,\n",
       "   0.3911318778991699,\n",
       "   0.3859151601791382,\n",
       "   0.37491777539253235,\n",
       "   0.38019630312919617,\n",
       "   0.3725135326385498,\n",
       "   0.400319904088974,\n",
       "   0.3902404308319092,\n",
       "   0.40816742181777954,\n",
       "   0.37450215220451355,\n",
       "   0.38389188051223755,\n",
       "   0.3570948541164398,\n",
       "   0.35226452350616455,\n",
       "   0.36910295486450195,\n",
       "   0.3917750418186188,\n",
       "   0.4007474184036255,\n",
       "   0.38695842027664185,\n",
       "   0.3631123900413513,\n",
       "   0.3527749180793762,\n",
       "   0.37050920724868774,\n",
       "   0.37862446904182434,\n",
       "   0.38776445388793945,\n",
       "   0.36958232522010803,\n",
       "   0.3716335892677307,\n",
       "   0.36481425166130066,\n",
       "   0.36677783727645874,\n",
       "   0.3832608163356781,\n",
       "   0.35841894149780273,\n",
       "   0.3619993031024933,\n",
       "   0.41447389125823975,\n",
       "   0.36003735661506653,\n",
       "   0.39835476875305176,\n",
       "   0.3531740605831146,\n",
       "   0.3600879907608032,\n",
       "   0.39047640562057495,\n",
       "   0.3852176070213318,\n",
       "   0.3480018675327301,\n",
       "   0.361381471157074,\n",
       "   0.3700146973133087,\n",
       "   0.34786099195480347,\n",
       "   0.3510488271713257,\n",
       "   0.3637489676475525,\n",
       "   0.38283219933509827,\n",
       "   0.3704739511013031,\n",
       "   0.3582976162433624,\n",
       "   0.38355588912963867,\n",
       "   0.3702215254306793,\n",
       "   0.370781809091568,\n",
       "   0.373287558555603,\n",
       "   0.35909274220466614,\n",
       "   0.3618841767311096,\n",
       "   0.38506823778152466,\n",
       "   0.371511310338974,\n",
       "   0.38415250182151794,\n",
       "   0.37086984515190125,\n",
       "   0.3765188753604889,\n",
       "   0.35179510712623596,\n",
       "   0.36708661913871765,\n",
       "   0.39131444692611694,\n",
       "   0.3693358600139618,\n",
       "   0.3433299958705902,\n",
       "   0.3490496575832367,\n",
       "   0.3550266921520233,\n",
       "   0.34530097246170044,\n",
       "   0.3368314206600189,\n",
       "   0.3664610683917999,\n",
       "   0.3677273392677307,\n",
       "   0.38225632905960083,\n",
       "   0.35952022671699524,\n",
       "   0.3851313889026642,\n",
       "   0.3699706494808197,\n",
       "   0.359279602766037,\n",
       "   0.3592323958873749,\n",
       "   0.35523587465286255,\n",
       "   0.3523160517215729,\n",
       "   0.3672644793987274,\n",
       "   0.33643046021461487,\n",
       "   0.3377651870250702,\n",
       "   0.3794609010219574,\n",
       "   0.3648241460323334,\n",
       "   0.38905492424964905,\n",
       "   0.3480526804924011,\n",
       "   0.3693559765815735,\n",
       "   0.3398387134075165,\n",
       "   0.3723738193511963,\n",
       "   0.35140034556388855,\n",
       "   0.3628808259963989,\n",
       "   0.35196009278297424,\n",
       "   0.3405059576034546,\n",
       "   0.35034558176994324,\n",
       "   0.3542424738407135,\n",
       "   0.3481908440589905,\n",
       "   0.36633795499801636,\n",
       "   0.3540433347225189,\n",
       "   0.3517972230911255,\n",
       "   0.3328676223754883,\n",
       "   0.3627872169017792,\n",
       "   0.313093900680542,\n",
       "   0.33536314964294434,\n",
       "   0.34942877292633057,\n",
       "   0.34858620166778564,\n",
       "   0.3498593270778656,\n",
       "   0.33797165751457214,\n",
       "   0.36237451434135437,\n",
       "   0.3415081799030304,\n",
       "   0.3456442654132843,\n",
       "   0.33788907527923584,\n",
       "   0.3603716194629669,\n",
       "   0.3559228777885437,\n",
       "   0.3296988606452942,\n",
       "   0.34844714403152466,\n",
       "   0.3417224884033203,\n",
       "   0.3279571533203125,\n",
       "   0.3658449947834015,\n",
       "   0.35655030608177185,\n",
       "   0.34395042061805725,\n",
       "   0.34269586205482483,\n",
       "   0.3473536968231201,\n",
       "   0.3515467643737793,\n",
       "   0.338164746761322,\n",
       "   0.3427816927433014,\n",
       "   0.37648338079452515,\n",
       "   0.33444133400917053,\n",
       "   0.3214598596096039,\n",
       "   0.33324986696243286,\n",
       "   0.3414820730686188,\n",
       "   0.32958680391311646,\n",
       "   0.3131193220615387,\n",
       "   0.3529060482978821,\n",
       "   0.325980007648468,\n",
       "   0.3423367440700531,\n",
       "   0.3368450701236725,\n",
       "   0.3445941209793091,\n",
       "   0.31473106145858765,\n",
       "   0.3695228695869446,\n",
       "   0.3344402611255646,\n",
       "   0.3469668924808502,\n",
       "   0.36404290795326233,\n",
       "   0.34087836742401123,\n",
       "   0.3411147892475128,\n",
       "   0.31679007411003113,\n",
       "   0.32810238003730774,\n",
       "   0.3363264501094818,\n",
       "   0.3294847011566162,\n",
       "   0.3277389109134674,\n",
       "   0.3479465842247009,\n",
       "   0.32559916377067566,\n",
       "   0.34446457028388977,\n",
       "   0.32889145612716675,\n",
       "   0.3493673801422119,\n",
       "   0.3484959900379181,\n",
       "   0.3450712561607361,\n",
       "   0.33546707034111023,\n",
       "   0.3247430920600891,\n",
       "   0.3235170543193817,\n",
       "   0.3336601257324219,\n",
       "   0.34402015805244446,\n",
       "   0.33060306310653687,\n",
       "   0.3515656292438507,\n",
       "   0.3333449363708496,\n",
       "   0.32601597905158997,\n",
       "   0.31991714239120483,\n",
       "   0.3280743658542633,\n",
       "   0.33351606130599976,\n",
       "   0.31169161200523376,\n",
       "   0.333928644657135,\n",
       "   0.32960009574890137,\n",
       "   0.3303125202655792,\n",
       "   0.29575756192207336,\n",
       "   0.3479014039039612,\n",
       "   0.3323182165622711,\n",
       "   0.35468411445617676,\n",
       "   0.32283928990364075,\n",
       "   0.3279914855957031,\n",
       "   0.332068532705307,\n",
       "   0.3027831017971039,\n",
       "   0.3262893855571747,\n",
       "   0.32869255542755127,\n",
       "   0.32885655760765076,\n",
       "   0.32300668954849243,\n",
       "   0.2967856228351593,\n",
       "   0.33477863669395447,\n",
       "   0.31813493371009827,\n",
       "   0.30716195702552795,\n",
       "   0.31308048963546753,\n",
       "   0.3317802846431732,\n",
       "   0.31076329946517944,\n",
       "   0.3328631520271301,\n",
       "   0.3186643719673157,\n",
       "   0.31339117884635925,\n",
       "   0.31997472047805786,\n",
       "   0.3224455714225769,\n",
       "   0.308138370513916,\n",
       "   0.33316394686698914,\n",
       "   0.34998130798339844,\n",
       "   0.33476167917251587,\n",
       "   0.3343106508255005,\n",
       "   0.3088904321193695,\n",
       "   0.3160336911678314,\n",
       "   0.2986331582069397,\n",
       "   0.3074682056903839,\n",
       "   0.29878437519073486,\n",
       "   0.31117624044418335,\n",
       "   0.32323095202445984,\n",
       "   0.31295692920684814,\n",
       "   0.3147452175617218,\n",
       "   0.2999119758605957,\n",
       "   0.3209214508533478,\n",
       "   0.3070264458656311,\n",
       "   0.31672418117523193,\n",
       "   0.32785606384277344,\n",
       "   0.31482312083244324,\n",
       "   0.3117064833641052,\n",
       "   0.320875883102417,\n",
       "   0.3118413984775543,\n",
       "   0.3106626570224762,\n",
       "   0.3058564066886902,\n",
       "   0.30676931142807007,\n",
       "   0.31650975346565247,\n",
       "   0.31580957770347595,\n",
       "   0.3209669888019562,\n",
       "   0.3153609335422516,\n",
       "   0.3261692225933075,\n",
       "   0.3061199188232422,\n",
       "   0.3009912669658661,\n",
       "   0.31084197759628296,\n",
       "   0.3247208893299103,\n",
       "   0.30984073877334595,\n",
       "   0.3225519359111786,\n",
       "   0.3373588025569916,\n",
       "   0.32658836245536804,\n",
       "   0.2881082594394684,\n",
       "   0.2997050881385803,\n",
       "   0.3186818063259125,\n",
       "   0.310180127620697,\n",
       "   0.3101639449596405,\n",
       "   0.29908981919288635,\n",
       "   0.2917053997516632,\n",
       "   0.3162958323955536,\n",
       "   0.307975709438324,\n",
       "   0.2923465669155121,\n",
       "   0.31560131907463074,\n",
       "   0.3129176199436188,\n",
       "   0.3013436198234558,\n",
       "   0.309931218624115,\n",
       "   0.293979287147522,\n",
       "   0.3059309720993042,\n",
       "   0.30058518052101135,\n",
       "   0.29567140340805054,\n",
       "   0.32836979627609253,\n",
       "   0.2911451756954193,\n",
       "   0.32084834575653076,\n",
       "   0.31399351358413696,\n",
       "   0.28920620679855347,\n",
       "   0.28687477111816406,\n",
       "   0.2941054105758667,\n",
       "   0.3228822350502014,\n",
       "   0.2945364713668823,\n",
       "   0.30867257714271545,\n",
       "   0.3057449460029602,\n",
       "   0.2909349501132965,\n",
       "   0.28943008184432983,\n",
       "   0.3110949397087097,\n",
       "   0.291617214679718,\n",
       "   0.29122087359428406,\n",
       "   0.2866953909397125,\n",
       "   0.303804486989975,\n",
       "   0.31500834226608276,\n",
       "   0.312685489654541,\n",
       "   0.3084804117679596,\n",
       "   0.2872130274772644,\n",
       "   0.29094499349594116,\n",
       "   0.289623498916626,\n",
       "   0.29037991166114807,\n",
       "   0.2974693775177002,\n",
       "   0.29334384202957153,\n",
       "   0.3048303425312042,\n",
       "   0.3247157335281372,\n",
       "   0.3033920228481293,\n",
       "   0.28100860118865967,\n",
       "   0.31052637100219727,\n",
       "   0.30635035037994385,\n",
       "   0.30005449056625366,\n",
       "   0.281198114156723,\n",
       "   0.2896297574043274,\n",
       "   0.29099220037460327,\n",
       "   0.29158487915992737,\n",
       "   0.28886473178863525,\n",
       "   0.2847691774368286,\n",
       "   0.2794753909111023,\n",
       "   0.28598013520240784,\n",
       "   0.2876313030719757,\n",
       "   0.2786456048488617,\n",
       "   0.2904808223247528,\n",
       "   0.2872771620750427,\n",
       "   0.30657804012298584,\n",
       "   0.2891031801700592,\n",
       "   0.28170400857925415,\n",
       "   0.29141750931739807,\n",
       "   0.2819857895374298,\n",
       "   0.27889567613601685,\n",
       "   0.3088008165359497,\n",
       "   0.2889072597026825,\n",
       "   0.29569941759109497,\n",
       "   0.2873329818248749,\n",
       "   0.2719855308532715,\n",
       "   0.2779960632324219,\n",
       "   0.27775809168815613,\n",
       "   0.27939364314079285,\n",
       "   0.2841992676258087,\n",
       "   0.2693583071231842,\n",
       "   0.2733074128627777,\n",
       "   0.2942449748516083,\n",
       "   0.28950831294059753,\n",
       "   0.2807711958885193,\n",
       "   0.2965565323829651,\n",
       "   0.2848135232925415,\n",
       "   0.2839842140674591,\n",
       "   0.3014131188392639,\n",
       "   0.2965696156024933,\n",
       "   0.2725512385368347,\n",
       "   0.28884077072143555,\n",
       "   0.28298258781433105,\n",
       "   0.2765001356601715,\n",
       "   0.295499712228775,\n",
       "   0.2959287166595459,\n",
       "   0.2624519467353821,\n",
       "   0.29147598147392273,\n",
       "   0.27305909991264343,\n",
       "   0.29824838042259216,\n",
       "   0.28225788474082947,\n",
       "   0.28493812680244446,\n",
       "   0.2822331190109253,\n",
       "   0.26932328939437866,\n",
       "   0.2566595673561096,\n",
       "   0.28633520007133484,\n",
       "   0.2958253026008606,\n",
       "   0.28522804379463196,\n",
       "   0.2764347791671753,\n",
       "   0.2590305805206299,\n",
       "   0.2855187654495239,\n",
       "   0.2862738370895386,\n",
       "   0.28320789337158203,\n",
       "   0.27972835302352905,\n",
       "   0.2764364182949066,\n",
       "   0.2911224663257599,\n",
       "   0.26731884479522705,\n",
       "   0.2808607816696167,\n",
       "   0.2755093276500702,\n",
       "   0.2648715376853943,\n",
       "   0.2825758755207062,\n",
       "   0.274421364068985,\n",
       "   0.26069432497024536,\n",
       "   0.27504685521125793,\n",
       "   0.2936006486415863,\n",
       "   0.2724175751209259,\n",
       "   0.29774177074432373,\n",
       "   0.2757638990879059,\n",
       "   0.26896584033966064,\n",
       "   0.28624895215034485,\n",
       "   0.27929747104644775,\n",
       "   0.26559123396873474,\n",
       "   0.2741755247116089,\n",
       "   0.2762302756309509,\n",
       "   0.27696749567985535,\n",
       "   0.26670682430267334,\n",
       "   0.2823648154735565,\n",
       "   0.2679753601551056,\n",
       "   0.2716561555862427,\n",
       "   0.2557031810283661,\n",
       "   0.2658195495605469,\n",
       "   0.27452316880226135,\n",
       "   0.25507527589797974,\n",
       "   0.27502328157424927,\n",
       "   0.28430429100990295,\n",
       "   0.2752862274646759,\n",
       "   0.2739308178424835,\n",
       "   0.2821578085422516,\n",
       "   0.2587560713291168,\n",
       "   0.2717228829860687,\n",
       "   0.2788139283657074,\n",
       "   0.26757240295410156,\n",
       "   0.2743149399757385,\n",
       "   0.2598724961280823,\n",
       "   0.27922436594963074]},\n",
       " 2: {'lr': 1e-05,\n",
       "  'best_loss_epoch': 485,\n",
       "  'best_acc_epoch': 410,\n",
       "  'best_r2_epoch': 18,\n",
       "  'pce_loss': [0.8262105584144592,\n",
       "   0.5499904155731201,\n",
       "   0.33273249864578247,\n",
       "   0.20727509260177612,\n",
       "   0.1188245564699173,\n",
       "   0.07284437119960785,\n",
       "   0.05535903200507164,\n",
       "   0.03858498856425285,\n",
       "   0.03030700795352459,\n",
       "   0.025109991431236267,\n",
       "   0.017439140006899834,\n",
       "   0.015704067423939705,\n",
       "   0.012201757170259953,\n",
       "   0.009073689579963684,\n",
       "   0.007317577488720417,\n",
       "   0.007181274704635143,\n",
       "   0.007318304851651192,\n",
       "   0.008876591920852661,\n",
       "   0.011844532564282417,\n",
       "   0.015892185270786285,\n",
       "   0.020735319703817368,\n",
       "   0.026430819183588028,\n",
       "   0.031006526201963425,\n",
       "   0.034178320318460464,\n",
       "   0.03891969844698906,\n",
       "   0.04032996669411659,\n",
       "   0.04245549812912941,\n",
       "   0.04691818729043007,\n",
       "   0.04965116083621979,\n",
       "   0.051627565175294876,\n",
       "   0.05714397504925728,\n",
       "   0.059562165290117264,\n",
       "   0.05936526507139206,\n",
       "   0.06095687672495842,\n",
       "   0.060245078057050705,\n",
       "   0.05898835510015488,\n",
       "   0.06217404082417488,\n",
       "   0.062085457146167755,\n",
       "   0.059025831520557404,\n",
       "   0.05923297256231308,\n",
       "   0.06146107614040375,\n",
       "   0.062056031078100204,\n",
       "   0.061721742153167725,\n",
       "   0.061917245388031006,\n",
       "   0.06547985225915909,\n",
       "   0.06797046959400177,\n",
       "   0.06781352311372757,\n",
       "   0.06644490361213684,\n",
       "   0.06626313179731369,\n",
       "   0.06484787166118622,\n",
       "   0.06375788897275925,\n",
       "   0.05915036424994469,\n",
       "   0.05753668025135994,\n",
       "   0.05708201974630356,\n",
       "   0.053019408136606216,\n",
       "   0.054659076035022736,\n",
       "   0.05206996947526932,\n",
       "   0.050190165638923645,\n",
       "   0.05118092894554138,\n",
       "   0.04893699660897255,\n",
       "   0.045703403651714325,\n",
       "   0.043498579412698746,\n",
       "   0.040755048394203186,\n",
       "   0.03791726008057594,\n",
       "   0.034463707357645035,\n",
       "   0.03356359526515007,\n",
       "   0.03213295340538025,\n",
       "   0.030668627470731735,\n",
       "   0.029850251972675323,\n",
       "   0.029747022315859795,\n",
       "   0.02921237051486969,\n",
       "   0.03050834871828556,\n",
       "   0.02973792888224125,\n",
       "   0.02856617048382759,\n",
       "   0.02930835820734501,\n",
       "   0.028645923361182213,\n",
       "   0.02988569438457489,\n",
       "   0.03064061515033245,\n",
       "   0.030362965539097786,\n",
       "   0.028322147205471992,\n",
       "   0.02813468873500824,\n",
       "   0.028137341141700745,\n",
       "   0.0269029438495636,\n",
       "   0.02620876021683216,\n",
       "   0.025867704302072525,\n",
       "   0.02486511878669262,\n",
       "   0.024695491418242455,\n",
       "   0.023683782666921616,\n",
       "   0.024076594039797783,\n",
       "   0.023289943113923073,\n",
       "   0.022243686020374298,\n",
       "   0.02232281118631363,\n",
       "   0.02215241640806198,\n",
       "   0.02174411155283451,\n",
       "   0.022114692255854607,\n",
       "   0.021360179409384727,\n",
       "   0.020786037668585777,\n",
       "   0.020696748048067093,\n",
       "   0.02058916911482811,\n",
       "   0.021098418161273003,\n",
       "   0.021029574796557426,\n",
       "   0.02204028144478798,\n",
       "   0.022896280512213707,\n",
       "   0.024405010044574738,\n",
       "   0.02504534088075161,\n",
       "   0.024803578853607178,\n",
       "   0.02462385781109333,\n",
       "   0.023294605314731598,\n",
       "   0.02380010299384594,\n",
       "   0.023937344551086426,\n",
       "   0.024097515270113945,\n",
       "   0.023706121370196342,\n",
       "   0.023207610473036766,\n",
       "   0.023797187954187393,\n",
       "   0.023695290088653564,\n",
       "   0.023687271401286125,\n",
       "   0.02300407551229,\n",
       "   0.021748416125774384,\n",
       "   0.021316997706890106,\n",
       "   0.023406099528074265,\n",
       "   0.022943370044231415,\n",
       "   0.02408873289823532,\n",
       "   0.024443745613098145,\n",
       "   0.024182388558983803,\n",
       "   0.02325812540948391,\n",
       "   0.022342253476381302,\n",
       "   0.02287457324564457,\n",
       "   0.023057609796524048,\n",
       "   0.022930551320314407,\n",
       "   0.02350192703306675,\n",
       "   0.02292788214981556,\n",
       "   0.023527534678578377,\n",
       "   0.021895064041018486,\n",
       "   0.022025147452950478,\n",
       "   0.022381829097867012,\n",
       "   0.022214500233530998,\n",
       "   0.021790962666273117,\n",
       "   0.022149115800857544,\n",
       "   0.022092305123806,\n",
       "   0.022308096289634705,\n",
       "   0.021971644833683968,\n",
       "   0.021529482677578926,\n",
       "   0.02199273742735386,\n",
       "   0.02156948484480381,\n",
       "   0.020755745470523834,\n",
       "   0.020319625735282898,\n",
       "   0.020297080278396606,\n",
       "   0.020219361409544945,\n",
       "   0.020847661420702934,\n",
       "   0.020586945116519928,\n",
       "   0.020082559436559677,\n",
       "   0.020370256155729294,\n",
       "   0.020272010937333107,\n",
       "   0.02036711946129799,\n",
       "   0.02117576263844967,\n",
       "   0.02165396884083748,\n",
       "   0.02242801897227764,\n",
       "   0.022054409608244896,\n",
       "   0.022769099101424217,\n",
       "   0.02315378375351429,\n",
       "   0.023306680843234062,\n",
       "   0.02366143837571144,\n",
       "   0.02364969439804554,\n",
       "   0.02363482117652893,\n",
       "   0.024920117110013962,\n",
       "   0.0236480925232172,\n",
       "   0.023182395845651627,\n",
       "   0.023046495392918587,\n",
       "   0.023201577365398407,\n",
       "   0.022920718416571617,\n",
       "   0.023520104587078094,\n",
       "   0.02346123568713665,\n",
       "   0.02338513918220997,\n",
       "   0.023621685802936554,\n",
       "   0.023870864883065224,\n",
       "   0.024140717461705208,\n",
       "   0.0236524511128664,\n",
       "   0.02329639531672001,\n",
       "   0.023346640169620514,\n",
       "   0.023546360433101654,\n",
       "   0.023800121620297432,\n",
       "   0.02341696433722973,\n",
       "   0.02346671372652054,\n",
       "   0.02304868772625923,\n",
       "   0.022682595998048782,\n",
       "   0.022558407858014107,\n",
       "   0.022017739713191986,\n",
       "   0.022321635857224464,\n",
       "   0.02133237197995186,\n",
       "   0.021682586520910263,\n",
       "   0.02177305519580841,\n",
       "   0.02258945442736149,\n",
       "   0.022308751940727234,\n",
       "   0.022788820788264275,\n",
       "   0.02281213365495205,\n",
       "   0.02480785734951496,\n",
       "   0.0258905328810215,\n",
       "   0.02641889452934265,\n",
       "   0.027839098125696182,\n",
       "   0.027508610859513283,\n",
       "   0.026824181899428368,\n",
       "   0.026160022243857384,\n",
       "   0.026003221049904823,\n",
       "   0.02579161897301674,\n",
       "   0.02535868249833584,\n",
       "   0.024832868948578835,\n",
       "   0.023983726277947426,\n",
       "   0.02486506663262844,\n",
       "   0.025666125118732452,\n",
       "   0.02529461681842804,\n",
       "   0.02516118250787258,\n",
       "   0.02442074939608574,\n",
       "   0.02510075457394123,\n",
       "   0.02497778832912445,\n",
       "   0.025220002979040146,\n",
       "   0.023861534893512726,\n",
       "   0.023394128307700157,\n",
       "   0.02327680215239525,\n",
       "   0.022898592054843903,\n",
       "   0.022980226203799248,\n",
       "   0.021931815892457962,\n",
       "   0.020934313535690308,\n",
       "   0.020976193249225616,\n",
       "   0.021424176171422005,\n",
       "   0.02095206081867218,\n",
       "   0.02052326314151287,\n",
       "   0.02027120254933834,\n",
       "   0.020402291789650917,\n",
       "   0.020251912996172905,\n",
       "   0.019991574808955193,\n",
       "   0.01982230693101883,\n",
       "   0.020577192306518555,\n",
       "   0.020350661128759384,\n",
       "   0.020635521039366722,\n",
       "   0.021097170189023018,\n",
       "   0.021579299122095108,\n",
       "   0.02279089204967022,\n",
       "   0.023270079866051674,\n",
       "   0.023477885872125626,\n",
       "   0.023622002452611923,\n",
       "   0.023891044780611992,\n",
       "   0.024477999657392502,\n",
       "   0.024493388831615448,\n",
       "   0.023907504975795746,\n",
       "   0.02351909875869751,\n",
       "   0.023990482091903687,\n",
       "   0.023537954315543175,\n",
       "   0.02363569289445877,\n",
       "   0.023038065060973167,\n",
       "   0.022819682955741882,\n",
       "   0.022198352962732315,\n",
       "   0.022404517978429794,\n",
       "   0.02211586758494377,\n",
       "   0.021935325115919113,\n",
       "   0.021678317338228226,\n",
       "   0.021712111309170723,\n",
       "   0.02205520123243332,\n",
       "   0.02277427911758423,\n",
       "   0.022887475788593292,\n",
       "   0.022842559963464737,\n",
       "   0.023600412532687187,\n",
       "   0.023786475881934166,\n",
       "   0.023818528279662132,\n",
       "   0.02465202473104,\n",
       "   0.024587269872426987,\n",
       "   0.02457103319466114,\n",
       "   0.024752100929617882,\n",
       "   0.025732429698109627,\n",
       "   0.025192124769091606,\n",
       "   0.026389380916953087,\n",
       "   0.026241997256875038,\n",
       "   0.025980882346630096,\n",
       "   0.02611343003809452,\n",
       "   0.025621235370635986,\n",
       "   0.02505757473409176,\n",
       "   0.024277184158563614,\n",
       "   0.023798491805791855,\n",
       "   0.023047422990202904,\n",
       "   0.023363713175058365,\n",
       "   0.023742789402604103,\n",
       "   0.023921078070998192,\n",
       "   0.024448847398161888,\n",
       "   0.023878110572695732,\n",
       "   0.023510104045271873,\n",
       "   0.022621309384703636,\n",
       "   0.023029230535030365,\n",
       "   0.022485611960291862,\n",
       "   0.022363385185599327,\n",
       "   0.022214367985725403,\n",
       "   0.022552553564310074,\n",
       "   0.022902531549334526,\n",
       "   0.022426795214414597,\n",
       "   0.021845636889338493,\n",
       "   0.021946365013718605,\n",
       "   0.021987242624163628,\n",
       "   0.022095786407589912,\n",
       "   0.022381875663995743,\n",
       "   0.022565437480807304,\n",
       "   0.02223549783229828,\n",
       "   0.022797835990786552,\n",
       "   0.022193172946572304,\n",
       "   0.02200065180659294,\n",
       "   0.022231873124837875,\n",
       "   0.02218501642346382,\n",
       "   0.021590525284409523,\n",
       "   0.02173173986375332,\n",
       "   0.022907206788659096,\n",
       "   0.023480240255594254,\n",
       "   0.023531954735517502,\n",
       "   0.024564608931541443,\n",
       "   0.0245869942009449,\n",
       "   0.02501356601715088,\n",
       "   0.025411756709218025,\n",
       "   0.025049390271306038,\n",
       "   0.025024930015206337,\n",
       "   0.02507610432803631,\n",
       "   0.024828646332025528,\n",
       "   0.026190897449851036,\n",
       "   0.025816798210144043,\n",
       "   0.02526240237057209,\n",
       "   0.02539108693599701,\n",
       "   0.025041457265615463,\n",
       "   0.025585990399122238,\n",
       "   0.025130756199359894,\n",
       "   0.02522142603993416,\n",
       "   0.0249356497079134,\n",
       "   0.024192485958337784,\n",
       "   0.025251343846321106,\n",
       "   0.025304432958364487,\n",
       "   0.024933822453022003,\n",
       "   0.02472652494907379,\n",
       "   0.025024667382240295,\n",
       "   0.025181293487548828,\n",
       "   0.02456117421388626,\n",
       "   0.02389359660446644,\n",
       "   0.023672938346862793,\n",
       "   0.023441586643457413,\n",
       "   0.02253689616918564,\n",
       "   0.0229378379881382,\n",
       "   0.022942563518881798,\n",
       "   0.0232072900980711,\n",
       "   0.02259182743728161,\n",
       "   0.022407708689570427,\n",
       "   0.02211698703467846,\n",
       "   0.02186962217092514,\n",
       "   0.02153601124882698,\n",
       "   0.021470127627253532,\n",
       "   0.021428141742944717,\n",
       "   0.021045630797743797,\n",
       "   0.021767517551779747,\n",
       "   0.021444298326969147,\n",
       "   0.021472886204719543,\n",
       "   0.02135508693754673,\n",
       "   0.021634478121995926,\n",
       "   0.021818963810801506,\n",
       "   0.021902834996581078,\n",
       "   0.022062474861741066,\n",
       "   0.022273067384958267,\n",
       "   0.022223729640245438,\n",
       "   0.022713840007781982,\n",
       "   0.022552013397216797,\n",
       "   0.022631272673606873,\n",
       "   0.022471632808446884,\n",
       "   0.022704219445586205,\n",
       "   0.022494662553071976,\n",
       "   0.022447429597377777,\n",
       "   0.02298106625676155,\n",
       "   0.02297343872487545,\n",
       "   0.02412647008895874,\n",
       "   0.023186812177300453,\n",
       "   0.023507095873355865,\n",
       "   0.02314450405538082,\n",
       "   0.02235720492899418,\n",
       "   0.02313021570444107,\n",
       "   0.02347978577017784,\n",
       "   0.022884372621774673,\n",
       "   0.022981900721788406,\n",
       "   0.023293130099773407,\n",
       "   0.023304026573896408,\n",
       "   0.02315712720155716,\n",
       "   0.023315943777561188,\n",
       "   0.023689821362495422,\n",
       "   0.02334652468562126,\n",
       "   0.023222511634230614,\n",
       "   0.02306148037314415,\n",
       "   0.022904956713318825,\n",
       "   0.02334594912827015,\n",
       "   0.023027578368782997,\n",
       "   0.022764742374420166,\n",
       "   0.02244010753929615,\n",
       "   0.022503145039081573,\n",
       "   0.022595753893256187,\n",
       "   0.02271099202334881,\n",
       "   0.022515621036291122,\n",
       "   0.022561639547348022,\n",
       "   0.023061176761984825,\n",
       "   0.022571049630641937,\n",
       "   0.022851334884762764,\n",
       "   0.022523730993270874,\n",
       "   0.022247329354286194,\n",
       "   0.02199210599064827,\n",
       "   0.021230656653642654,\n",
       "   0.022165991365909576,\n",
       "   0.02226889878511429,\n",
       "   0.022601552307605743,\n",
       "   0.022490454837679863,\n",
       "   0.022628532722592354,\n",
       "   0.02227197028696537,\n",
       "   0.022119315341114998,\n",
       "   0.021265169605612755,\n",
       "   0.02083973027765751,\n",
       "   0.02076774835586548,\n",
       "   0.020934954285621643,\n",
       "   0.020563364028930664,\n",
       "   0.020223082974553108,\n",
       "   0.02104661427438259,\n",
       "   0.02052251808345318,\n",
       "   0.020391201600432396,\n",
       "   0.020976729691028595,\n",
       "   0.021216748282313347,\n",
       "   0.020940599963068962,\n",
       "   0.02069426141679287,\n",
       "   0.020223570987582207,\n",
       "   0.02021637000143528,\n",
       "   0.020534256473183632,\n",
       "   0.0205500777810812,\n",
       "   0.02066933363676071,\n",
       "   0.02065570466220379,\n",
       "   0.02131625823676586,\n",
       "   0.022150840610265732,\n",
       "   0.02236534282565117,\n",
       "   0.022301385179162025,\n",
       "   0.022267725318670273,\n",
       "   0.022168142721056938,\n",
       "   0.02227891981601715,\n",
       "   0.0221840962767601,\n",
       "   0.02214166149497032,\n",
       "   0.022409211844205856,\n",
       "   0.0219296682626009,\n",
       "   0.021608784794807434,\n",
       "   0.021934110671281815,\n",
       "   0.0221848227083683,\n",
       "   0.02155047282576561,\n",
       "   0.0211668461561203,\n",
       "   0.020752092823386192,\n",
       "   0.020551014691591263,\n",
       "   0.02058822475373745,\n",
       "   0.021040508523583412,\n",
       "   0.02152913436293602,\n",
       "   0.021750329062342644,\n",
       "   0.021917633712291718,\n",
       "   0.02193768508732319,\n",
       "   0.021570807322859764,\n",
       "   0.02176865190267563,\n",
       "   0.02142605185508728,\n",
       "   0.021674901247024536,\n",
       "   0.02219546213746071,\n",
       "   0.022683503106236458,\n",
       "   0.023244671523571014,\n",
       "   0.023591963574290276,\n",
       "   0.023932505398988724,\n",
       "   0.02370811253786087,\n",
       "   0.023538632318377495,\n",
       "   0.024262653663754463,\n",
       "   0.024060847237706184,\n",
       "   0.023909784853458405,\n",
       "   0.023605499416589737,\n",
       "   0.02375522442162037,\n",
       "   0.023497458547353745,\n",
       "   0.023353897035121918,\n",
       "   0.023386240005493164,\n",
       "   0.0225288774818182,\n",
       "   0.022294700145721436,\n",
       "   0.02239428646862507,\n",
       "   0.022160591557621956,\n",
       "   0.022253474220633507,\n",
       "   0.02227886952459812,\n",
       "   0.022326195612549782,\n",
       "   0.02253188006579876,\n",
       "   0.02323656901717186,\n",
       "   0.023163411766290665,\n",
       "   0.02307738922536373,\n",
       "   0.02340548485517502,\n",
       "   0.02395266480743885,\n",
       "   0.02370133250951767,\n",
       "   0.02331826649606228,\n",
       "   0.023436017334461212,\n",
       "   0.02340046502649784,\n",
       "   0.023386726155877113,\n",
       "   0.023029640316963196,\n",
       "   0.02249746024608612,\n",
       "   0.02230139821767807,\n",
       "   0.02261911705136299,\n",
       "   0.02260284498333931,\n",
       "   0.023324191570281982,\n",
       "   0.023600028827786446,\n",
       "   0.02359025552868843,\n",
       "   0.024063222110271454,\n",
       "   0.024533258751034737,\n",
       "   0.02490049973130226],\n",
       "  'voc_loss': [0.3724364638328552,\n",
       "   0.28052380681037903,\n",
       "   0.1684228479862213,\n",
       "   0.13892506062984467,\n",
       "   0.09479895979166031,\n",
       "   0.0644325539469719,\n",
       "   0.04150323197245598,\n",
       "   0.024200258776545525,\n",
       "   0.014124005101621151,\n",
       "   0.008904136717319489,\n",
       "   0.005109378136694431,\n",
       "   0.003547504311427474,\n",
       "   0.003792193252593279,\n",
       "   0.004609411116689444,\n",
       "   0.005772760137915611,\n",
       "   0.006609764415770769,\n",
       "   0.006523488089442253,\n",
       "   0.00724420789629221,\n",
       "   0.006524370517581701,\n",
       "   0.006204807199537754,\n",
       "   0.005701279267668724,\n",
       "   0.005723072215914726,\n",
       "   0.005772383883595467,\n",
       "   0.006191546563059092,\n",
       "   0.006524665281176567,\n",
       "   0.00726284459233284,\n",
       "   0.008564231917262077,\n",
       "   0.010292526334524155,\n",
       "   0.011417885310947895,\n",
       "   0.0126577727496624,\n",
       "   0.015566153451800346,\n",
       "   0.019280480220913887,\n",
       "   0.019556747749447823,\n",
       "   0.021305542439222336,\n",
       "   0.022141220048069954,\n",
       "   0.023101672530174255,\n",
       "   0.024447310715913773,\n",
       "   0.025429585948586464,\n",
       "   0.024819154292345047,\n",
       "   0.026255806908011436,\n",
       "   0.02968316525220871,\n",
       "   0.030886346474289894,\n",
       "   0.03365945443511009,\n",
       "   0.03417662903666496,\n",
       "   0.03789303079247475,\n",
       "   0.039083436131477356,\n",
       "   0.041011545807123184,\n",
       "   0.042159873992204666,\n",
       "   0.04350178688764572,\n",
       "   0.0450785830616951,\n",
       "   0.04698491841554642,\n",
       "   0.04744737595319748,\n",
       "   0.04823620244860649,\n",
       "   0.0519786961376667,\n",
       "   0.05325358361005783,\n",
       "   0.05468478426337242,\n",
       "   0.05513666570186615,\n",
       "   0.057095006108284,\n",
       "   0.05460691079497337,\n",
       "   0.0572972372174263,\n",
       "   0.061850860714912415,\n",
       "   0.06467827409505844,\n",
       "   0.06594938784837723,\n",
       "   0.0660388246178627,\n",
       "   0.068790964782238,\n",
       "   0.07178351283073425,\n",
       "   0.07189241796731949,\n",
       "   0.07102419435977936,\n",
       "   0.07194584608078003,\n",
       "   0.07431240379810333,\n",
       "   0.07565907388925552,\n",
       "   0.07233219593763351,\n",
       "   0.07202078402042389,\n",
       "   0.07651591300964355,\n",
       "   0.07500935345888138,\n",
       "   0.07653740793466568,\n",
       "   0.07443495839834213,\n",
       "   0.07622004300355911,\n",
       "   0.07869109511375427,\n",
       "   0.07985368371009827,\n",
       "   0.08199378103017807,\n",
       "   0.08347535133361816,\n",
       "   0.08633400499820709,\n",
       "   0.08595061302185059,\n",
       "   0.08750896900892258,\n",
       "   0.08769303560256958,\n",
       "   0.08611534535884857,\n",
       "   0.08804106712341309,\n",
       "   0.08934833854436874,\n",
       "   0.09220781177282333,\n",
       "   0.09288810193538666,\n",
       "   0.09569917619228363,\n",
       "   0.09498719871044159,\n",
       "   0.09664179384708405,\n",
       "   0.1026487872004509,\n",
       "   0.10048587620258331,\n",
       "   0.09846564382314682,\n",
       "   0.09887484461069107,\n",
       "   0.10265188664197922,\n",
       "   0.10571359843015671,\n",
       "   0.10499230027198792,\n",
       "   0.10395428538322449,\n",
       "   0.1042180135846138,\n",
       "   0.10384519398212433,\n",
       "   0.10494481772184372,\n",
       "   0.10261660069227219,\n",
       "   0.10272659361362457,\n",
       "   0.10393701493740082,\n",
       "   0.10553839802742004,\n",
       "   0.1046573743224144,\n",
       "   0.10149770975112915,\n",
       "   0.1025417372584343,\n",
       "   0.10420265793800354,\n",
       "   0.10590796172618866,\n",
       "   0.1044873371720314,\n",
       "   0.10585261881351471,\n",
       "   0.10894131660461426,\n",
       "   0.10973972082138062,\n",
       "   0.10908685624599457,\n",
       "   0.1090068593621254,\n",
       "   0.11220838129520416,\n",
       "   0.11619146913290024,\n",
       "   0.11999194324016571,\n",
       "   0.11887479573488235,\n",
       "   0.11653436720371246,\n",
       "   0.1123574823141098,\n",
       "   0.11562789976596832,\n",
       "   0.11513151973485947,\n",
       "   0.10989955812692642,\n",
       "   0.10952206701040268,\n",
       "   0.10384256392717361,\n",
       "   0.10162744671106339,\n",
       "   0.09760809689760208,\n",
       "   0.09878478944301605,\n",
       "   0.09877355396747589,\n",
       "   0.10336389392614365,\n",
       "   0.1075618788599968,\n",
       "   0.10608457773923874,\n",
       "   0.1035572960972786,\n",
       "   0.1047019213438034,\n",
       "   0.10307751595973969,\n",
       "   0.10231152921915054,\n",
       "   0.10264728963375092,\n",
       "   0.10066477954387665,\n",
       "   0.10253327339887619,\n",
       "   0.10678791254758835,\n",
       "   0.10224733501672745,\n",
       "   0.10050830245018005,\n",
       "   0.10404567420482635,\n",
       "   0.1044962927699089,\n",
       "   0.10421819239854813,\n",
       "   0.10147330164909363,\n",
       "   0.10323256254196167,\n",
       "   0.10170542448759079,\n",
       "   0.10312977433204651,\n",
       "   0.10437949001789093,\n",
       "   0.10621040314435959,\n",
       "   0.10516130179166794,\n",
       "   0.10547026991844177,\n",
       "   0.1104651540517807,\n",
       "   0.10749972611665726,\n",
       "   0.1091214045882225,\n",
       "   0.11260677874088287,\n",
       "   0.11481833457946777,\n",
       "   0.11526688188314438,\n",
       "   0.11427301913499832,\n",
       "   0.11565446108579636,\n",
       "   0.11751940846443176,\n",
       "   0.11862869560718536,\n",
       "   0.12109480798244476,\n",
       "   0.12390805780887604,\n",
       "   0.12460965663194656,\n",
       "   0.12706774473190308,\n",
       "   0.12508904933929443,\n",
       "   0.12814398109912872,\n",
       "   0.1301940232515335,\n",
       "   0.12924569845199585,\n",
       "   0.1309662163257599,\n",
       "   0.12811177968978882,\n",
       "   0.12631753087043762,\n",
       "   0.12227851152420044,\n",
       "   0.12640321254730225,\n",
       "   0.12730592489242554,\n",
       "   0.12795960903167725,\n",
       "   0.1267571598291397,\n",
       "   0.12788259983062744,\n",
       "   0.1305987536907196,\n",
       "   0.1307944506406784,\n",
       "   0.13361117243766785,\n",
       "   0.13179633021354675,\n",
       "   0.13557764887809753,\n",
       "   0.1343415081501007,\n",
       "   0.13532163202762604,\n",
       "   0.13639184832572937,\n",
       "   0.13862726092338562,\n",
       "   0.13813111186027527,\n",
       "   0.13576607406139374,\n",
       "   0.13586969673633575,\n",
       "   0.14024454355239868,\n",
       "   0.1400517374277115,\n",
       "   0.14211370050907135,\n",
       "   0.14343582093715668,\n",
       "   0.1426108181476593,\n",
       "   0.14268742501735687,\n",
       "   0.14158737659454346,\n",
       "   0.1426362693309784,\n",
       "   0.14019803702831268,\n",
       "   0.1413334459066391,\n",
       "   0.13920916616916656,\n",
       "   0.13470114767551422,\n",
       "   0.13317471742630005,\n",
       "   0.13008055090904236,\n",
       "   0.13130243122577667,\n",
       "   0.13090503215789795,\n",
       "   0.12999041378498077,\n",
       "   0.12784741818904877,\n",
       "   0.12956716120243073,\n",
       "   0.12949976325035095,\n",
       "   0.12909884750843048,\n",
       "   0.1291833370923996,\n",
       "   0.13199150562286377,\n",
       "   0.1337394118309021,\n",
       "   0.1375146359205246,\n",
       "   0.13792140781879425,\n",
       "   0.13588696718215942,\n",
       "   0.14119185507297516,\n",
       "   0.1365518867969513,\n",
       "   0.1353929489850998,\n",
       "   0.1370895653963089,\n",
       "   0.13600672781467438,\n",
       "   0.1376432329416275,\n",
       "   0.13826948404312134,\n",
       "   0.14246638119220734,\n",
       "   0.1420985460281372,\n",
       "   0.14038611948490143,\n",
       "   0.13928642868995667,\n",
       "   0.1395871937274933,\n",
       "   0.13885757327079773,\n",
       "   0.13799495995044708,\n",
       "   0.13466256856918335,\n",
       "   0.13525523245334625,\n",
       "   0.13451842963695526,\n",
       "   0.13254061341285706,\n",
       "   0.1314353197813034,\n",
       "   0.13099156320095062,\n",
       "   0.1340692639350891,\n",
       "   0.13532252609729767,\n",
       "   0.13689596951007843,\n",
       "   0.1393478661775589,\n",
       "   0.1410224586725235,\n",
       "   0.13983121514320374,\n",
       "   0.13930851221084595,\n",
       "   0.1377248466014862,\n",
       "   0.13973121345043182,\n",
       "   0.1398395150899887,\n",
       "   0.14099223911762238,\n",
       "   0.13999830186367035,\n",
       "   0.14271891117095947,\n",
       "   0.14234404265880585,\n",
       "   0.1414976418018341,\n",
       "   0.14136508107185364,\n",
       "   0.1417902410030365,\n",
       "   0.1414869874715805,\n",
       "   0.1432638317346573,\n",
       "   0.14166595041751862,\n",
       "   0.1401795744895935,\n",
       "   0.13901835680007935,\n",
       "   0.1375475376844406,\n",
       "   0.13803599774837494,\n",
       "   0.13376496732234955,\n",
       "   0.13120241463184357,\n",
       "   0.13109362125396729,\n",
       "   0.13072803616523743,\n",
       "   0.13015709817409515,\n",
       "   0.13231270015239716,\n",
       "   0.13468313217163086,\n",
       "   0.13503915071487427,\n",
       "   0.13524900376796722,\n",
       "   0.13280527293682098,\n",
       "   0.1340695321559906,\n",
       "   0.13366080820560455,\n",
       "   0.1333915889263153,\n",
       "   0.1353486031293869,\n",
       "   0.13218757510185242,\n",
       "   0.1293494552373886,\n",
       "   0.13288980722427368,\n",
       "   0.13267095386981964,\n",
       "   0.13032712042331696,\n",
       "   0.13028697669506073,\n",
       "   0.1327955573797226,\n",
       "   0.13217589259147644,\n",
       "   0.13264918327331543,\n",
       "   0.1344965249300003,\n",
       "   0.13251343369483948,\n",
       "   0.13087838888168335,\n",
       "   0.1285906732082367,\n",
       "   0.12608803808689117,\n",
       "   0.12502223253250122,\n",
       "   0.12290165573358536,\n",
       "   0.12392028421163559,\n",
       "   0.1232670322060585,\n",
       "   0.1242532730102539,\n",
       "   0.12305977940559387,\n",
       "   0.12091437727212906,\n",
       "   0.12045425176620483,\n",
       "   0.12331388890743256,\n",
       "   0.12548241019248962,\n",
       "   0.12242870777845383,\n",
       "   0.11935939639806747,\n",
       "   0.12224946171045303,\n",
       "   0.12062998861074448,\n",
       "   0.11778733134269714,\n",
       "   0.1173902153968811,\n",
       "   0.11524490267038345,\n",
       "   0.11482705175876617,\n",
       "   0.11254063993692398,\n",
       "   0.11184114962816238,\n",
       "   0.11170706152915955,\n",
       "   0.11357298493385315,\n",
       "   0.11305901408195496,\n",
       "   0.11060326546430588,\n",
       "   0.10834053158760071,\n",
       "   0.10946183651685715,\n",
       "   0.1072058379650116,\n",
       "   0.10508129000663757,\n",
       "   0.10256047546863556,\n",
       "   0.1054503321647644,\n",
       "   0.10401924699544907,\n",
       "   0.10269352793693542,\n",
       "   0.1007775366306305,\n",
       "   0.09901317954063416,\n",
       "   0.09732045233249664,\n",
       "   0.09548743814229965,\n",
       "   0.09372909367084503,\n",
       "   0.09192399680614471,\n",
       "   0.09024092555046082,\n",
       "   0.09164991974830627,\n",
       "   0.0913754552602768,\n",
       "   0.08872140944004059,\n",
       "   0.08883288502693176,\n",
       "   0.09160818159580231,\n",
       "   0.08715853840112686,\n",
       "   0.08661295473575592,\n",
       "   0.08605659008026123,\n",
       "   0.0850004255771637,\n",
       "   0.08432126045227051,\n",
       "   0.08323527127504349,\n",
       "   0.08232374489307404,\n",
       "   0.08232930302619934,\n",
       "   0.08069358766078949,\n",
       "   0.07943528145551682,\n",
       "   0.07858660817146301,\n",
       "   0.07786103338003159,\n",
       "   0.07713700830936432,\n",
       "   0.0756555050611496,\n",
       "   0.07461223006248474,\n",
       "   0.07565052807331085,\n",
       "   0.07599377632141113,\n",
       "   0.07543577253818512,\n",
       "   0.0731147900223732,\n",
       "   0.07174194604158401,\n",
       "   0.07138197124004364,\n",
       "   0.07199432700872421,\n",
       "   0.07278723269701004,\n",
       "   0.07217323780059814,\n",
       "   0.07104039937257767,\n",
       "   0.07177276164293289,\n",
       "   0.0696675106883049,\n",
       "   0.07076006382703781,\n",
       "   0.06987671554088593,\n",
       "   0.06933712959289551,\n",
       "   0.07095080614089966,\n",
       "   0.07197660207748413,\n",
       "   0.07260356098413467,\n",
       "   0.07397948205471039,\n",
       "   0.07330171018838882,\n",
       "   0.07147687673568726,\n",
       "   0.0699792131781578,\n",
       "   0.07224088162183762,\n",
       "   0.07188143581151962,\n",
       "   0.07172015309333801,\n",
       "   0.07185327261686325,\n",
       "   0.07114792615175247,\n",
       "   0.07205091416835785,\n",
       "   0.07350719720125198,\n",
       "   0.07279760390520096,\n",
       "   0.07211799174547195,\n",
       "   0.07278847694396973,\n",
       "   0.07396090775728226,\n",
       "   0.07352732121944427,\n",
       "   0.07406190782785416,\n",
       "   0.07509051263332367,\n",
       "   0.075492724776268,\n",
       "   0.07624758780002594,\n",
       "   0.07635138183832169,\n",
       "   0.075987309217453,\n",
       "   0.07519710063934326,\n",
       "   0.07488293200731277,\n",
       "   0.0747784897685051,\n",
       "   0.07438277453184128,\n",
       "   0.07550233602523804,\n",
       "   0.07348925620317459,\n",
       "   0.07279857993125916,\n",
       "   0.07143420726060867,\n",
       "   0.07182862609624863,\n",
       "   0.07132266461849213,\n",
       "   0.07098515331745148,\n",
       "   0.07089155167341232,\n",
       "   0.07182759046554565,\n",
       "   0.07029882818460464,\n",
       "   0.06877017766237259,\n",
       "   0.06849893927574158,\n",
       "   0.07034128904342651,\n",
       "   0.06823975592851639,\n",
       "   0.06729280203580856,\n",
       "   0.0675388053059578,\n",
       "   0.06736032664775848,\n",
       "   0.06872472167015076,\n",
       "   0.0674353688955307,\n",
       "   0.06891930103302002,\n",
       "   0.06974142789840698,\n",
       "   0.06904682517051697,\n",
       "   0.06902249902486801,\n",
       "   0.06815434992313385,\n",
       "   0.06882836669683456,\n",
       "   0.06794901937246323,\n",
       "   0.06862064450979233,\n",
       "   0.07021384686231613,\n",
       "   0.07050176709890366,\n",
       "   0.0724218413233757,\n",
       "   0.07266082614660263,\n",
       "   0.07324133813381195,\n",
       "   0.07346823811531067,\n",
       "   0.07359626889228821,\n",
       "   0.07431232929229736,\n",
       "   0.07453171908855438,\n",
       "   0.07833871990442276,\n",
       "   0.07562945038080215,\n",
       "   0.07512558996677399,\n",
       "   0.07511378079652786,\n",
       "   0.07567711174488068,\n",
       "   0.07591405510902405,\n",
       "   0.07692291587591171,\n",
       "   0.07665158063173294,\n",
       "   0.0762239396572113,\n",
       "   0.0763404369354248,\n",
       "   0.07626605033874512,\n",
       "   0.07724598795175552,\n",
       "   0.07711876928806305,\n",
       "   0.07564723491668701,\n",
       "   0.07346727699041367,\n",
       "   0.07282636314630508,\n",
       "   0.07265888899564743,\n",
       "   0.07115335017442703,\n",
       "   0.07002631574869156,\n",
       "   0.06919852644205093,\n",
       "   0.06874983757734299,\n",
       "   0.0705949142575264,\n",
       "   0.07198154926300049,\n",
       "   0.0716819241642952,\n",
       "   0.07232969999313354,\n",
       "   0.07141676545143127,\n",
       "   0.07200141251087189,\n",
       "   0.07074747234582901,\n",
       "   0.07069128006696701,\n",
       "   0.07048413157463074,\n",
       "   0.06825623661279678,\n",
       "   0.06814265251159668,\n",
       "   0.06756587326526642,\n",
       "   0.0676538348197937,\n",
       "   0.06666473299264908,\n",
       "   0.06684409826993942,\n",
       "   0.06735321879386902,\n",
       "   0.06719894707202911,\n",
       "   0.06690878421068192,\n",
       "   0.06567928940057755,\n",
       "   0.06544966250658035,\n",
       "   0.06336752325296402,\n",
       "   0.06257115304470062,\n",
       "   0.06190795823931694,\n",
       "   0.06208771839737892,\n",
       "   0.06201953813433647,\n",
       "   0.06194012612104416,\n",
       "   0.061424799263477325,\n",
       "   0.06075426563620567,\n",
       "   0.059808678925037384,\n",
       "   0.059698496013879776,\n",
       "   0.060211990028619766,\n",
       "   0.05997935310006142,\n",
       "   0.05681297928094864,\n",
       "   0.057118989527225494,\n",
       "   0.05827400088310242,\n",
       "   0.05841901898384094,\n",
       "   0.0581488162279129,\n",
       "   0.05926160886883736,\n",
       "   0.0596928596496582,\n",
       "   0.060925181955099106,\n",
       "   0.06074317544698715,\n",
       "   0.06113341823220253,\n",
       "   0.061086948961019516],\n",
       "  'jsc_loss': [15.021902084350586,\n",
       "   9.97669506072998,\n",
       "   6.724930763244629,\n",
       "   4.858356952667236,\n",
       "   3.56475830078125,\n",
       "   2.7702832221984863,\n",
       "   2.190457582473755,\n",
       "   1.7886626720428467,\n",
       "   1.4805179834365845,\n",
       "   1.235393762588501,\n",
       "   1.03333580493927,\n",
       "   0.9068326354026794,\n",
       "   0.8152920007705688,\n",
       "   0.7324042320251465,\n",
       "   0.6444914937019348,\n",
       "   0.586821436882019,\n",
       "   0.5335814356803894,\n",
       "   0.4919498860836029,\n",
       "   0.45207270979881287,\n",
       "   0.41077524423599243,\n",
       "   0.3820120096206665,\n",
       "   0.3627126216888428,\n",
       "   0.350967139005661,\n",
       "   0.3406600058078766,\n",
       "   0.31798693537712097,\n",
       "   0.30036720633506775,\n",
       "   0.2873464524745941,\n",
       "   0.2635727822780609,\n",
       "   0.24845512211322784,\n",
       "   0.2344147115945816,\n",
       "   0.21774719655513763,\n",
       "   0.20583391189575195,\n",
       "   0.19709666073322296,\n",
       "   0.19191643595695496,\n",
       "   0.18771755695343018,\n",
       "   0.17995744943618774,\n",
       "   0.16747017204761505,\n",
       "   0.15711097419261932,\n",
       "   0.15259017050266266,\n",
       "   0.1494584083557129,\n",
       "   0.15318822860717773,\n",
       "   0.1544569581747055,\n",
       "   0.1543067842721939,\n",
       "   0.15430526435375214,\n",
       "   0.1511506736278534,\n",
       "   0.1499006152153015,\n",
       "   0.15475288033485413,\n",
       "   0.15542402863502502,\n",
       "   0.151550754904747,\n",
       "   0.14948585629463196,\n",
       "   0.1450040489435196,\n",
       "   0.14329569041728973,\n",
       "   0.1320199817419052,\n",
       "   0.12800973653793335,\n",
       "   0.11952174454927444,\n",
       "   0.11801221966743469,\n",
       "   0.11324533820152283,\n",
       "   0.11240995675325394,\n",
       "   0.11253594607114792,\n",
       "   0.1077272966504097,\n",
       "   0.11060876399278641,\n",
       "   0.11512961238622665,\n",
       "   0.11777468770742416,\n",
       "   0.11996687203645706,\n",
       "   0.12166231125593185,\n",
       "   0.12202315032482147,\n",
       "   0.12587477266788483,\n",
       "   0.12798336148262024,\n",
       "   0.12944181263446808,\n",
       "   0.13216620683670044,\n",
       "   0.1368081271648407,\n",
       "   0.13866208493709564,\n",
       "   0.13287220895290375,\n",
       "   0.13488391041755676,\n",
       "   0.12331622838973999,\n",
       "   0.12913040816783905,\n",
       "   0.12996244430541992,\n",
       "   0.12992992997169495,\n",
       "   0.1307641714811325,\n",
       "   0.12746982276439667,\n",
       "   0.12399541586637497,\n",
       "   0.12383218109607697,\n",
       "   0.12419687211513519,\n",
       "   0.12023050338029861,\n",
       "   0.12129701673984528,\n",
       "   0.12190347164869308,\n",
       "   0.11936893314123154,\n",
       "   0.11933742463588715,\n",
       "   0.11820316314697266,\n",
       "   0.11570683866739273,\n",
       "   0.11352542042732239,\n",
       "   0.10917925089597702,\n",
       "   0.11169886589050293,\n",
       "   0.10654816776514053,\n",
       "   0.10561205446720123,\n",
       "   0.10741961002349854,\n",
       "   0.1052059605717659,\n",
       "   0.10666340589523315,\n",
       "   0.10334443300962448,\n",
       "   0.1042599231004715,\n",
       "   0.10404925048351288,\n",
       "   0.10294745117425919,\n",
       "   0.10225534439086914,\n",
       "   0.09918713569641113,\n",
       "   0.09786490350961685,\n",
       "   0.09827392548322678,\n",
       "   0.09633871912956238,\n",
       "   0.09423258155584335,\n",
       "   0.09169141203165054,\n",
       "   0.09113193303346634,\n",
       "   0.09391756355762482,\n",
       "   0.08979231864213943,\n",
       "   0.08741152286529541,\n",
       "   0.08604778349399567,\n",
       "   0.08374393731355667,\n",
       "   0.07925203442573547,\n",
       "   0.07905007153749466,\n",
       "   0.08094816654920578,\n",
       "   0.07921189814805984,\n",
       "   0.07737009227275848,\n",
       "   0.07745978236198425,\n",
       "   0.0763111487030983,\n",
       "   0.07963370531797409,\n",
       "   0.08124865591526031,\n",
       "   0.08344053477048874,\n",
       "   0.08319154381752014,\n",
       "   0.08375456929206848,\n",
       "   0.08662885427474976,\n",
       "   0.08310845494270325,\n",
       "   0.0785890743136406,\n",
       "   0.07733919471502304,\n",
       "   0.0763823464512825,\n",
       "   0.07383440434932709,\n",
       "   0.07191278040409088,\n",
       "   0.070948027074337,\n",
       "   0.07260976731777191,\n",
       "   0.07117980718612671,\n",
       "   0.06834513694047928,\n",
       "   0.07212450355291367,\n",
       "   0.07440541684627533,\n",
       "   0.07175365835428238,\n",
       "   0.06654182821512222,\n",
       "   0.06581605225801468,\n",
       "   0.06592344492673874,\n",
       "   0.0660800114274025,\n",
       "   0.066206194460392,\n",
       "   0.06595000624656677,\n",
       "   0.06561657786369324,\n",
       "   0.06709057092666626,\n",
       "   0.0678965300321579,\n",
       "   0.06942862272262573,\n",
       "   0.06821560859680176,\n",
       "   0.06589223444461823,\n",
       "   0.0632471814751625,\n",
       "   0.062071919441223145,\n",
       "   0.05942073464393616,\n",
       "   0.059495024383068085,\n",
       "   0.05606769397854805,\n",
       "   0.0568363256752491,\n",
       "   0.05424528568983078,\n",
       "   0.05498737096786499,\n",
       "   0.05275631695985794,\n",
       "   0.052593231201171875,\n",
       "   0.05177764222025871,\n",
       "   0.04978565871715546,\n",
       "   0.04923398792743683,\n",
       "   0.05317298322916031,\n",
       "   0.051704611629247665,\n",
       "   0.0504566915333271,\n",
       "   0.04765079915523529,\n",
       "   0.0469224639236927,\n",
       "   0.04334378242492676,\n",
       "   0.04331554099917412,\n",
       "   0.0418475866317749,\n",
       "   0.038953062146902084,\n",
       "   0.03750195354223251,\n",
       "   0.03648068383336067,\n",
       "   0.037088263779878616,\n",
       "   0.034511078149080276,\n",
       "   0.03202016279101372,\n",
       "   0.030573757365345955,\n",
       "   0.03147558867931366,\n",
       "   0.0318710133433342,\n",
       "   0.033332858234643936,\n",
       "   0.03333674371242523,\n",
       "   0.0342264361679554,\n",
       "   0.03251780942082405,\n",
       "   0.03168042004108429,\n",
       "   0.03239704295992851,\n",
       "   0.03069816157221794,\n",
       "   0.03038022853434086,\n",
       "   0.03163823485374451,\n",
       "   0.03136216849088669,\n",
       "   0.032101746648550034,\n",
       "   0.032245926558971405,\n",
       "   0.032721228897571564,\n",
       "   0.03303932771086693,\n",
       "   0.03412129357457161,\n",
       "   0.03199648857116699,\n",
       "   0.03228319063782692,\n",
       "   0.03303884342312813,\n",
       "   0.03327123820781708,\n",
       "   0.03337007761001587,\n",
       "   0.03531961888074875,\n",
       "   0.03702424094080925,\n",
       "   0.03652793914079666,\n",
       "   0.035632453858852386,\n",
       "   0.035651788115501404,\n",
       "   0.035486381500959396,\n",
       "   0.03442925959825516,\n",
       "   0.03468957915902138,\n",
       "   0.03442401438951492,\n",
       "   0.031749263405799866,\n",
       "   0.03145809844136238,\n",
       "   0.03263077512383461,\n",
       "   0.032463882118463516,\n",
       "   0.03261657804250717,\n",
       "   0.032552894204854965,\n",
       "   0.032643962651491165,\n",
       "   0.03258371353149414,\n",
       "   0.03157191351056099,\n",
       "   0.02999272383749485,\n",
       "   0.027759453281760216,\n",
       "   0.025525182485580444,\n",
       "   0.024852044880390167,\n",
       "   0.023561231791973114,\n",
       "   0.023773204535245895,\n",
       "   0.023976247757673264,\n",
       "   0.02382134646177292,\n",
       "   0.02277216501533985,\n",
       "   0.020920420065522194,\n",
       "   0.02081836201250553,\n",
       "   0.020760919898748398,\n",
       "   0.021353228017687798,\n",
       "   0.019975511357188225,\n",
       "   0.01987430267035961,\n",
       "   0.02006053738296032,\n",
       "   0.019779538735747337,\n",
       "   0.019631873816251755,\n",
       "   0.021007146686315536,\n",
       "   0.021426819264888763,\n",
       "   0.020622391253709793,\n",
       "   0.020440030843019485,\n",
       "   0.02093934826552868,\n",
       "   0.021386798471212387,\n",
       "   0.02187829650938511,\n",
       "   0.02240963652729988,\n",
       "   0.02127007767558098,\n",
       "   0.02156064100563526,\n",
       "   0.021384332329034805,\n",
       "   0.021037692204117775,\n",
       "   0.0205546785145998,\n",
       "   0.01853911206126213,\n",
       "   0.018672676756978035,\n",
       "   0.018717074766755104,\n",
       "   0.019931910559535027,\n",
       "   0.019792696461081505,\n",
       "   0.0195588581264019,\n",
       "   0.02051667869091034,\n",
       "   0.019548576325178146,\n",
       "   0.019024763256311417,\n",
       "   0.01952354609966278,\n",
       "   0.01811889559030533,\n",
       "   0.018254194408655167,\n",
       "   0.018338751047849655,\n",
       "   0.01867201365530491,\n",
       "   0.017949284985661507,\n",
       "   0.018247948959469795,\n",
       "   0.01868937909603119,\n",
       "   0.01715913787484169,\n",
       "   0.016055390238761902,\n",
       "   0.01676558330655098,\n",
       "   0.016252627596259117,\n",
       "   0.016573449596762657,\n",
       "   0.01518024317920208,\n",
       "   0.014730243012309074,\n",
       "   0.014320822432637215,\n",
       "   0.012916442938148975,\n",
       "   0.012336612679064274,\n",
       "   0.011821344494819641,\n",
       "   0.011730078607797623,\n",
       "   0.011364820413291454,\n",
       "   0.011453897692263126,\n",
       "   0.010759583674371243,\n",
       "   0.010177366435527802,\n",
       "   0.009360217489302158,\n",
       "   0.008594919927418232,\n",
       "   0.008352606557309628,\n",
       "   0.00907442718744278,\n",
       "   0.008752220310270786,\n",
       "   0.008596559055149555,\n",
       "   0.008081001229584217,\n",
       "   0.00792428944259882,\n",
       "   0.007655284367501736,\n",
       "   0.007651241030544043,\n",
       "   0.007657638750970364,\n",
       "   0.007372425869107246,\n",
       "   0.006958284415304661,\n",
       "   0.006686890963464975,\n",
       "   0.006731470115482807,\n",
       "   0.007156246341764927,\n",
       "   0.007205579429864883,\n",
       "   0.007195370737463236,\n",
       "   0.007003260776400566,\n",
       "   0.007497997954487801,\n",
       "   0.0073904613964259624,\n",
       "   0.007065587677061558,\n",
       "   0.007021636236459017,\n",
       "   0.006725558079779148,\n",
       "   0.006714358925819397,\n",
       "   0.006383079569786787,\n",
       "   0.006370251998305321,\n",
       "   0.006442529149353504,\n",
       "   0.006388755049556494,\n",
       "   0.006375128868967295,\n",
       "   0.006762397009879351,\n",
       "   0.00778516661375761,\n",
       "   0.007604264188557863,\n",
       "   0.007480408065021038,\n",
       "   0.0076496656984090805,\n",
       "   0.007707330398261547,\n",
       "   0.0074844141490757465,\n",
       "   0.007814382202923298,\n",
       "   0.007885373197495937,\n",
       "   0.007360114715993404,\n",
       "   0.007273188326507807,\n",
       "   0.007105762138962746,\n",
       "   0.007625375874340534,\n",
       "   0.00740296533331275,\n",
       "   0.00699722021818161,\n",
       "   0.006946996320039034,\n",
       "   0.007096294779330492,\n",
       "   0.007605310063809156,\n",
       "   0.007333023939281702,\n",
       "   0.007788101211190224,\n",
       "   0.007490083109587431,\n",
       "   0.006881142966449261,\n",
       "   0.007059148047119379,\n",
       "   0.007171184290200472,\n",
       "   0.006987071130424738,\n",
       "   0.007033987436443567,\n",
       "   0.007153765764087439,\n",
       "   0.007089738268405199,\n",
       "   0.0074000577442348,\n",
       "   0.006998066324740648,\n",
       "   0.007574776187539101,\n",
       "   0.007624599151313305,\n",
       "   0.007248201407492161,\n",
       "   0.007011480629444122,\n",
       "   0.006672810297459364,\n",
       "   0.007002460770308971,\n",
       "   0.006731335539370775,\n",
       "   0.006702822167426348,\n",
       "   0.006818885914981365,\n",
       "   0.0070532821118831635,\n",
       "   0.006801357492804527,\n",
       "   0.006283480674028397,\n",
       "   0.006218588445335627,\n",
       "   0.005808296147733927,\n",
       "   0.005588073283433914,\n",
       "   0.0057497103698551655,\n",
       "   0.005650187376886606,\n",
       "   0.005337732844054699,\n",
       "   0.00562812527641654,\n",
       "   0.005992302671074867,\n",
       "   0.0059968880377709866,\n",
       "   0.005930108483880758,\n",
       "   0.005952168256044388,\n",
       "   0.005908805411309004,\n",
       "   0.005991795565932989,\n",
       "   0.006030277814716101,\n",
       "   0.005950023885816336,\n",
       "   0.006060756742954254,\n",
       "   0.005969286896288395,\n",
       "   0.006066520232707262,\n",
       "   0.006395797710865736,\n",
       "   0.006685810163617134,\n",
       "   0.006908966228365898,\n",
       "   0.0071685900911688805,\n",
       "   0.00769323855638504,\n",
       "   0.008234868757426739,\n",
       "   0.008184764534235,\n",
       "   0.008514720015227795,\n",
       "   0.008755343034863472,\n",
       "   0.008616672828793526,\n",
       "   0.008457333780825138,\n",
       "   0.0076379235833883286,\n",
       "   0.008185984566807747,\n",
       "   0.00864060502499342,\n",
       "   0.008508512750267982,\n",
       "   0.00849959533661604,\n",
       "   0.007754359394311905,\n",
       "   0.007796779274940491,\n",
       "   0.007843696512281895,\n",
       "   0.007246609311550856,\n",
       "   0.007642598357051611,\n",
       "   0.007226763293147087,\n",
       "   0.007276758551597595,\n",
       "   0.00726892426609993,\n",
       "   0.007131773978471756,\n",
       "   0.007251329720020294,\n",
       "   0.006872287951409817,\n",
       "   0.006675281096249819,\n",
       "   0.007080448791384697,\n",
       "   0.007385856471955776,\n",
       "   0.0077749211341142654,\n",
       "   0.007881134748458862,\n",
       "   0.00797217432409525,\n",
       "   0.007914322428405285,\n",
       "   0.008320555090904236,\n",
       "   0.008326483890414238,\n",
       "   0.008297317661345005,\n",
       "   0.008853497914969921,\n",
       "   0.009007061831653118,\n",
       "   0.009184359572827816,\n",
       "   0.009189930744469166,\n",
       "   0.009260470047593117,\n",
       "   0.008592881262302399,\n",
       "   0.00844592321664095,\n",
       "   0.00780085101723671,\n",
       "   0.007975650951266289,\n",
       "   0.00818715151399374,\n",
       "   0.007886079140007496,\n",
       "   0.007846182212233543,\n",
       "   0.00812871940433979,\n",
       "   0.008468260988593102,\n",
       "   0.007998887449502945,\n",
       "   0.007496307138353586,\n",
       "   0.007148653734475374,\n",
       "   0.0073102982714772224,\n",
       "   0.0072157904505729675,\n",
       "   0.007628312800079584,\n",
       "   0.008440172299742699,\n",
       "   0.008840998634696007,\n",
       "   0.009199189953505993,\n",
       "   0.009121032431721687,\n",
       "   0.009593104012310505,\n",
       "   0.010234436951577663,\n",
       "   0.010019046254456043,\n",
       "   0.010015087202191353,\n",
       "   0.010105306282639503,\n",
       "   0.01001392025500536,\n",
       "   0.009707414545118809,\n",
       "   0.009596986696124077,\n",
       "   0.00900720339268446,\n",
       "   0.008808705024421215,\n",
       "   0.008629345335066319,\n",
       "   0.008037563413381577,\n",
       "   0.007543185725808144,\n",
       "   0.007379321847110987,\n",
       "   0.0066057718358933926,\n",
       "   0.007324655540287495,\n",
       "   0.0070484899915754795,\n",
       "   0.007296060677617788,\n",
       "   0.007480816449970007,\n",
       "   0.007308863569051027,\n",
       "   0.007267414592206478,\n",
       "   0.0071609183214604855,\n",
       "   0.00758874649181962,\n",
       "   0.00817430391907692,\n",
       "   0.008448842912912369,\n",
       "   0.00838976725935936,\n",
       "   0.008292486891150475,\n",
       "   0.008399306796491146,\n",
       "   0.00815743301063776,\n",
       "   0.007878877222537994,\n",
       "   0.0077300602570176125,\n",
       "   0.007515289355069399,\n",
       "   0.007739713881164789,\n",
       "   0.008095445111393929,\n",
       "   0.0076516238041222095,\n",
       "   0.00780577352270484,\n",
       "   0.007561986800283194,\n",
       "   0.007854564115405083,\n",
       "   0.007941643707454205,\n",
       "   0.008632510900497437,\n",
       "   0.008508099243044853,\n",
       "   0.008848381228744984,\n",
       "   0.009259306825697422,\n",
       "   0.00884462334215641,\n",
       "   0.008680633269250393,\n",
       "   0.008014284074306488,\n",
       "   0.0076431771740317345,\n",
       "   0.007637730333954096,\n",
       "   0.00778502132743597,\n",
       "   0.007674103137105703,\n",
       "   0.007831156253814697,\n",
       "   0.007819638587534428,\n",
       "   0.007531114853918552,\n",
       "   0.007643779739737511,\n",
       "   0.007424124516546726,\n",
       "   0.0075135910883545876,\n",
       "   0.007669500075280666,\n",
       "   0.006873191334307194,\n",
       "   0.0069673312827944756,\n",
       "   0.007012451998889446,\n",
       "   0.006917266175150871,\n",
       "   0.006931548938155174,\n",
       "   0.007444567047059536,\n",
       "   0.007823031395673752],\n",
       "  'ff_loss': [0.28572985529899597,\n",
       "   0.22544343769550323,\n",
       "   0.18187105655670166,\n",
       "   0.15760289132595062,\n",
       "   0.12666583061218262,\n",
       "   0.11554159224033356,\n",
       "   0.09108048677444458,\n",
       "   0.07342706620693207,\n",
       "   0.053727589547634125,\n",
       "   0.04116170108318329,\n",
       "   0.031491294503211975,\n",
       "   0.026210689917206764,\n",
       "   0.02422533556818962,\n",
       "   0.022072933614253998,\n",
       "   0.020500170066952705,\n",
       "   0.01776151731610298,\n",
       "   0.017227597534656525,\n",
       "   0.016600539907813072,\n",
       "   0.015750005841255188,\n",
       "   0.015219748951494694,\n",
       "   0.014866815879940987,\n",
       "   0.014710315503180027,\n",
       "   0.015173010528087616,\n",
       "   0.015133166685700417,\n",
       "   0.01592429354786873,\n",
       "   0.016820499673485756,\n",
       "   0.019057704135775566,\n",
       "   0.02074206806719303,\n",
       "   0.022547436878085136,\n",
       "   0.025280483067035675,\n",
       "   0.027506263926625252,\n",
       "   0.030097076669335365,\n",
       "   0.031248006969690323,\n",
       "   0.032482631504535675,\n",
       "   0.03761045262217522,\n",
       "   0.03942957520484924,\n",
       "   0.040488772094249725,\n",
       "   0.04381473362445831,\n",
       "   0.045434433966875076,\n",
       "   0.04774792492389679,\n",
       "   0.05000242963433266,\n",
       "   0.05454442277550697,\n",
       "   0.059259962290525436,\n",
       "   0.058870237320661545,\n",
       "   0.059758663177490234,\n",
       "   0.06263566017150879,\n",
       "   0.06684551388025284,\n",
       "   0.07150205969810486,\n",
       "   0.07505163550376892,\n",
       "   0.07725401967763901,\n",
       "   0.0848073661327362,\n",
       "   0.09091228246688843,\n",
       "   0.09554574638605118,\n",
       "   0.09338365495204926,\n",
       "   0.1010337695479393,\n",
       "   0.10648555308580399,\n",
       "   0.10888239741325378,\n",
       "   0.1098049208521843,\n",
       "   0.10857656598091125,\n",
       "   0.10522912442684174,\n",
       "   0.10495992004871368,\n",
       "   0.10139857232570648,\n",
       "   0.09998804330825806,\n",
       "   0.10197615623474121,\n",
       "   0.09626143425703049,\n",
       "   0.09374801069498062,\n",
       "   0.09411575645208359,\n",
       "   0.09132964164018631,\n",
       "   0.08408335596323013,\n",
       "   0.08030246198177338,\n",
       "   0.08058691024780273,\n",
       "   0.0790087953209877,\n",
       "   0.07910895347595215,\n",
       "   0.07523798197507858,\n",
       "   0.07564500719308853,\n",
       "   0.07574640214443207,\n",
       "   0.07485461980104446,\n",
       "   0.07646971195936203,\n",
       "   0.07420152425765991,\n",
       "   0.07355066388845444,\n",
       "   0.0708027333021164,\n",
       "   0.07257498055696487,\n",
       "   0.07221470028162003,\n",
       "   0.07525917887687683,\n",
       "   0.07428433746099472,\n",
       "   0.07037533074617386,\n",
       "   0.06913753598928452,\n",
       "   0.07161969691514969,\n",
       "   0.07299669831991196,\n",
       "   0.07547309249639511,\n",
       "   0.07662774622440338,\n",
       "   0.07854051142930984,\n",
       "   0.0812077522277832,\n",
       "   0.07891107350587845,\n",
       "   0.08100540190935135,\n",
       "   0.08244658261537552,\n",
       "   0.08177755773067474,\n",
       "   0.0827617347240448,\n",
       "   0.0832742378115654,\n",
       "   0.08020665496587753,\n",
       "   0.0778878927230835,\n",
       "   0.08219297230243683,\n",
       "   0.08018480241298676,\n",
       "   0.08006300032138824,\n",
       "   0.07782591134309769,\n",
       "   0.07723437249660492,\n",
       "   0.07732736319303513,\n",
       "   0.07828634232282639,\n",
       "   0.07894323766231537,\n",
       "   0.0748804584145546,\n",
       "   0.07546191662549973,\n",
       "   0.07316393405199051,\n",
       "   0.0753401517868042,\n",
       "   0.07449301332235336,\n",
       "   0.07301396131515503,\n",
       "   0.07282158732414246,\n",
       "   0.07076971977949142,\n",
       "   0.06993921846151352,\n",
       "   0.06910943984985352,\n",
       "   0.06909987330436707,\n",
       "   0.07056740671396255,\n",
       "   0.06938894838094711,\n",
       "   0.06800489872694016,\n",
       "   0.06939096003770828,\n",
       "   0.06637846678495407,\n",
       "   0.06724753230810165,\n",
       "   0.06590437144041061,\n",
       "   0.0647583156824112,\n",
       "   0.06149417534470558,\n",
       "   0.0581875704228878,\n",
       "   0.05557269975543022,\n",
       "   0.05475443974137306,\n",
       "   0.057078342884778976,\n",
       "   0.058546602725982666,\n",
       "   0.05801839753985405,\n",
       "   0.05974842607975006,\n",
       "   0.060165490955114365,\n",
       "   0.05966687574982643,\n",
       "   0.06408599019050598,\n",
       "   0.06352268159389496,\n",
       "   0.06542261689901352,\n",
       "   0.06730152666568756,\n",
       "   0.0654853954911232,\n",
       "   0.06516101211309433,\n",
       "   0.06520882993936539,\n",
       "   0.06724261492490768,\n",
       "   0.06752132624387741,\n",
       "   0.06739555299282074,\n",
       "   0.06535948812961578,\n",
       "   0.06640392541885376,\n",
       "   0.07073160260915756,\n",
       "   0.06911821663379669,\n",
       "   0.07008916884660721,\n",
       "   0.0689656212925911,\n",
       "   0.07004571706056595,\n",
       "   0.06897883862257004,\n",
       "   0.06638799607753754,\n",
       "   0.0652201846241951,\n",
       "   0.0638093426823616,\n",
       "   0.06497442722320557,\n",
       "   0.06489061564207077,\n",
       "   0.06695498526096344,\n",
       "   0.06747163087129593,\n",
       "   0.0661623552441597,\n",
       "   0.06837610900402069,\n",
       "   0.0696396753191948,\n",
       "   0.0702313557267189,\n",
       "   0.06873167306184769,\n",
       "   0.06925453990697861,\n",
       "   0.06848776340484619,\n",
       "   0.06809782236814499,\n",
       "   0.0718429684638977,\n",
       "   0.07178796827793121,\n",
       "   0.07240550220012665,\n",
       "   0.07329600304365158,\n",
       "   0.07366553694009781,\n",
       "   0.07229035347700119,\n",
       "   0.06952685117721558,\n",
       "   0.06875761598348618,\n",
       "   0.06851065158843994,\n",
       "   0.06890852004289627,\n",
       "   0.06750026345252991,\n",
       "   0.06724511086940765,\n",
       "   0.06534428149461746,\n",
       "   0.0659923106431961,\n",
       "   0.06762699037790298,\n",
       "   0.06701675802469254,\n",
       "   0.06805787980556488,\n",
       "   0.06775917857885361,\n",
       "   0.06640986353158951,\n",
       "   0.06859111785888672,\n",
       "   0.06879718601703644,\n",
       "   0.06733912229537964,\n",
       "   0.06512351334095001,\n",
       "   0.06658091396093369,\n",
       "   0.06715937703847885,\n",
       "   0.06871378421783447,\n",
       "   0.06793879717588425,\n",
       "   0.06910920143127441,\n",
       "   0.07008633017539978,\n",
       "   0.07110234349966049,\n",
       "   0.07063394039869308,\n",
       "   0.07079900801181793,\n",
       "   0.07242804020643234,\n",
       "   0.07127512246370316,\n",
       "   0.06930092722177505,\n",
       "   0.06951290369033813,\n",
       "   0.06681141257286072,\n",
       "   0.06746377050876617,\n",
       "   0.07127087563276291,\n",
       "   0.07308243960142136,\n",
       "   0.07402954250574112,\n",
       "   0.07488373667001724,\n",
       "   0.07626885920763016,\n",
       "   0.07566073536872864,\n",
       "   0.07362114638090134,\n",
       "   0.07196423411369324,\n",
       "   0.07135941088199615,\n",
       "   0.07073991745710373,\n",
       "   0.072386234998703,\n",
       "   0.07365189492702484,\n",
       "   0.07162730395793915,\n",
       "   0.07003612071275711,\n",
       "   0.06699586659669876,\n",
       "   0.0651836171746254,\n",
       "   0.060950618237257004,\n",
       "   0.06396564841270447,\n",
       "   0.06334637105464935,\n",
       "   0.06316229701042175,\n",
       "   0.06133131682872772,\n",
       "   0.06301617622375488,\n",
       "   0.061537403613328934,\n",
       "   0.06141160801053047,\n",
       "   0.061084549874067307,\n",
       "   0.060749419033527374,\n",
       "   0.06017322465777397,\n",
       "   0.06259332597255707,\n",
       "   0.06115860491991043,\n",
       "   0.06077107414603233,\n",
       "   0.058839891105890274,\n",
       "   0.05804852396249771,\n",
       "   0.0576423816382885,\n",
       "   0.05935455858707428,\n",
       "   0.06097228080034256,\n",
       "   0.06288965791463852,\n",
       "   0.06592360138893127,\n",
       "   0.06858716905117035,\n",
       "   0.06948450207710266,\n",
       "   0.07159416377544403,\n",
       "   0.07313640415668488,\n",
       "   0.07584501802921295,\n",
       "   0.0758650153875351,\n",
       "   0.07667580246925354,\n",
       "   0.07776758819818497,\n",
       "   0.07804613560438156,\n",
       "   0.07768987119197845,\n",
       "   0.0782708004117012,\n",
       "   0.07979311048984528,\n",
       "   0.0837775319814682,\n",
       "   0.0829261913895607,\n",
       "   0.08200541883707047,\n",
       "   0.08238456398248672,\n",
       "   0.08157267421483994,\n",
       "   0.07918701320886612,\n",
       "   0.07562103867530823,\n",
       "   0.07433797419071198,\n",
       "   0.07151918113231659,\n",
       "   0.07044697552919388,\n",
       "   0.07063765823841095,\n",
       "   0.06921292096376419,\n",
       "   0.06929780542850494,\n",
       "   0.06777482479810715,\n",
       "   0.06779593229293823,\n",
       "   0.06503208726644516,\n",
       "   0.06527826935052872,\n",
       "   0.0657808929681778,\n",
       "   0.06578008085489273,\n",
       "   0.06736994534730911,\n",
       "   0.06736235320568085,\n",
       "   0.06731373816728592,\n",
       "   0.06721793860197067,\n",
       "   0.06553618609905243,\n",
       "   0.06408899277448654,\n",
       "   0.0645095705986023,\n",
       "   0.06424722075462341,\n",
       "   0.06612521409988403,\n",
       "   0.06662894785404205,\n",
       "   0.06435959786176682,\n",
       "   0.06404740363359451,\n",
       "   0.06410744786262512,\n",
       "   0.06344849616289139,\n",
       "   0.06184123829007149,\n",
       "   0.06359416991472244,\n",
       "   0.06616751104593277,\n",
       "   0.06827942281961441,\n",
       "   0.06786239147186279,\n",
       "   0.06936415284872055,\n",
       "   0.06841343641281128,\n",
       "   0.07164378464221954,\n",
       "   0.07128920406103134,\n",
       "   0.07134290784597397,\n",
       "   0.06738044321537018,\n",
       "   0.06658601015806198,\n",
       "   0.06422887742519379,\n",
       "   0.06061863526701927,\n",
       "   0.06041090935468674,\n",
       "   0.05746181681752205,\n",
       "   0.06105748936533928,\n",
       "   0.05967509746551514,\n",
       "   0.059167664498090744,\n",
       "   0.06064597889780998,\n",
       "   0.06038419529795647,\n",
       "   0.05914567410945892,\n",
       "   0.059823453426361084,\n",
       "   0.06094793602824211,\n",
       "   0.06185241416096687,\n",
       "   0.05959372967481613,\n",
       "   0.06042240560054779,\n",
       "   0.0617271326482296,\n",
       "   0.0628058984875679,\n",
       "   0.06349200010299683,\n",
       "   0.06434255838394165,\n",
       "   0.06426210701465607,\n",
       "   0.06437230110168457,\n",
       "   0.06405076384544373,\n",
       "   0.06435750424861908,\n",
       "   0.06571369618177414,\n",
       "   0.06640572100877762,\n",
       "   0.06554523855447769,\n",
       "   0.06304296851158142,\n",
       "   0.06233370304107666,\n",
       "   0.061857838183641434,\n",
       "   0.06208600103855133,\n",
       "   0.060765959322452545,\n",
       "   0.06122852489352226,\n",
       "   0.06037294119596481,\n",
       "   0.06117122620344162,\n",
       "   0.06115839630365372,\n",
       "   0.06518661975860596,\n",
       "   0.06483496725559235,\n",
       "   0.06467805802822113,\n",
       "   0.06628508120775223,\n",
       "   0.06517389416694641,\n",
       "   0.06355015188455582,\n",
       "   0.06232328340411186,\n",
       "   0.06406750530004501,\n",
       "   0.06079402565956116,\n",
       "   0.05988258868455887,\n",
       "   0.058452900499105453,\n",
       "   0.05905083939433098,\n",
       "   0.05949517339468002,\n",
       "   0.06013408303260803,\n",
       "   0.06019006296992302,\n",
       "   0.05859316140413284,\n",
       "   0.05930914357304573,\n",
       "   0.06006954610347748,\n",
       "   0.0610584020614624,\n",
       "   0.06091465428471565,\n",
       "   0.05707256868481636,\n",
       "   0.056948158890008926,\n",
       "   0.0566064827144146,\n",
       "   0.05519629269838333,\n",
       "   0.05601677671074867,\n",
       "   0.054551757872104645,\n",
       "   0.050900042057037354,\n",
       "   0.04850715398788452,\n",
       "   0.04736264422535896,\n",
       "   0.04728607088327408,\n",
       "   0.048372093588113785,\n",
       "   0.04635144770145416,\n",
       "   0.04658057913184166,\n",
       "   0.0468515120446682,\n",
       "   0.04363785684108734,\n",
       "   0.04379472881555557,\n",
       "   0.043527666479349136,\n",
       "   0.04381102696061134,\n",
       "   0.044411029666662216,\n",
       "   0.04626923054456711,\n",
       "   0.04843077436089516,\n",
       "   0.050643641501665115,\n",
       "   0.051885996013879776,\n",
       "   0.05622825399041176,\n",
       "   0.06155138090252876,\n",
       "   0.0647687092423439,\n",
       "   0.06742952764034271,\n",
       "   0.06710594892501831,\n",
       "   0.06752046197652817,\n",
       "   0.06973935663700104,\n",
       "   0.06994642317295074,\n",
       "   0.07173152267932892,\n",
       "   0.07119780033826828,\n",
       "   0.06968027353286743,\n",
       "   0.06710737943649292,\n",
       "   0.06438921391963959,\n",
       "   0.0629117488861084,\n",
       "   0.05988569185137749,\n",
       "   0.059248778969049454,\n",
       "   0.05783163383603096,\n",
       "   0.054313089698553085,\n",
       "   0.054041557013988495,\n",
       "   0.05367865040898323,\n",
       "   0.0507601760327816,\n",
       "   0.049569275230169296,\n",
       "   0.04964028671383858,\n",
       "   0.04681236669421196,\n",
       "   0.04464656859636307,\n",
       "   0.04223649576306343,\n",
       "   0.04071234166622162,\n",
       "   0.039974749088287354,\n",
       "   0.04056757315993309,\n",
       "   0.040842730551958084,\n",
       "   0.04238348454236984,\n",
       "   0.0444827601313591,\n",
       "   0.04582781344652176,\n",
       "   0.04876599460840225,\n",
       "   0.049325425177812576,\n",
       "   0.04863949492573738,\n",
       "   0.05133649334311485,\n",
       "   0.05240822955965996,\n",
       "   0.05589348077774048,\n",
       "   0.05824454873800278,\n",
       "   0.059890761971473694,\n",
       "   0.06233741343021393,\n",
       "   0.062169644981622696,\n",
       "   0.06419689953327179,\n",
       "   0.06637831777334213,\n",
       "   0.0668664276599884,\n",
       "   0.06485768407583237,\n",
       "   0.06481867283582687,\n",
       "   0.0654844120144844,\n",
       "   0.0649690330028534,\n",
       "   0.06457152962684631,\n",
       "   0.06542971730232239,\n",
       "   0.0646979808807373,\n",
       "   0.061383429914712906,\n",
       "   0.05974753201007843,\n",
       "   0.059711284935474396,\n",
       "   0.05997936800122261,\n",
       "   0.061091337352991104,\n",
       "   0.06184126064181328,\n",
       "   0.0638074055314064,\n",
       "   0.06316542625427246,\n",
       "   0.06537418812513351,\n",
       "   0.06423942744731903,\n",
       "   0.06287538260221481,\n",
       "   0.061028853058815,\n",
       "   0.05970023572444916,\n",
       "   0.057548005133867264,\n",
       "   0.05400168523192406,\n",
       "   0.05315406620502472,\n",
       "   0.05174083262681961,\n",
       "   0.05010555312037468,\n",
       "   0.04824555292725563,\n",
       "   0.0484686903655529,\n",
       "   0.04800044000148773,\n",
       "   0.04816869646310806,\n",
       "   0.04725492000579834,\n",
       "   0.0468643382191658,\n",
       "   0.047206681221723557,\n",
       "   0.04892601817846298,\n",
       "   0.05084311217069626,\n",
       "   0.05036503076553345,\n",
       "   0.04985024034976959,\n",
       "   0.05190271511673927,\n",
       "   0.0522802360355854,\n",
       "   0.05080392584204674,\n",
       "   0.05044180899858475,\n",
       "   0.04960136115550995,\n",
       "   0.0496647022664547,\n",
       "   0.050185609608888626,\n",
       "   0.05151453986763954,\n",
       "   0.052424829453229904,\n",
       "   0.052299801260232925,\n",
       "   0.05149712786078453,\n",
       "   0.05138642340898514,\n",
       "   0.049457207322120667,\n",
       "   0.05027256906032562,\n",
       "   0.0482102707028389,\n",
       "   0.048542652279138565,\n",
       "   0.04684454947710037,\n",
       "   0.047381043434143066,\n",
       "   0.04726506024599075,\n",
       "   0.046393021941185,\n",
       "   0.04746837168931961,\n",
       "   0.04720610752701759,\n",
       "   0.04796326160430908,\n",
       "   0.04885169491171837,\n",
       "   0.0489317923784256,\n",
       "   0.05023664981126785,\n",
       "   0.05151008069515228,\n",
       "   0.052643999457359314,\n",
       "   0.0538051538169384,\n",
       "   0.05170227587223053,\n",
       "   0.05244746059179306,\n",
       "   0.05473136901855469,\n",
       "   0.05690391734242439,\n",
       "   0.057326629757881165,\n",
       "   0.057565994560718536,\n",
       "   0.0584329329431057,\n",
       "   0.059746336191892624],\n",
       "  'test_losses': [16.506278961896896,\n",
       "   11.032652720808983,\n",
       "   7.407957166433334,\n",
       "   5.362159997224808,\n",
       "   3.9050476476550102,\n",
       "   3.0231017395853996,\n",
       "   2.378400333225727,\n",
       "   1.9248749855905771,\n",
       "   1.5786765860393643,\n",
       "   1.31056959182024,\n",
       "   1.0873756175860763,\n",
       "   0.9522948970552534,\n",
       "   0.8555112867616117,\n",
       "   0.7681602663360536,\n",
       "   0.6780820013955235,\n",
       "   0.6183739933185279,\n",
       "   0.5646508261561394,\n",
       "   0.5246712258085608,\n",
       "   0.4861916187219322,\n",
       "   0.44809198565781116,\n",
       "   0.4233154244720936,\n",
       "   0.40957682859152555,\n",
       "   0.4029190596193075,\n",
       "   0.39616303937509656,\n",
       "   0.3793555926531553,\n",
       "   0.36478051729500294,\n",
       "   0.35742388665676117,\n",
       "   0.34152556397020817,\n",
       "   0.33207160513848066,\n",
       "   0.32398053258657455,\n",
       "   0.3179635889828205,\n",
       "   0.31477363407611847,\n",
       "   0.30726668052375317,\n",
       "   0.3066614866256714,\n",
       "   0.30771430768072605,\n",
       "   0.3014770522713661,\n",
       "   0.29458029568195343,\n",
       "   0.28844075091183186,\n",
       "   0.2818695902824402,\n",
       "   0.2826951127499342,\n",
       "   0.29433489963412285,\n",
       "   0.3019437585026026,\n",
       "   0.30894794315099716,\n",
       "   0.30926937609910965,\n",
       "   0.31428221985697746,\n",
       "   0.31959018111228943,\n",
       "   0.3304234631359577,\n",
       "   0.3355308659374714,\n",
       "   0.33636730909347534,\n",
       "   0.3366663306951523,\n",
       "   0.34055422246456146,\n",
       "   0.34080571308732033,\n",
       "   0.3333386108279228,\n",
       "   0.33045410737395287,\n",
       "   0.3268285058438778,\n",
       "   0.33384163305163383,\n",
       "   0.3293343707919121,\n",
       "   0.3295000493526459,\n",
       "   0.32690035179257393,\n",
       "   0.3191906549036503,\n",
       "   0.32312294840812683,\n",
       "   0.3247050382196903,\n",
       "   0.32446716725826263,\n",
       "   0.3258991129696369,\n",
       "   0.3211784176528454,\n",
       "   0.3211182691156864,\n",
       "   0.32401590049266815,\n",
       "   0.32100582495331764,\n",
       "   0.31532126665115356,\n",
       "   0.31652809493243694,\n",
       "   0.32226648181676865,\n",
       "   0.3205114249140024,\n",
       "   0.31373987533152103,\n",
       "   0.3152039758861065,\n",
       "   0.3032789472490549,\n",
       "   0.310060141608119,\n",
       "   0.3091377168893814,\n",
       "   0.31326030008494854,\n",
       "   0.3140197563916445,\n",
       "   0.30919631756842136,\n",
       "   0.3049266189336777,\n",
       "   0.30801985412836075,\n",
       "   0.3096485212445259,\n",
       "   0.3076490554958582,\n",
       "   0.3089580275118351,\n",
       "   0.30483695678412914,\n",
       "   0.2993173059076071,\n",
       "   0.30268197134137154,\n",
       "   0.30462479405105114,\n",
       "   0.30667768605053425,\n",
       "   0.3052849546074867,\n",
       "   0.3057417497038841,\n",
       "   0.3100462332367897,\n",
       "   0.30384514667093754,\n",
       "   0.3113809358328581,\n",
       "   0.3117122482508421,\n",
       "   0.30623519979417324,\n",
       "   0.3089967332780361,\n",
       "   0.3098597265779972,\n",
       "   0.31127859465777874,\n",
       "   0.3079590182751417,\n",
       "   0.3111349903047085,\n",
       "   0.3095544409006834,\n",
       "   0.30750034004449844,\n",
       "   0.30568097345530987,\n",
       "   0.30292847752571106,\n",
       "   0.3010165337473154,\n",
       "   0.29975054413080215,\n",
       "   0.2999731507152319,\n",
       "   0.29460711032152176,\n",
       "   0.29497470520436764,\n",
       "   0.2892041113227606,\n",
       "   0.2901619430631399,\n",
       "   0.2902459464967251,\n",
       "   0.28494052588939667,\n",
       "   0.28161351196467876,\n",
       "   0.28176518343389034,\n",
       "   0.2823755219578743,\n",
       "   0.27872519195079803,\n",
       "   0.2788829244673252,\n",
       "   0.2831789404153824,\n",
       "   0.28598029911518097,\n",
       "   0.2920742928981781,\n",
       "   0.29369680024683475,\n",
       "   0.2896114941686392,\n",
       "   0.2851388119161129,\n",
       "   0.288161413744092,\n",
       "   0.28957629948854446,\n",
       "   0.27743273973464966,\n",
       "   0.2698006387799978,\n",
       "   0.25968234054744244,\n",
       "   0.2562917675822973,\n",
       "   0.25041590817272663,\n",
       "   0.2512693200260401,\n",
       "   0.25012180767953396,\n",
       "   0.2579365875571966,\n",
       "   0.260698139667511,\n",
       "   0.256245706230402,\n",
       "   0.26186009496450424,\n",
       "   0.2649381160736084,\n",
       "   0.26222543604671955,\n",
       "   0.25768436677753925,\n",
       "   0.25594147481024265,\n",
       "   0.2533187214285135,\n",
       "   0.2545778602361679,\n",
       "   0.26055634766817093,\n",
       "   0.25601574778556824,\n",
       "   0.253739794716239,\n",
       "   0.25734339468181133,\n",
       "   0.2593836933374405,\n",
       "   0.2644609771668911,\n",
       "   0.25917738303542137,\n",
       "   0.2594859767705202,\n",
       "   0.2542853467166424,\n",
       "   0.25642317347228527,\n",
       "   0.2544330321252346,\n",
       "   0.25452144257724285,\n",
       "   0.24850359000265598,\n",
       "   0.2488850373774767,\n",
       "   0.25283865071833134,\n",
       "   0.2506843935698271,\n",
       "   0.2524941451847553,\n",
       "   0.2563213352113962,\n",
       "   0.2563931532204151,\n",
       "   0.2583487667143345,\n",
       "   0.25679477490484715,\n",
       "   0.2622411958873272,\n",
       "   0.2610021885484457,\n",
       "   0.2615415044128895,\n",
       "   0.26015408895909786,\n",
       "   0.2624484486877918,\n",
       "   0.2632576432079077,\n",
       "   0.2655563931912184,\n",
       "   0.26296382397413254,\n",
       "   0.2642639111727476,\n",
       "   0.26550223119556904,\n",
       "   0.2616691868752241,\n",
       "   0.2608777265995741,\n",
       "   0.2547271139919758,\n",
       "   0.25039470568299294,\n",
       "   0.2455609105527401,\n",
       "   0.24879602901637554,\n",
       "   0.24988876283168793,\n",
       "   0.24968543648719788,\n",
       "   0.24876881018280983,\n",
       "   0.25229443423449993,\n",
       "   0.2521510608494282,\n",
       "   0.25285438634455204,\n",
       "   0.2550997659564018,\n",
       "   0.25058694183826447,\n",
       "   0.2563220504671335,\n",
       "   0.25736638344824314,\n",
       "   0.2563316747546196,\n",
       "   0.2564059291034937,\n",
       "   0.26026623509824276,\n",
       "   0.26281957514584064,\n",
       "   0.26340971887111664,\n",
       "   0.26434868201613426,\n",
       "   0.26918933168053627,\n",
       "   0.26992986910045147,\n",
       "   0.27307906933128834,\n",
       "   0.2735010217875242,\n",
       "   0.2727831248193979,\n",
       "   0.2762267030775547,\n",
       "   0.2752454224973917,\n",
       "   0.27329800464212894,\n",
       "   0.26932712085545063,\n",
       "   0.26866171322762966,\n",
       "   0.2678254432976246,\n",
       "   0.2656958997249603,\n",
       "   0.26610791869461536,\n",
       "   0.26295485720038414,\n",
       "   0.263036185875535,\n",
       "   0.26360977813601494,\n",
       "   0.26350192725658417,\n",
       "   0.25779398158192635,\n",
       "   0.2575421016663313,\n",
       "   0.2566888704895973,\n",
       "   0.2553813196718693,\n",
       "   0.257133511826396,\n",
       "   0.25914712995290756,\n",
       "   0.2562937531620264,\n",
       "   0.25628640316426754,\n",
       "   0.25186663307249546,\n",
       "   0.24687469005584717,\n",
       "   0.24622696824371815,\n",
       "   0.24456194229424,\n",
       "   0.24311785958707333,\n",
       "   0.24432512186467648,\n",
       "   0.24010178446769714,\n",
       "   0.2414021361619234,\n",
       "   0.24120244197547436,\n",
       "   0.2449895702302456,\n",
       "   0.24517184495925903,\n",
       "   0.24220822006464005,\n",
       "   0.24091325514018536,\n",
       "   0.2450319491326809,\n",
       "   0.24306579679250717,\n",
       "   0.2418757937848568,\n",
       "   0.23813160881400108,\n",
       "   0.23862162046134472,\n",
       "   0.23726120218634605,\n",
       "   0.23682859167456627,\n",
       "   0.2372544538229704,\n",
       "   0.23878711834549904,\n",
       "   0.24586164392530918,\n",
       "   0.24985728599131107,\n",
       "   0.25128624215722084,\n",
       "   0.25554073601961136,\n",
       "   0.25836287811398506,\n",
       "   0.2589122783392668,\n",
       "   0.25813272409141064,\n",
       "   0.25505562871694565,\n",
       "   0.25810680352151394,\n",
       "   0.2582810427993536,\n",
       "   0.2603261321783066,\n",
       "   0.2601169999688864,\n",
       "   0.2648451589047909,\n",
       "   0.2695257291197777,\n",
       "   0.2668149694800377,\n",
       "   0.2659956756979227,\n",
       "   0.26748482696712017,\n",
       "   0.2649970855563879,\n",
       "   0.2653570640832186,\n",
       "   0.2602130100131035,\n",
       "   0.25776059553027153,\n",
       "   0.2532389238476753,\n",
       "   0.2519748918712139,\n",
       "   0.2525551598519087,\n",
       "   0.24652640707790852,\n",
       "   0.24279760755598545,\n",
       "   0.2416149117052555,\n",
       "   0.2408900260925293,\n",
       "   0.23738387040793896,\n",
       "   0.2378287874162197,\n",
       "   0.23947145231068134,\n",
       "   0.23893854580819607,\n",
       "   0.23858281504362822,\n",
       "   0.23586795199662447,\n",
       "   0.23694740422070026,\n",
       "   0.23652990348637104,\n",
       "   0.23474144283682108,\n",
       "   0.2347696041688323,\n",
       "   0.23096683342009783,\n",
       "   0.22639535181224346,\n",
       "   0.23140446934849024,\n",
       "   0.2303804336115718,\n",
       "   0.22540271002799273,\n",
       "   0.22562317550182343,\n",
       "   0.22820777911692858,\n",
       "   0.2271234793588519,\n",
       "   0.22499821800738573,\n",
       "   0.22786062117666006,\n",
       "   0.2282825941219926,\n",
       "   0.22879629535600543,\n",
       "   0.22620648983865976,\n",
       "   0.22520649246871471,\n",
       "   0.22295939084142447,\n",
       "   0.22346782917156816,\n",
       "   0.2247387943789363,\n",
       "   0.2239593593403697,\n",
       "   0.2208399474620819,\n",
       "   0.21907303342595696,\n",
       "   0.21433153189718723,\n",
       "   0.21016141027212143,\n",
       "   0.21284699952229857,\n",
       "   0.21291702147573233,\n",
       "   0.21398807363584638,\n",
       "   0.20929200667887926,\n",
       "   0.21269609406590462,\n",
       "   0.21224604127928615,\n",
       "   0.2095553446561098,\n",
       "   0.20839017536491156,\n",
       "   0.20650650141760707,\n",
       "   0.20717504667118192,\n",
       "   0.2062315554358065,\n",
       "   0.20404869224876165,\n",
       "   0.20592462876811624,\n",
       "   0.20859732385724783,\n",
       "   0.20877698063850403,\n",
       "   0.20719368290156126,\n",
       "   0.20520896138623357,\n",
       "   0.20712431613355875,\n",
       "   0.204594268463552,\n",
       "   0.20171359460800886,\n",
       "   0.19912681775167584,\n",
       "   0.20246227644383907,\n",
       "   0.20330168772488832,\n",
       "   0.20094616478309035,\n",
       "   0.19575154781341553,\n",
       "   0.19302040385082364,\n",
       "   0.19129925267770886,\n",
       "   0.19036004273220897,\n",
       "   0.18638925114646554,\n",
       "   0.18483421951532364,\n",
       "   0.18177688820287585,\n",
       "   0.18314387556165457,\n",
       "   0.18212989578023553,\n",
       "   0.18401705147698522,\n",
       "   0.18359748693183064,\n",
       "   0.1865275171585381,\n",
       "   0.18318921281024814,\n",
       "   0.18128429586067796,\n",
       "   0.1791237867437303,\n",
       "   0.17619139747694135,\n",
       "   0.1774995531886816,\n",
       "   0.17312402371317148,\n",
       "   0.17088267672806978,\n",
       "   0.16883931495249271,\n",
       "   0.16818475490435958,\n",
       "   0.16737721394747496,\n",
       "   0.16692491294816136,\n",
       "   0.16610900545492768,\n",
       "   0.16418353375047445,\n",
       "   0.16383689455688,\n",
       "   0.16338596865534782,\n",
       "   0.16505488567054272,\n",
       "   0.16540008643642068,\n",
       "   0.16054036701098084,\n",
       "   0.15836486220359802,\n",
       "   0.15665015252307057,\n",
       "   0.15485972398892045,\n",
       "   0.15582046937197447,\n",
       "   0.15567133529111743,\n",
       "   0.15156024508178234,\n",
       "   0.14799187099561095,\n",
       "   0.14804658060893416,\n",
       "   0.14587918855249882,\n",
       "   0.14916743291541934,\n",
       "   0.14540677098557353,\n",
       "   0.14545508241280913,\n",
       "   0.146896846126765,\n",
       "   0.1440324205905199,\n",
       "   0.1454977924004197,\n",
       "   0.14705345453694463,\n",
       "   0.14639290748164058,\n",
       "   0.145555617287755,\n",
       "   0.14645054005086422,\n",
       "   0.15114427264779806,\n",
       "   0.15337544307112694,\n",
       "   0.15515696164220572,\n",
       "   0.15995611250400543,\n",
       "   0.1645605517551303,\n",
       "   0.16879747807979584,\n",
       "   0.17261487804353237,\n",
       "   0.17126584332436323,\n",
       "   0.1706223264336586,\n",
       "   0.1737413965165615,\n",
       "   0.1753126783296466,\n",
       "   0.17620746418833733,\n",
       "   0.17626244854182005,\n",
       "   0.1751208994537592,\n",
       "   0.17310787551105022,\n",
       "   0.17099611926823854,\n",
       "   0.16907137958332896,\n",
       "   0.16657677618786693,\n",
       "   0.16424369253218174,\n",
       "   0.1628426592797041,\n",
       "   0.15888423472642899,\n",
       "   0.15780343487858772,\n",
       "   0.15842442214488983,\n",
       "   0.15235237684100866,\n",
       "   0.15120912762358785,\n",
       "   0.15042384155094624,\n",
       "   0.1486284015700221,\n",
       "   0.14623460918664932,\n",
       "   0.14373131655156612,\n",
       "   0.14184803795069456,\n",
       "   0.1418359773233533,\n",
       "   0.14045212604105473,\n",
       "   0.13877912238240242,\n",
       "   0.1399474898353219,\n",
       "   0.14461250137537718,\n",
       "   0.14363799523562193,\n",
       "   0.14546623919159174,\n",
       "   0.14710077550262213,\n",
       "   0.14578280970454216,\n",
       "   0.1490452978760004,\n",
       "   0.1492662513628602,\n",
       "   0.15383038111031055,\n",
       "   0.156902227550745,\n",
       "   0.15781900007277727,\n",
       "   0.15946956258267164,\n",
       "   0.15838654711842537,\n",
       "   0.16168824210762978,\n",
       "   0.16334567591547966,\n",
       "   0.1641552932560444,\n",
       "   0.16322354273870587,\n",
       "   0.16378535190597177,\n",
       "   0.16736739221960306,\n",
       "   0.16721099242568016,\n",
       "   0.16774256573989987,\n",
       "   0.16960585303604603,\n",
       "   0.16930339112877846,\n",
       "   0.1671738689765334,\n",
       "   0.1655843798071146,\n",
       "   0.16978477034717798,\n",
       "   0.16825246717780828,\n",
       "   0.16816564183682203,\n",
       "   0.16857891343533993,\n",
       "   0.1715239342302084,\n",
       "   0.17127822432667017,\n",
       "   0.17355499137192965,\n",
       "   0.17165484093129635,\n",
       "   0.16885861847549677,\n",
       "   0.16672900971025229,\n",
       "   0.16518385615199804,\n",
       "   0.16387206502258778,\n",
       "   0.16019277460873127,\n",
       "   0.15793095203116536,\n",
       "   0.1537315151654184,\n",
       "   0.15219425689429045,\n",
       "   0.1495237392373383,\n",
       "   0.14868675312027335,\n",
       "   0.14693362405523658,\n",
       "   0.14635098772123456,\n",
       "   0.1454676343128085,\n",
       "   0.14730367390438914,\n",
       "   0.15002164850011468,\n",
       "   0.15237420983612537,\n",
       "   0.1555541604757309,\n",
       "   0.15387967601418495,\n",
       "   0.15368277207016945,\n",
       "   0.1553121479228139,\n",
       "   0.15518979635089636,\n",
       "   0.15307671949267387,\n",
       "   0.15003360528498888,\n",
       "   0.1490145274437964,\n",
       "   0.14846774796023965,\n",
       "   0.14928878657519817,\n",
       "   0.149217136669904,\n",
       "   0.14960357872769237,\n",
       "   0.14950970700010657,\n",
       "   0.1489449255168438,\n",
       "   0.1483974428847432,\n",
       "   0.14602248184382915,\n",
       "   0.14650920033454895,\n",
       "   0.14275237079709768,\n",
       "   0.14290499221533537,\n",
       "   0.14083370007574558,\n",
       "   0.14131280686706305,\n",
       "   0.14037627167999744,\n",
       "   0.1393818100914359,\n",
       "   0.14048356609418988,\n",
       "   0.1394467270001769,\n",
       "   0.13876431016251445,\n",
       "   0.13981736451387405,\n",
       "   0.14036388602107763,\n",
       "   0.14113384392112494,\n",
       "   0.13899648003280163,\n",
       "   0.13968457374721766,\n",
       "   0.14189414400607347,\n",
       "   0.14040991198271513,\n",
       "   0.14007231313735247,\n",
       "   0.1442845007404685,\n",
       "   0.1472092578187585,\n",
       "   0.14875933341681957,\n",
       "   0.14930394105613232,\n",
       "   0.1515441769734025,\n",
       "   0.15355681627988815],\n",
       "  'pce_acc': [127.1121597290039,\n",
       "   102.18635559082031,\n",
       "   77.94038391113281,\n",
       "   60.61510467529297,\n",
       "   44.509071350097656,\n",
       "   32.88905715942383,\n",
       "   27.489990234375,\n",
       "   22.468032836914062,\n",
       "   20.017608642578125,\n",
       "   18.381956100463867,\n",
       "   15.764670372009277,\n",
       "   15.098682403564453,\n",
       "   13.091082572937012,\n",
       "   11.165849685668945,\n",
       "   10.488106727600098,\n",
       "   10.032812118530273,\n",
       "   9.530566215515137,\n",
       "   10.737363815307617,\n",
       "   13.170930862426758,\n",
       "   15.347578048706055,\n",
       "   17.95792007446289,\n",
       "   20.498865127563477,\n",
       "   22.094507217407227,\n",
       "   23.247617721557617,\n",
       "   24.907211303710938,\n",
       "   25.06068992614746,\n",
       "   25.899919509887695,\n",
       "   27.22041893005371,\n",
       "   28.01336097717285,\n",
       "   28.39691925048828,\n",
       "   29.66867446899414,\n",
       "   30.08705711364746,\n",
       "   30.067583084106445,\n",
       "   30.47325897216797,\n",
       "   30.356210708618164,\n",
       "   29.960281372070312,\n",
       "   30.806028366088867,\n",
       "   30.59473991394043,\n",
       "   29.868988037109375,\n",
       "   29.780799865722656,\n",
       "   30.356403350830078,\n",
       "   30.393505096435547,\n",
       "   30.19438934326172,\n",
       "   29.872026443481445,\n",
       "   30.49496078491211,\n",
       "   31.117231369018555,\n",
       "   30.931568145751953,\n",
       "   30.542922973632812,\n",
       "   30.528297424316406,\n",
       "   30.056425094604492,\n",
       "   29.73317527770996,\n",
       "   28.529199600219727,\n",
       "   28.08282470703125,\n",
       "   27.84653091430664,\n",
       "   26.78028678894043,\n",
       "   27.133195877075195,\n",
       "   26.460002899169922,\n",
       "   25.79616355895996,\n",
       "   26.053836822509766,\n",
       "   25.336456298828125,\n",
       "   24.394941329956055,\n",
       "   23.670269012451172,\n",
       "   22.712352752685547,\n",
       "   21.66353988647461,\n",
       "   20.439239501953125,\n",
       "   20.150747299194336,\n",
       "   19.532602310180664,\n",
       "   19.05748176574707,\n",
       "   18.70610809326172,\n",
       "   18.683307647705078,\n",
       "   18.500272750854492,\n",
       "   18.996065139770508,\n",
       "   18.723556518554688,\n",
       "   18.134212493896484,\n",
       "   18.355649948120117,\n",
       "   18.06761932373047,\n",
       "   18.537784576416016,\n",
       "   18.845884323120117,\n",
       "   18.71400260925293,\n",
       "   17.892248153686523,\n",
       "   17.948129653930664,\n",
       "   18.029911041259766,\n",
       "   17.689857482910156,\n",
       "   17.345712661743164,\n",
       "   17.36809730529785,\n",
       "   16.84288787841797,\n",
       "   16.762210845947266,\n",
       "   16.285629272460938,\n",
       "   16.5654239654541,\n",
       "   16.11185646057129,\n",
       "   15.61909008026123,\n",
       "   15.47510051727295,\n",
       "   15.346256256103516,\n",
       "   15.162168502807617,\n",
       "   15.45279598236084,\n",
       "   15.277987480163574,\n",
       "   15.084226608276367,\n",
       "   14.866325378417969,\n",
       "   14.786759376525879,\n",
       "   14.734437942504883,\n",
       "   14.615752220153809,\n",
       "   14.95383358001709,\n",
       "   15.420858383178711,\n",
       "   15.955984115600586,\n",
       "   16.177976608276367,\n",
       "   16.043529510498047,\n",
       "   16.09172821044922,\n",
       "   15.508416175842285,\n",
       "   15.663619041442871,\n",
       "   15.720708847045898,\n",
       "   15.712091445922852,\n",
       "   15.570127487182617,\n",
       "   15.261533737182617,\n",
       "   15.5092191696167,\n",
       "   15.474751472473145,\n",
       "   15.435708999633789,\n",
       "   15.077011108398438,\n",
       "   14.523542404174805,\n",
       "   14.333091735839844,\n",
       "   15.282308578491211,\n",
       "   15.096796035766602,\n",
       "   15.563072204589844,\n",
       "   15.810516357421875,\n",
       "   15.61886978149414,\n",
       "   15.153135299682617,\n",
       "   14.786214828491211,\n",
       "   15.06336784362793,\n",
       "   15.19520092010498,\n",
       "   15.055088996887207,\n",
       "   15.08139419555664,\n",
       "   14.78271484375,\n",
       "   14.885656356811523,\n",
       "   14.113758087158203,\n",
       "   14.056070327758789,\n",
       "   14.10177993774414,\n",
       "   13.997490882873535,\n",
       "   13.789664268493652,\n",
       "   13.875062942504883,\n",
       "   13.811163902282715,\n",
       "   13.866159439086914,\n",
       "   13.704630851745605,\n",
       "   13.547100067138672,\n",
       "   13.724109649658203,\n",
       "   13.58447265625,\n",
       "   13.25529670715332,\n",
       "   13.185986518859863,\n",
       "   13.207433700561523,\n",
       "   13.258931159973145,\n",
       "   13.557807922363281,\n",
       "   13.474093437194824,\n",
       "   13.258960723876953,\n",
       "   13.480429649353027,\n",
       "   13.391457557678223,\n",
       "   13.518482208251953,\n",
       "   13.881458282470703,\n",
       "   14.07929515838623,\n",
       "   14.479530334472656,\n",
       "   14.407003402709961,\n",
       "   14.812141418457031,\n",
       "   15.050021171569824,\n",
       "   15.180593490600586,\n",
       "   15.475809097290039,\n",
       "   15.571989059448242,\n",
       "   15.553081512451172,\n",
       "   16.1900691986084,\n",
       "   15.692262649536133,\n",
       "   15.615081787109375,\n",
       "   15.50763988494873,\n",
       "   15.578564643859863,\n",
       "   15.402481079101562,\n",
       "   15.67265510559082,\n",
       "   15.640239715576172,\n",
       "   15.624444007873535,\n",
       "   15.637872695922852,\n",
       "   15.562847137451172,\n",
       "   15.717487335205078,\n",
       "   15.49236011505127,\n",
       "   15.336000442504883,\n",
       "   15.27723217010498,\n",
       "   15.348349571228027,\n",
       "   15.366019248962402,\n",
       "   15.100127220153809,\n",
       "   15.10821533203125,\n",
       "   14.90896224975586,\n",
       "   14.668642044067383,\n",
       "   14.592795372009277,\n",
       "   14.326292037963867,\n",
       "   14.49554443359375,\n",
       "   14.075541496276855,\n",
       "   14.215019226074219,\n",
       "   14.29613208770752,\n",
       "   14.585373878479004,\n",
       "   14.43609619140625,\n",
       "   14.616351127624512,\n",
       "   14.698174476623535,\n",
       "   15.494624137878418,\n",
       "   15.958333015441895,\n",
       "   16.234233856201172,\n",
       "   16.88666343688965,\n",
       "   16.726776123046875,\n",
       "   16.41329002380371,\n",
       "   16.141267776489258,\n",
       "   16.084442138671875,\n",
       "   15.9609956741333,\n",
       "   15.776104927062988,\n",
       "   15.597688674926758,\n",
       "   15.27070426940918,\n",
       "   15.551201820373535,\n",
       "   15.955904006958008,\n",
       "   15.746909141540527,\n",
       "   15.730956077575684,\n",
       "   15.311071395874023,\n",
       "   15.547901153564453,\n",
       "   15.408863067626953,\n",
       "   15.488828659057617,\n",
       "   14.89719295501709,\n",
       "   14.777697563171387,\n",
       "   14.680843353271484,\n",
       "   14.478127479553223,\n",
       "   14.60727596282959,\n",
       "   14.267848014831543,\n",
       "   14.037055015563965,\n",
       "   14.03516674041748,\n",
       "   14.185254096984863,\n",
       "   14.075800895690918,\n",
       "   13.979358673095703,\n",
       "   13.92521858215332,\n",
       "   13.936688423156738,\n",
       "   13.857931137084961,\n",
       "   13.799110412597656,\n",
       "   13.770708084106445,\n",
       "   13.852134704589844,\n",
       "   13.846235275268555,\n",
       "   14.011770248413086,\n",
       "   14.200173377990723,\n",
       "   14.342442512512207,\n",
       "   14.769315719604492,\n",
       "   15.10743522644043,\n",
       "   15.175925254821777,\n",
       "   15.11064624786377,\n",
       "   15.247993469238281,\n",
       "   15.424245834350586,\n",
       "   15.376472473144531,\n",
       "   15.151877403259277,\n",
       "   15.066729545593262,\n",
       "   15.400193214416504,\n",
       "   15.271395683288574,\n",
       "   15.334580421447754,\n",
       "   15.096518516540527,\n",
       "   14.977767944335938,\n",
       "   14.721292495727539,\n",
       "   14.678878784179688,\n",
       "   14.497298240661621,\n",
       "   14.445082664489746,\n",
       "   14.267927169799805,\n",
       "   14.31995964050293,\n",
       "   14.390082359313965,\n",
       "   14.612971305847168,\n",
       "   14.690637588500977,\n",
       "   14.75536060333252,\n",
       "   15.277502059936523,\n",
       "   15.447150230407715,\n",
       "   15.425094604492188,\n",
       "   15.81299114227295,\n",
       "   15.866454124450684,\n",
       "   15.848599433898926,\n",
       "   15.922194480895996,\n",
       "   16.247901916503906,\n",
       "   16.11659049987793,\n",
       "   16.692914962768555,\n",
       "   16.816877365112305,\n",
       "   16.684335708618164,\n",
       "   16.798091888427734,\n",
       "   16.448644638061523,\n",
       "   16.284523010253906,\n",
       "   16.105430603027344,\n",
       "   15.832204818725586,\n",
       "   15.509269714355469,\n",
       "   15.549098014831543,\n",
       "   15.754429817199707,\n",
       "   15.752431869506836,\n",
       "   16.05774688720703,\n",
       "   15.800990104675293,\n",
       "   15.477866172790527,\n",
       "   15.003718376159668,\n",
       "   15.133033752441406,\n",
       "   14.870579719543457,\n",
       "   14.799861907958984,\n",
       "   14.77657699584961,\n",
       "   15.01579475402832,\n",
       "   15.116299629211426,\n",
       "   14.868614196777344,\n",
       "   14.669597625732422,\n",
       "   14.673185348510742,\n",
       "   14.574305534362793,\n",
       "   14.653593063354492,\n",
       "   14.820796012878418,\n",
       "   14.831884384155273,\n",
       "   14.77086353302002,\n",
       "   15.030041694641113,\n",
       "   14.921696662902832,\n",
       "   14.991447448730469,\n",
       "   14.987934112548828,\n",
       "   14.936921119689941,\n",
       "   14.72342586517334,\n",
       "   14.685994148254395,\n",
       "   15.178123474121094,\n",
       "   15.493029594421387,\n",
       "   15.487930297851562,\n",
       "   15.948101997375488,\n",
       "   16.019763946533203,\n",
       "   16.271692276000977,\n",
       "   16.442441940307617,\n",
       "   16.23164939880371,\n",
       "   16.040485382080078,\n",
       "   16.119972229003906,\n",
       "   15.898811340332031,\n",
       "   16.548410415649414,\n",
       "   16.493457794189453,\n",
       "   16.265602111816406,\n",
       "   16.185867309570312,\n",
       "   16.107725143432617,\n",
       "   16.139854431152344,\n",
       "   15.977437973022461,\n",
       "   15.999448776245117,\n",
       "   15.750198364257812,\n",
       "   15.470211029052734,\n",
       "   15.869039535522461,\n",
       "   15.810614585876465,\n",
       "   15.568197250366211,\n",
       "   15.532546043395996,\n",
       "   15.643302917480469,\n",
       "   15.67223834991455,\n",
       "   15.492631912231445,\n",
       "   15.161914825439453,\n",
       "   15.071521759033203,\n",
       "   14.971782684326172,\n",
       "   14.669879913330078,\n",
       "   14.79034423828125,\n",
       "   14.77768611907959,\n",
       "   14.861066818237305,\n",
       "   14.663702011108398,\n",
       "   14.591938018798828,\n",
       "   14.542616844177246,\n",
       "   14.48111343383789,\n",
       "   14.361828804016113,\n",
       "   14.366909980773926,\n",
       "   14.298160552978516,\n",
       "   14.190814971923828,\n",
       "   14.342559814453125,\n",
       "   14.218833923339844,\n",
       "   14.281441688537598,\n",
       "   14.31309986114502,\n",
       "   14.333154678344727,\n",
       "   14.433831214904785,\n",
       "   14.558638572692871,\n",
       "   14.576188087463379,\n",
       "   14.650080680847168,\n",
       "   14.691573143005371,\n",
       "   14.821098327636719,\n",
       "   14.751726150512695,\n",
       "   14.750665664672852,\n",
       "   14.642707824707031,\n",
       "   14.735014915466309,\n",
       "   14.632908821105957,\n",
       "   14.6620512008667,\n",
       "   14.81785774230957,\n",
       "   14.878374099731445,\n",
       "   15.295083999633789,\n",
       "   14.882548332214355,\n",
       "   14.952251434326172,\n",
       "   14.890569686889648,\n",
       "   14.63490104675293,\n",
       "   14.912017822265625,\n",
       "   15.064075469970703,\n",
       "   14.822988510131836,\n",
       "   14.728005409240723,\n",
       "   14.740406036376953,\n",
       "   14.753894805908203,\n",
       "   14.713342666625977,\n",
       "   14.73552131652832,\n",
       "   14.850886344909668,\n",
       "   14.727614402770996,\n",
       "   14.684393882751465,\n",
       "   14.683967590332031,\n",
       "   14.61952018737793,\n",
       "   14.710134506225586,\n",
       "   14.566797256469727,\n",
       "   14.473106384277344,\n",
       "   14.344152450561523,\n",
       "   14.337666511535645,\n",
       "   14.328728675842285,\n",
       "   14.341062545776367,\n",
       "   14.310012817382812,\n",
       "   14.318031311035156,\n",
       "   14.466227531433105,\n",
       "   14.296703338623047,\n",
       "   14.383252143859863,\n",
       "   14.301122665405273,\n",
       "   14.228741645812988,\n",
       "   14.170254707336426,\n",
       "   13.942755699157715,\n",
       "   14.180353164672852,\n",
       "   14.231649398803711,\n",
       "   14.348363876342773,\n",
       "   14.35983943939209,\n",
       "   14.436623573303223,\n",
       "   14.320405960083008,\n",
       "   14.275492668151855,\n",
       "   13.971558570861816,\n",
       "   13.834322929382324,\n",
       "   13.767127990722656,\n",
       "   13.81408405303955,\n",
       "   13.676433563232422,\n",
       "   13.517875671386719,\n",
       "   13.784642219543457,\n",
       "   13.599605560302734,\n",
       "   13.559744834899902,\n",
       "   13.70816707611084,\n",
       "   13.71600341796875,\n",
       "   13.634696960449219,\n",
       "   13.528621673583984,\n",
       "   13.346221923828125,\n",
       "   13.334650993347168,\n",
       "   13.584250450134277,\n",
       "   13.595756530761719,\n",
       "   13.649540901184082,\n",
       "   13.645861625671387,\n",
       "   13.914111137390137,\n",
       "   14.312161445617676,\n",
       "   14.377617835998535,\n",
       "   14.403984069824219,\n",
       "   14.344996452331543,\n",
       "   14.336729049682617,\n",
       "   14.434021949768066,\n",
       "   14.379731178283691,\n",
       "   14.384690284729004,\n",
       "   14.471733093261719,\n",
       "   14.209407806396484,\n",
       "   14.019712448120117,\n",
       "   14.209464073181152,\n",
       "   14.288267135620117,\n",
       "   13.985191345214844,\n",
       "   13.901564598083496,\n",
       "   13.610029220581055,\n",
       "   13.52823257446289,\n",
       "   13.511821746826172,\n",
       "   13.661348342895508,\n",
       "   13.883140563964844,\n",
       "   13.97716236114502,\n",
       "   14.04880428314209,\n",
       "   14.011308670043945,\n",
       "   13.808690071105957,\n",
       "   13.940234184265137,\n",
       "   13.801529884338379,\n",
       "   13.857420921325684,\n",
       "   14.113533020019531,\n",
       "   14.36489486694336,\n",
       "   14.656609535217285,\n",
       "   14.78915786743164,\n",
       "   14.912388801574707,\n",
       "   14.790977478027344,\n",
       "   14.67434310913086,\n",
       "   15.088123321533203,\n",
       "   14.936396598815918,\n",
       "   14.813261985778809,\n",
       "   14.627710342407227,\n",
       "   14.719822883605957,\n",
       "   14.70467758178711,\n",
       "   14.76315689086914,\n",
       "   14.763458251953125,\n",
       "   14.441893577575684,\n",
       "   14.312073707580566,\n",
       "   14.415346145629883,\n",
       "   14.297582626342773,\n",
       "   14.235183715820312,\n",
       "   14.211013793945312,\n",
       "   14.24836540222168,\n",
       "   14.216012001037598,\n",
       "   14.564238548278809,\n",
       "   14.410013198852539,\n",
       "   14.414776802062988,\n",
       "   14.547734260559082,\n",
       "   14.875923156738281,\n",
       "   14.708605766296387,\n",
       "   14.568130493164062,\n",
       "   14.632536888122559,\n",
       "   14.655271530151367,\n",
       "   14.689144134521484,\n",
       "   14.45798110961914,\n",
       "   14.270586967468262,\n",
       "   14.173182487487793,\n",
       "   14.394536972045898,\n",
       "   14.235885620117188,\n",
       "   14.626993179321289,\n",
       "   14.782645225524902,\n",
       "   14.732909202575684,\n",
       "   14.97374439239502,\n",
       "   15.210805892944336,\n",
       "   15.431051254272461],\n",
       "  'voc_acc': [72.3319320678711,\n",
       "   61.13083267211914,\n",
       "   47.14910888671875,\n",
       "   42.889862060546875,\n",
       "   34.905338287353516,\n",
       "   27.83539581298828,\n",
       "   21.785245895385742,\n",
       "   15.93502426147461,\n",
       "   11.72909164428711,\n",
       "   9.399761199951172,\n",
       "   7.643867492675781,\n",
       "   6.635149002075195,\n",
       "   7.009552955627441,\n",
       "   7.2360405921936035,\n",
       "   7.6228156089782715,\n",
       "   8.181378364562988,\n",
       "   8.375052452087402,\n",
       "   8.950498580932617,\n",
       "   8.565731048583984,\n",
       "   8.345027923583984,\n",
       "   7.889702320098877,\n",
       "   7.893653392791748,\n",
       "   8.027766227722168,\n",
       "   8.327727317810059,\n",
       "   8.496217727661133,\n",
       "   8.820792198181152,\n",
       "   9.32752513885498,\n",
       "   10.071736335754395,\n",
       "   10.657488822937012,\n",
       "   11.236381530761719,\n",
       "   12.69101333618164,\n",
       "   14.46055793762207,\n",
       "   14.542052268981934,\n",
       "   15.300719261169434,\n",
       "   15.636316299438477,\n",
       "   16.037086486816406,\n",
       "   16.591968536376953,\n",
       "   17.004680633544922,\n",
       "   16.719337463378906,\n",
       "   17.254560470581055,\n",
       "   18.833858489990234,\n",
       "   19.224756240844727,\n",
       "   20.239652633666992,\n",
       "   20.4414005279541,\n",
       "   21.75297737121582,\n",
       "   22.13075828552246,\n",
       "   22.763269424438477,\n",
       "   23.193498611450195,\n",
       "   23.720924377441406,\n",
       "   24.185943603515625,\n",
       "   24.8236083984375,\n",
       "   24.926799774169922,\n",
       "   25.240690231323242,\n",
       "   26.453340530395508,\n",
       "   26.770517349243164,\n",
       "   27.15452003479004,\n",
       "   27.288171768188477,\n",
       "   27.882829666137695,\n",
       "   27.159908294677734,\n",
       "   28.006237030029297,\n",
       "   29.276365280151367,\n",
       "   30.043092727661133,\n",
       "   30.424251556396484,\n",
       "   30.46118927001953,\n",
       "   31.171688079833984,\n",
       "   31.92291831970215,\n",
       "   31.94422721862793,\n",
       "   31.743562698364258,\n",
       "   32.002906799316406,\n",
       "   32.63188934326172,\n",
       "   32.9034538269043,\n",
       "   32.09564971923828,\n",
       "   32.062496185302734,\n",
       "   33.16080856323242,\n",
       "   32.840084075927734,\n",
       "   33.2225341796875,\n",
       "   32.7690544128418,\n",
       "   33.20995330810547,\n",
       "   33.7757682800293,\n",
       "   34.024906158447266,\n",
       "   34.51815414428711,\n",
       "   34.8315315246582,\n",
       "   35.46471405029297,\n",
       "   35.40945816040039,\n",
       "   35.75423812866211,\n",
       "   35.80799102783203,\n",
       "   35.453582763671875,\n",
       "   35.860599517822266,\n",
       "   36.15247344970703,\n",
       "   36.73859786987305,\n",
       "   36.862056732177734,\n",
       "   37.44514465332031,\n",
       "   37.28874588012695,\n",
       "   37.63282775878906,\n",
       "   38.85504913330078,\n",
       "   38.41102981567383,\n",
       "   37.978965759277344,\n",
       "   38.069175720214844,\n",
       "   38.84080123901367,\n",
       "   39.40793991088867,\n",
       "   39.2597541809082,\n",
       "   39.070701599121094,\n",
       "   39.12017822265625,\n",
       "   39.05577087402344,\n",
       "   39.27176284790039,\n",
       "   38.812076568603516,\n",
       "   38.819801330566406,\n",
       "   39.06071472167969,\n",
       "   39.36820983886719,\n",
       "   39.1804084777832,\n",
       "   38.54286193847656,\n",
       "   38.73714828491211,\n",
       "   39.0874137878418,\n",
       "   39.413780212402344,\n",
       "   39.134796142578125,\n",
       "   39.400211334228516,\n",
       "   40.01156997680664,\n",
       "   40.18853759765625,\n",
       "   40.046634674072266,\n",
       "   40.033626556396484,\n",
       "   40.63326644897461,\n",
       "   41.361106872558594,\n",
       "   42.06226348876953,\n",
       "   41.8004035949707,\n",
       "   41.336421966552734,\n",
       "   40.57814025878906,\n",
       "   41.21675491333008,\n",
       "   41.0594596862793,\n",
       "   40.04217529296875,\n",
       "   39.913150787353516,\n",
       "   38.76933670043945,\n",
       "   38.30695343017578,\n",
       "   37.43173599243164,\n",
       "   37.6384391784668,\n",
       "   37.62062072753906,\n",
       "   38.57273864746094,\n",
       "   39.39012908935547,\n",
       "   39.057106018066406,\n",
       "   38.46809387207031,\n",
       "   38.67272186279297,\n",
       "   38.353302001953125,\n",
       "   38.18300247192383,\n",
       "   38.22616195678711,\n",
       "   37.82398223876953,\n",
       "   38.17529296875,\n",
       "   39.00508499145508,\n",
       "   38.04355239868164,\n",
       "   37.66851806640625,\n",
       "   38.30617904663086,\n",
       "   38.37022399902344,\n",
       "   38.2910270690918,\n",
       "   37.72552490234375,\n",
       "   38.11965560913086,\n",
       "   37.790748596191406,\n",
       "   38.10584259033203,\n",
       "   38.36200714111328,\n",
       "   38.67110824584961,\n",
       "   38.38272476196289,\n",
       "   38.398475646972656,\n",
       "   39.382266998291016,\n",
       "   38.713409423828125,\n",
       "   39.000244140625,\n",
       "   39.71887969970703,\n",
       "   40.1732063293457,\n",
       "   40.24528503417969,\n",
       "   39.99980545043945,\n",
       "   40.23889923095703,\n",
       "   40.51820755004883,\n",
       "   40.74033737182617,\n",
       "   41.10460662841797,\n",
       "   41.58065414428711,\n",
       "   41.70250701904297,\n",
       "   42.1154899597168,\n",
       "   41.711395263671875,\n",
       "   42.241432189941406,\n",
       "   42.55970001220703,\n",
       "   42.30496597290039,\n",
       "   42.56721496582031,\n",
       "   42.0113639831543,\n",
       "   41.59541702270508,\n",
       "   40.800811767578125,\n",
       "   41.50808334350586,\n",
       "   41.66633605957031,\n",
       "   41.71710968017578,\n",
       "   41.488956451416016,\n",
       "   41.67131423950195,\n",
       "   42.18388748168945,\n",
       "   42.17401885986328,\n",
       "   42.70076370239258,\n",
       "   42.327476501464844,\n",
       "   42.952999114990234,\n",
       "   42.73159408569336,\n",
       "   42.8947868347168,\n",
       "   43.05436325073242,\n",
       "   43.456581115722656,\n",
       "   43.36659240722656,\n",
       "   42.98069381713867,\n",
       "   42.97935104370117,\n",
       "   43.74083709716797,\n",
       "   43.749481201171875,\n",
       "   44.10527420043945,\n",
       "   44.360538482666016,\n",
       "   44.266422271728516,\n",
       "   44.25031661987305,\n",
       "   44.11720275878906,\n",
       "   44.26881408691406,\n",
       "   43.78805160522461,\n",
       "   43.976158142089844,\n",
       "   43.567787170410156,\n",
       "   42.73031234741211,\n",
       "   42.479740142822266,\n",
       "   41.984771728515625,\n",
       "   42.18694305419922,\n",
       "   42.15135955810547,\n",
       "   41.98448944091797,\n",
       "   41.5788459777832,\n",
       "   41.9707145690918,\n",
       "   41.93047332763672,\n",
       "   41.90602111816406,\n",
       "   41.94289016723633,\n",
       "   42.44585418701172,\n",
       "   42.78038024902344,\n",
       "   43.41938018798828,\n",
       "   43.47372055053711,\n",
       "   43.08418273925781,\n",
       "   44.02375030517578,\n",
       "   43.27158737182617,\n",
       "   42.977447509765625,\n",
       "   43.252899169921875,\n",
       "   43.040164947509766,\n",
       "   43.3119010925293,\n",
       "   43.35648727416992,\n",
       "   44.1083869934082,\n",
       "   44.011592864990234,\n",
       "   43.693355560302734,\n",
       "   43.43341064453125,\n",
       "   43.40096664428711,\n",
       "   43.19432830810547,\n",
       "   43.03012466430664,\n",
       "   42.453147888183594,\n",
       "   42.575443267822266,\n",
       "   42.42127990722656,\n",
       "   42.056331634521484,\n",
       "   41.861419677734375,\n",
       "   41.782676696777344,\n",
       "   42.27467346191406,\n",
       "   42.51874542236328,\n",
       "   42.81326675415039,\n",
       "   43.33487319946289,\n",
       "   43.769466400146484,\n",
       "   43.521602630615234,\n",
       "   43.35615158081055,\n",
       "   43.010765075683594,\n",
       "   43.346092224121094,\n",
       "   43.33100128173828,\n",
       "   43.55880355834961,\n",
       "   43.37482833862305,\n",
       "   43.85932540893555,\n",
       "   43.84071350097656,\n",
       "   43.615135192871094,\n",
       "   43.62125015258789,\n",
       "   43.62907791137695,\n",
       "   43.53321075439453,\n",
       "   43.8758659362793,\n",
       "   43.55706024169922,\n",
       "   43.20991516113281,\n",
       "   43.0262336730957,\n",
       "   42.61101531982422,\n",
       "   42.721214294433594,\n",
       "   41.906105041503906,\n",
       "   41.40895462036133,\n",
       "   41.36763381958008,\n",
       "   41.28374099731445,\n",
       "   41.10141372680664,\n",
       "   41.46704864501953,\n",
       "   42.00611877441406,\n",
       "   42.1032829284668,\n",
       "   42.122745513916016,\n",
       "   41.69659423828125,\n",
       "   41.9581184387207,\n",
       "   41.905086517333984,\n",
       "   41.887901306152344,\n",
       "   42.30467987060547,\n",
       "   41.60814666748047,\n",
       "   41.009159088134766,\n",
       "   41.72078323364258,\n",
       "   41.645938873291016,\n",
       "   41.240196228027344,\n",
       "   41.170902252197266,\n",
       "   41.58985900878906,\n",
       "   41.513710021972656,\n",
       "   41.64460754394531,\n",
       "   42.050907135009766,\n",
       "   41.70405960083008,\n",
       "   41.31254577636719,\n",
       "   40.868709564208984,\n",
       "   40.43132019042969,\n",
       "   40.312713623046875,\n",
       "   39.8681755065918,\n",
       "   40.08319854736328,\n",
       "   40.00419235229492,\n",
       "   40.110958099365234,\n",
       "   39.91920471191406,\n",
       "   39.52107620239258,\n",
       "   39.49449920654297,\n",
       "   40.09297561645508,\n",
       "   40.50236129760742,\n",
       "   39.91972351074219,\n",
       "   39.32114791870117,\n",
       "   39.88334655761719,\n",
       "   39.6123046875,\n",
       "   39.03108596801758,\n",
       "   38.94570541381836,\n",
       "   38.5474853515625,\n",
       "   38.45470428466797,\n",
       "   37.99781799316406,\n",
       "   37.90896224975586,\n",
       "   37.82021713256836,\n",
       "   38.198753356933594,\n",
       "   38.13642120361328,\n",
       "   37.653770446777344,\n",
       "   37.25619888305664,\n",
       "   37.5385627746582,\n",
       "   37.14200210571289,\n",
       "   36.64595031738281,\n",
       "   36.06522750854492,\n",
       "   36.697059631347656,\n",
       "   36.47306823730469,\n",
       "   36.208274841308594,\n",
       "   35.808414459228516,\n",
       "   35.419246673583984,\n",
       "   35.061466217041016,\n",
       "   34.68470764160156,\n",
       "   34.313201904296875,\n",
       "   33.88044357299805,\n",
       "   33.420745849609375,\n",
       "   33.84635925292969,\n",
       "   33.848148345947266,\n",
       "   33.26567840576172,\n",
       "   33.34177017211914,\n",
       "   33.97651290893555,\n",
       "   32.982913970947266,\n",
       "   32.98589324951172,\n",
       "   32.98164367675781,\n",
       "   32.779537200927734,\n",
       "   32.72785568237305,\n",
       "   32.44615173339844,\n",
       "   32.1988639831543,\n",
       "   32.209136962890625,\n",
       "   31.838788986206055,\n",
       "   31.599763870239258,\n",
       "   31.3531494140625,\n",
       "   31.177425384521484,\n",
       "   31.06839370727539,\n",
       "   30.642515182495117,\n",
       "   30.385892868041992,\n",
       "   30.76247215270996,\n",
       "   30.81130027770996,\n",
       "   30.564212799072266,\n",
       "   29.964614868164062,\n",
       "   29.593944549560547,\n",
       "   29.498144149780273,\n",
       "   29.673194885253906,\n",
       "   29.843341827392578,\n",
       "   29.62118911743164,\n",
       "   29.247310638427734,\n",
       "   29.402503967285156,\n",
       "   28.8699951171875,\n",
       "   29.2501277923584,\n",
       "   28.9700870513916,\n",
       "   28.885576248168945,\n",
       "   29.29762840270996,\n",
       "   29.58722496032715,\n",
       "   29.86817741394043,\n",
       "   30.267105102539062,\n",
       "   30.100622177124023,\n",
       "   29.702455520629883,\n",
       "   29.33335304260254,\n",
       "   29.951446533203125,\n",
       "   29.896894454956055,\n",
       "   29.84990692138672,\n",
       "   29.922670364379883,\n",
       "   29.714519500732422,\n",
       "   30.033523559570312,\n",
       "   30.409130096435547,\n",
       "   30.210208892822266,\n",
       "   30.0212345123291,\n",
       "   30.233169555664062,\n",
       "   30.52239990234375,\n",
       "   30.452547073364258,\n",
       "   30.684667587280273,\n",
       "   30.88435173034668,\n",
       "   31.02789306640625,\n",
       "   31.236461639404297,\n",
       "   31.240324020385742,\n",
       "   31.184022903442383,\n",
       "   30.990747451782227,\n",
       "   30.918874740600586,\n",
       "   30.966169357299805,\n",
       "   30.947378158569336,\n",
       "   31.20363426208496,\n",
       "   30.735857009887695,\n",
       "   30.495527267456055,\n",
       "   30.1845645904541,\n",
       "   30.30754280090332,\n",
       "   30.166744232177734,\n",
       "   30.085643768310547,\n",
       "   30.028175354003906,\n",
       "   30.23841094970703,\n",
       "   29.86248779296875,\n",
       "   29.409282684326172,\n",
       "   29.34772300720215,\n",
       "   29.832149505615234,\n",
       "   29.291719436645508,\n",
       "   29.05664825439453,\n",
       "   29.106870651245117,\n",
       "   28.99028968811035,\n",
       "   29.335100173950195,\n",
       "   29.02066993713379,\n",
       "   29.414966583251953,\n",
       "   29.658905029296875,\n",
       "   29.48160171508789,\n",
       "   29.507680892944336,\n",
       "   29.24848175048828,\n",
       "   29.445659637451172,\n",
       "   29.19912338256836,\n",
       "   29.390562057495117,\n",
       "   29.870588302612305,\n",
       "   29.982412338256836,\n",
       "   30.474788665771484,\n",
       "   30.621370315551758,\n",
       "   30.733642578125,\n",
       "   30.684968948364258,\n",
       "   30.676424026489258,\n",
       "   30.884605407714844,\n",
       "   30.944320678710938,\n",
       "   31.821151733398438,\n",
       "   31.09644317626953,\n",
       "   30.94162368774414,\n",
       "   31.00248908996582,\n",
       "   31.17925262451172,\n",
       "   31.28032112121582,\n",
       "   31.530132293701172,\n",
       "   31.513490676879883,\n",
       "   31.388011932373047,\n",
       "   31.399965286254883,\n",
       "   31.41585350036621,\n",
       "   31.710445404052734,\n",
       "   31.746482849121094,\n",
       "   31.350671768188477,\n",
       "   30.808713912963867,\n",
       "   30.628061294555664,\n",
       "   30.67694091796875,\n",
       "   30.29886245727539,\n",
       "   30.098575592041016,\n",
       "   29.93720817565918,\n",
       "   29.827503204345703,\n",
       "   30.362070083618164,\n",
       "   30.6966552734375,\n",
       "   30.673852920532227,\n",
       "   30.811277389526367,\n",
       "   30.6082820892334,\n",
       "   30.782833099365234,\n",
       "   30.47403335571289,\n",
       "   30.481287002563477,\n",
       "   30.346101760864258,\n",
       "   29.801544189453125,\n",
       "   29.70863914489746,\n",
       "   29.647790908813477,\n",
       "   29.620880126953125,\n",
       "   29.33424949645996,\n",
       "   29.39041519165039,\n",
       "   29.524898529052734,\n",
       "   29.486339569091797,\n",
       "   29.388566970825195,\n",
       "   29.009172439575195,\n",
       "   28.9544734954834,\n",
       "   28.368087768554688,\n",
       "   28.120800018310547,\n",
       "   27.938302993774414,\n",
       "   27.990949630737305,\n",
       "   27.996793746948242,\n",
       "   27.97265625,\n",
       "   27.762121200561523,\n",
       "   27.561914443969727,\n",
       "   27.35249137878418,\n",
       "   27.306047439575195,\n",
       "   27.458545684814453,\n",
       "   27.412912368774414,\n",
       "   26.534563064575195,\n",
       "   26.6534366607666,\n",
       "   27.025297164916992,\n",
       "   26.983060836791992,\n",
       "   26.89553451538086,\n",
       "   27.199352264404297,\n",
       "   27.317567825317383,\n",
       "   27.66234588623047,\n",
       "   27.584705352783203,\n",
       "   27.73223876953125,\n",
       "   27.808502197265625],\n",
       "  'jsc_acc': [505.74688720703125,\n",
       "   410.2769470214844,\n",
       "   334.8154296875,\n",
       "   282.6647644042969,\n",
       "   239.59329223632812,\n",
       "   208.3497772216797,\n",
       "   182.20921325683594,\n",
       "   161.4827880859375,\n",
       "   143.68423461914062,\n",
       "   128.0264434814453,\n",
       "   114.09635925292969,\n",
       "   104.10957336425781,\n",
       "   97.21588134765625,\n",
       "   91.4532699584961,\n",
       "   85.93413543701172,\n",
       "   81.86115264892578,\n",
       "   77.38118743896484,\n",
       "   73.92972564697266,\n",
       "   70.82585144042969,\n",
       "   67.41645812988281,\n",
       "   64.95864868164062,\n",
       "   63.329620361328125,\n",
       "   62.46107864379883,\n",
       "   61.78455352783203,\n",
       "   60.113704681396484,\n",
       "   58.79227066040039,\n",
       "   57.620121002197266,\n",
       "   55.37632751464844,\n",
       "   53.987796783447266,\n",
       "   52.64493179321289,\n",
       "   50.86522674560547,\n",
       "   49.62307357788086,\n",
       "   48.792667388916016,\n",
       "   48.10169219970703,\n",
       "   47.749725341796875,\n",
       "   47.02232360839844,\n",
       "   45.68484115600586,\n",
       "   44.221134185791016,\n",
       "   43.385536193847656,\n",
       "   42.915992736816406,\n",
       "   43.31308364868164,\n",
       "   43.43464279174805,\n",
       "   43.22203826904297,\n",
       "   42.97089385986328,\n",
       "   42.341190338134766,\n",
       "   42.16551208496094,\n",
       "   42.75749969482422,\n",
       "   42.83478927612305,\n",
       "   42.28388214111328,\n",
       "   42.054691314697266,\n",
       "   41.4333381652832,\n",
       "   41.200927734375,\n",
       "   39.589195251464844,\n",
       "   39.05567932128906,\n",
       "   37.80793380737305,\n",
       "   37.582061767578125,\n",
       "   36.88731002807617,\n",
       "   36.8159294128418,\n",
       "   36.90501022338867,\n",
       "   36.2039909362793,\n",
       "   36.7557258605957,\n",
       "   37.41233825683594,\n",
       "   37.78347396850586,\n",
       "   38.09625244140625,\n",
       "   38.340789794921875,\n",
       "   38.41981887817383,\n",
       "   39.10953140258789,\n",
       "   39.43350601196289,\n",
       "   39.63346862792969,\n",
       "   40.11004638671875,\n",
       "   40.81383514404297,\n",
       "   41.038814544677734,\n",
       "   40.26782989501953,\n",
       "   40.584354400634766,\n",
       "   38.87944030761719,\n",
       "   39.78288650512695,\n",
       "   39.90036392211914,\n",
       "   39.92375183105469,\n",
       "   40.020042419433594,\n",
       "   39.590118408203125,\n",
       "   39.088661193847656,\n",
       "   39.12521743774414,\n",
       "   39.16618347167969,\n",
       "   38.56988525390625,\n",
       "   38.72370910644531,\n",
       "   38.767486572265625,\n",
       "   38.39650344848633,\n",
       "   38.355648040771484,\n",
       "   38.20303726196289,\n",
       "   37.797515869140625,\n",
       "   37.483219146728516,\n",
       "   36.773250579833984,\n",
       "   37.12729263305664,\n",
       "   36.24764633178711,\n",
       "   36.071414947509766,\n",
       "   36.33993911743164,\n",
       "   35.95522689819336,\n",
       "   36.20120620727539,\n",
       "   35.6473274230957,\n",
       "   35.775123596191406,\n",
       "   35.674007415771484,\n",
       "   35.48725128173828,\n",
       "   35.345279693603516,\n",
       "   34.81739807128906,\n",
       "   34.53577423095703,\n",
       "   34.57991409301758,\n",
       "   34.22615051269531,\n",
       "   33.84286880493164,\n",
       "   33.3812370300293,\n",
       "   33.286312103271484,\n",
       "   33.82663345336914,\n",
       "   33.084774017333984,\n",
       "   32.8173942565918,\n",
       "   32.57743835449219,\n",
       "   32.2368278503418,\n",
       "   31.269994735717773,\n",
       "   31.256067276000977,\n",
       "   31.600666046142578,\n",
       "   31.233062744140625,\n",
       "   30.85563850402832,\n",
       "   30.84893798828125,\n",
       "   30.575817108154297,\n",
       "   31.138532638549805,\n",
       "   31.398954391479492,\n",
       "   31.810945510864258,\n",
       "   31.75412368774414,\n",
       "   31.823354721069336,\n",
       "   32.34990310668945,\n",
       "   31.768781661987305,\n",
       "   30.89400863647461,\n",
       "   30.643722534179688,\n",
       "   30.47240447998047,\n",
       "   29.991859436035156,\n",
       "   29.624393463134766,\n",
       "   29.444746017456055,\n",
       "   29.798316955566406,\n",
       "   29.518260955810547,\n",
       "   28.950410842895508,\n",
       "   29.681665420532227,\n",
       "   30.08636474609375,\n",
       "   29.529857635498047,\n",
       "   28.59263038635254,\n",
       "   28.43413734436035,\n",
       "   28.41838836669922,\n",
       "   28.410303115844727,\n",
       "   28.39293098449707,\n",
       "   28.29737091064453,\n",
       "   28.210111618041992,\n",
       "   28.497072219848633,\n",
       "   28.615528106689453,\n",
       "   28.941890716552734,\n",
       "   28.7141170501709,\n",
       "   28.249042510986328,\n",
       "   27.68310546875,\n",
       "   27.444988250732422,\n",
       "   26.877239227294922,\n",
       "   26.875782012939453,\n",
       "   26.087669372558594,\n",
       "   26.234071731567383,\n",
       "   25.671472549438477,\n",
       "   25.87161636352539,\n",
       "   25.4219913482666,\n",
       "   25.38614845275879,\n",
       "   25.241500854492188,\n",
       "   24.741371154785156,\n",
       "   24.566730499267578,\n",
       "   25.381916046142578,\n",
       "   25.089466094970703,\n",
       "   24.78792953491211,\n",
       "   24.125436782836914,\n",
       "   23.98065757751465,\n",
       "   23.106714248657227,\n",
       "   23.114452362060547,\n",
       "   22.765972137451172,\n",
       "   22.010845184326172,\n",
       "   21.62404441833496,\n",
       "   21.311038970947266,\n",
       "   21.484983444213867,\n",
       "   20.785293579101562,\n",
       "   20.106882095336914,\n",
       "   19.706981658935547,\n",
       "   19.963905334472656,\n",
       "   20.08146858215332,\n",
       "   20.528226852416992,\n",
       "   20.573196411132812,\n",
       "   20.76068115234375,\n",
       "   20.26082420349121,\n",
       "   20.011274337768555,\n",
       "   20.193450927734375,\n",
       "   19.682199478149414,\n",
       "   19.582082748413086,\n",
       "   19.91650390625,\n",
       "   19.848054885864258,\n",
       "   20.066585540771484,\n",
       "   20.066633224487305,\n",
       "   20.21595001220703,\n",
       "   20.3101749420166,\n",
       "   20.627614974975586,\n",
       "   20.00813865661621,\n",
       "   20.070262908935547,\n",
       "   20.230466842651367,\n",
       "   20.285642623901367,\n",
       "   20.29753875732422,\n",
       "   20.8072566986084,\n",
       "   21.229080200195312,\n",
       "   21.078323364257812,\n",
       "   20.85641860961914,\n",
       "   20.85892677307129,\n",
       "   20.800121307373047,\n",
       "   20.50859260559082,\n",
       "   20.59000587463379,\n",
       "   20.527856826782227,\n",
       "   19.78342628479004,\n",
       "   19.67133903503418,\n",
       "   19.955158233642578,\n",
       "   19.910438537597656,\n",
       "   19.985321044921875,\n",
       "   19.965473175048828,\n",
       "   19.99831199645996,\n",
       "   20.003963470458984,\n",
       "   19.72111701965332,\n",
       "   19.321542739868164,\n",
       "   18.63923454284668,\n",
       "   17.897790908813477,\n",
       "   17.721181869506836,\n",
       "   17.29059410095215,\n",
       "   17.36962127685547,\n",
       "   17.441373825073242,\n",
       "   17.354398727416992,\n",
       "   17.024845123291016,\n",
       "   16.39055824279785,\n",
       "   16.362628936767578,\n",
       "   16.308330535888672,\n",
       "   16.526519775390625,\n",
       "   16.01175308227539,\n",
       "   15.991847038269043,\n",
       "   16.145362854003906,\n",
       "   15.989786148071289,\n",
       "   15.935620307922363,\n",
       "   16.474708557128906,\n",
       "   16.712900161743164,\n",
       "   16.392581939697266,\n",
       "   16.313426971435547,\n",
       "   16.507362365722656,\n",
       "   16.64837074279785,\n",
       "   16.813961029052734,\n",
       "   17.017728805541992,\n",
       "   16.497337341308594,\n",
       "   16.628507614135742,\n",
       "   16.460535049438477,\n",
       "   16.296287536621094,\n",
       "   16.061447143554688,\n",
       "   15.359038352966309,\n",
       "   15.372209548950195,\n",
       "   15.351035118103027,\n",
       "   15.782716751098633,\n",
       "   15.751899719238281,\n",
       "   15.722025871276855,\n",
       "   16.085569381713867,\n",
       "   15.81298542022705,\n",
       "   15.678451538085938,\n",
       "   15.836423873901367,\n",
       "   15.267151832580566,\n",
       "   15.267461776733398,\n",
       "   15.215188026428223,\n",
       "   15.322540283203125,\n",
       "   15.160717010498047,\n",
       "   15.301214218139648,\n",
       "   15.407747268676758,\n",
       "   14.815308570861816,\n",
       "   14.293050765991211,\n",
       "   14.533263206481934,\n",
       "   14.282293319702148,\n",
       "   14.385299682617188,\n",
       "   13.836060523986816,\n",
       "   13.6072998046875,\n",
       "   13.453727722167969,\n",
       "   12.82351016998291,\n",
       "   12.544743537902832,\n",
       "   12.295502662658691,\n",
       "   12.260772705078125,\n",
       "   12.079828262329102,\n",
       "   12.12624454498291,\n",
       "   11.803313255310059,\n",
       "   11.512075424194336,\n",
       "   11.111712455749512,\n",
       "   10.730660438537598,\n",
       "   10.587197303771973,\n",
       "   10.924592971801758,\n",
       "   10.754161834716797,\n",
       "   10.667839050292969,\n",
       "   10.404006004333496,\n",
       "   10.317866325378418,\n",
       "   10.172734260559082,\n",
       "   10.15085506439209,\n",
       "   10.120144844055176,\n",
       "   9.98556137084961,\n",
       "   9.743896484375,\n",
       "   9.579318046569824,\n",
       "   9.609374046325684,\n",
       "   9.95121955871582,\n",
       "   9.933977127075195,\n",
       "   9.892412185668945,\n",
       "   9.720122337341309,\n",
       "   9.985783576965332,\n",
       "   9.945479393005371,\n",
       "   9.709492683410645,\n",
       "   9.687068939208984,\n",
       "   9.487588882446289,\n",
       "   9.468230247497559,\n",
       "   9.236042022705078,\n",
       "   9.228121757507324,\n",
       "   9.328043937683105,\n",
       "   9.321975708007812,\n",
       "   9.334000587463379,\n",
       "   9.64928913116455,\n",
       "   10.431185722351074,\n",
       "   10.30885124206543,\n",
       "   10.30977725982666,\n",
       "   10.32085132598877,\n",
       "   10.297487258911133,\n",
       "   10.062735557556152,\n",
       "   10.356071472167969,\n",
       "   10.340497970581055,\n",
       "   9.976532936096191,\n",
       "   9.944066047668457,\n",
       "   9.840835571289062,\n",
       "   10.18664836883545,\n",
       "   10.099479675292969,\n",
       "   9.921841621398926,\n",
       "   9.98194694519043,\n",
       "   9.999765396118164,\n",
       "   10.337248802185059,\n",
       "   10.27553939819336,\n",
       "   10.602850914001465,\n",
       "   10.394510269165039,\n",
       "   9.859861373901367,\n",
       "   9.94394302368164,\n",
       "   9.96460247039795,\n",
       "   9.836017608642578,\n",
       "   9.712786674499512,\n",
       "   9.84588623046875,\n",
       "   9.771757125854492,\n",
       "   9.977531433105469,\n",
       "   9.735788345336914,\n",
       "   10.036087036132812,\n",
       "   10.030817985534668,\n",
       "   9.831068992614746,\n",
       "   9.71871566772461,\n",
       "   9.556007385253906,\n",
       "   9.81097412109375,\n",
       "   9.696477890014648,\n",
       "   9.700602531433105,\n",
       "   9.831364631652832,\n",
       "   10.071027755737305,\n",
       "   9.93964672088623,\n",
       "   9.520236015319824,\n",
       "   9.432106971740723,\n",
       "   9.056253433227539,\n",
       "   8.82264518737793,\n",
       "   8.97148323059082,\n",
       "   8.889593124389648,\n",
       "   8.503849983215332,\n",
       "   8.818052291870117,\n",
       "   9.172532081604004,\n",
       "   9.108792304992676,\n",
       "   9.079757690429688,\n",
       "   9.089715003967285,\n",
       "   9.018648147583008,\n",
       "   9.02434253692627,\n",
       "   9.047642707824707,\n",
       "   9.065868377685547,\n",
       "   9.176060676574707,\n",
       "   9.074373245239258,\n",
       "   9.122930526733398,\n",
       "   9.363450050354004,\n",
       "   9.61203384399414,\n",
       "   9.771190643310547,\n",
       "   9.884035110473633,\n",
       "   10.154814720153809,\n",
       "   10.549986839294434,\n",
       "   10.69729995727539,\n",
       "   11.042668342590332,\n",
       "   11.241018295288086,\n",
       "   11.137788772583008,\n",
       "   11.049832344055176,\n",
       "   10.552544593811035,\n",
       "   10.91026782989502,\n",
       "   11.203811645507812,\n",
       "   11.11741828918457,\n",
       "   11.136165618896484,\n",
       "   10.657366752624512,\n",
       "   10.683830261230469,\n",
       "   10.714932441711426,\n",
       "   10.275388717651367,\n",
       "   10.574819564819336,\n",
       "   10.245712280273438,\n",
       "   10.288246154785156,\n",
       "   10.278454780578613,\n",
       "   10.1598482131958,\n",
       "   10.25033187866211,\n",
       "   9.967912673950195,\n",
       "   9.800518035888672,\n",
       "   10.121560096740723,\n",
       "   10.347075462341309,\n",
       "   10.628876686096191,\n",
       "   10.665048599243164,\n",
       "   10.658171653747559,\n",
       "   10.571160316467285,\n",
       "   10.764042854309082,\n",
       "   10.75456428527832,\n",
       "   10.567561149597168,\n",
       "   10.826749801635742,\n",
       "   10.908259391784668,\n",
       "   11.013151168823242,\n",
       "   10.894993782043457,\n",
       "   10.939847946166992,\n",
       "   10.6460599899292,\n",
       "   10.372762680053711,\n",
       "   10.08195972442627,\n",
       "   10.184022903442383,\n",
       "   10.498072624206543,\n",
       "   10.255260467529297,\n",
       "   10.271044731140137,\n",
       "   10.432661056518555,\n",
       "   10.587156295776367,\n",
       "   10.232695579528809,\n",
       "   9.927148818969727,\n",
       "   9.732019424438477,\n",
       "   9.6969633102417,\n",
       "   9.617974281311035,\n",
       "   9.847970962524414,\n",
       "   10.31508731842041,\n",
       "   10.608551979064941,\n",
       "   10.814719200134277,\n",
       "   10.817811965942383,\n",
       "   11.122700691223145,\n",
       "   11.358098983764648,\n",
       "   11.229364395141602,\n",
       "   11.181839942932129,\n",
       "   11.165329933166504,\n",
       "   11.089860916137695,\n",
       "   10.924538612365723,\n",
       "   10.864912033081055,\n",
       "   10.57634162902832,\n",
       "   10.469926834106445,\n",
       "   10.395905494689941,\n",
       "   10.116425514221191,\n",
       "   9.862832069396973,\n",
       "   9.761895179748535,\n",
       "   9.262453079223633,\n",
       "   9.724854469299316,\n",
       "   9.566540718078613,\n",
       "   9.7239408493042,\n",
       "   9.825553894042969,\n",
       "   9.732074737548828,\n",
       "   9.711485862731934,\n",
       "   9.626561164855957,\n",
       "   9.895105361938477,\n",
       "   10.225382804870605,\n",
       "   10.369582176208496,\n",
       "   10.338305473327637,\n",
       "   10.290396690368652,\n",
       "   10.34195327758789,\n",
       "   10.213237762451172,\n",
       "   10.05710220336914,\n",
       "   9.972811698913574,\n",
       "   9.844103813171387,\n",
       "   9.974627494812012,\n",
       "   10.16908073425293,\n",
       "   9.917222023010254,\n",
       "   10.01206111907959,\n",
       "   9.863255500793457,\n",
       "   10.027029037475586,\n",
       "   10.070008277893066,\n",
       "   10.428650856018066,\n",
       "   10.377004623413086,\n",
       "   10.542496681213379,\n",
       "   10.72420597076416,\n",
       "   10.513399124145508,\n",
       "   10.42612361907959,\n",
       "   10.1034574508667,\n",
       "   9.895217895507812,\n",
       "   9.891304016113281,\n",
       "   9.953400611877441,\n",
       "   9.890850067138672,\n",
       "   9.989447593688965,\n",
       "   9.985442161560059,\n",
       "   9.822877883911133,\n",
       "   9.889886856079102,\n",
       "   9.767264366149902,\n",
       "   9.824414253234863,\n",
       "   9.907157897949219,\n",
       "   9.411905288696289,\n",
       "   9.469188690185547,\n",
       "   9.488484382629395,\n",
       "   9.4268159866333,\n",
       "   9.444616317749023,\n",
       "   9.759723663330078,\n",
       "   9.973952293395996],\n",
       "  'ff_acc': [58.50895690917969,\n",
       "   51.85251998901367,\n",
       "   46.51647186279297,\n",
       "   43.194984436035156,\n",
       "   38.747833251953125,\n",
       "   37.00007247924805,\n",
       "   32.745216369628906,\n",
       "   29.278873443603516,\n",
       "   24.713228225708008,\n",
       "   21.721071243286133,\n",
       "   18.95315933227539,\n",
       "   17.110984802246094,\n",
       "   16.487571716308594,\n",
       "   15.752704620361328,\n",
       "   15.219369888305664,\n",
       "   13.860919952392578,\n",
       "   13.838529586791992,\n",
       "   13.507548332214355,\n",
       "   13.230846405029297,\n",
       "   12.961925506591797,\n",
       "   12.92225456237793,\n",
       "   12.81330394744873,\n",
       "   13.09335994720459,\n",
       "   13.128763198852539,\n",
       "   13.679657936096191,\n",
       "   14.349250793457031,\n",
       "   15.633766174316406,\n",
       "   16.30760955810547,\n",
       "   17.010250091552734,\n",
       "   17.805835723876953,\n",
       "   18.680723190307617,\n",
       "   19.842893600463867,\n",
       "   20.42243194580078,\n",
       "   21.037160873413086,\n",
       "   23.166587829589844,\n",
       "   23.939775466918945,\n",
       "   24.377017974853516,\n",
       "   25.658967971801758,\n",
       "   26.269880294799805,\n",
       "   27.121376037597656,\n",
       "   27.9001407623291,\n",
       "   29.34223747253418,\n",
       "   30.76205825805664,\n",
       "   30.653026580810547,\n",
       "   30.926258087158203,\n",
       "   31.778121948242188,\n",
       "   32.94562530517578,\n",
       "   34.186153411865234,\n",
       "   35.05644989013672,\n",
       "   35.60020065307617,\n",
       "   37.35206604003906,\n",
       "   38.80796432495117,\n",
       "   39.81303024291992,\n",
       "   39.21010971069336,\n",
       "   40.856719970703125,\n",
       "   42.000770568847656,\n",
       "   42.48843765258789,\n",
       "   42.63849639892578,\n",
       "   42.40070343017578,\n",
       "   41.75193786621094,\n",
       "   41.77968978881836,\n",
       "   41.016475677490234,\n",
       "   40.692481994628906,\n",
       "   41.105133056640625,\n",
       "   39.7529182434082,\n",
       "   39.06898880004883,\n",
       "   39.12194061279297,\n",
       "   38.44097137451172,\n",
       "   36.68712615966797,\n",
       "   35.947113037109375,\n",
       "   36.073123931884766,\n",
       "   35.62568283081055,\n",
       "   35.70136260986328,\n",
       "   34.77836990356445,\n",
       "   34.89069366455078,\n",
       "   34.98467254638672,\n",
       "   34.675262451171875,\n",
       "   34.96378707885742,\n",
       "   34.299766540527344,\n",
       "   34.25260543823242,\n",
       "   33.503204345703125,\n",
       "   33.886131286621094,\n",
       "   33.801727294921875,\n",
       "   34.626583099365234,\n",
       "   34.446067810058594,\n",
       "   33.308536529541016,\n",
       "   33.018070220947266,\n",
       "   33.643287658691406,\n",
       "   34.02008819580078,\n",
       "   34.60843276977539,\n",
       "   34.921756744384766,\n",
       "   35.29242706298828,\n",
       "   35.850521087646484,\n",
       "   35.25768280029297,\n",
       "   35.78229904174805,\n",
       "   36.05931854248047,\n",
       "   35.70479202270508,\n",
       "   35.775230407714844,\n",
       "   35.8450927734375,\n",
       "   35.02826690673828,\n",
       "   34.34904861450195,\n",
       "   35.62205123901367,\n",
       "   35.10954666137695,\n",
       "   34.93193054199219,\n",
       "   34.455665588378906,\n",
       "   34.410552978515625,\n",
       "   34.49174880981445,\n",
       "   34.997032165527344,\n",
       "   35.132606506347656,\n",
       "   34.094627380371094,\n",
       "   34.163265228271484,\n",
       "   33.656776428222656,\n",
       "   34.02250289916992,\n",
       "   33.62158966064453,\n",
       "   33.0341682434082,\n",
       "   32.9529914855957,\n",
       "   32.3817024230957,\n",
       "   31.978872299194336,\n",
       "   31.885265350341797,\n",
       "   31.797033309936523,\n",
       "   32.1286735534668,\n",
       "   31.89653778076172,\n",
       "   31.466611862182617,\n",
       "   31.66887092590332,\n",
       "   30.85845947265625,\n",
       "   30.95686149597168,\n",
       "   30.446252822875977,\n",
       "   30.111562728881836,\n",
       "   29.35637855529785,\n",
       "   28.505765914916992,\n",
       "   27.76087188720703,\n",
       "   27.48447036743164,\n",
       "   27.981657028198242,\n",
       "   28.261167526245117,\n",
       "   27.97865867614746,\n",
       "   28.45325469970703,\n",
       "   28.49372673034668,\n",
       "   28.46766471862793,\n",
       "   29.440340042114258,\n",
       "   29.270299911499023,\n",
       "   29.623735427856445,\n",
       "   29.995161056518555,\n",
       "   29.675493240356445,\n",
       "   29.56907081604004,\n",
       "   29.470476150512695,\n",
       "   30.059263229370117,\n",
       "   30.061786651611328,\n",
       "   30.109560012817383,\n",
       "   29.61819076538086,\n",
       "   29.678455352783203,\n",
       "   30.69184112548828,\n",
       "   30.344575881958008,\n",
       "   30.726524353027344,\n",
       "   30.664548873901367,\n",
       "   30.927631378173828,\n",
       "   30.67546272277832,\n",
       "   30.131351470947266,\n",
       "   29.88155746459961,\n",
       "   29.660324096679688,\n",
       "   30.043701171875,\n",
       "   30.102951049804688,\n",
       "   30.53949546813965,\n",
       "   30.606037139892578,\n",
       "   30.32575225830078,\n",
       "   30.75969886779785,\n",
       "   30.961257934570312,\n",
       "   31.17814064025879,\n",
       "   30.75798797607422,\n",
       "   30.735584259033203,\n",
       "   30.49054718017578,\n",
       "   30.256834030151367,\n",
       "   31.05608558654785,\n",
       "   31.102672576904297,\n",
       "   31.26897430419922,\n",
       "   31.316614151000977,\n",
       "   31.45233726501465,\n",
       "   31.079357147216797,\n",
       "   30.404653549194336,\n",
       "   30.04922866821289,\n",
       "   29.968408584594727,\n",
       "   30.031145095825195,\n",
       "   29.741182327270508,\n",
       "   29.670806884765625,\n",
       "   29.34688377380371,\n",
       "   29.393890380859375,\n",
       "   29.751068115234375,\n",
       "   29.616201400756836,\n",
       "   29.806856155395508,\n",
       "   29.761638641357422,\n",
       "   29.5198974609375,\n",
       "   29.99644660949707,\n",
       "   30.107311248779297,\n",
       "   29.813823699951172,\n",
       "   29.266813278198242,\n",
       "   29.57100486755371,\n",
       "   29.73463249206543,\n",
       "   30.270755767822266,\n",
       "   30.125104904174805,\n",
       "   30.205825805664062,\n",
       "   30.431726455688477,\n",
       "   30.62710189819336,\n",
       "   30.548078536987305,\n",
       "   30.598522186279297,\n",
       "   30.81818199157715,\n",
       "   30.50876808166504,\n",
       "   30.096370697021484,\n",
       "   30.120901107788086,\n",
       "   29.584476470947266,\n",
       "   29.717910766601562,\n",
       "   30.477863311767578,\n",
       "   30.813053131103516,\n",
       "   31.05702781677246,\n",
       "   31.28484535217285,\n",
       "   31.651432037353516,\n",
       "   31.753829956054688,\n",
       "   31.406253814697266,\n",
       "   31.188730239868164,\n",
       "   31.108001708984375,\n",
       "   31.038272857666016,\n",
       "   31.48941993713379,\n",
       "   31.80105972290039,\n",
       "   31.450197219848633,\n",
       "   31.032487869262695,\n",
       "   30.416440963745117,\n",
       "   30.018871307373047,\n",
       "   28.851259231567383,\n",
       "   29.47917938232422,\n",
       "   29.258956909179688,\n",
       "   29.139766693115234,\n",
       "   28.659149169921875,\n",
       "   28.80840301513672,\n",
       "   28.414573669433594,\n",
       "   28.40381622314453,\n",
       "   28.38628578186035,\n",
       "   28.299468994140625,\n",
       "   28.290462493896484,\n",
       "   28.754013061523438,\n",
       "   28.484329223632812,\n",
       "   28.412050247192383,\n",
       "   28.022666931152344,\n",
       "   27.795612335205078,\n",
       "   27.676040649414062,\n",
       "   28.18977165222168,\n",
       "   28.46403694152832,\n",
       "   28.74034881591797,\n",
       "   29.384521484375,\n",
       "   30.161930084228516,\n",
       "   30.690326690673828,\n",
       "   31.23619842529297,\n",
       "   31.54998779296875,\n",
       "   32.18704605102539,\n",
       "   32.19752883911133,\n",
       "   32.37937545776367,\n",
       "   32.63534164428711,\n",
       "   32.639503479003906,\n",
       "   32.598533630371094,\n",
       "   32.8003044128418,\n",
       "   33.15461349487305,\n",
       "   33.989444732666016,\n",
       "   33.92714309692383,\n",
       "   33.83436584472656,\n",
       "   33.88133239746094,\n",
       "   33.76273727416992,\n",
       "   33.339210510253906,\n",
       "   32.64093780517578,\n",
       "   32.42679977416992,\n",
       "   31.770559310913086,\n",
       "   31.409772872924805,\n",
       "   31.45899772644043,\n",
       "   31.090801239013672,\n",
       "   31.097681045532227,\n",
       "   30.80742645263672,\n",
       "   30.822467803955078,\n",
       "   30.198373794555664,\n",
       "   30.218549728393555,\n",
       "   30.228588104248047,\n",
       "   30.18037986755371,\n",
       "   30.628124237060547,\n",
       "   30.61927604675293,\n",
       "   30.560550689697266,\n",
       "   30.447568893432617,\n",
       "   30.191743850708008,\n",
       "   29.687246322631836,\n",
       "   29.731487274169922,\n",
       "   29.678577423095703,\n",
       "   30.002885818481445,\n",
       "   30.062496185302734,\n",
       "   29.639488220214844,\n",
       "   29.47617530822754,\n",
       "   29.450908660888672,\n",
       "   29.36416244506836,\n",
       "   29.017057418823242,\n",
       "   29.470544815063477,\n",
       "   30.135948181152344,\n",
       "   30.621746063232422,\n",
       "   30.48253631591797,\n",
       "   30.93108558654785,\n",
       "   30.833179473876953,\n",
       "   31.665897369384766,\n",
       "   31.70033073425293,\n",
       "   31.69159507751465,\n",
       "   30.83333969116211,\n",
       "   30.659687042236328,\n",
       "   30.057106018066406,\n",
       "   29.24858283996582,\n",
       "   29.192102432250977,\n",
       "   28.493019104003906,\n",
       "   29.451730728149414,\n",
       "   29.19168472290039,\n",
       "   29.10120391845703,\n",
       "   29.405641555786133,\n",
       "   29.417461395263672,\n",
       "   29.131784439086914,\n",
       "   29.282072067260742,\n",
       "   29.57843589782715,\n",
       "   29.810041427612305,\n",
       "   29.302635192871094,\n",
       "   29.467376708984375,\n",
       "   29.76921844482422,\n",
       "   29.956993103027344,\n",
       "   30.138578414916992,\n",
       "   30.333948135375977,\n",
       "   30.341825485229492,\n",
       "   30.4082088470459,\n",
       "   30.340805053710938,\n",
       "   30.44352149963379,\n",
       "   30.792057037353516,\n",
       "   30.905860900878906,\n",
       "   30.642980575561523,\n",
       "   30.12055778503418,\n",
       "   29.98052215576172,\n",
       "   29.825899124145508,\n",
       "   29.835105895996094,\n",
       "   29.52900505065918,\n",
       "   29.632699966430664,\n",
       "   29.511016845703125,\n",
       "   29.71567726135254,\n",
       "   29.659181594848633,\n",
       "   30.581607818603516,\n",
       "   30.51066780090332,\n",
       "   30.50263786315918,\n",
       "   30.780094146728516,\n",
       "   30.54065704345703,\n",
       "   30.19074821472168,\n",
       "   29.907373428344727,\n",
       "   30.341657638549805,\n",
       "   29.59823989868164,\n",
       "   29.37125015258789,\n",
       "   29.007431030273438,\n",
       "   29.2047176361084,\n",
       "   29.2482967376709,\n",
       "   29.45992660522461,\n",
       "   29.4444522857666,\n",
       "   28.981075286865234,\n",
       "   29.1079044342041,\n",
       "   29.36074447631836,\n",
       "   29.603925704956055,\n",
       "   29.545751571655273,\n",
       "   28.60521125793457,\n",
       "   28.478872299194336,\n",
       "   28.39241600036621,\n",
       "   28.117183685302734,\n",
       "   28.302091598510742,\n",
       "   28.00433349609375,\n",
       "   27.14405059814453,\n",
       "   26.47644805908203,\n",
       "   26.16109275817871,\n",
       "   26.138465881347656,\n",
       "   26.45811653137207,\n",
       "   25.900808334350586,\n",
       "   25.89908790588379,\n",
       "   25.94866180419922,\n",
       "   25.118186950683594,\n",
       "   25.211578369140625,\n",
       "   25.09857749938965,\n",
       "   25.225234985351562,\n",
       "   25.422985076904297,\n",
       "   25.983015060424805,\n",
       "   26.550344467163086,\n",
       "   27.139202117919922,\n",
       "   27.487213134765625,\n",
       "   28.557613372802734,\n",
       "   29.85076904296875,\n",
       "   30.61746597290039,\n",
       "   31.239992141723633,\n",
       "   31.19378089904785,\n",
       "   31.28783416748047,\n",
       "   31.768619537353516,\n",
       "   31.847116470336914,\n",
       "   32.22983932495117,\n",
       "   32.117427825927734,\n",
       "   31.8447322845459,\n",
       "   31.265336990356445,\n",
       "   30.682226181030273,\n",
       "   30.378679275512695,\n",
       "   29.634056091308594,\n",
       "   29.47627067565918,\n",
       "   29.117525100708008,\n",
       "   28.254222869873047,\n",
       "   28.18531608581543,\n",
       "   28.056747436523438,\n",
       "   27.24921989440918,\n",
       "   26.914501190185547,\n",
       "   26.840627670288086,\n",
       "   26.00555992126465,\n",
       "   25.396442413330078,\n",
       "   24.79098892211914,\n",
       "   24.219295501708984,\n",
       "   23.959857940673828,\n",
       "   24.18878746032715,\n",
       "   24.323549270629883,\n",
       "   24.789112091064453,\n",
       "   25.313335418701172,\n",
       "   25.812793731689453,\n",
       "   26.637454986572266,\n",
       "   26.968612670898438,\n",
       "   26.772462844848633,\n",
       "   27.48297691345215,\n",
       "   27.78945541381836,\n",
       "   28.646936416625977,\n",
       "   29.273948669433594,\n",
       "   29.729005813598633,\n",
       "   30.286041259765625,\n",
       "   30.215164184570312,\n",
       "   30.680747985839844,\n",
       "   31.164968490600586,\n",
       "   31.285419464111328,\n",
       "   30.834808349609375,\n",
       "   30.951976776123047,\n",
       "   31.12417221069336,\n",
       "   30.995187759399414,\n",
       "   30.876991271972656,\n",
       "   31.06985092163086,\n",
       "   30.94651985168457,\n",
       "   30.133262634277344,\n",
       "   29.728912353515625,\n",
       "   29.731918334960938,\n",
       "   29.849464416503906,\n",
       "   30.136537551879883,\n",
       "   30.330244064331055,\n",
       "   30.77414321899414,\n",
       "   30.641075134277344,\n",
       "   31.167221069335938,\n",
       "   30.889240264892578,\n",
       "   30.583595275878906,\n",
       "   30.1568660736084,\n",
       "   29.85251808166504,\n",
       "   29.36406707763672,\n",
       "   28.505123138427734,\n",
       "   28.301311492919922,\n",
       "   27.945444107055664,\n",
       "   27.510112762451172,\n",
       "   27.009296417236328,\n",
       "   27.064653396606445,\n",
       "   26.94254493713379,\n",
       "   26.950138092041016,\n",
       "   26.70234489440918,\n",
       "   26.556640625,\n",
       "   26.67934226989746,\n",
       "   27.18602180480957,\n",
       "   27.670278549194336,\n",
       "   27.554454803466797,\n",
       "   27.4464168548584,\n",
       "   28.00545310974121,\n",
       "   28.1041202545166,\n",
       "   27.688100814819336,\n",
       "   27.586591720581055,\n",
       "   27.336421966552734,\n",
       "   27.347341537475586,\n",
       "   27.471214294433594,\n",
       "   27.831247329711914,\n",
       "   28.077667236328125,\n",
       "   28.052881240844727,\n",
       "   27.81831932067871,\n",
       "   27.79887580871582,\n",
       "   27.28988265991211,\n",
       "   27.523958206176758,\n",
       "   26.97635841369629,\n",
       "   27.072492599487305,\n",
       "   26.615610122680664,\n",
       "   26.754432678222656,\n",
       "   26.726457595825195,\n",
       "   26.48581886291504,\n",
       "   26.767284393310547,\n",
       "   26.7154598236084,\n",
       "   26.923423767089844,\n",
       "   27.157411575317383,\n",
       "   27.166494369506836,\n",
       "   27.518770217895508,\n",
       "   27.862545013427734,\n",
       "   28.19087791442871,\n",
       "   28.512351989746094,\n",
       "   27.983123779296875,\n",
       "   28.153980255126953,\n",
       "   28.7413387298584,\n",
       "   29.293474197387695,\n",
       "   29.406373977661133,\n",
       "   29.454391479492188,\n",
       "   29.675966262817383,\n",
       "   29.952302932739258],\n",
       "  'test_accs': [763.6999359130859,\n",
       "   625.4466552734375,\n",
       "   506.42139434814453,\n",
       "   429.3647155761719,\n",
       "   357.7555351257324,\n",
       "   306.07430267333984,\n",
       "   264.2296657562256,\n",
       "   229.1647186279297,\n",
       "   200.14416313171387,\n",
       "   177.52923202514648,\n",
       "   156.45805644989014,\n",
       "   142.95438957214355,\n",
       "   133.8040885925293,\n",
       "   125.60786485671997,\n",
       "   119.26442766189575,\n",
       "   113.93626308441162,\n",
       "   109.12533569335938,\n",
       "   107.12513637542725,\n",
       "   105.79335975646973,\n",
       "   104.07098960876465,\n",
       "   103.72852563858032,\n",
       "   104.53544282913208,\n",
       "   105.67671203613281,\n",
       "   106.48866176605225,\n",
       "   107.19679164886475,\n",
       "   107.02300357818604,\n",
       "   108.48133182525635,\n",
       "   108.97609233856201,\n",
       "   109.66889667510986,\n",
       "   110.08406829833984,\n",
       "   111.90563774108887,\n",
       "   114.01358222961426,\n",
       "   113.82473468780518,\n",
       "   114.91283130645752,\n",
       "   116.90884017944336,\n",
       "   116.9594669342041,\n",
       "   117.4598560333252,\n",
       "   117.47952270507812,\n",
       "   116.24374198913574,\n",
       "   117.07272911071777,\n",
       "   120.40348625183105,\n",
       "   122.3951416015625,\n",
       "   124.41813850402832,\n",
       "   123.93734741210938,\n",
       "   125.5153865814209,\n",
       "   127.19162368774414,\n",
       "   129.39796257019043,\n",
       "   130.7573642730713,\n",
       "   131.5895538330078,\n",
       "   131.89726066589355,\n",
       "   133.34218788146973,\n",
       "   133.46489143371582,\n",
       "   132.72574043273926,\n",
       "   132.56566047668457,\n",
       "   132.21545791625977,\n",
       "   133.87054824829102,\n",
       "   133.12392234802246,\n",
       "   133.13341903686523,\n",
       "   132.51945877075195,\n",
       "   131.29862213134766,\n",
       "   132.20672225952148,\n",
       "   132.14217567443848,\n",
       "   131.6125602722168,\n",
       "   131.32611465454102,\n",
       "   129.7046356201172,\n",
       "   129.56247329711914,\n",
       "   129.70830154418945,\n",
       "   128.67552185058594,\n",
       "   127.02960968017578,\n",
       "   127.37235641479492,\n",
       "   128.29068565368652,\n",
       "   127.75621223449707,\n",
       "   126.75524520874023,\n",
       "   126.65774536132812,\n",
       "   124.96586799621582,\n",
       "   126.05771255493164,\n",
       "   125.88246536254883,\n",
       "   126.9433765411377,\n",
       "   126.80957984924316,\n",
       "   125.75987815856934,\n",
       "   125.05814933776855,\n",
       "   125.8727912902832,\n",
       "   126.12248229980469,\n",
       "   125.95163917541504,\n",
       "   126.29211235046387,\n",
       "   124.72690200805664,\n",
       "   123.63036727905273,\n",
       "   124.1451644897461,\n",
       "   124.9410228729248,\n",
       "   125.25640296936035,\n",
       "   124.88612270355225,\n",
       "   124.98592281341553,\n",
       "   125.6128158569336,\n",
       "   124.30032539367676,\n",
       "   126.16155910491943,\n",
       "   126.08827495574951,\n",
       "   124.72321128845215,\n",
       "   124.91193771362305,\n",
       "   125.11998081207275,\n",
       "   124.94576835632324,\n",
       "   123.89856243133545,\n",
       "   125.13383769989014,\n",
       "   124.99586296081543,\n",
       "   124.76108360290527,\n",
       "   124.4411792755127,\n",
       "   123.84607315063477,\n",
       "   123.62942886352539,\n",
       "   123.40903186798096,\n",
       "   123.54567241668701,\n",
       "   122.28205680847168,\n",
       "   122.24485206604004,\n",
       "   121.04882621765137,\n",
       "   121.18884468078613,\n",
       "   121.12202739715576,\n",
       "   119.88054370880127,\n",
       "   119.05890655517578,\n",
       "   118.72635078430176,\n",
       "   118.29161834716797,\n",
       "   117.49805450439453,\n",
       "   117.96860694885254,\n",
       "   118.70767402648926,\n",
       "   119.39653396606445,\n",
       "   120.47792434692383,\n",
       "   120.48709869384766,\n",
       "   119.15896224975586,\n",
       "   118.0753402709961,\n",
       "   118.54973030090332,\n",
       "   118.71612644195557,\n",
       "   116.22242450714111,\n",
       "   114.39431953430176,\n",
       "   111.95664596557617,\n",
       "   111.14948463439941,\n",
       "   109.51901054382324,\n",
       "   109.58007049560547,\n",
       "   109.14580535888672,\n",
       "   110.82180118560791,\n",
       "   111.19178104400635,\n",
       "   110.35024452209473,\n",
       "   111.40126323699951,\n",
       "   111.89554595947266,\n",
       "   111.21152591705322,\n",
       "   110.3178939819336,\n",
       "   110.05990219116211,\n",
       "   109.39591407775879,\n",
       "   109.31136894226074,\n",
       "   110.64326572418213,\n",
       "   109.61014366149902,\n",
       "   109.24712085723877,\n",
       "   109.97924995422363,\n",
       "   110.13830089569092,\n",
       "   111.18371963500977,\n",
       "   110.26464748382568,\n",
       "   110.48668003082275,\n",
       "   109.65688514709473,\n",
       "   110.35992050170898,\n",
       "   109.99400424957275,\n",
       "   110.15777206420898,\n",
       "   108.75895500183105,\n",
       "   109.10501289367676,\n",
       "   110.14746189117432,\n",
       "   109.86857032775879,\n",
       "   110.43754005432129,\n",
       "   111.28305435180664,\n",
       "   111.29354095458984,\n",
       "   111.9364242553711,\n",
       "   111.22005653381348,\n",
       "   112.41403770446777,\n",
       "   111.87330150604248,\n",
       "   111.84241580963135,\n",
       "   111.12307167053223,\n",
       "   111.49080085754395,\n",
       "   111.50554656982422,\n",
       "   111.95705890655518,\n",
       "   111.38421440124512,\n",
       "   111.13173866271973,\n",
       "   111.35356903076172,\n",
       "   110.18772220611572,\n",
       "   109.7928524017334,\n",
       "   108.12311840057373,\n",
       "   107.01905727386475,\n",
       "   105.90495777130127,\n",
       "   106.31329822540283,\n",
       "   106.52682685852051,\n",
       "   106.50118255615234,\n",
       "   106.12468528747559,\n",
       "   106.77585887908936,\n",
       "   106.38720512390137,\n",
       "   106.4876937866211,\n",
       "   106.73139476776123,\n",
       "   105.74459266662598,\n",
       "   106.82766056060791,\n",
       "   107.34078311920166,\n",
       "   106.99276161193848,\n",
       "   107.00411319732666,\n",
       "   107.7923936843872,\n",
       "   108.81179904937744,\n",
       "   109.51995754241943,\n",
       "   109.96630477905273,\n",
       "   110.84146499633789,\n",
       "   110.97824668884277,\n",
       "   111.37613296508789,\n",
       "   111.33552742004395,\n",
       "   111.2469253540039,\n",
       "   111.8367509841919,\n",
       "   111.6311559677124,\n",
       "   111.04119682312012,\n",
       "   110.03607559204102,\n",
       "   109.97076320648193,\n",
       "   110.04172325134277,\n",
       "   109.46367740631104,\n",
       "   109.61375522613525,\n",
       "   108.88072776794434,\n",
       "   108.80311584472656,\n",
       "   108.88299369812012,\n",
       "   109.18230628967285,\n",
       "   107.79273128509521,\n",
       "   107.92246341705322,\n",
       "   107.6847915649414,\n",
       "   107.42073345184326,\n",
       "   108.04354953765869,\n",
       "   108.23587894439697,\n",
       "   107.5891752243042,\n",
       "   107.12626934051514,\n",
       "   105.97320652008057,\n",
       "   104.90003681182861,\n",
       "   104.14496231079102,\n",
       "   104.04560661315918,\n",
       "   103.6144666671753,\n",
       "   103.60499572753906,\n",
       "   102.52326965332031,\n",
       "   102.28157043457031,\n",
       "   101.98582458496094,\n",
       "   102.66676902770996,\n",
       "   102.9361686706543,\n",
       "   102.20475101470947,\n",
       "   102.05816268920898,\n",
       "   103.06965827941895,\n",
       "   102.77587890625,\n",
       "   102.55372047424316,\n",
       "   102.06116962432861,\n",
       "   102.33194923400879,\n",
       "   101.91414833068848,\n",
       "   101.93600273132324,\n",
       "   101.98469638824463,\n",
       "   102.23812580108643,\n",
       "   103.8733491897583,\n",
       "   104.96979999542236,\n",
       "   105.33551120758057,\n",
       "   106.29609775543213,\n",
       "   106.75775718688965,\n",
       "   106.72622871398926,\n",
       "   106.29400634765625,\n",
       "   105.2464771270752,\n",
       "   105.79872608184814,\n",
       "   105.58946704864502,\n",
       "   106.26001358032227,\n",
       "   106.31711483001709,\n",
       "   107.34893608093262,\n",
       "   108.60636520385742,\n",
       "   108.11062431335449,\n",
       "   108.41156959533691,\n",
       "   108.79398441314697,\n",
       "   107.9881944656372,\n",
       "   108.29552936553955,\n",
       "   107.2796401977539,\n",
       "   106.80785465240479,\n",
       "   105.87970447540283,\n",
       "   105.56990432739258,\n",
       "   105.70454978942871,\n",
       "   104.50512981414795,\n",
       "   103.61656379699707,\n",
       "   103.3926591873169,\n",
       "   103.18659400939941,\n",
       "   102.13373184204102,\n",
       "   101.80618190765381,\n",
       "   101.94743728637695,\n",
       "   101.56959533691406,\n",
       "   101.08364963531494,\n",
       "   100.40971183776855,\n",
       "   100.56860160827637,\n",
       "   100.36585998535156,\n",
       "   100.21722030639648,\n",
       "   99.91916084289551,\n",
       "   98.62081336975098,\n",
       "   97.20353031158447,\n",
       "   97.96841526031494,\n",
       "   97.3096752166748,\n",
       "   96.26674365997314,\n",
       "   96.34824752807617,\n",
       "   96.81072425842285,\n",
       "   96.66201114654541,\n",
       "   95.9342851638794,\n",
       "   96.50891590118408,\n",
       "   96.68592739105225,\n",
       "   96.65945243835449,\n",
       "   96.12498378753662,\n",
       "   96.16876316070557,\n",
       "   95.7216739654541,\n",
       "   95.8842544555664,\n",
       "   96.42294502258301,\n",
       "   96.56870365142822,\n",
       "   95.86972236633301,\n",
       "   95.45923805236816,\n",
       "   94.23522567749023,\n",
       "   93.45229148864746,\n",
       "   93.91655158996582,\n",
       "   93.88299655914307,\n",
       "   94.55155277252197,\n",
       "   93.48835182189941,\n",
       "   94.40088272094727,\n",
       "   94.27375221252441,\n",
       "   93.94836139678955,\n",
       "   93.847975730896,\n",
       "   93.38318252563477,\n",
       "   93.40762615203857,\n",
       "   93.57712078094482,\n",
       "   93.54159450531006,\n",
       "   94.14485549926758,\n",
       "   94.77120685577393,\n",
       "   94.6798677444458,\n",
       "   94.27570343017578,\n",
       "   93.76060771942139,\n",
       "   94.37631416320801,\n",
       "   93.8681468963623,\n",
       "   92.96273708343506,\n",
       "   92.20301342010498,\n",
       "   92.80016326904297,\n",
       "   93.4346170425415,\n",
       "   92.76134967803955,\n",
       "   91.41901111602783,\n",
       "   90.91426181793213,\n",
       "   90.53043365478516,\n",
       "   90.52930068969727,\n",
       "   89.61037826538086,\n",
       "   89.27790927886963,\n",
       "   88.39779472351074,\n",
       "   88.39368057250977,\n",
       "   88.12115287780762,\n",
       "   88.60223293304443,\n",
       "   88.46614170074463,\n",
       "   89.05300426483154,\n",
       "   88.27259635925293,\n",
       "   87.89024543762207,\n",
       "   87.6925401687622,\n",
       "   86.90381240844727,\n",
       "   87.46742916107178,\n",
       "   86.44211959838867,\n",
       "   85.69934368133545,\n",
       "   85.1260986328125,\n",
       "   84.94207382202148,\n",
       "   84.87786865234375,\n",
       "   84.79099559783936,\n",
       "   84.63558006286621,\n",
       "   84.21398830413818,\n",
       "   84.25527858734131,\n",
       "   84.24492263793945,\n",
       "   84.46282196044922,\n",
       "   84.43923950195312,\n",
       "   82.91725063323975,\n",
       "   82.08723068237305,\n",
       "   81.70956993103027,\n",
       "   81.25558662414551,\n",
       "   81.12184429168701,\n",
       "   81.40074253082275,\n",
       "   80.57068061828613,\n",
       "   79.49460220336914,\n",
       "   79.46121215820312,\n",
       "   78.97655010223389,\n",
       "   80.02197647094727,\n",
       "   78.77778625488281,\n",
       "   78.78455829620361,\n",
       "   79.20272827148438,\n",
       "   78.51637363433838,\n",
       "   79.06614685058594,\n",
       "   79.55268859863281,\n",
       "   79.51229572296143,\n",
       "   79.46547985076904,\n",
       "   79.82796478271484,\n",
       "   81.13972091674805,\n",
       "   81.90425395965576,\n",
       "   82.6226282119751,\n",
       "   84.02847003936768,\n",
       "   85.3355712890625,\n",
       "   86.57640171051025,\n",
       "   87.47087860107422,\n",
       "   87.07334232330322,\n",
       "   86.57174777984619,\n",
       "   87.47885417938232,\n",
       "   88.04643440246582,\n",
       "   88.14395713806152,\n",
       "   88.27592754364014,\n",
       "   87.71517944335938,\n",
       "   87.31812286376953,\n",
       "   86.94363307952881,\n",
       "   86.21242332458496,\n",
       "   85.85912609100342,\n",
       "   85.00943374633789,\n",
       "   84.70789813995361,\n",
       "   83.79996967315674,\n",
       "   83.52128410339355,\n",
       "   83.68096828460693,\n",
       "   81.89574527740479,\n",
       "   81.39089965820312,\n",
       "   81.37840175628662,\n",
       "   81.00854206085205,\n",
       "   80.5519027709961,\n",
       "   79.97830486297607,\n",
       "   79.22604846954346,\n",
       "   79.044921875,\n",
       "   78.7868766784668,\n",
       "   78.3217191696167,\n",
       "   78.47152423858643,\n",
       "   79.7863187789917,\n",
       "   79.68920612335205,\n",
       "   80.22513008117676,\n",
       "   80.75511932373047,\n",
       "   80.30220603942871,\n",
       "   81.02388191223145,\n",
       "   80.8910551071167,\n",
       "   81.85986614227295,\n",
       "   82.75157356262207,\n",
       "   83.23730182647705,\n",
       "   83.39520454406738,\n",
       "   83.0693416595459,\n",
       "   84.14331912994385,\n",
       "   84.54700469970703,\n",
       "   84.55821800231934,\n",
       "   84.2784070968628,\n",
       "   84.5805196762085,\n",
       "   85.60808563232422,\n",
       "   85.61215019226074,\n",
       "   85.86258888244629,\n",
       "   86.41490364074707,\n",
       "   86.56822490692139,\n",
       "   86.26660919189453,\n",
       "   85.87077617645264,\n",
       "   87.06046104431152,\n",
       "   86.7757396697998,\n",
       "   86.51693344116211,\n",
       "   86.53428554534912,\n",
       "   87.32818984985352,\n",
       "   87.29952430725098,\n",
       "   87.60708332061768,\n",
       "   87.16920757293701,\n",
       "   86.15797805786133,\n",
       "   85.55499076843262,\n",
       "   85.17609882354736,\n",
       "   84.85228633880615,\n",
       "   83.99757862091064,\n",
       "   83.39104080200195,\n",
       "   82.06541538238525,\n",
       "   81.8743371963501,\n",
       "   81.06146812438965,\n",
       "   81.02769088745117,\n",
       "   80.66820430755615,\n",
       "   80.4768419265747,\n",
       "   80.35486698150635,\n",
       "   80.91016674041748,\n",
       "   81.92771244049072,\n",
       "   82.87441539764404,\n",
       "   83.7635269165039,\n",
       "   83.29201984405518,\n",
       "   83.19398975372314,\n",
       "   83.9095630645752,\n",
       "   83.73504161834717,\n",
       "   82.90456676483154,\n",
       "   81.98865795135498,\n",
       "   81.60898780822754,\n",
       "   81.67443752288818,\n",
       "   82.02433204650879,\n",
       "   81.84617710113525,\n",
       "   81.92203712463379,\n",
       "   81.75310897827148,\n",
       "   81.74703407287598,\n",
       "   81.55503368377686,\n",
       "   80.96288967132568,\n",
       "   81.06645011901855,\n",
       "   80.13530826568604,\n",
       "   80.13351058959961,\n",
       "   79.6315507888794,\n",
       "   79.58151912689209,\n",
       "   79.24148559570312,\n",
       "   78.90142726898193,\n",
       "   79.29663276672363,\n",
       "   78.93938064575195,\n",
       "   78.73489570617676,\n",
       "   79.0854434967041,\n",
       "   79.26575374603271,\n",
       "   79.44370460510254,\n",
       "   78.74497604370117,\n",
       "   78.88216590881348,\n",
       "   79.53524589538574,\n",
       "   79.26787948608398,\n",
       "   78.69730567932129,\n",
       "   80.03687286376953,\n",
       "   80.88217163085938,\n",
       "   81.22844505310059,\n",
       "   81.45745754241943,\n",
       "   82.37873458862305,\n",
       "   83.16580867767334],\n",
       "  'pce_r2': [-21.648945084881007,\n",
       "   -13.264736269560359,\n",
       "   -8.902197430864678,\n",
       "   -7.442418175350859,\n",
       "   -5.544907690021455,\n",
       "   -3.609389381629092,\n",
       "   -2.3976346524194256,\n",
       "   -1.4361374485118295,\n",
       "   -1.1161701133362243,\n",
       "   -1.122165227108558,\n",
       "   -0.6862442915121492,\n",
       "   -0.613761072561706,\n",
       "   -0.6061637204593706,\n",
       "   -0.6679265482002912,\n",
       "   -0.6251951055850671,\n",
       "   -0.43632370459652203,\n",
       "   -0.43039111167028166,\n",
       "   -0.34905597148463885,\n",
       "   -0.2568940353655962,\n",
       "   -0.3206985550647701,\n",
       "   -0.34402924363965637,\n",
       "   -0.4234759152110765,\n",
       "   -0.5183850190136041,\n",
       "   -0.5124076812915677,\n",
       "   -0.5269744158177498,\n",
       "   -0.6559414816421953,\n",
       "   -0.6195049731183699,\n",
       "   -0.7079950365928869,\n",
       "   -0.7296547980005841,\n",
       "   -0.7985418130601463,\n",
       "   -0.9362097149582396,\n",
       "   -1.0174838883467268,\n",
       "   -1.014623157769016,\n",
       "   -1.057764546871046,\n",
       "   -1.0237671077091828,\n",
       "   -1.0133736487176574,\n",
       "   -1.0635655657402725,\n",
       "   -1.1024221717873868,\n",
       "   -1.0310573514227355,\n",
       "   -1.0613316457682425,\n",
       "   -1.1101514567088517,\n",
       "   -1.1374417417570633,\n",
       "   -1.1587676463875107,\n",
       "   -1.3183716297966641,\n",
       "   -1.529851053463359,\n",
       "   -1.568435047865473,\n",
       "   -1.6406836110610126,\n",
       "   -1.6685675315028132,\n",
       "   -1.6509855856203073,\n",
       "   -1.7468706072550115,\n",
       "   -1.7148178564023473,\n",
       "   -1.6231257932096241,\n",
       "   -1.5725323920489007,\n",
       "   -1.5923767638585238,\n",
       "   -1.4527290828971027,\n",
       "   -1.524466360715441,\n",
       "   -1.4496595326342088,\n",
       "   -1.5044280331904298,\n",
       "   -1.6179527398971212,\n",
       "   -1.5916692627360236,\n",
       "   -1.499904589095729,\n",
       "   -1.5015353777742404,\n",
       "   -1.3981610931634902,\n",
       "   -1.2886003251571077,\n",
       "   -1.1373477313321732,\n",
       "   -1.1387569075088795,\n",
       "   -1.068154872348916,\n",
       "   -0.9570652494505765,\n",
       "   -0.8891131434828452,\n",
       "   -0.9240343726476206,\n",
       "   -0.89836069609372,\n",
       "   -1.0515931897965092,\n",
       "   -1.1076945647606853,\n",
       "   -0.9476928090039676,\n",
       "   -1.0252955288228285,\n",
       "   -1.0134007661324747,\n",
       "   -1.1164609417557068,\n",
       "   -1.219001865367375,\n",
       "   -1.2224110143896225,\n",
       "   -1.0483863510574873,\n",
       "   -0.9525657461802368,\n",
       "   -0.8828463661765651,\n",
       "   -0.722494652362494,\n",
       "   -0.7262996304324254,\n",
       "   -0.6731891013421019,\n",
       "   -0.6866476995295501,\n",
       "   -0.6349061289739992,\n",
       "   -0.6045631757858292,\n",
       "   -0.5573903956195594,\n",
       "   -0.5262300435868117,\n",
       "   -0.4875132246683811,\n",
       "   -0.5533460825143119,\n",
       "   -0.5654846051652729,\n",
       "   -0.544171693210433,\n",
       "   -0.568443828028649,\n",
       "   -0.5007095011009244,\n",
       "   -0.4538447842814126,\n",
       "   -0.5130677660084189,\n",
       "   -0.5717877071139565,\n",
       "   -0.6739236202921703,\n",
       "   -0.6751881582196155,\n",
       "   -0.7644587179948976,\n",
       "   -0.8223696695243532,\n",
       "   -0.9513268907383923,\n",
       "   -0.9609460973511581,\n",
       "   -0.931892734701109,\n",
       "   -0.9210840634751165,\n",
       "   -0.7946868964300977,\n",
       "   -0.8289486859692574,\n",
       "   -0.8007357987959234,\n",
       "   -0.8299348343151798,\n",
       "   -0.7887574585567692,\n",
       "   -0.7630173032787813,\n",
       "   -0.8054133097673297,\n",
       "   -0.7769008638227606,\n",
       "   -0.7788603249402539,\n",
       "   -0.716306872618885,\n",
       "   -0.6193334219294955,\n",
       "   -0.5597319343834155,\n",
       "   -0.7056218180739051,\n",
       "   -0.6536731861566476,\n",
       "   -0.710265748373575,\n",
       "   -0.7223390341992617,\n",
       "   -0.7074636604113045,\n",
       "   -0.6449258318622111,\n",
       "   -0.556922642090437,\n",
       "   -0.6004623070030637,\n",
       "   -0.620260820820921,\n",
       "   -0.6343690684309764,\n",
       "   -0.7268427718709418,\n",
       "   -0.6740028403152885,\n",
       "   -0.7332277880658145,\n",
       "   -0.5892501505297205,\n",
       "   -0.6064505555061435,\n",
       "   -0.6154783661772998,\n",
       "   -0.6037845129719663,\n",
       "   -0.5420397608862983,\n",
       "   -0.5715430303777538,\n",
       "   -0.5684248973694099,\n",
       "   -0.5991281073940709,\n",
       "   -0.5861654898780426,\n",
       "   -0.5476241439377179,\n",
       "   -0.5792906762122294,\n",
       "   -0.5622312007958143,\n",
       "   -0.5033572948772369,\n",
       "   -0.461984601431185,\n",
       "   -0.4656446645605894,\n",
       "   -0.4786939278418121,\n",
       "   -0.5378906714809217,\n",
       "   -0.5150331980142617,\n",
       "   -0.4754044013957752,\n",
       "   -0.4906517521433309,\n",
       "   -0.4711059958571415,\n",
       "   -0.47688250333061966,\n",
       "   -0.5557467250112933,\n",
       "   -0.5959731743696697,\n",
       "   -0.665468266163874,\n",
       "   -0.6242269696812812,\n",
       "   -0.6559129907895795,\n",
       "   -0.6540423695111603,\n",
       "   -0.640783762377181,\n",
       "   -0.6212892437634239,\n",
       "   -0.5947414506955149,\n",
       "   -0.5982289092960615,\n",
       "   -0.6498543093226878,\n",
       "   -0.556867992236209,\n",
       "   -0.48756048329405366,\n",
       "   -0.49804171716691714,\n",
       "   -0.5067411154005148,\n",
       "   -0.49442877562496856,\n",
       "   -0.5199386944221527,\n",
       "   -0.501484034644103,\n",
       "   -0.4888608011256066,\n",
       "   -0.5179639353117567,\n",
       "   -0.5754365299152002,\n",
       "   -0.5799751997100699,\n",
       "   -0.5619725367800388,\n",
       "   -0.5239183177492415,\n",
       "   -0.5459232493624191,\n",
       "   -0.5669741721139985,\n",
       "   -0.5953354706758904,\n",
       "   -0.5776977576685767,\n",
       "   -0.585966493497577,\n",
       "   -0.5743779057375411,\n",
       "   -0.5628067176803877,\n",
       "   -0.5636279162638087,\n",
       "   -0.5194223347349076,\n",
       "   -0.5278324702907793,\n",
       "   -0.43879753130618204,\n",
       "   -0.47016444499490095,\n",
       "   -0.4592759544347398,\n",
       "   -0.5238922386126768,\n",
       "   -0.5040864945430401,\n",
       "   -0.5406997881493503,\n",
       "   -0.5431744995986658,\n",
       "   -0.7024045088437905,\n",
       "   -0.7787402049903414,\n",
       "   -0.8225613581867057,\n",
       "   -0.9252316853569882,\n",
       "   -0.8983845595576168,\n",
       "   -0.8477573630091575,\n",
       "   -0.799709147083504,\n",
       "   -0.7572737879338289,\n",
       "   -0.7500408073013949,\n",
       "   -0.7333039503368075,\n",
       "   -0.693788196953047,\n",
       "   -0.6323250386545227,\n",
       "   -0.7098051009504132,\n",
       "   -0.7733932864347108,\n",
       "   -0.7427415935581712,\n",
       "   -0.7321587361540771,\n",
       "   -0.6653454408990629,\n",
       "   -0.7204549567285232,\n",
       "   -0.7117011671401772,\n",
       "   -0.6987736992348657,\n",
       "   -0.5820543317684523,\n",
       "   -0.5358493831982671,\n",
       "   -0.5298040157284771,\n",
       "   -0.516283342230258,\n",
       "   -0.5070076107905339,\n",
       "   -0.41913655590916066,\n",
       "   -0.3453413461632986,\n",
       "   -0.3457442333643763,\n",
       "   -0.3644194281036057,\n",
       "   -0.3191984716961829,\n",
       "   -0.29102267859233577,\n",
       "   -0.27726703457679136,\n",
       "   -0.2898244597621735,\n",
       "   -0.2867253748494276,\n",
       "   -0.2720040465934377,\n",
       "   -0.25448436757240933,\n",
       "   -0.33891571978064206,\n",
       "   -0.3184063782544182,\n",
       "   -0.3051444909541883,\n",
       "   -0.31376083952364486,\n",
       "   -0.33612108486375125,\n",
       "   -0.408932204285096,\n",
       "   -0.39824285144868443,\n",
       "   -0.4221501239187797,\n",
       "   -0.43511404389565067,\n",
       "   -0.4546720151932486,\n",
       "   -0.48777277386161244,\n",
       "   -0.48086941906112335,\n",
       "   -0.43623657590642995,\n",
       "   -0.3877215323246854,\n",
       "   -0.39390892481945605,\n",
       "   -0.34662976988820926,\n",
       "   -0.34465224580153864,\n",
       "   -0.3282699310344104,\n",
       "   -0.3400750995193418,\n",
       "   -0.31447116175199863,\n",
       "   -0.3676629445806925,\n",
       "   -0.375146156940404,\n",
       "   -0.3637612391421685,\n",
       "   -0.37638946718904864,\n",
       "   -0.3878797629511683,\n",
       "   -0.4330289256035291,\n",
       "   -0.4776057243398024,\n",
       "   -0.47783193577301897,\n",
       "   -0.4665329189049081,\n",
       "   -0.5095515716271601,\n",
       "   -0.4998486512641669,\n",
       "   -0.49419202103034765,\n",
       "   -0.5630112561685505,\n",
       "   -0.5338081606372624,\n",
       "   -0.5233064212907967,\n",
       "   -0.5416410012073452,\n",
       "   -0.6125105744768113,\n",
       "   -0.5645879748867599,\n",
       "   -0.625496473776165,\n",
       "   -0.5803324105656555,\n",
       "   -0.5491470707223272,\n",
       "   -0.5506151741065317,\n",
       "   -0.5398003981732209,\n",
       "   -0.49413396188822367,\n",
       "   -0.42302953032795365,\n",
       "   -0.3951398823194596,\n",
       "   -0.34680923083006254,\n",
       "   -0.38620058909029753,\n",
       "   -0.3928352760808871,\n",
       "   -0.4018863495301688,\n",
       "   -0.43763091373393803,\n",
       "   -0.39685939109100343,\n",
       "   -0.4149184110682802,\n",
       "   -0.36808022781373695,\n",
       "   -0.41440676294975987,\n",
       "   -0.38771120960008054,\n",
       "   -0.37865826203644404,\n",
       "   -0.35581581006296026,\n",
       "   -0.36283763146459913,\n",
       "   -0.3875916726323172,\n",
       "   -0.3650159492630216,\n",
       "   -0.31758593335657803,\n",
       "   -0.3375845568093012,\n",
       "   -0.36540636213250677,\n",
       "   -0.3562657855100113,\n",
       "   -0.3774096957186095,\n",
       "   -0.39854355282358944,\n",
       "   -0.3570909282112076,\n",
       "   -0.38714875182464503,\n",
       "   -0.3196617652448088,\n",
       "   -0.2878220134474563,\n",
       "   -0.30378006496021537,\n",
       "   -0.3000528365608357,\n",
       "   -0.2859180975479845,\n",
       "   -0.3056920360502968,\n",
       "   -0.35804727528256497,\n",
       "   -0.36395853359350827,\n",
       "   -0.37699240104878284,\n",
       "   -0.4391239600891612,\n",
       "   -0.44218849164934526,\n",
       "   -0.45440526265397474,\n",
       "   -0.45287958333674294,\n",
       "   -0.4659129749694897,\n",
       "   -0.47607294258601995,\n",
       "   -0.461354715992667,\n",
       "   -0.4725225969226714,\n",
       "   -0.5520586010177968,\n",
       "   -0.5130505004586472,\n",
       "   -0.4933449878553109,\n",
       "   -0.5341932939195833,\n",
       "   -0.471000493587276,\n",
       "   -0.53931897671913,\n",
       "   -0.5156491189556625,\n",
       "   -0.5326394022980661,\n",
       "   -0.5258500937837938,\n",
       "   -0.48627658874419444,\n",
       "   -0.5639055260836736,\n",
       "   -0.5606419551597361,\n",
       "   -0.5626688176960624,\n",
       "   -0.5355953111368479,\n",
       "   -0.530951852094385,\n",
       "   -0.5558498438121922,\n",
       "   -0.503996463930414,\n",
       "   -0.47619614468172067,\n",
       "   -0.4609139230365995,\n",
       "   -0.4496723596953298,\n",
       "   -0.40364721869652764,\n",
       "   -0.44176258265443624,\n",
       "   -0.4445393878231849,\n",
       "   -0.468216995092835,\n",
       "   -0.4345357880840037,\n",
       "   -0.4265838321953974,\n",
       "   -0.4006603449030848,\n",
       "   -0.38094838586506685,\n",
       "   -0.36208215018373213,\n",
       "   -0.35737111000178357,\n",
       "   -0.37031754115723525,\n",
       "   -0.33687760039084025,\n",
       "   -0.39690591766298233,\n",
       "   -0.3749870782860556,\n",
       "   -0.36243498320506284,\n",
       "   -0.35141192037880553,\n",
       "   -0.37431788805634336,\n",
       "   -0.3868660729716267,\n",
       "   -0.3613428032097623,\n",
       "   -0.3538168607367824,\n",
       "   -0.36095244862047204,\n",
       "   -0.3485235748733597,\n",
       "   -0.37554841940262995,\n",
       "   -0.3769770500959333,\n",
       "   -0.3928901048431981,\n",
       "   -0.38949527426472796,\n",
       "   -0.4026203376559143,\n",
       "   -0.40396716053508674,\n",
       "   -0.4003484779331019,\n",
       "   -0.44384458440272145,\n",
       "   -0.4519376495527261,\n",
       "   -0.5391509134031118,\n",
       "   -0.48304223172078276,\n",
       "   -0.5187132839943811,\n",
       "   -0.48371974217842095,\n",
       "   -0.4185123871109646,\n",
       "   -0.47502672864138806,\n",
       "   -0.493305687897315,\n",
       "   -0.45633961196011863,\n",
       "   -0.4984802918406346,\n",
       "   -0.5418767661787922,\n",
       "   -0.5286113573010562,\n",
       "   -0.5097632713435281,\n",
       "   -0.5316426122255278,\n",
       "   -0.5676430563831625,\n",
       "   -0.5401712430100643,\n",
       "   -0.5263318617727695,\n",
       "   -0.4998750962452716,\n",
       "   -0.49188482677981593,\n",
       "   -0.527648818132515,\n",
       "   -0.5139112594753297,\n",
       "   -0.49617081527357176,\n",
       "   -0.4832743397503074,\n",
       "   -0.48833266024978417,\n",
       "   -0.5069636031095934,\n",
       "   -0.5181803835126559,\n",
       "   -0.507988208952088,\n",
       "   -0.5124394540414303,\n",
       "   -0.5483843431188999,\n",
       "   -0.5185543776699488,\n",
       "   -0.5372202209862584,\n",
       "   -0.5013446993002373,\n",
       "   -0.48447292025538946,\n",
       "   -0.46267646941918206,\n",
       "   -0.4060835186154159,\n",
       "   -0.48548404569243764,\n",
       "   -0.48329476982649733,\n",
       "   -0.482407809788054,\n",
       "   -0.46534274670976594,\n",
       "   -0.493166394067889,\n",
       "   -0.4785823986526909,\n",
       "   -0.4727753090279454,\n",
       "   -0.4210028043351284,\n",
       "   -0.3877704978753207,\n",
       "   -0.38746857027410386,\n",
       "   -0.41098033787480404,\n",
       "   -0.3872282418457058,\n",
       "   -0.3763953889988143,\n",
       "   -0.44570616252109163,\n",
       "   -0.40633295259329816,\n",
       "   -0.40072602889443054,\n",
       "   -0.44964470476513996,\n",
       "   -0.490657174875194,\n",
       "   -0.46815525982599704,\n",
       "   -0.4502713055867644,\n",
       "   -0.4275712485886334,\n",
       "   -0.4303225834356659,\n",
       "   -0.4519220932290604,\n",
       "   -0.45946094010708105,\n",
       "   -0.46683099199814126,\n",
       "   -0.44025602424297294,\n",
       "   -0.4824567684980503,\n",
       "   -0.5338505901676711,\n",
       "   -0.5614987926600499,\n",
       "   -0.5494118523506621,\n",
       "   -0.5627983352553125,\n",
       "   -0.5308406923659879,\n",
       "   -0.5595190964487462,\n",
       "   -0.5675298997514566,\n",
       "   -0.5611276522230118,\n",
       "   -0.5736443385162735,\n",
       "   -0.5554594474728187,\n",
       "   -0.5453360864535517,\n",
       "   -0.5780503150873844,\n",
       "   -0.5847108533044922,\n",
       "   -0.5307142828516431,\n",
       "   -0.4800016928789441,\n",
       "   -0.4551051206455774,\n",
       "   -0.4382549157888205,\n",
       "   -0.45524963037663246,\n",
       "   -0.4967302760241168,\n",
       "   -0.541318789783781,\n",
       "   -0.5725127248917457,\n",
       "   -0.5731479385310843,\n",
       "   -0.5626378246750632,\n",
       "   -0.5214377706471098,\n",
       "   -0.53489208974478,\n",
       "   -0.5099175756643977,\n",
       "   -0.5340663777765575,\n",
       "   -0.5799357658226136,\n",
       "   -0.6204901968442067,\n",
       "   -0.6470094624097356,\n",
       "   -0.677016748334212,\n",
       "   -0.7008071914894438,\n",
       "   -0.6930105266188327,\n",
       "   -0.6854378759579443,\n",
       "   -0.7453959671576944,\n",
       "   -0.7198336229857463,\n",
       "   -0.6961919032812458,\n",
       "   -0.673319642060485,\n",
       "   -0.6791031970696766,\n",
       "   -0.65596428308711,\n",
       "   -0.6328468614077025,\n",
       "   -0.6407089094451914,\n",
       "   -0.5609761816885217,\n",
       "   -0.5385523832691528,\n",
       "   -0.5565660794115244,\n",
       "   -0.5422757276230905,\n",
       "   -0.5509667898222508,\n",
       "   -0.5330926762427197,\n",
       "   -0.5405301442556136,\n",
       "   -0.5732255402925373,\n",
       "   -0.6244743705079268,\n",
       "   -0.6180994155639656,\n",
       "   -0.6280608941725858,\n",
       "   -0.6427046883782628,\n",
       "   -0.6777559682368159,\n",
       "   -0.6585787408932915,\n",
       "   -0.6183352087293201,\n",
       "   -0.6319712396002704,\n",
       "   -0.6420198356453781,\n",
       "   -0.6276126977816094,\n",
       "   -0.6210887148284414,\n",
       "   -0.6040781293397988,\n",
       "   -0.595259418061423,\n",
       "   -0.6240111720046639,\n",
       "   -0.6359189749143319,\n",
       "   -0.6964915415298361,\n",
       "   -0.7300399281191594,\n",
       "   -0.7358982555700897,\n",
       "   -0.7671900339244151,\n",
       "   -0.8093841887347382,\n",
       "   -0.8527858489746787],\n",
       "  'voc_r2': [-16.616692374713658,\n",
       "   -9.86969142591034,\n",
       "   -9.791648510548116,\n",
       "   -10.963077665561642,\n",
       "   -9.817701244529678,\n",
       "   -7.398558505055849,\n",
       "   -6.90172375413105,\n",
       "   -5.840041000889915,\n",
       "   -4.330720760345663,\n",
       "   -3.116202552524668,\n",
       "   -1.7477212638885353,\n",
       "   -0.8453759352214618,\n",
       "   -0.6470142006521911,\n",
       "   -0.7420915283404754,\n",
       "   -0.9542848521633118,\n",
       "   -0.8208884471863964,\n",
       "   -0.5479083437335379,\n",
       "   -0.4888812212682221,\n",
       "   -0.265312759209416,\n",
       "   -0.13597201033493755,\n",
       "   0.004536581164075182,\n",
       "   0.04965001610492148,\n",
       "   0.08903707630082569,\n",
       "   0.035644953372686694,\n",
       "   0.018806483242374772,\n",
       "   -0.050998473112617626,\n",
       "   -0.27947196265918883,\n",
       "   -0.5155481247462694,\n",
       "   -0.6621117456602954,\n",
       "   -0.7358265803837087,\n",
       "   -1.108670869670863,\n",
       "   -1.750954199694887,\n",
       "   -1.675275425208456,\n",
       "   -1.9289484422396854,\n",
       "   -1.9907240693121824,\n",
       "   -2.0832474965092147,\n",
       "   -2.2084633102625144,\n",
       "   -2.313170879640769,\n",
       "   -2.197494550892263,\n",
       "   -2.26919139741067,\n",
       "   -2.9307589717326965,\n",
       "   -2.9246129527185536,\n",
       "   -3.151591542727658,\n",
       "   -3.238618780970355,\n",
       "   -3.6367713234161716,\n",
       "   -3.6797965705664124,\n",
       "   -3.85529551288643,\n",
       "   -4.095196747967666,\n",
       "   -4.487897872891774,\n",
       "   -4.5351910321639055,\n",
       "   -4.879006213826608,\n",
       "   -4.812228472373616,\n",
       "   -5.132644666195864,\n",
       "   -6.088672423616811,\n",
       "   -6.026133896167945,\n",
       "   -6.139412602621291,\n",
       "   -6.259524653096321,\n",
       "   -6.7930483835579,\n",
       "   -6.279322946384451,\n",
       "   -7.231498224449863,\n",
       "   -8.294951630886281,\n",
       "   -8.981592418085507,\n",
       "   -9.833662841552758,\n",
       "   -9.996214206549396,\n",
       "   -10.75944317674359,\n",
       "   -11.717814165273857,\n",
       "   -11.600104263520226,\n",
       "   -11.656820329645418,\n",
       "   -12.41112407517884,\n",
       "   -14.031202862560775,\n",
       "   -13.461009917173655,\n",
       "   -12.334716706191847,\n",
       "   -12.988886610305254,\n",
       "   -14.522056255983497,\n",
       "   -14.770812868304779,\n",
       "   -15.661746947471872,\n",
       "   -15.817127175403854,\n",
       "   -17.09912195576304,\n",
       "   -17.85243405104706,\n",
       "   -17.15578746665041,\n",
       "   -17.874653188085453,\n",
       "   -17.716227037307707,\n",
       "   -18.599492001108477,\n",
       "   -19.145855736723558,\n",
       "   -19.890424063276296,\n",
       "   -20.72614030311866,\n",
       "   -19.803019994744073,\n",
       "   -19.83245108152412,\n",
       "   -20.489073600287846,\n",
       "   -20.757609513792296,\n",
       "   -20.02944495811079,\n",
       "   -21.260788147525627,\n",
       "   -20.994285554146245,\n",
       "   -21.74289744723984,\n",
       "   -23.944234808223957,\n",
       "   -22.538390272463896,\n",
       "   -20.876130421734185,\n",
       "   -21.341550701319466,\n",
       "   -23.30048515440485,\n",
       "   -22.585980291933456,\n",
       "   -22.298470561619062,\n",
       "   -22.628028670865092,\n",
       "   -22.473203481112456,\n",
       "   -22.258838684601322,\n",
       "   -22.63581394581383,\n",
       "   -21.568943490624605,\n",
       "   -21.005749998484102,\n",
       "   -20.884040413983758,\n",
       "   -20.605063558784124,\n",
       "   -19.630965724401165,\n",
       "   -18.407887928156907,\n",
       "   -18.165752185735766,\n",
       "   -18.90013724436839,\n",
       "   -18.84882485062071,\n",
       "   -18.464557612627704,\n",
       "   -18.54603224948935,\n",
       "   -19.481409821170455,\n",
       "   -20.28286553078351,\n",
       "   -19.59308396902023,\n",
       "   -19.561122504715843,\n",
       "   -19.83349375119222,\n",
       "   -19.97999697354684,\n",
       "   -20.888796994694534,\n",
       "   -18.90882538001079,\n",
       "   -17.702383165363447,\n",
       "   -17.50712439487453,\n",
       "   -18.886858251592326,\n",
       "   -17.243408275861093,\n",
       "   -15.853437491177182,\n",
       "   -14.783781518861236,\n",
       "   -13.306538652643855,\n",
       "   -12.724051593965113,\n",
       "   -11.459229966590435,\n",
       "   -11.292499046926178,\n",
       "   -11.1728932459095,\n",
       "   -12.087090486304422,\n",
       "   -12.5109517388534,\n",
       "   -11.86778029332288,\n",
       "   -10.687559783816848,\n",
       "   -10.633231482606968,\n",
       "   -10.483515329941545,\n",
       "   -10.205996307640863,\n",
       "   -10.046625098410383,\n",
       "   -9.819586006609775,\n",
       "   -9.880163744762083,\n",
       "   -10.260756511602986,\n",
       "   -9.327305786563011,\n",
       "   -9.006265379792303,\n",
       "   -8.887835765187809,\n",
       "   -8.809960464718655,\n",
       "   -8.660359581644823,\n",
       "   -8.3560713914184,\n",
       "   -8.770685633328798,\n",
       "   -8.531810995430932,\n",
       "   -8.828428539321996,\n",
       "   -8.987095289673395,\n",
       "   -8.837379978823344,\n",
       "   -8.302465076821969,\n",
       "   -8.082353152286675,\n",
       "   -8.550146513304815,\n",
       "   -7.860328801750487,\n",
       "   -7.804243284908086,\n",
       "   -8.251288628414743,\n",
       "   -8.563440411648605,\n",
       "   -8.523210290572315,\n",
       "   -8.150783351401754,\n",
       "   -8.161623137667826,\n",
       "   -7.964519846876204,\n",
       "   -8.127848979637657,\n",
       "   -7.879189208217809,\n",
       "   -7.8813377624338194,\n",
       "   -7.932736660643368,\n",
       "   -7.946196336825441,\n",
       "   -7.644013921501289,\n",
       "   -7.766918064384042,\n",
       "   -7.697279385457897,\n",
       "   -7.362030140554326,\n",
       "   -7.2829698791474105,\n",
       "   -7.003449270885826,\n",
       "   -6.603038039032802,\n",
       "   -6.24863610700452,\n",
       "   -6.313238408870055,\n",
       "   -6.3460547298662195,\n",
       "   -6.192044515991901,\n",
       "   -6.121046505952154,\n",
       "   -6.123357077943893,\n",
       "   -6.332325499914853,\n",
       "   -6.216226409066527,\n",
       "   -6.422131965992559,\n",
       "   -6.196435093620111,\n",
       "   -6.249909922714994,\n",
       "   -6.17996840241818,\n",
       "   -6.1886372047571445,\n",
       "   -6.140139084363077,\n",
       "   -6.276942325741768,\n",
       "   -6.243717691576089,\n",
       "   -6.22673872954329,\n",
       "   -6.182151583996077,\n",
       "   -6.380860509746427,\n",
       "   -6.500494337146803,\n",
       "   -6.596850793633558,\n",
       "   -6.724477567068908,\n",
       "   -6.810225313033471,\n",
       "   -6.7241779462692515,\n",
       "   -6.849136043910132,\n",
       "   -6.803703514064676,\n",
       "   -6.504758712761653,\n",
       "   -6.535956014993797,\n",
       "   -6.309116130150112,\n",
       "   -6.003067904150726,\n",
       "   -5.986290770705709,\n",
       "   -5.990304710343125,\n",
       "   -6.00028894663469,\n",
       "   -6.080517924653187,\n",
       "   -6.027049469086905,\n",
       "   -5.881322012020145,\n",
       "   -6.192725749531127,\n",
       "   -6.1217996857606005,\n",
       "   -6.237014190810245,\n",
       "   -6.302577515935721,\n",
       "   -6.449227807273643,\n",
       "   -6.616853751767384,\n",
       "   -6.714700526575624,\n",
       "   -6.697518238669988,\n",
       "   -6.524851879211075,\n",
       "   -6.840234921570549,\n",
       "   -6.791816310683586,\n",
       "   -6.477893769812193,\n",
       "   -6.495061564320946,\n",
       "   -6.392831543133184,\n",
       "   -6.43848295597331,\n",
       "   -6.28031660427106,\n",
       "   -6.552375802227124,\n",
       "   -6.444433678193953,\n",
       "   -6.299352156957275,\n",
       "   -6.065289627040761,\n",
       "   -5.866110516740658,\n",
       "   -5.648110210709574,\n",
       "   -5.5680238922423495,\n",
       "   -5.445028490744465,\n",
       "   -5.505342344865719,\n",
       "   -5.408387234267185,\n",
       "   -5.299026679422032,\n",
       "   -5.25793857940106,\n",
       "   -5.247786122958911,\n",
       "   -5.259965188679984,\n",
       "   -5.3550211401511145,\n",
       "   -5.450352259949278,\n",
       "   -5.750621371571306,\n",
       "   -6.167903498218774,\n",
       "   -6.011042032805385,\n",
       "   -5.80057917263028,\n",
       "   -5.576369655768578,\n",
       "   -5.617803779669765,\n",
       "   -5.540298049363193,\n",
       "   -5.64221319420221,\n",
       "   -5.567993129992609,\n",
       "   -5.687664833149405,\n",
       "   -5.77495448257833,\n",
       "   -5.564521406677025,\n",
       "   -5.62541667768899,\n",
       "   -5.506901620089971,\n",
       "   -5.402461410271576,\n",
       "   -5.537613900087301,\n",
       "   -5.391326506216188,\n",
       "   -5.1753164123216875,\n",
       "   -5.165470014781005,\n",
       "   -4.836359525445215,\n",
       "   -4.898504183884206,\n",
       "   -4.64202771930861,\n",
       "   -4.500284286990787,\n",
       "   -4.462567813128474,\n",
       "   -4.4242929328195375,\n",
       "   -4.300018062133393,\n",
       "   -4.331435787006532,\n",
       "   -4.578580850427047,\n",
       "   -4.636770256505217,\n",
       "   -4.61276545209184,\n",
       "   -4.546559063610092,\n",
       "   -4.643579924207658,\n",
       "   -4.662178648021984,\n",
       "   -4.701605770053647,\n",
       "   -4.884135829181113,\n",
       "   -4.561835355436704,\n",
       "   -4.334723272718517,\n",
       "   -4.555801722490005,\n",
       "   -4.496576595808754,\n",
       "   -4.440206778024997,\n",
       "   -4.347163641254455,\n",
       "   -4.383272836475819,\n",
       "   -4.41341223190025,\n",
       "   -4.497048619530293,\n",
       "   -4.682165585925417,\n",
       "   -4.632436790186755,\n",
       "   -4.431463682421106,\n",
       "   -4.317431938316775,\n",
       "   -4.265920824152896,\n",
       "   -4.346577788524961,\n",
       "   -4.210591134020421,\n",
       "   -4.288461388391421,\n",
       "   -4.335428860408467,\n",
       "   -4.269146443965963,\n",
       "   -4.277312827306072,\n",
       "   -4.211333515940774,\n",
       "   -4.28916334930123,\n",
       "   -4.484504021371576,\n",
       "   -4.572516892712469,\n",
       "   -4.438521362642606,\n",
       "   -4.286882717515912,\n",
       "   -4.417336184032847,\n",
       "   -4.398962969245487,\n",
       "   -4.234068977439596,\n",
       "   -4.203692579156925,\n",
       "   -4.1386657698501645,\n",
       "   -4.112310721989361,\n",
       "   -4.0069986622776455,\n",
       "   -4.049680923671026,\n",
       "   -3.9596938459111666,\n",
       "   -4.04091354098132,\n",
       "   -4.069330256034984,\n",
       "   -3.981675683218488,\n",
       "   -3.972564015204587,\n",
       "   -4.095189676935823,\n",
       "   -4.08523663358562,\n",
       "   -3.9178663973056134,\n",
       "   -3.7424499990401854,\n",
       "   -3.902830148963849,\n",
       "   -3.944175973601925,\n",
       "   -3.9033839381236106,\n",
       "   -3.830621516926829,\n",
       "   -3.733635129904002,\n",
       "   -3.6686171757640746,\n",
       "   -3.6126919303322387,\n",
       "   -3.5509614781046963,\n",
       "   -3.4347712261680075,\n",
       "   -3.2687464334581326,\n",
       "   -3.4501099957416663,\n",
       "   -3.517250354716696,\n",
       "   -3.420800737016444,\n",
       "   -3.4913442374792325,\n",
       "   -3.6384578954016398,\n",
       "   -3.4474438527614364,\n",
       "   -3.574555520323811,\n",
       "   -3.708271282221955,\n",
       "   -3.71023358972315,\n",
       "   -3.826136771997631,\n",
       "   -3.7279093136322334,\n",
       "   -3.6353818598997423,\n",
       "   -3.644812379919756,\n",
       "   -3.5750621192135412,\n",
       "   -3.5892062733523664,\n",
       "   -3.4959808135288633,\n",
       "   -3.461006845422169,\n",
       "   -3.5099265631738747,\n",
       "   -3.349902304759545,\n",
       "   -3.303224143474994,\n",
       "   -3.5160008436729138,\n",
       "   -3.489722066421047,\n",
       "   -3.3228319574303145,\n",
       "   -3.175933979662724,\n",
       "   -3.0757133498294564,\n",
       "   -3.0489750935685143,\n",
       "   -3.095473399089146,\n",
       "   -3.101342420377062,\n",
       "   -2.990806289259189,\n",
       "   -2.8488381700563954,\n",
       "   -2.842105380085573,\n",
       "   -2.751097654254191,\n",
       "   -2.900090719876018,\n",
       "   -2.8020592918308784,\n",
       "   -2.8328144763299195,\n",
       "   -2.9010590464552823,\n",
       "   -2.9828526975495406,\n",
       "   -3.145496309875589,\n",
       "   -3.2804946833299438,\n",
       "   -3.2531633132738476,\n",
       "   -3.2305025816246635,\n",
       "   -3.1683957728657157,\n",
       "   -3.3433624032293014,\n",
       "   -3.374800836408647,\n",
       "   -3.357409177375576,\n",
       "   -3.420724706019338,\n",
       "   -3.346158053952494,\n",
       "   -3.5302166572623594,\n",
       "   -3.641397680518663,\n",
       "   -3.575347244226222,\n",
       "   -3.506178580592266,\n",
       "   -3.614165325285499,\n",
       "   -3.6832177408030535,\n",
       "   -3.7150176282448406,\n",
       "   -3.908987117091046,\n",
       "   -3.8828698680005296,\n",
       "   -3.9866123911804756,\n",
       "   -4.0610055168464045,\n",
       "   -4.028760148240549,\n",
       "   -4.060110547401392,\n",
       "   -3.9915942238337703,\n",
       "   -3.979993155707759,\n",
       "   -4.099744149356856,\n",
       "   -4.2294192265774475,\n",
       "   -4.2612687777853155,\n",
       "   -4.175127473583883,\n",
       "   -4.014747118059606,\n",
       "   -3.986678807259434,\n",
       "   -4.062763172240415,\n",
       "   -4.012540974816334,\n",
       "   -3.987211953691088,\n",
       "   -3.9188244703276913,\n",
       "   -3.9401729414340636,\n",
       "   -3.8546302052617705,\n",
       "   -3.6583039329379394,\n",
       "   -3.6452957682468297,\n",
       "   -3.7747131895929718,\n",
       "   -3.6345570794658055,\n",
       "   -3.59317887656165,\n",
       "   -3.5896205987668326,\n",
       "   -3.4703668741843634,\n",
       "   -3.5307727873522614,\n",
       "   -3.4670796911844635,\n",
       "   -3.5681781819939653,\n",
       "   -3.6672647169765815,\n",
       "   -3.619110825786258,\n",
       "   -3.667206610430849,\n",
       "   -3.560549052920484,\n",
       "   -3.630761424062573,\n",
       "   -3.5459801308857495,\n",
       "   -3.61564882865773,\n",
       "   -3.8289821063632514,\n",
       "   -3.8999472503523487,\n",
       "   -4.02857610754544,\n",
       "   -4.1897492347099785,\n",
       "   -4.170044734841684,\n",
       "   -3.995160769465974,\n",
       "   -3.9262918738298103,\n",
       "   -4.019780736361489,\n",
       "   -4.037074879843958,\n",
       "   -4.185041188234417,\n",
       "   -3.899713364113837,\n",
       "   -3.8084198146476025,\n",
       "   -3.8911144681555934,\n",
       "   -3.983843089824953,\n",
       "   -4.065571997432869,\n",
       "   -4.115500502233187,\n",
       "   -4.185314709660224,\n",
       "   -4.112461795058701,\n",
       "   -4.080813263519902,\n",
       "   -4.124595274816704,\n",
       "   -4.293524804926526,\n",
       "   -4.40951697476701,\n",
       "   -4.2381039353377785,\n",
       "   -4.073792230379124,\n",
       "   -3.9930059290735693,\n",
       "   -4.141529634112983,\n",
       "   -4.040696661877167,\n",
       "   -4.130947073194846,\n",
       "   -4.154641747925842,\n",
       "   -4.13666512489225,\n",
       "   -4.40592239465468,\n",
       "   -4.490020976229913,\n",
       "   -4.581766880787266,\n",
       "   -4.5664050533096,\n",
       "   -4.532795074610254,\n",
       "   -4.630174677234851,\n",
       "   -4.559361293770776,\n",
       "   -4.597677193878149,\n",
       "   -4.404409005137985,\n",
       "   -4.2809222373460845,\n",
       "   -4.149464899908538,\n",
       "   -4.2869592984945,\n",
       "   -4.195456838049589,\n",
       "   -4.071635611032502,\n",
       "   -4.102417806705539,\n",
       "   -4.134497423264218,\n",
       "   -4.135766949761167,\n",
       "   -4.074828044221626,\n",
       "   -3.888557618087268,\n",
       "   -3.8724041853147035,\n",
       "   -3.666964542189306,\n",
       "   -3.5564849092206003,\n",
       "   -3.5049940571998306,\n",
       "   -3.527562395392601,\n",
       "   -3.5536549653131173,\n",
       "   -3.5447118548670904,\n",
       "   -3.4123793125810975,\n",
       "   -3.344046591231548,\n",
       "   -3.356891077966581,\n",
       "   -3.3314034685386087,\n",
       "   -3.377571760791797,\n",
       "   -3.3811519469988047,\n",
       "   -3.1849154111075686,\n",
       "   -3.2457519560133186,\n",
       "   -3.3972882711875307,\n",
       "   -3.2799858919273888,\n",
       "   -3.23340380294938,\n",
       "   -3.28826574203633,\n",
       "   -3.31563981624325,\n",
       "   -3.3998935111067814,\n",
       "   -3.341335442811048,\n",
       "   -3.4335141557787816,\n",
       "   -3.565192527966678],\n",
       "  'jsc_r2': [-16.43194721278734,\n",
       "   -13.49278654970917,\n",
       "   -11.086529087726333,\n",
       "   -9.300822866821052,\n",
       "   -7.495483271000241,\n",
       "   -5.994166907339518,\n",
       "   -4.835707601339823,\n",
       "   -3.951649672313395,\n",
       "   -3.274042933370012,\n",
       "   -2.7529935088384034,\n",
       "   -2.373858753275793,\n",
       "   -2.065566961327936,\n",
       "   -1.7761197962863382,\n",
       "   -1.61725801768424,\n",
       "   -1.5620935507844154,\n",
       "   -1.590638632261519,\n",
       "   -1.6503436471995254,\n",
       "   -1.683068771202855,\n",
       "   -1.7786907720793832,\n",
       "   -1.8603730228045032,\n",
       "   -1.9202668985447198,\n",
       "   -1.9825800153495434,\n",
       "   -2.0689000719389856,\n",
       "   -2.092938278843202,\n",
       "   -2.045172834399734,\n",
       "   -1.9820335368975934,\n",
       "   -1.9140285835916773,\n",
       "   -1.8346565812120992,\n",
       "   -1.7635387517743903,\n",
       "   -1.7207533879769858,\n",
       "   -1.6173529882020774,\n",
       "   -1.552183228752154,\n",
       "   -1.5213923127021651,\n",
       "   -1.5090494189506032,\n",
       "   -1.4987499797702917,\n",
       "   -1.4265299867374925,\n",
       "   -1.2999139523768486,\n",
       "   -1.3254812934088154,\n",
       "   -1.3997129300412978,\n",
       "   -1.4137654582956714,\n",
       "   -1.39837055405378,\n",
       "   -1.438959981138777,\n",
       "   -1.4834342795320636,\n",
       "   -1.553841614278395,\n",
       "   -1.6330056834261697,\n",
       "   -1.7408677954539802,\n",
       "   -1.8219141448522693,\n",
       "   -1.9261042868866327,\n",
       "   -1.9678204751706883,\n",
       "   -1.9409536618025989,\n",
       "   -2.0095052146164933,\n",
       "   -2.006734207433622,\n",
       "   -1.8900238573743287,\n",
       "   -1.9176100357436536,\n",
       "   -1.8607167548376182,\n",
       "   -1.9149904855393438,\n",
       "   -1.855615488194645,\n",
       "   -1.8539621470873024,\n",
       "   -1.8286135669785706,\n",
       "   -1.9184577892347554,\n",
       "   -1.9159511936327762,\n",
       "   -1.9331621489346338,\n",
       "   -2.0116943373087732,\n",
       "   -1.896579970513614,\n",
       "   -1.8607585386885983,\n",
       "   -1.8499236936746573,\n",
       "   -1.7686113604851434,\n",
       "   -1.7647983763248547,\n",
       "   -1.7791165821678936,\n",
       "   -1.7715776600578863,\n",
       "   -1.867264774599045,\n",
       "   -1.949498640676489,\n",
       "   -1.9517051160311767,\n",
       "   -2.0112317264899042,\n",
       "   -1.9623437076423116,\n",
       "   -1.9852499038398759,\n",
       "   -2.0858700517870785,\n",
       "   -2.0141355688825557,\n",
       "   -2.011518647618897,\n",
       "   -1.981584480966474,\n",
       "   -2.0345125128006383,\n",
       "   -2.0906515492474154,\n",
       "   -2.1376011278898917,\n",
       "   -2.1660059673697254,\n",
       "   -2.268835666369129,\n",
       "   -2.4295840270705575,\n",
       "   -2.3790154318019012,\n",
       "   -2.3938999187895402,\n",
       "   -2.34534256442204,\n",
       "   -2.358697134299799,\n",
       "   -2.3110729265476033,\n",
       "   -2.264535898500853,\n",
       "   -2.4200731358476615,\n",
       "   -2.5212898295188504,\n",
       "   -2.6908594781751205,\n",
       "   -2.6135139050647105,\n",
       "   -2.6155593914910527,\n",
       "   -2.5435046397837238,\n",
       "   -2.7469844140390967,\n",
       "   -2.681005280870986,\n",
       "   -2.525295504543248,\n",
       "   -2.3603707941555974,\n",
       "   -2.3726988955307857,\n",
       "   -2.371204043010519,\n",
       "   -2.2705220150203975,\n",
       "   -2.2470359630256818,\n",
       "   -2.3365016328067534,\n",
       "   -2.2118262496942402,\n",
       "   -2.1473813316256543,\n",
       "   -2.101034042033332,\n",
       "   -1.9831271822630523,\n",
       "   -1.9568090051611753,\n",
       "   -1.833133111087884,\n",
       "   -1.8640377624797986,\n",
       "   -1.859688101228496,\n",
       "   -1.9860843485335118,\n",
       "   -1.9608508337689825,\n",
       "   -1.9341243645042554,\n",
       "   -2.0058025049847887,\n",
       "   -1.9905540112797167,\n",
       "   -2.015633310636412,\n",
       "   -2.0351692103403565,\n",
       "   -2.330515408605867,\n",
       "   -2.547757036088933,\n",
       "   -2.739364166132654,\n",
       "   -2.7466877948575203,\n",
       "   -2.613006538566613,\n",
       "   -2.687217430879265,\n",
       "   -2.8245524531178443,\n",
       "   -2.735379756295686,\n",
       "   -2.7456157074105776,\n",
       "   -2.6495563358617096,\n",
       "   -2.701256721571395,\n",
       "   -2.6732772352905374,\n",
       "   -2.760977288463891,\n",
       "   -2.9617096019252984,\n",
       "   -2.9101406668538874,\n",
       "   -2.997770171270268,\n",
       "   -2.9892898302438686,\n",
       "   -3.009481702176495,\n",
       "   -2.8019553477631787,\n",
       "   -2.5850538522909807,\n",
       "   -2.583142340023418,\n",
       "   -2.5054539952277755,\n",
       "   -2.6273711960153916,\n",
       "   -2.6018077742409966,\n",
       "   -2.738445232702683,\n",
       "   -2.799809287120684,\n",
       "   -2.9977297158139167,\n",
       "   -2.8728390965784802,\n",
       "   -3.2145023299121913,\n",
       "   -3.323132875535375,\n",
       "   -3.435113731960077,\n",
       "   -3.319900891739411,\n",
       "   -3.395397484860198,\n",
       "   -3.337441716776456,\n",
       "   -3.3530552640690328,\n",
       "   -3.2324756713875864,\n",
       "   -3.327903787143292,\n",
       "   -3.1573894189813663,\n",
       "   -3.453882538552497,\n",
       "   -3.5383237470510496,\n",
       "   -3.561655688303749,\n",
       "   -3.6386166273572105,\n",
       "   -3.512550410093933,\n",
       "   -3.5017189927588603,\n",
       "   -3.471362308980318,\n",
       "   -3.548988061284624,\n",
       "   -3.479965619358323,\n",
       "   -3.479991161010176,\n",
       "   -3.498660650289577,\n",
       "   -3.3104992407328773,\n",
       "   -3.4959608629057373,\n",
       "   -3.631760943731262,\n",
       "   -3.4431199791031055,\n",
       "   -3.4278348715735225,\n",
       "   -3.403255282051483,\n",
       "   -3.5332763376036684,\n",
       "   -3.361457946978377,\n",
       "   -3.2636126164550783,\n",
       "   -3.1904517222324804,\n",
       "   -3.413911287244618,\n",
       "   -3.517764881981692,\n",
       "   -3.791567757867864,\n",
       "   -3.8714877301472024,\n",
       "   -4.073300952419716,\n",
       "   -3.848163487326679,\n",
       "   -3.7405027969090057,\n",
       "   -3.8351176736597905,\n",
       "   -3.7168255570711732,\n",
       "   -3.5877264800345543,\n",
       "   -3.8027988673000594,\n",
       "   -3.814120265272912,\n",
       "   -4.062818564020365,\n",
       "   -4.038097494992319,\n",
       "   -3.927451326276465,\n",
       "   -3.9335392914350384,\n",
       "   -4.041239441892468,\n",
       "   -4.1210939531288115,\n",
       "   -4.041336928661674,\n",
       "   -3.9698776111060443,\n",
       "   -3.9934149465081923,\n",
       "   -3.94074459260099,\n",
       "   -3.9778266019152495,\n",
       "   -4.054078275879871,\n",
       "   -3.8848419535691487,\n",
       "   -3.7769885848705265,\n",
       "   -3.919496128833144,\n",
       "   -4.0767397149328835,\n",
       "   -4.069153716877287,\n",
       "   -3.8970901810890446,\n",
       "   -4.074054750645046,\n",
       "   -4.080912934497321,\n",
       "   -4.340903018387225,\n",
       "   -4.438239032714379,\n",
       "   -4.582676209463776,\n",
       "   -4.580799546829136,\n",
       "   -4.900459829919829,\n",
       "   -5.043875200028873,\n",
       "   -5.097566144628926,\n",
       "   -4.836887355214188,\n",
       "   -4.816342835263337,\n",
       "   -4.381877616682058,\n",
       "   -3.9706571648422893,\n",
       "   -3.763382526285257,\n",
       "   -3.651508932420528,\n",
       "   -3.7739299510258935,\n",
       "   -3.8986936814697,\n",
       "   -3.7850247072467074,\n",
       "   -3.5655128198905377,\n",
       "   -3.3783900545379737,\n",
       "   -3.3271161732182035,\n",
       "   -3.4135801110712647,\n",
       "   -3.6189172747490144,\n",
       "   -3.3416349281384274,\n",
       "   -3.36775380900327,\n",
       "   -3.471547488183245,\n",
       "   -3.5306189435298547,\n",
       "   -3.6518194083175404,\n",
       "   -4.134909456068725,\n",
       "   -4.311403498766197,\n",
       "   -4.168151432639862,\n",
       "   -4.226999490766652,\n",
       "   -4.519004876599423,\n",
       "   -4.806196149861208,\n",
       "   -5.144662378086494,\n",
       "   -5.373592758503434,\n",
       "   -5.189804481668268,\n",
       "   -5.357773946003139,\n",
       "   -5.511884396969966,\n",
       "   -5.641908924287077,\n",
       "   -5.655082442897273,\n",
       "   -5.11623451782853,\n",
       "   -5.354631751599197,\n",
       "   -5.6904717985654205,\n",
       "   -6.441151739599164,\n",
       "   -6.527333389741782,\n",
       "   -6.525708604753785,\n",
       "   -7.178492200329416,\n",
       "   -6.620022325346186,\n",
       "   -6.276468848753306,\n",
       "   -6.904777079898542,\n",
       "   -6.254410642183298,\n",
       "   -6.590803496063075,\n",
       "   -7.180119760462047,\n",
       "   -7.657185967245557,\n",
       "   -6.855000250403286,\n",
       "   -7.067754073149134,\n",
       "   -7.661416897774306,\n",
       "   -6.585824225658204,\n",
       "   -6.252521982890638,\n",
       "   -6.916030137218944,\n",
       "   -6.803767093504102,\n",
       "   -7.004616602563372,\n",
       "   -6.198350914676764,\n",
       "   -6.242965849847001,\n",
       "   -5.926060902744268,\n",
       "   -4.981635686269892,\n",
       "   -4.536018708845989,\n",
       "   -4.451411351040536,\n",
       "   -4.508384471346693,\n",
       "   -4.17428936460611,\n",
       "   -4.304501134138078,\n",
       "   -4.17016952824241,\n",
       "   -3.603624291566538,\n",
       "   -2.401738766747225,\n",
       "   -1.832782274687553,\n",
       "   -1.7225048248909882,\n",
       "   -2.297723240455084,\n",
       "   -2.3477808335440757,\n",
       "   -2.408481204363583,\n",
       "   -1.8316789499827402,\n",
       "   -1.7907837110136002,\n",
       "   -1.657255772263429,\n",
       "   -1.537736931698222,\n",
       "   -1.4437396596047156,\n",
       "   -1.5250143912055907,\n",
       "   -1.3239979460509095,\n",
       "   -1.2783892857723118,\n",
       "   -1.3435544820359149,\n",
       "   -2.068049342722461,\n",
       "   -2.021700503667548,\n",
       "   -1.849983007423869,\n",
       "   -1.498896672655857,\n",
       "   -1.8745159655296,\n",
       "   -1.9023575922878924,\n",
       "   -1.390410352823158,\n",
       "   -1.415204972225454,\n",
       "   -1.0941749671334349,\n",
       "   -1.0408444746164243,\n",
       "   -0.7881888314210355,\n",
       "   -0.8017922919000748,\n",
       "   -1.1255912053313333,\n",
       "   -1.2192959327758777,\n",
       "   -1.3032124179371292,\n",
       "   -1.9713360372510307,\n",
       "   -4.296205982032018,\n",
       "   -3.869365783195537,\n",
       "   -3.957232029129119,\n",
       "   -4.074660727805015,\n",
       "   -4.085497934695365,\n",
       "   -3.2511897388797832,\n",
       "   -4.2743970435344325,\n",
       "   -4.382121240336263,\n",
       "   -3.1613962317726143,\n",
       "   -3.0232268140369003,\n",
       "   -2.712421697118351,\n",
       "   -3.8305923593046414,\n",
       "   -3.5364392634698563,\n",
       "   -2.9592864200998483,\n",
       "   -3.0678945298277362,\n",
       "   -3.165152134249116,\n",
       "   -4.222722126674015,\n",
       "   -3.9334968076386687,\n",
       "   -5.3154736232585345,\n",
       "   -4.304179729493169,\n",
       "   -2.5585554256721625,\n",
       "   -2.7544123122607824,\n",
       "   -2.7710409282689863,\n",
       "   -2.459962695490961,\n",
       "   -2.087229062220075,\n",
       "   -2.433919781091248,\n",
       "   -2.188353321922533,\n",
       "   -2.8256065185991517,\n",
       "   -2.1847809420491062,\n",
       "   -2.8596872076717297,\n",
       "   -2.391291294925103,\n",
       "   -2.123834442762009,\n",
       "   -2.0647262396908914,\n",
       "   -1.8080075699810765,\n",
       "   -2.476812735926139,\n",
       "   -2.224243241640899,\n",
       "   -2.308384088597846,\n",
       "   -2.695437827361762,\n",
       "   -3.61637738427948,\n",
       "   -3.206815240356325,\n",
       "   -2.040868818801081,\n",
       "   -1.8880222038946028,\n",
       "   -1.2634847657080126,\n",
       "   -0.9721946320165125,\n",
       "   -1.169427854095539,\n",
       "   -1.0652022060766533,\n",
       "   -0.6009062606321218,\n",
       "   -0.9270133706131687,\n",
       "   -1.4423578821591239,\n",
       "   -1.2134686342528873,\n",
       "   -1.1982580160133818,\n",
       "   -1.2158729753319815,\n",
       "   -1.0976600946382793,\n",
       "   -1.0199786391778773,\n",
       "   -1.09532878178294,\n",
       "   -1.2283545495513257,\n",
       "   -1.4118716339169612,\n",
       "   -1.291939450331368,\n",
       "   -1.3202687031833569,\n",
       "   -1.7257484995995687,\n",
       "   -2.4050773075572005,\n",
       "   -2.8772111052014058,\n",
       "   -3.2141624410303837,\n",
       "   -4.1900905647599185,\n",
       "   -6.4255160272994,\n",
       "   -8.017383032346235,\n",
       "   -11.7360247313016,\n",
       "   -14.363206995043386,\n",
       "   -12.999311005063811,\n",
       "   -11.906129876166531,\n",
       "   -7.398158016365475,\n",
       "   -11.056714219433255,\n",
       "   -15.2772837495068,\n",
       "   -14.562250614589663,\n",
       "   -15.287698333183624,\n",
       "   -9.146567140666928,\n",
       "   -9.702361860525428,\n",
       "   -10.029108909052027,\n",
       "   -6.374994737802874,\n",
       "   -8.649323111295429,\n",
       "   -6.202633756329966,\n",
       "   -6.50678059006324,\n",
       "   -6.426488687462681,\n",
       "   -5.6782866443878905,\n",
       "   -6.2357499586322485,\n",
       "   -4.702124838071848,\n",
       "   -3.9600248534479423,\n",
       "   -5.524606805614028,\n",
       "   -6.915892440090363,\n",
       "   -9.332788894204956,\n",
       "   -9.802825505109357,\n",
       "   -9.812537889815966,\n",
       "   -9.158844601388065,\n",
       "   -11.235346766415987,\n",
       "   -11.196363194738566,\n",
       "   -9.388487182366923,\n",
       "   -11.59217810390867,\n",
       "   -12.65624782061707,\n",
       "   -13.792712290443406,\n",
       "   -12.528482704811076,\n",
       "   -12.990084215485616,\n",
       "   -10.363063407476295,\n",
       "   -8.23824801779826,\n",
       "   -6.105117648988883,\n",
       "   -6.7483028548264254,\n",
       "   -8.833815342137855,\n",
       "   -7.005892909132168,\n",
       "   -7.02721019598223,\n",
       "   -8.218182902085646,\n",
       "   -9.434700963456596,\n",
       "   -6.819730385360091,\n",
       "   -5.0581604146372,\n",
       "   -4.163400549005318,\n",
       "   -4.1403004405519646,\n",
       "   -3.804840761078335,\n",
       "   -4.793825382467065,\n",
       "   -7.375582554576763,\n",
       "   -9.285494680532102,\n",
       "   -10.944034813103391,\n",
       "   -11.139469401185957,\n",
       "   -13.725698795054145,\n",
       "   -15.146964657736856,\n",
       "   -14.036220279477188,\n",
       "   -13.791739872654343,\n",
       "   -12.561582434802103,\n",
       "   -10.971978192626489,\n",
       "   -10.014445515148473,\n",
       "   -9.677757957371348,\n",
       "   -7.465603122007018,\n",
       "   -7.392344590674782,\n",
       "   -6.597505588861815,\n",
       "   -4.620495250603287,\n",
       "   -3.4998164460393175,\n",
       "   -2.9313700159213916,\n",
       "   -1.6682867810192468,\n",
       "   -2.452303003944436,\n",
       "   -1.9688684839988926,\n",
       "   -2.2992521340407195,\n",
       "   -2.8732582077474347,\n",
       "   -2.3297976281645947,\n",
       "   -2.1787815061432356,\n",
       "   -2.174171786470265,\n",
       "   -2.764888599296458,\n",
       "   -4.021484632346659,\n",
       "   -4.25460986416158,\n",
       "   -4.103700950942333,\n",
       "   -3.8773315549345764,\n",
       "   -4.224036504850675,\n",
       "   -3.8280496171424643,\n",
       "   -3.319927447442222,\n",
       "   -2.7540134877522817,\n",
       "   -2.4435025011015394,\n",
       "   -2.810182681722277,\n",
       "   -3.7003258746387235,\n",
       "   -2.9646615704149784,\n",
       "   -2.8921457032088442,\n",
       "   -2.2649196649746246,\n",
       "   -2.8459914877582504,\n",
       "   -3.1174687645250065,\n",
       "   -4.695451993636834,\n",
       "   -4.54443445337922,\n",
       "   -5.507716739489922,\n",
       "   -7.021043254223597,\n",
       "   -5.275799996475541,\n",
       "   -4.725009538231366,\n",
       "   -3.624843587787372,\n",
       "   -2.6431157658904714,\n",
       "   -2.588062943426057,\n",
       "   -2.524827595638137,\n",
       "   -2.424343515443192,\n",
       "   -2.7951599113319907,\n",
       "   -3.064344263832802,\n",
       "   -2.6245571497857534,\n",
       "   -2.8618859307204687,\n",
       "   -2.30625665286848,\n",
       "   -2.5033039968229085,\n",
       "   -3.0147093893393713,\n",
       "   -1.7778269683713845,\n",
       "   -1.9841161425079363,\n",
       "   -1.8098423935237449,\n",
       "   -1.8125004416062498,\n",
       "   -1.8946140308331323,\n",
       "   -2.614217852158947,\n",
       "   -3.061905741976931],\n",
       "  'ff_r2': [-0.5111847618507122,\n",
       "   -0.5633397028440976,\n",
       "   -0.5935868287997821,\n",
       "   -0.710770704664305,\n",
       "   -0.656299046570415,\n",
       "   -0.7791402276637245,\n",
       "   -0.6734706136448942,\n",
       "   -0.5959048545398722,\n",
       "   -0.4428222234404968,\n",
       "   -0.3501370668584951,\n",
       "   -0.2629633607536037,\n",
       "   -0.19147627727276806,\n",
       "   -0.2447246088118169,\n",
       "   -0.22246281905698018,\n",
       "   -0.2832648420799839,\n",
       "   -0.27666545879353666,\n",
       "   -0.3824036932481032,\n",
       "   -0.4483491977250391,\n",
       "   -0.48503924224399064,\n",
       "   -0.6199742414892131,\n",
       "   -0.776151982305539,\n",
       "   -1.0236759018022799,\n",
       "   -1.2340636342674984,\n",
       "   -1.3605628287483373,\n",
       "   -1.5947715145478134,\n",
       "   -1.828085600706466,\n",
       "   -2.2488781405228786,\n",
       "   -2.4235058980947812,\n",
       "   -2.8874475097396624,\n",
       "   -3.254757407835992,\n",
       "   -3.594561466054591,\n",
       "   -4.243127804565997,\n",
       "   -4.973883751097089,\n",
       "   -5.404723462336104,\n",
       "   -5.836622140263354,\n",
       "   -6.2054391527675365,\n",
       "   -6.351361465315894,\n",
       "   -7.494880632266405,\n",
       "   -8.313860364029429,\n",
       "   -9.737459375519855,\n",
       "   -10.928106030702802,\n",
       "   -13.260482345329965,\n",
       "   -14.337216742516297,\n",
       "   -15.021792896564008,\n",
       "   -15.770969757468315,\n",
       "   -15.974026202954846,\n",
       "   -18.885531811164796,\n",
       "   -21.30448606020511,\n",
       "   -23.74795131700694,\n",
       "   -24.679677300254117,\n",
       "   -26.902863947138087,\n",
       "   -29.554943221185248,\n",
       "   -31.187540987591596,\n",
       "   -27.96661718144539,\n",
       "   -28.969346977425438,\n",
       "   -28.983545565645993,\n",
       "   -29.328217788468425,\n",
       "   -28.47761392591824,\n",
       "   -28.98128745980121,\n",
       "   -28.992015335881078,\n",
       "   -30.147568387325094,\n",
       "   -29.842438537308347,\n",
       "   -29.11965401780079,\n",
       "   -29.603540685241214,\n",
       "   -27.295732623855027,\n",
       "   -25.422059508518593,\n",
       "   -25.239063742578466,\n",
       "   -24.37306761178855,\n",
       "   -22.093130765523714,\n",
       "   -23.40432633971926,\n",
       "   -24.419222786743333,\n",
       "   -23.2529990077029,\n",
       "   -23.819656942363387,\n",
       "   -23.26124855284546,\n",
       "   -23.00808070302784,\n",
       "   -23.414252771732507,\n",
       "   -22.435395622088958,\n",
       "   -21.75007566361943,\n",
       "   -20.82155274700185,\n",
       "   -22.260088383994752,\n",
       "   -21.29272367573702,\n",
       "   -20.84038420194208,\n",
       "   -21.27866393152361,\n",
       "   -22.48536023718789,\n",
       "   -22.86706642764139,\n",
       "   -20.716002093455188,\n",
       "   -20.731818647348696,\n",
       "   -20.70473452662262,\n",
       "   -21.114731643137734,\n",
       "   -20.82224630320934,\n",
       "   -21.331362472420096,\n",
       "   -20.252806510458555,\n",
       "   -19.513546572130632,\n",
       "   -18.830014772425958,\n",
       "   -19.380780779175137,\n",
       "   -18.71808039371538,\n",
       "   -16.70425699790807,\n",
       "   -15.241619490436989,\n",
       "   -15.00700740934695,\n",
       "   -14.243185653102866,\n",
       "   -13.451431252852988,\n",
       "   -15.324713278261637,\n",
       "   -14.91270736920861,\n",
       "   -13.825192802903926,\n",
       "   -14.196060301041406,\n",
       "   -14.924381302425246,\n",
       "   -15.407061754338152,\n",
       "   -17.88043015882343,\n",
       "   -17.560223593838867,\n",
       "   -17.132450340035803,\n",
       "   -16.564891176347828,\n",
       "   -17.233167932171305,\n",
       "   -15.841070543547474,\n",
       "   -14.468167350094513,\n",
       "   -13.014565391128698,\n",
       "   -12.859530359007927,\n",
       "   -12.501215572224657,\n",
       "   -11.43838357763292,\n",
       "   -12.11756438890718,\n",
       "   -11.733682859474722,\n",
       "   -11.452585673316728,\n",
       "   -11.810631002304904,\n",
       "   -11.334111975907819,\n",
       "   -10.565788581679184,\n",
       "   -10.187233973515264,\n",
       "   -9.514107999118616,\n",
       "   -8.480611965718332,\n",
       "   -7.978900953829921,\n",
       "   -7.829896537605496,\n",
       "   -7.073701060007977,\n",
       "   -6.174574316046764,\n",
       "   -5.712143641633955,\n",
       "   -5.255302779970008,\n",
       "   -4.942686950696961,\n",
       "   -4.448029177673597,\n",
       "   -4.620922574566975,\n",
       "   -4.472329610290571,\n",
       "   -4.7325555291479535,\n",
       "   -4.623683079377324,\n",
       "   -4.494169389149064,\n",
       "   -4.31129612364092,\n",
       "   -4.182257800944793,\n",
       "   -4.393189781243131,\n",
       "   -4.330384551151602,\n",
       "   -4.062766435617891,\n",
       "   -4.365635911808746,\n",
       "   -4.2566739911705955,\n",
       "   -4.427284577454717,\n",
       "   -4.31935487354202,\n",
       "   -3.899215577383738,\n",
       "   -3.9254132440622893,\n",
       "   -3.8367275858021115,\n",
       "   -4.0282840713644195,\n",
       "   -4.4530392393008755,\n",
       "   -4.529661706749959,\n",
       "   -4.503991176480165,\n",
       "   -4.626559286021307,\n",
       "   -4.733787926600021,\n",
       "   -5.169064656656131,\n",
       "   -5.654292634425309,\n",
       "   -6.115508102836786,\n",
       "   -6.011041634652994,\n",
       "   -5.73328831903517,\n",
       "   -5.8887885882078885,\n",
       "   -5.611276247847578,\n",
       "   -5.3582271705648035,\n",
       "   -5.855971016154934,\n",
       "   -5.457478377958018,\n",
       "   -4.944667011962501,\n",
       "   -4.784001015735418,\n",
       "   -4.370191791066585,\n",
       "   -4.362624180276017,\n",
       "   -4.582450206494105,\n",
       "   -4.773182375769061,\n",
       "   -4.403162978547128,\n",
       "   -4.5660676530617295,\n",
       "   -4.335844264735251,\n",
       "   -4.177417399428724,\n",
       "   -3.7127771410348522,\n",
       "   -3.6037605767409966,\n",
       "   -3.5150059326577985,\n",
       "   -3.470117598816162,\n",
       "   -3.3918309748529687,\n",
       "   -3.6389823839977202,\n",
       "   -3.2563044517321167,\n",
       "   -3.2124434997219318,\n",
       "   -3.1514613473315434,\n",
       "   -2.9694030350266023,\n",
       "   -3.2131352481461546,\n",
       "   -3.4956736560677415,\n",
       "   -3.5046893056237396,\n",
       "   -3.7033050570490813,\n",
       "   -3.737902099975982,\n",
       "   -3.504718872915044,\n",
       "   -3.4703977180543903,\n",
       "   -3.5690071691169223,\n",
       "   -4.048542855398093,\n",
       "   -4.0956417841508665,\n",
       "   -3.718978859840731,\n",
       "   -3.749852345814488,\n",
       "   -3.724171267467135,\n",
       "   -3.7159250448470678,\n",
       "   -3.756930965878313,\n",
       "   -3.4763061814685754,\n",
       "   -3.244118945871005,\n",
       "   -3.142867154916064,\n",
       "   -2.9892314450926785,\n",
       "   -3.07759158285959,\n",
       "   -3.0507039164963397,\n",
       "   -2.9227646898715083,\n",
       "   -2.951956665897172,\n",
       "   -3.1115415795244914,\n",
       "   -3.221031561108391,\n",
       "   -3.433832202702213,\n",
       "   -3.866246344867861,\n",
       "   -4.012307208166141,\n",
       "   -4.452474263601958,\n",
       "   -4.596417820073019,\n",
       "   -4.688625012291823,\n",
       "   -4.94146700003119,\n",
       "   -5.061213036709474,\n",
       "   -5.404326792379831,\n",
       "   -4.97344394022242,\n",
       "   -5.084821115772224,\n",
       "   -5.050134963125937,\n",
       "   -4.356250447827716,\n",
       "   -4.13092315119142,\n",
       "   -3.8913418504443467,\n",
       "   -3.715548893540025,\n",
       "   -3.6294034706593905,\n",
       "   -3.1731572627574334,\n",
       "   -2.938514091540374,\n",
       "   -2.7637729142621206,\n",
       "   -2.548135047867932,\n",
       "   -2.6095567341092023,\n",
       "   -2.3912796066083972,\n",
       "   -2.354398903685481,\n",
       "   -2.3542767493304857,\n",
       "   -2.3293752254077402,\n",
       "   -2.342054487365547,\n",
       "   -2.3362599900921657,\n",
       "   -2.3393226469250084,\n",
       "   -2.1073625905465887,\n",
       "   -2.246378026284413,\n",
       "   -2.557842321424848,\n",
       "   -2.8469088493075136,\n",
       "   -3.235275287455175,\n",
       "   -3.801597177475707,\n",
       "   -4.054209186353456,\n",
       "   -4.01336719809667,\n",
       "   -4.166961100066489,\n",
       "   -4.134763306089621,\n",
       "   -4.134006817402201,\n",
       "   -4.20932529863584,\n",
       "   -4.054915447277681,\n",
       "   -4.129720101121666,\n",
       "   -4.344126724563522,\n",
       "   -4.469673596963556,\n",
       "   -4.641510097652765,\n",
       "   -5.053442512936649,\n",
       "   -5.550987075863899,\n",
       "   -5.414614864577019,\n",
       "   -5.483929098905614,\n",
       "   -5.827660269539234,\n",
       "   -5.928796116309282,\n",
       "   -6.396022029155317,\n",
       "   -6.005830136707244,\n",
       "   -5.273825312871567,\n",
       "   -5.230013234554572,\n",
       "   -4.9573415408386134,\n",
       "   -4.944300468961757,\n",
       "   -5.112157744216096,\n",
       "   -5.202526679805817,\n",
       "   -5.201842657965448,\n",
       "   -4.949449095243316,\n",
       "   -4.698167895695746,\n",
       "   -4.545260640796078,\n",
       "   -4.7971071855621075,\n",
       "   -4.77782194267689,\n",
       "   -4.589725746402262,\n",
       "   -4.297731855484912,\n",
       "   -4.517382469127762,\n",
       "   -4.050410514810498,\n",
       "   -3.9529990987048818,\n",
       "   -4.011140182421182,\n",
       "   -3.74923537234155,\n",
       "   -3.640703936377845,\n",
       "   -3.801463591771559,\n",
       "   -3.5847165313687546,\n",
       "   -3.4899612598349776,\n",
       "   -3.62720585154172,\n",
       "   -3.6807916893252344,\n",
       "   -3.7583255385986893,\n",
       "   -3.911212934764551,\n",
       "   -3.982679748237322,\n",
       "   -3.908226275418519,\n",
       "   -4.203104823922094,\n",
       "   -4.498468351840026,\n",
       "   -4.9861088458826925,\n",
       "   -5.593558847211217,\n",
       "   -5.4836108177416465,\n",
       "   -5.582403673540158,\n",
       "   -5.865295731540879,\n",
       "   -5.5941477038758025,\n",
       "   -5.84712346899968,\n",
       "   -6.052814385857522,\n",
       "   -5.774232071163395,\n",
       "   -6.45049984826774,\n",
       "   -7.074637368611937,\n",
       "   -7.282671007770066,\n",
       "   -6.530738195943067,\n",
       "   -6.994969602508339,\n",
       "   -6.827203885235626,\n",
       "   -6.89440843542061,\n",
       "   -7.134218042568616,\n",
       "   -7.188634003564518,\n",
       "   -7.044716189367541,\n",
       "   -6.678227833056868,\n",
       "   -6.616566039385566,\n",
       "   -6.086790354987414,\n",
       "   -6.115308407575225,\n",
       "   -6.044352140558222,\n",
       "   -6.271056772954149,\n",
       "   -6.436232483151359,\n",
       "   -6.205572221128915,\n",
       "   -6.461136870263711,\n",
       "   -6.826816847330968,\n",
       "   -6.322527548962932,\n",
       "   -5.820667284560511,\n",
       "   -6.170734642368587,\n",
       "   -6.339049892485179,\n",
       "   -6.161587663144222,\n",
       "   -6.115917744504161,\n",
       "   -6.170391397630806,\n",
       "   -6.172996489061295,\n",
       "   -7.070377522380694,\n",
       "   -7.375913863872309,\n",
       "   -6.830953751378416,\n",
       "   -6.952620418443736,\n",
       "   -7.283010660490843,\n",
       "   -7.913776298964532,\n",
       "   -7.089752632278024,\n",
       "   -7.160573251635645,\n",
       "   -7.158050342003033,\n",
       "   -6.997109420987962,\n",
       "   -7.327772221526571,\n",
       "   -7.335939902393122,\n",
       "   -7.1119431682176355,\n",
       "   -6.469944181185123,\n",
       "   -6.667213427593625,\n",
       "   -6.2792601252839555,\n",
       "   -6.439965937202218,\n",
       "   -6.090456333159712,\n",
       "   -5.561905115655724,\n",
       "   -5.315622734810657,\n",
       "   -5.765495570727984,\n",
       "   -5.668984315078811,\n",
       "   -5.707833010770347,\n",
       "   -5.746729240560179,\n",
       "   -5.353554442711379,\n",
       "   -5.460973290807957,\n",
       "   -5.91870821772541,\n",
       "   -5.990028967496372,\n",
       "   -6.492974703420561,\n",
       "   -7.117752384488014,\n",
       "   -6.985869279590311,\n",
       "   -6.941233300796701,\n",
       "   -6.837683877469739,\n",
       "   -7.2148841406347355,\n",
       "   -7.901228106670507,\n",
       "   -7.909026782628752,\n",
       "   -7.521458838924223,\n",
       "   -8.001591404574434,\n",
       "   -8.81152623510227,\n",
       "   -8.183711897482661,\n",
       "   -7.65933920746104,\n",
       "   -7.900385378101452,\n",
       "   -8.345471201290625,\n",
       "   -7.868522515076849,\n",
       "   -7.902047112856488,\n",
       "   -8.346749088890173,\n",
       "   -8.087326069197845,\n",
       "   -8.562162172118144,\n",
       "   -8.824308739210885,\n",
       "   -9.094330455746313,\n",
       "   -9.393812435208924,\n",
       "   -9.391560048189216,\n",
       "   -8.983612094753255,\n",
       "   -9.23709495818182,\n",
       "   -8.702951879290332,\n",
       "   -8.451239133532988,\n",
       "   -8.983202456433174,\n",
       "   -8.700686632090282,\n",
       "   -9.170769996016325,\n",
       "   -10.09389200277248,\n",
       "   -9.907396688107813,\n",
       "   -9.652682773656887,\n",
       "   -8.7786455261299,\n",
       "   -8.40412205002618,\n",
       "   -8.80610381852796,\n",
       "   -8.096512440938369,\n",
       "   -7.471614335024791,\n",
       "   -7.366683105515483,\n",
       "   -6.524266119432371,\n",
       "   -6.019087655587526,\n",
       "   -5.844589909066814,\n",
       "   -6.374801657318377,\n",
       "   -5.6939318290992045,\n",
       "   -5.384171655849944,\n",
       "   -5.478410720379134,\n",
       "   -5.605371662587517,\n",
       "   -5.479249830337034,\n",
       "   -5.06146205250058,\n",
       "   -5.441453090888096,\n",
       "   -5.607714047670413,\n",
       "   -6.516347371472583,\n",
       "   -6.21296882298395,\n",
       "   -6.287647456474079,\n",
       "   -6.434987727478831,\n",
       "   -6.266702564530679,\n",
       "   -6.479735383241183,\n",
       "   -6.936781170260983,\n",
       "   -6.785843290456379,\n",
       "   -6.216924741056507,\n",
       "   -5.922269288359906,\n",
       "   -5.602255407489121,\n",
       "   -5.53258126538467,\n",
       "   -5.421453835404844,\n",
       "   -6.017150088131525,\n",
       "   -6.115023900100924,\n",
       "   -5.922682426789207,\n",
       "   -5.631583959998636,\n",
       "   -5.592086098679891,\n",
       "   -5.798706361776763,\n",
       "   -5.408051739505914,\n",
       "   -5.33025244204119,\n",
       "   -5.454494513995509,\n",
       "   -5.768636013424318,\n",
       "   -6.071299731740584,\n",
       "   -6.365856445386916,\n",
       "   -6.257864209152016,\n",
       "   -6.849244183150585,\n",
       "   -7.012095451953062,\n",
       "   -6.987387552087403,\n",
       "   -7.154265540893924,\n",
       "   -7.473033256331016,\n",
       "   -7.55986877579195,\n",
       "   -7.688862159284531,\n",
       "   -7.347828450552347,\n",
       "   -7.147783387904349,\n",
       "   -7.040947303346748,\n",
       "   -6.427859574706109,\n",
       "   -6.031602468575571,\n",
       "   -6.088299826566339,\n",
       "   -6.023647785345272,\n",
       "   -5.564626489507223,\n",
       "   -5.464192514985762,\n",
       "   -5.097842203637482,\n",
       "   -5.148009977916779,\n",
       "   -5.389105168042421,\n",
       "   -5.156680143482718,\n",
       "   -5.144006088058592,\n",
       "   -5.53343509003087,\n",
       "   -5.7914795647948605,\n",
       "   -6.024819849348261,\n",
       "   -5.841456437645366,\n",
       "   -5.792569534359036,\n",
       "   -5.920341719579851,\n",
       "   -5.9446876199782706,\n",
       "   -6.26941000167069,\n",
       "   -6.306143883527879,\n",
       "   -6.220477051250311,\n",
       "   -6.391443329138897,\n",
       "   -6.204398289498471,\n",
       "   -6.163568826976474,\n",
       "   -6.082720918158321,\n",
       "   -6.358692334040865,\n",
       "   -6.479189923939076,\n",
       "   -6.662758259626084,\n",
       "   -6.517850135740798,\n",
       "   -6.501821496688377,\n",
       "   -6.323557580838386,\n",
       "   -6.067432696869516,\n",
       "   -6.145536868199706,\n",
       "   -6.094292468520807,\n",
       "   -6.1979795530447666,\n",
       "   -6.124391618740937,\n",
       "   -5.871979740869325,\n",
       "   -6.0901417459395875,\n",
       "   -6.117900771434657,\n",
       "   -6.3590798772010055,\n",
       "   -6.503980225618496,\n",
       "   -6.550323683987629,\n",
       "   -6.440468848438028,\n",
       "   -6.665230666558335,\n",
       "   -6.950464141596128,\n",
       "   -7.225596685852469,\n",
       "   -7.59905108866119,\n",
       "   -7.640285757063696,\n",
       "   -8.219019717157972],\n",
       "  'test_r2s': [-55.20876943423272,\n",
       "   -37.19055394802397,\n",
       "   -30.373961857938912,\n",
       "   -28.417089412397857,\n",
       "   -23.514391252121786,\n",
       "   -17.781255021688185,\n",
       "   -14.808536621535193,\n",
       "   -11.82373297625501,\n",
       "   -9.163756030492397,\n",
       "   -7.341498355330124,\n",
       "   -5.070787669430081,\n",
       "   -3.716180246383872,\n",
       "   -3.2740223262097166,\n",
       "   -3.2497389132819867,\n",
       "   -3.424838350612778,\n",
       "   -3.1245162428379745,\n",
       "   -3.011046795851448,\n",
       "   -2.969355161680755,\n",
       "   -2.785936808898386,\n",
       "   -2.9370178296934237,\n",
       "   -3.03591154332584,\n",
       "   -3.380081816257978,\n",
       "   -3.7323116489192625,\n",
       "   -3.9302638355104205,\n",
       "   -4.1481122815229226,\n",
       "   -4.517059092358872,\n",
       "   -5.061883659892114,\n",
       "   -5.4817056406460365,\n",
       "   -6.042752805174932,\n",
       "   -6.509879189256833,\n",
       "   -7.256795038885771,\n",
       "   -8.563749121359765,\n",
       "   -9.185174646776726,\n",
       "   -9.90048587039744,\n",
       "   -10.34986329705501,\n",
       "   -10.7285902847319,\n",
       "   -10.92330429369553,\n",
       "   -12.235954977103376,\n",
       "   -12.942125196385724,\n",
       "   -14.481747876994438,\n",
       "   -16.36738701319813,\n",
       "   -18.761497020944358,\n",
       "   -20.131010211163527,\n",
       "   -21.132624921609423,\n",
       "   -22.570597817774015,\n",
       "   -22.963125616840713,\n",
       "   -26.203425079964507,\n",
       "   -28.99435462656222,\n",
       "   -31.85465525068971,\n",
       "   -32.902692601475636,\n",
       "   -35.50619323198354,\n",
       "   -37.99703169420211,\n",
       "   -39.78274190321069,\n",
       "   -37.565276404664374,\n",
       "   -38.308926711328105,\n",
       "   -38.56241501452207,\n",
       "   -38.8930174623936,\n",
       "   -38.629052489753875,\n",
       "   -38.70717671306135,\n",
       "   -39.73364061230172,\n",
       "   -41.85837580093988,\n",
       "   -42.258728482102725,\n",
       "   -42.36317228982581,\n",
       "   -42.78493518746133,\n",
       "   -41.05328207061939,\n",
       "   -40.12855427497599,\n",
       "   -39.675934238932754,\n",
       "   -38.751751567209396,\n",
       "   -37.17248456635329,\n",
       "   -40.131141234985535,\n",
       "   -40.64585817460976,\n",
       "   -38.58880754436775,\n",
       "   -39.86794323346051,\n",
       "   -40.74222934432282,\n",
       "   -40.76653280779776,\n",
       "   -42.074650389176725,\n",
       "   -41.4548537910356,\n",
       "   -42.0823350536324,\n",
       "   -41.90791646005742,\n",
       "   -42.44584668266913,\n",
       "   -42.15445512280335,\n",
       "   -41.53010915467377,\n",
       "   -42.73825171288448,\n",
       "   -44.5235215717136,\n",
       "   -45.69951525862892,\n",
       "   -44.55837412317396,\n",
       "   -43.548760202868664,\n",
       "   -43.535648702722106,\n",
       "   -44.50653820346718,\n",
       "   -44.464782994888246,\n",
       "   -44.15939358174687,\n",
       "   -44.33147663899935,\n",
       "   -43.493389867289814,\n",
       "   -43.63837374239508,\n",
       "   -46.58431889360286,\n",
       "   -44.37069407234491,\n",
       "   -40.649791595414726,\n",
       "   -39.6397425975486,\n",
       "   -41.626264684904854,\n",
       "   -40.184094846199486,\n",
       "   -38.95038547723492,\n",
       "   -41.07757146127722,\n",
       "   -40.580979415376206,\n",
       "   -39.40656242125416,\n",
       "   -40.063342359226795,\n",
       "   -39.67225349077664,\n",
       "   -39.67039744910413,\n",
       "   -41.770983718931525,\n",
       "   -41.1416171702179,\n",
       "   -39.66518590526623,\n",
       "   -37.78584112108297,\n",
       "   -38.14448658162502,\n",
       "   -37.33735820228253,\n",
       "   -35.986443272962354,\n",
       "   -34.11571196880766,\n",
       "   -34.17050728197104,\n",
       "   -34.65978309978298,\n",
       "   -34.27470689485018,\n",
       "   -34.27618279729562,\n",
       "   -33.99098119354419,\n",
       "   -33.955385921302,\n",
       "   -34.53606293456568,\n",
       "   -35.27576341340748,\n",
       "   -32.72983465819021,\n",
       "   -31.273907136873575,\n",
       "   -30.324842830941108,\n",
       "   -30.580939062880333,\n",
       "   -28.5297874813912,\n",
       "   -27.1422555503315,\n",
       "   -25.31970510703584,\n",
       "   -22.900731516416485,\n",
       "   -21.81897935952659,\n",
       "   -20.00503961866156,\n",
       "   -19.51491378841982,\n",
       "   -18.997378078224287,\n",
       "   -20.27350717576866,\n",
       "   -20.435461776884157,\n",
       "   -20.169649024118854,\n",
       "   -18.868957590807447,\n",
       "   -18.736010681326597,\n",
       "   -18.182932291223686,\n",
       "   -17.520932104814356,\n",
       "   -17.60224789588916,\n",
       "   -17.217655753784967,\n",
       "   -17.073658671272604,\n",
       "   -17.690184799083912,\n",
       "   -16.788069674996876,\n",
       "   -16.712053172209515,\n",
       "   -16.74281102602467,\n",
       "   -16.097048336695135,\n",
       "   -16.275679557015078,\n",
       "   -16.006583604899216,\n",
       "   -16.705189432510437,\n",
       "   -16.78163362980184,\n",
       "   -17.309234455943447,\n",
       "   -17.424501357299683,\n",
       "   -17.48246279507756,\n",
       "   -16.892955644490858,\n",
       "   -17.235234586875677,\n",
       "   -18.015870936222647,\n",
       "   -18.07050320551695,\n",
       "   -17.974897910375553,\n",
       "   -18.140974086449177,\n",
       "   -18.689074536509764,\n",
       "   -18.296891257836513,\n",
       "   -17.567597506961626,\n",
       "   -17.97651694609713,\n",
       "   -17.469028003285764,\n",
       "   -17.059222726358996,\n",
       "   -16.637610160588373,\n",
       "   -16.270128898212135,\n",
       "   -16.107344116296364,\n",
       "   -16.51346820735089,\n",
       "   -16.566921176313368,\n",
       "   -16.188637551949476,\n",
       "   -16.27115710980322,\n",
       "   -15.6631022241211,\n",
       "   -15.517581933929044,\n",
       "   -14.623607608261473,\n",
       "   -14.037385404342874,\n",
       "   -13.54942923257069,\n",
       "   -13.774965052599413,\n",
       "   -13.841617080198457,\n",
       "   -14.196972563595025,\n",
       "   -13.811645405511861,\n",
       "   -13.97272944634935,\n",
       "   -13.851372669307985,\n",
       "   -13.453964711292913,\n",
       "   -13.909182419104688,\n",
       "   -13.879098751753926,\n",
       "   -13.801601662808029,\n",
       "   -14.209964565379998,\n",
       "   -14.24474606454908,\n",
       "   -14.248376309447837,\n",
       "   -14.328612038387142,\n",
       "   -14.442580695813266,\n",
       "   -14.987561081366763,\n",
       "   -15.141594168226117,\n",
       "   -15.146165008072959,\n",
       "   -15.190068171180581,\n",
       "   -15.138657035215894,\n",
       "   -15.233526705507671,\n",
       "   -15.265174659446602,\n",
       "   -14.928351536954473,\n",
       "   -14.880637215997815,\n",
       "   -14.525200819502935,\n",
       "   -13.903303781379382,\n",
       "   -14.242848827636944,\n",
       "   -14.209953048014047,\n",
       "   -13.737727904457694,\n",
       "   -13.567496353846003,\n",
       "   -13.841246481411726,\n",
       "   -14.022688398968924,\n",
       "   -14.566954312882803,\n",
       "   -15.03030854590401,\n",
       "   -15.058359761418515,\n",
       "   -15.761848943160487,\n",
       "   -16.148481351481927,\n",
       "   -16.4857977453612,\n",
       "   -16.84861827138637,\n",
       "   -16.766464755106465,\n",
       "   -17.18286472557385,\n",
       "   -16.41576631684448,\n",
       "   -16.117415947388107,\n",
       "   -15.657567840318453,\n",
       "   -15.139016980411128,\n",
       "   -14.97393644747769,\n",
       "   -14.557753761488412,\n",
       "   -14.282360539957107,\n",
       "   -13.85975188027655,\n",
       "   -13.244514640841127,\n",
       "   -12.884862588810279,\n",
       "   -13.048135205814926,\n",
       "   -12.916630491765087,\n",
       "   -12.564304658728549,\n",
       "   -12.160444127516179,\n",
       "   -12.100989112894482,\n",
       "   -11.931248755018599,\n",
       "   -11.971368649886411,\n",
       "   -12.357106478074389,\n",
       "   -12.60767784891733,\n",
       "   -12.403634087693668,\n",
       "   -12.114258179796398,\n",
       "   -12.459558058191325,\n",
       "   -12.999546126569653,\n",
       "   -13.645445340893449,\n",
       "   -14.310518955997932,\n",
       "   -14.786406164894789,\n",
       "   -15.490874434962311,\n",
       "   -16.03323019280475,\n",
       "   -16.134383218910948,\n",
       "   -15.958087866197866,\n",
       "   -15.201757147939713,\n",
       "   -15.54552206904697,\n",
       "   -15.662074762395344,\n",
       "   -16.60096479787421,\n",
       "   -16.872482169901442,\n",
       "   -17.16065275920655,\n",
       "   -18.07278871633353,\n",
       "   -17.70451916386477,\n",
       "   -17.962424173933357,\n",
       "   -18.326142215829698,\n",
       "   -17.634993172390836,\n",
       "   -18.51908892185816,\n",
       "   -19.03405054362478,\n",
       "   -19.751830830013358,\n",
       "   -18.567941403098878,\n",
       "   -17.79044948594273,\n",
       "   -18.354522291099844,\n",
       "   -16.81068995958159,\n",
       "   -16.27743914940884,\n",
       "   -17.03990276528584,\n",
       "   -16.98120188023599,\n",
       "   -17.046277720835434,\n",
       "   -15.973369758814837,\n",
       "   -15.942744126297747,\n",
       "   -15.503231682365024,\n",
       "   -14.738317554753902,\n",
       "   -14.246600304223268,\n",
       "   -14.077552297731344,\n",
       "   -13.870181324383758,\n",
       "   -13.830908517521456,\n",
       "   -13.63590686922069,\n",
       "   -13.099922393452278,\n",
       "   -12.317567974519974,\n",
       "   -11.12118262452854,\n",
       "   -10.357774016474233,\n",
       "   -10.342833456723987,\n",
       "   -10.585419223141255,\n",
       "   -10.583852561319471,\n",
       "   -10.83669096043787,\n",
       "   -10.374535208101289,\n",
       "   -10.548860768894285,\n",
       "   -10.538490054024036,\n",
       "   -10.317286724489156,\n",
       "   -10.025663658850021,\n",
       "   -10.371449734999189,\n",
       "   -10.567587639239486,\n",
       "   -10.832180193886632,\n",
       "   -11.612723469463198,\n",
       "   -12.206750786117382,\n",
       "   -12.161072634621124,\n",
       "   -12.296371631231036,\n",
       "   -11.604430729033268,\n",
       "   -12.296720881378494,\n",
       "   -12.745368035567287,\n",
       "   -12.095206591981587,\n",
       "   -12.668184716729309,\n",
       "   -12.832687454310067,\n",
       "   -13.179975626508499,\n",
       "   -12.160078488258932,\n",
       "   -12.485236134501985,\n",
       "   -12.609367253060627,\n",
       "   -12.718283113016142,\n",
       "   -13.025814125081126,\n",
       "   -13.628323419085861,\n",
       "   -15.863125691993258,\n",
       "   -15.059346063181367,\n",
       "   -15.12776210995465,\n",
       "   -14.724126326682725,\n",
       "   -14.716675319408662,\n",
       "   -13.739106388229867,\n",
       "   -15.179962470143536,\n",
       "   -15.419239476028904,\n",
       "   -13.817474252505209,\n",
       "   -13.75266377712459,\n",
       "   -13.928345282157363,\n",
       "   -14.661201407953172,\n",
       "   -13.821132441313715,\n",
       "   -13.523311397091327,\n",
       "   -13.676174863353765,\n",
       "   -13.526308825251798,\n",
       "   -14.507181645322607,\n",
       "   -14.158846147304585,\n",
       "   -15.399437483169557,\n",
       "   -15.104217608368595,\n",
       "   -13.834251644981467,\n",
       "   -13.506263637052422,\n",
       "   -13.586224666383604,\n",
       "   -13.678856981284222,\n",
       "   -14.107680251679081,\n",
       "   -13.405652054214713,\n",
       "   -13.350065926077386,\n",
       "   -14.092588487727225,\n",
       "   -13.273072338625285,\n",
       "   -14.375678351379664,\n",
       "   -13.812511620952241,\n",
       "   -13.241477012036622,\n",
       "   -12.51636040118661,\n",
       "   -12.447189034451224,\n",
       "   -12.720266212848518,\n",
       "   -12.522624975577042,\n",
       "   -12.211259187558532,\n",
       "   -12.141587394247704,\n",
       "   -12.668768496821308,\n",
       "   -12.636877757769064,\n",
       "   -11.579670838289587,\n",
       "   -11.44652972970647,\n",
       "   -10.681569538571866,\n",
       "   -9.877231473793245,\n",
       "   -10.083091544828886,\n",
       "   -10.425775622213775,\n",
       "   -10.075903901482366,\n",
       "   -10.923950832066707,\n",
       "   -11.954883716441413,\n",
       "   -11.448524561832695,\n",
       "   -11.425441281298376,\n",
       "   -11.256592156608637,\n",
       "   -11.751785868552144,\n",
       "   -12.206308269400045,\n",
       "   -12.355883324735991,\n",
       "   -12.134592177109251,\n",
       "   -12.814828123151901,\n",
       "   -13.723988723950615,\n",
       "   -13.277780971893277,\n",
       "   -13.094590632294574,\n",
       "   -14.03444555912395,\n",
       "   -14.932954845536539,\n",
       "   -14.95465871663759,\n",
       "   -15.976701785368581,\n",
       "   -18.661316905790677,\n",
       "   -20.093076863946578,\n",
       "   -24.184516200382305,\n",
       "   -27.2440642532894,\n",
       "   -26.23491423757406,\n",
       "   -25.367174382381492,\n",
       "   -20.823545463279473,\n",
       "   -24.16840289894734,\n",
       "   -28.693767263765245,\n",
       "   -27.463494461875143,\n",
       "   -28.13625724405744,\n",
       "   -22.519603068210223,\n",
       "   -22.90784126730884,\n",
       "   -23.768872630866845,\n",
       "   -21.010086342857335,\n",
       "   -23.165214689923534,\n",
       "   -20.365465131490573,\n",
       "   -19.802639492887156,\n",
       "   -19.431699586145953,\n",
       "   -19.198282609748688,\n",
       "   -19.056207646775114,\n",
       "   -16.754950165295938,\n",
       "   -15.82693912271547,\n",
       "   -16.51884650213233,\n",
       "   -17.48015107770636,\n",
       "   -19.65526252479787,\n",
       "   -20.65800551018671,\n",
       "   -19.903876587895553,\n",
       "   -18.955964507700017,\n",
       "   -20.98939049639202,\n",
       "   -20.847809288139345,\n",
       "   -18.90050135122489,\n",
       "   -20.839333683877026,\n",
       "   -22.119486232816676,\n",
       "   -23.370000603674285,\n",
       "   -23.080156837571586,\n",
       "   -23.079752865247226,\n",
       "   -20.582209680197067,\n",
       "   -18.589960141226694,\n",
       "   -16.430655570388723,\n",
       "   -17.363458214870185,\n",
       "   -19.83997864377186,\n",
       "   -17.88651405860803,\n",
       "   -17.235006573394887,\n",
       "   -18.223135707737185,\n",
       "   -19.042397441938547,\n",
       "   -16.434791471400633,\n",
       "   -14.748852380648268,\n",
       "   -14.562954655987241,\n",
       "   -14.817751038366,\n",
       "   -14.47877121523757,\n",
       "   -15.144865929658048,\n",
       "   -17.52562775797794,\n",
       "   -19.54133360850466,\n",
       "   -20.93138638541954,\n",
       "   -21.07432662282256,\n",
       "   -23.92636214950708,\n",
       "   -25.388958373791283,\n",
       "   -24.471399273338193,\n",
       "   -24.594046872650402,\n",
       "   -23.381340048866456,\n",
       "   -22.471505226514434,\n",
       "   -21.672755752186365,\n",
       "   -21.330461911997922,\n",
       "   -19.18743557860522,\n",
       "   -19.38444602631452,\n",
       "   -18.737219269847103,\n",
       "   -17.09961249083846,\n",
       "   -15.798480661142456,\n",
       "   -14.889770064055265,\n",
       "   -13.356174253276203,\n",
       "   -13.435806332399178,\n",
       "   -12.663438357334556,\n",
       "   -12.963140712229006,\n",
       "   -13.53777064195195,\n",
       "   -12.583132243374218,\n",
       "   -12.359574911843861,\n",
       "   -12.298426581606634,\n",
       "   -13.049929015852886,\n",
       "   -14.669373429510557,\n",
       "   -14.67850225244334,\n",
       "   -14.473512640230012,\n",
       "   -14.726379198158241,\n",
       "   -15.320273330574006,\n",
       "   -15.170380283354621,\n",
       "   -14.26198479350682,\n",
       "   -13.500824901517888,\n",
       "   -13.192412317659606,\n",
       "   -13.697793883282158,\n",
       "   -14.798039575766705,\n",
       "   -13.98314997442055,\n",
       "   -13.776016742853216,\n",
       "   -13.329412800646892,\n",
       "   -13.742722806429413,\n",
       "   -13.898141363346197,\n",
       "   -15.217697319704675,\n",
       "   -15.308623648977509,\n",
       "   -16.194401349873917,\n",
       "   -17.81351196336282,\n",
       "   -15.923118559924095,\n",
       "   -15.37249284587631,\n",
       "   -14.13011702811146,\n",
       "   -12.89796500600534,\n",
       "   -12.823735092443677,\n",
       "   -12.621745396283785,\n",
       "   -12.597549355183858,\n",
       "   -12.882926238211807,\n",
       "   -12.955915601139303,\n",
       "   -12.723463540505755,\n",
       "   -12.785790828091136,\n",
       "   -12.515166615422604,\n",
       "   -12.999831911690357,\n",
       "   -13.469030137259054,\n",
       "   -12.087618594673124,\n",
       "   -12.634104092632437,\n",
       "   -12.805986279482283,\n",
       "   -13.17388889413559,\n",
       "   -13.602190596229786,\n",
       "   -14.497401953736162,\n",
       "   -15.698903836076259],\n",
       "  'train_pce_loss': [0.7302777171134949,\n",
       "   0.6630434989929199,\n",
       "   0.6779241561889648,\n",
       "   0.6041350960731506,\n",
       "   0.6774753332138062,\n",
       "   0.6269028782844543,\n",
       "   0.6496851444244385,\n",
       "   0.5503286123275757,\n",
       "   0.6101468801498413,\n",
       "   0.5329838991165161,\n",
       "   0.5720386505126953,\n",
       "   0.5668820738792419,\n",
       "   0.604973554611206,\n",
       "   0.5187991261482239,\n",
       "   0.5554453730583191,\n",
       "   0.554458737373352,\n",
       "   0.561646580696106,\n",
       "   0.5468299984931946,\n",
       "   0.5959728956222534,\n",
       "   0.5650154948234558,\n",
       "   0.5423132181167603,\n",
       "   0.49553626775741577,\n",
       "   0.5441405177116394,\n",
       "   0.43800047039985657,\n",
       "   0.5418853759765625,\n",
       "   0.5437703728675842,\n",
       "   0.5305477976799011,\n",
       "   0.5342027544975281,\n",
       "   0.5259017944335938,\n",
       "   0.43526336550712585,\n",
       "   0.528170108795166,\n",
       "   0.46242690086364746,\n",
       "   0.5175996422767639,\n",
       "   0.5797732472419739,\n",
       "   0.48486649990081787,\n",
       "   0.48014408349990845,\n",
       "   0.4947466254234314,\n",
       "   0.4948253035545349,\n",
       "   0.4864288568496704,\n",
       "   0.4740346670150757,\n",
       "   0.4616048336029053,\n",
       "   0.45533469319343567,\n",
       "   0.45140135288238525,\n",
       "   0.4876561462879181,\n",
       "   0.45486193895339966,\n",
       "   0.45531803369522095,\n",
       "   0.5218250155448914,\n",
       "   0.48179760575294495,\n",
       "   0.45272141695022583,\n",
       "   0.49653109908103943,\n",
       "   0.5171653032302856,\n",
       "   0.456804096698761,\n",
       "   0.41430291533470154,\n",
       "   0.4294181764125824,\n",
       "   0.4428482949733734,\n",
       "   0.4132751524448395,\n",
       "   0.5063015222549438,\n",
       "   0.4671449065208435,\n",
       "   0.4642263352870941,\n",
       "   0.4124303162097931,\n",
       "   0.4123377799987793,\n",
       "   0.44289523363113403,\n",
       "   0.4774515926837921,\n",
       "   0.47072702646255493,\n",
       "   0.4500220715999603,\n",
       "   0.456106036901474,\n",
       "   0.43724122643470764,\n",
       "   0.4465632736682892,\n",
       "   0.42465195059776306,\n",
       "   0.4766668379306793,\n",
       "   0.46000418066978455,\n",
       "   0.44095274806022644,\n",
       "   0.4049084782600403,\n",
       "   0.42547476291656494,\n",
       "   0.3804731070995331,\n",
       "   0.3695439398288727,\n",
       "   0.4526049792766571,\n",
       "   0.3715028166770935,\n",
       "   0.5041621923446655,\n",
       "   0.4465217590332031,\n",
       "   0.3972732424736023,\n",
       "   0.38347646594047546,\n",
       "   0.397887647151947,\n",
       "   0.38202255964279175,\n",
       "   0.37177300453186035,\n",
       "   0.38610774278640747,\n",
       "   0.4147014617919922,\n",
       "   0.4081006646156311,\n",
       "   0.39788365364074707,\n",
       "   0.44545361399650574,\n",
       "   0.45437347888946533,\n",
       "   0.42839518189430237,\n",
       "   0.3787771165370941,\n",
       "   0.36518213152885437,\n",
       "   0.420615017414093,\n",
       "   0.39436179399490356,\n",
       "   0.3897571563720703,\n",
       "   0.3693968653678894,\n",
       "   0.4018551707267761,\n",
       "   0.40597593784332275,\n",
       "   0.3935779631137848,\n",
       "   0.3568517565727234,\n",
       "   0.3725511431694031,\n",
       "   0.3772519528865814,\n",
       "   0.36043962836265564,\n",
       "   0.3637491762638092,\n",
       "   0.3911948502063751,\n",
       "   0.35773396492004395,\n",
       "   0.3861936330795288,\n",
       "   0.38445568084716797,\n",
       "   0.3665477931499481,\n",
       "   0.41122058033943176,\n",
       "   0.3247867822647095,\n",
       "   0.40112900733947754,\n",
       "   0.3198372721672058,\n",
       "   0.3569866418838501,\n",
       "   0.3385063409805298,\n",
       "   0.3709094524383545,\n",
       "   0.37713751196861267,\n",
       "   0.4011504650115967,\n",
       "   0.35878923535346985,\n",
       "   0.3326396942138672,\n",
       "   0.34521380066871643,\n",
       "   0.3739685118198395,\n",
       "   0.34280794858932495,\n",
       "   0.339129239320755,\n",
       "   0.33313724398612976,\n",
       "   0.3900359272956848,\n",
       "   0.35707780718803406,\n",
       "   0.34731099009513855,\n",
       "   0.3427391052246094,\n",
       "   0.38492271304130554,\n",
       "   0.35146185755729675,\n",
       "   0.37156715989112854,\n",
       "   0.36926141381263733,\n",
       "   0.34434616565704346,\n",
       "   0.339269757270813,\n",
       "   0.3773590326309204,\n",
       "   0.3191928565502167,\n",
       "   0.36427178978919983,\n",
       "   0.3651924729347229,\n",
       "   0.3090866804122925,\n",
       "   0.34265077114105225,\n",
       "   0.3313281834125519,\n",
       "   0.35450851917266846,\n",
       "   0.34148335456848145,\n",
       "   0.3133396804332733,\n",
       "   0.341377854347229,\n",
       "   0.35345351696014404,\n",
       "   0.37474489212036133,\n",
       "   0.2978064715862274,\n",
       "   0.3147102892398834,\n",
       "   0.34699514508247375,\n",
       "   0.30912891030311584,\n",
       "   0.33941805362701416,\n",
       "   0.31797829270362854,\n",
       "   0.37976688146591187,\n",
       "   0.2976002097129822,\n",
       "   0.2941059470176697,\n",
       "   0.3153242766857147,\n",
       "   0.2900792062282562,\n",
       "   0.31105682253837585,\n",
       "   0.32336699962615967,\n",
       "   0.31595420837402344,\n",
       "   0.32175683975219727,\n",
       "   0.28212806582450867,\n",
       "   0.33470508456230164,\n",
       "   0.2711731195449829,\n",
       "   0.3324330747127533,\n",
       "   0.3443969190120697,\n",
       "   0.3263774812221527,\n",
       "   0.3632132411003113,\n",
       "   0.3115549087524414,\n",
       "   0.319881796836853,\n",
       "   0.32716280221939087,\n",
       "   0.31445616483688354,\n",
       "   0.3045766055583954,\n",
       "   0.35965630412101746,\n",
       "   0.3282751739025116,\n",
       "   0.3218322992324829,\n",
       "   0.30074676871299744,\n",
       "   0.30780723690986633,\n",
       "   0.30260658264160156,\n",
       "   0.29679057002067566,\n",
       "   0.30979862809181213,\n",
       "   0.27686190605163574,\n",
       "   0.271344393491745,\n",
       "   0.3136216700077057,\n",
       "   0.2976894974708557,\n",
       "   0.31854960322380066,\n",
       "   0.32581043243408203,\n",
       "   0.2932587265968323,\n",
       "   0.3071347773075104,\n",
       "   0.31830963492393494,\n",
       "   0.3066920340061188,\n",
       "   0.322769433259964,\n",
       "   0.2563990354537964,\n",
       "   0.3118124306201935,\n",
       "   0.3146514296531677,\n",
       "   0.28475332260131836,\n",
       "   0.274749755859375,\n",
       "   0.2794671058654785,\n",
       "   0.2607015073299408,\n",
       "   0.28616684675216675,\n",
       "   0.2859813868999481,\n",
       "   0.314678430557251,\n",
       "   0.2778567671775818,\n",
       "   0.29868805408477783,\n",
       "   0.2772929072380066,\n",
       "   0.33448970317840576,\n",
       "   0.2700522541999817,\n",
       "   0.3104482591152191,\n",
       "   0.24602502584457397,\n",
       "   0.2854700982570648,\n",
       "   0.27705317735671997,\n",
       "   0.28458642959594727,\n",
       "   0.28815823793411255,\n",
       "   0.2924520671367645,\n",
       "   0.263484925031662,\n",
       "   0.2936958968639374,\n",
       "   0.2870977222919464,\n",
       "   0.2947925925254822,\n",
       "   0.31562909483909607,\n",
       "   0.2918727397918701,\n",
       "   0.27311939001083374,\n",
       "   0.27934393286705017,\n",
       "   0.2777601182460785,\n",
       "   0.3085355758666992,\n",
       "   0.26784268021583557,\n",
       "   0.3006441593170166,\n",
       "   0.29206013679504395,\n",
       "   0.2787644565105438,\n",
       "   0.27423858642578125,\n",
       "   0.28125566244125366,\n",
       "   0.2835127115249634,\n",
       "   0.25951895117759705,\n",
       "   0.2583981454372406,\n",
       "   0.26301610469818115,\n",
       "   0.27660930156707764,\n",
       "   0.2859494388103485,\n",
       "   0.2827581763267517,\n",
       "   0.2667841613292694,\n",
       "   0.2734656035900116,\n",
       "   0.271114706993103,\n",
       "   0.29206928610801697,\n",
       "   0.2805146872997284,\n",
       "   0.2626107335090637,\n",
       "   0.26530301570892334,\n",
       "   0.26431095600128174,\n",
       "   0.2737910747528076,\n",
       "   0.26249516010284424,\n",
       "   0.2534339129924774,\n",
       "   0.2665911018848419,\n",
       "   0.2430068403482437,\n",
       "   0.2623215317726135,\n",
       "   0.2782030701637268,\n",
       "   0.2696145474910736,\n",
       "   0.2600809931755066,\n",
       "   0.2840423583984375,\n",
       "   0.26880574226379395,\n",
       "   0.2956203818321228,\n",
       "   0.23852665722370148,\n",
       "   0.24715425074100494,\n",
       "   0.27357134222984314,\n",
       "   0.2332901805639267,\n",
       "   0.276309072971344,\n",
       "   0.241407111287117,\n",
       "   0.2768003046512604,\n",
       "   0.2765643894672394,\n",
       "   0.2886434495449066,\n",
       "   0.2467423528432846,\n",
       "   0.2697700262069702,\n",
       "   0.23873105645179749,\n",
       "   0.2685052454471588,\n",
       "   0.27034687995910645,\n",
       "   0.2517257630825043,\n",
       "   0.25209105014801025,\n",
       "   0.24570249021053314,\n",
       "   0.2951560616493225,\n",
       "   0.24269352853298187,\n",
       "   0.2297448068857193,\n",
       "   0.2744956612586975,\n",
       "   0.2933642566204071,\n",
       "   0.24170348048210144,\n",
       "   0.25870445370674133,\n",
       "   0.2769463062286377,\n",
       "   0.25819993019104004,\n",
       "   0.26113206148147583,\n",
       "   0.267901211977005,\n",
       "   0.23926758766174316,\n",
       "   0.24508225917816162,\n",
       "   0.23318345844745636,\n",
       "   0.2705974876880646,\n",
       "   0.2631298303604126,\n",
       "   0.264573335647583,\n",
       "   0.24488718807697296,\n",
       "   0.2585604190826416,\n",
       "   0.2672693729400635,\n",
       "   0.2563837170600891,\n",
       "   0.2549596130847931,\n",
       "   0.2698019742965698,\n",
       "   0.24164263904094696,\n",
       "   0.27162855863571167,\n",
       "   0.233582004904747,\n",
       "   0.25768178701400757,\n",
       "   0.23865510523319244,\n",
       "   0.25114500522613525,\n",
       "   0.2485164999961853,\n",
       "   0.24651914834976196,\n",
       "   0.22347143292427063,\n",
       "   0.2630026042461395,\n",
       "   0.2619612514972687,\n",
       "   0.2610420882701874,\n",
       "   0.2450248748064041,\n",
       "   0.226254403591156,\n",
       "   0.2506905496120453,\n",
       "   0.2261437028646469,\n",
       "   0.23263199627399445,\n",
       "   0.27769896388053894,\n",
       "   0.22662459313869476,\n",
       "   0.2395014613866806,\n",
       "   0.23967710137367249,\n",
       "   0.25844383239746094,\n",
       "   0.23176826536655426,\n",
       "   0.23244252800941467,\n",
       "   0.27031078934669495,\n",
       "   0.2559009790420532,\n",
       "   0.2608073055744171,\n",
       "   0.24341146647930145,\n",
       "   0.23142889142036438,\n",
       "   0.24793505668640137,\n",
       "   0.2598574757575989,\n",
       "   0.23968176543712616,\n",
       "   0.2218693196773529,\n",
       "   0.2352282702922821,\n",
       "   0.2240867167711258,\n",
       "   0.2281879186630249,\n",
       "   0.25769492983818054,\n",
       "   0.2597384452819824,\n",
       "   0.24831122159957886,\n",
       "   0.23431691527366638,\n",
       "   0.2186676263809204,\n",
       "   0.22047923505306244,\n",
       "   0.22025884687900543,\n",
       "   0.23614758253097534,\n",
       "   0.24711178243160248,\n",
       "   0.24765853583812714,\n",
       "   0.248172327876091,\n",
       "   0.23097755014896393,\n",
       "   0.23676662147045135,\n",
       "   0.23179183900356293,\n",
       "   0.25111645460128784,\n",
       "   0.2410588562488556,\n",
       "   0.23426814377307892,\n",
       "   0.22093544900417328,\n",
       "   0.25761523842811584,\n",
       "   0.273163378238678,\n",
       "   0.26916372776031494,\n",
       "   0.22005100548267365,\n",
       "   0.24051161110401154,\n",
       "   0.20946350693702698,\n",
       "   0.22591885924339294,\n",
       "   0.2072327733039856,\n",
       "   0.22518223524093628,\n",
       "   0.21301482617855072,\n",
       "   0.21768124401569366,\n",
       "   0.213856503367424,\n",
       "   0.22586628794670105,\n",
       "   0.2431391179561615,\n",
       "   0.21838906407356262,\n",
       "   0.2259991317987442,\n",
       "   0.21709302067756653,\n",
       "   0.22858045995235443,\n",
       "   0.22318260371685028,\n",
       "   0.2281014323234558,\n",
       "   0.2364598959684372,\n",
       "   0.21283958852291107,\n",
       "   0.20167578756809235,\n",
       "   0.21762610971927643,\n",
       "   0.21136632561683655,\n",
       "   0.24727724492549896,\n",
       "   0.21020203828811646,\n",
       "   0.23077963292598724,\n",
       "   0.23239026963710785,\n",
       "   0.2392905205488205,\n",
       "   0.20707252621650696,\n",
       "   0.24422243237495422,\n",
       "   0.21623528003692627,\n",
       "   0.2146657258272171,\n",
       "   0.22099629044532776,\n",
       "   0.2051066756248474,\n",
       "   0.21638095378875732,\n",
       "   0.2286764681339264,\n",
       "   0.23322471976280212,\n",
       "   0.21649636328220367,\n",
       "   0.23669825494289398,\n",
       "   0.21064865589141846,\n",
       "   0.2227504849433899,\n",
       "   0.21551543474197388,\n",
       "   0.21867835521697998,\n",
       "   0.21546873450279236,\n",
       "   0.20506896078586578,\n",
       "   0.22123049199581146,\n",
       "   0.20889373123645782,\n",
       "   0.22368505597114563,\n",
       "   0.21281853318214417,\n",
       "   0.22288286685943604,\n",
       "   0.20569312572479248,\n",
       "   0.2393067181110382,\n",
       "   0.21447177231311798,\n",
       "   0.19975921511650085,\n",
       "   0.2415379285812378,\n",
       "   0.21840573847293854,\n",
       "   0.21394798159599304,\n",
       "   0.22476109862327576,\n",
       "   0.22246600687503815,\n",
       "   0.2257542759180069,\n",
       "   0.2163432538509369,\n",
       "   0.23061658442020416,\n",
       "   0.19113804399967194,\n",
       "   0.21659234166145325,\n",
       "   0.2129123955965042,\n",
       "   0.21003423631191254,\n",
       "   0.2074829638004303,\n",
       "   0.19637154042720795,\n",
       "   0.2235696017742157,\n",
       "   0.21199683845043182,\n",
       "   0.21036486327648163,\n",
       "   0.2174774557352066,\n",
       "   0.21250247955322266,\n",
       "   0.22319296002388,\n",
       "   0.21554487943649292,\n",
       "   0.18664605915546417,\n",
       "   0.2064562290906906,\n",
       "   0.20194193720817566,\n",
       "   0.21027296781539917,\n",
       "   0.21095460653305054,\n",
       "   0.1920473575592041,\n",
       "   0.2127256542444229,\n",
       "   0.2064705640077591,\n",
       "   0.20289459824562073,\n",
       "   0.21492965519428253,\n",
       "   0.19493161141872406,\n",
       "   0.20570668578147888,\n",
       "   0.21417610347270966,\n",
       "   0.18998044729232788,\n",
       "   0.2307908982038498,\n",
       "   0.18984423577785492,\n",
       "   0.21332679688930511,\n",
       "   0.20487388968467712,\n",
       "   0.2131286859512329,\n",
       "   0.18965686857700348,\n",
       "   0.19763480126857758,\n",
       "   0.20874790847301483,\n",
       "   0.1991547793149948,\n",
       "   0.2003600001335144,\n",
       "   0.2179984748363495,\n",
       "   0.2366311401128769,\n",
       "   0.18518945574760437,\n",
       "   0.22448404133319855,\n",
       "   0.21266339719295502,\n",
       "   0.18784640729427338,\n",
       "   0.21380743384361267,\n",
       "   0.2103390395641327,\n",
       "   0.19213148951530457,\n",
       "   0.2146378606557846,\n",
       "   0.2056901901960373,\n",
       "   0.22274668514728546,\n",
       "   0.19081316888332367,\n",
       "   0.19641010463237762,\n",
       "   0.1904153823852539,\n",
       "   0.19017931818962097,\n",
       "   0.18257494270801544,\n",
       "   0.19083479046821594,\n",
       "   0.20154693722724915,\n",
       "   0.19757236540317535,\n",
       "   0.20713084936141968,\n",
       "   0.19118551909923553,\n",
       "   0.20327991247177124,\n",
       "   0.2028687745332718,\n",
       "   0.20523318648338318,\n",
       "   0.2042209506034851,\n",
       "   0.18777905404567719,\n",
       "   0.21293775737285614,\n",
       "   0.2015613615512848,\n",
       "   0.19802604615688324,\n",
       "   0.19699443876743317,\n",
       "   0.23395390808582306,\n",
       "   0.20059475302696228,\n",
       "   0.20878742635250092,\n",
       "   0.19944916665554047,\n",
       "   0.20333155989646912,\n",
       "   0.19485561549663544,\n",
       "   0.18723973631858826,\n",
       "   0.1957526057958603,\n",
       "   0.1999834179878235,\n",
       "   0.20856419205665588,\n",
       "   0.20039582252502441,\n",
       "   0.20123060047626495,\n",
       "   0.20572581887245178],\n",
       "  'train_voc_loss': [0.5255117416381836,\n",
       "   0.45563432574272156,\n",
       "   0.4351794421672821,\n",
       "   0.40522557497024536,\n",
       "   0.44035804271698,\n",
       "   0.42041996121406555,\n",
       "   0.4486849308013916,\n",
       "   0.4068915843963623,\n",
       "   0.3907853960990906,\n",
       "   0.40684014558792114,\n",
       "   0.5762620568275452,\n",
       "   0.4102398753166199,\n",
       "   0.41597360372543335,\n",
       "   0.3823027014732361,\n",
       "   0.3591558337211609,\n",
       "   0.33796757459640503,\n",
       "   0.40016403794288635,\n",
       "   0.36348757147789,\n",
       "   0.3664740025997162,\n",
       "   0.35932743549346924,\n",
       "   0.4028259217739105,\n",
       "   0.3488394320011139,\n",
       "   0.37599924206733704,\n",
       "   0.3428506553173065,\n",
       "   0.35790103673934937,\n",
       "   0.3532000780105591,\n",
       "   0.40248963236808777,\n",
       "   0.34223127365112305,\n",
       "   0.31798481941223145,\n",
       "   0.3547101318836212,\n",
       "   0.32183513045310974,\n",
       "   0.30485397577285767,\n",
       "   0.33433619141578674,\n",
       "   0.336961030960083,\n",
       "   0.3508022427558899,\n",
       "   0.3391420841217041,\n",
       "   0.34667330980300903,\n",
       "   0.3209964632987976,\n",
       "   0.35185444355010986,\n",
       "   0.34547215700149536,\n",
       "   0.34926262497901917,\n",
       "   0.33974167704582214,\n",
       "   0.34793034195899963,\n",
       "   0.34273460507392883,\n",
       "   0.3503088355064392,\n",
       "   0.3365464210510254,\n",
       "   0.32848283648490906,\n",
       "   0.3086657226085663,\n",
       "   0.3681696057319641,\n",
       "   0.3245050311088562,\n",
       "   0.30461958050727844,\n",
       "   0.33502039313316345,\n",
       "   0.3324168920516968,\n",
       "   0.31656140089035034,\n",
       "   0.3390794098377228,\n",
       "   0.33201277256011963,\n",
       "   0.3431570529937744,\n",
       "   0.3211740255355835,\n",
       "   0.35319915413856506,\n",
       "   0.3416544795036316,\n",
       "   0.31222009658813477,\n",
       "   0.32629653811454773,\n",
       "   0.3386141061782837,\n",
       "   0.31807026267051697,\n",
       "   0.34384146332740784,\n",
       "   0.3294351100921631,\n",
       "   0.3363667130470276,\n",
       "   0.30273833870887756,\n",
       "   0.3314998745918274,\n",
       "   0.319263756275177,\n",
       "   0.33582133054733276,\n",
       "   0.36072415113449097,\n",
       "   0.30346888303756714,\n",
       "   0.3132438063621521,\n",
       "   0.32023224234580994,\n",
       "   0.3272513151168823,\n",
       "   0.315653920173645,\n",
       "   0.30576252937316895,\n",
       "   0.30848804116249084,\n",
       "   0.29265329241752625,\n",
       "   0.28646785020828247,\n",
       "   0.31926092505455017,\n",
       "   0.3083665668964386,\n",
       "   0.2928597629070282,\n",
       "   0.299279123544693,\n",
       "   0.3651973605155945,\n",
       "   0.28777575492858887,\n",
       "   0.3196348249912262,\n",
       "   0.29505354166030884,\n",
       "   0.32741034030914307,\n",
       "   0.2929733395576477,\n",
       "   0.3065745234489441,\n",
       "   0.31213486194610596,\n",
       "   0.32963332533836365,\n",
       "   0.299045205116272,\n",
       "   0.3111018240451813,\n",
       "   0.29346662759780884,\n",
       "   0.323781281709671,\n",
       "   0.29461684823036194,\n",
       "   0.3014039099216461,\n",
       "   0.3073793053627014,\n",
       "   0.3223986327648163,\n",
       "   0.29566365480422974,\n",
       "   0.3036760985851288,\n",
       "   0.29662081599235535,\n",
       "   0.2824348211288452,\n",
       "   0.27141451835632324,\n",
       "   0.28578850626945496,\n",
       "   0.29318302869796753,\n",
       "   0.2895353138446808,\n",
       "   0.31990665197372437,\n",
       "   0.28146904706954956,\n",
       "   0.30066752433776855,\n",
       "   0.28998926281929016,\n",
       "   0.28920844197273254,\n",
       "   0.2844582498073578,\n",
       "   0.28217849135398865,\n",
       "   0.2869115471839905,\n",
       "   0.2974863648414612,\n",
       "   0.2762233316898346,\n",
       "   0.27287960052490234,\n",
       "   0.2991572320461273,\n",
       "   0.29422760009765625,\n",
       "   0.2683473825454712,\n",
       "   0.2682339549064636,\n",
       "   0.2786827087402344,\n",
       "   0.30264952778816223,\n",
       "   0.24653367698192596,\n",
       "   0.2873553931713104,\n",
       "   0.28992798924446106,\n",
       "   0.2886032462120056,\n",
       "   0.28395843505859375,\n",
       "   0.2694137692451477,\n",
       "   0.25875136256217957,\n",
       "   0.2903498709201813,\n",
       "   0.2844429910182953,\n",
       "   0.282818078994751,\n",
       "   0.27367234230041504,\n",
       "   0.2609454095363617,\n",
       "   0.26229578256607056,\n",
       "   0.2733222246170044,\n",
       "   0.29443448781967163,\n",
       "   0.2815520167350769,\n",
       "   0.2595217227935791,\n",
       "   0.27929627895355225,\n",
       "   0.25567466020584106,\n",
       "   0.2547495663166046,\n",
       "   0.2704143822193146,\n",
       "   0.2623041868209839,\n",
       "   0.27473047375679016,\n",
       "   0.2780683934688568,\n",
       "   0.24810855090618134,\n",
       "   0.2742455005645752,\n",
       "   0.26227402687072754,\n",
       "   0.27088069915771484,\n",
       "   0.2683340311050415,\n",
       "   0.2531062662601471,\n",
       "   0.2618976831436157,\n",
       "   0.2768206298351288,\n",
       "   0.25703564286231995,\n",
       "   0.24440528452396393,\n",
       "   0.2727645933628082,\n",
       "   0.2666013836860657,\n",
       "   0.2701028287410736,\n",
       "   0.24938218295574188,\n",
       "   0.2721157371997833,\n",
       "   0.272964745759964,\n",
       "   0.25062769651412964,\n",
       "   0.287596195936203,\n",
       "   0.25007861852645874,\n",
       "   0.2549411654472351,\n",
       "   0.22603951394557953,\n",
       "   0.24408015608787537,\n",
       "   0.24148227274417877,\n",
       "   0.2528301179409027,\n",
       "   0.2665036916732788,\n",
       "   0.2498258650302887,\n",
       "   0.2558608651161194,\n",
       "   0.26301518082618713,\n",
       "   0.23694348335266113,\n",
       "   0.24995654821395874,\n",
       "   0.2472946047782898,\n",
       "   0.25054335594177246,\n",
       "   0.24981248378753662,\n",
       "   0.26187315583229065,\n",
       "   0.2587503492832184,\n",
       "   0.2541615664958954,\n",
       "   0.2381541132926941,\n",
       "   0.256027489900589,\n",
       "   0.23008191585540771,\n",
       "   0.2388758361339569,\n",
       "   0.24417729675769806,\n",
       "   0.2144622951745987,\n",
       "   0.23364609479904175,\n",
       "   0.25742849707603455,\n",
       "   0.23888450860977173,\n",
       "   0.2574531137943268,\n",
       "   0.24457021057605743,\n",
       "   0.2276412546634674,\n",
       "   0.2385796308517456,\n",
       "   0.2272442728281021,\n",
       "   0.23905110359191895,\n",
       "   0.2381218820810318,\n",
       "   0.22479763627052307,\n",
       "   0.217417374253273,\n",
       "   0.2323668897151947,\n",
       "   0.25831472873687744,\n",
       "   0.23215565085411072,\n",
       "   0.2329556792974472,\n",
       "   0.22740164399147034,\n",
       "   0.21208839118480682,\n",
       "   0.22363759577274323,\n",
       "   0.24879153072834015,\n",
       "   0.24379979074001312,\n",
       "   0.23702989518642426,\n",
       "   0.21562796831130981,\n",
       "   0.225081205368042,\n",
       "   0.21926969289779663,\n",
       "   0.24188341200351715,\n",
       "   0.23897171020507812,\n",
       "   0.23652833700180054,\n",
       "   0.22600843012332916,\n",
       "   0.20674832165241241,\n",
       "   0.25469332933425903,\n",
       "   0.21489129960536957,\n",
       "   0.2176172435283661,\n",
       "   0.21617229282855988,\n",
       "   0.20807725191116333,\n",
       "   0.21985667943954468,\n",
       "   0.24112194776535034,\n",
       "   0.21649165451526642,\n",
       "   0.21043336391448975,\n",
       "   0.21866673231124878,\n",
       "   0.2331392914056778,\n",
       "   0.2192688137292862,\n",
       "   0.22635594010353088,\n",
       "   0.20670737326145172,\n",
       "   0.20422117412090302,\n",
       "   0.19936726987361908,\n",
       "   0.23017652332782745,\n",
       "   0.20566627383232117,\n",
       "   0.2060089409351349,\n",
       "   0.20277966558933258,\n",
       "   0.2129194736480713,\n",
       "   0.21522735059261322,\n",
       "   0.2105678915977478,\n",
       "   0.23478291928768158,\n",
       "   0.21079425513744354,\n",
       "   0.21921876072883606,\n",
       "   0.20668825507164001,\n",
       "   0.20428428053855896,\n",
       "   0.19801966845989227,\n",
       "   0.21949328482151031,\n",
       "   0.21428777277469635,\n",
       "   0.20739105343818665,\n",
       "   0.22563110291957855,\n",
       "   0.21655674278736115,\n",
       "   0.2065562605857849,\n",
       "   0.2091335952281952,\n",
       "   0.20236851274967194,\n",
       "   0.20862755179405212,\n",
       "   0.211077481508255,\n",
       "   0.1942632794380188,\n",
       "   0.22041234374046326,\n",
       "   0.20790110528469086,\n",
       "   0.1862664520740509,\n",
       "   0.2083146870136261,\n",
       "   0.2077031433582306,\n",
       "   0.18592004477977753,\n",
       "   0.20615066587924957,\n",
       "   0.21930786967277527,\n",
       "   0.21326924860477448,\n",
       "   0.19242802262306213,\n",
       "   0.20722660422325134,\n",
       "   0.1967984437942505,\n",
       "   0.19622668623924255,\n",
       "   0.21001611649990082,\n",
       "   0.20098675787448883,\n",
       "   0.21047645807266235,\n",
       "   0.20020692050457,\n",
       "   0.19472435116767883,\n",
       "   0.1929691731929779,\n",
       "   0.21173757314682007,\n",
       "   0.2130412608385086,\n",
       "   0.1917872577905655,\n",
       "   0.2079939842224121,\n",
       "   0.20456181466579437,\n",
       "   0.183842733502388,\n",
       "   0.1945665329694748,\n",
       "   0.2085285186767578,\n",
       "   0.188418447971344,\n",
       "   0.19390873610973358,\n",
       "   0.19127993285655975,\n",
       "   0.2024267166852951,\n",
       "   0.19457821547985077,\n",
       "   0.2140374481678009,\n",
       "   0.19609205424785614,\n",
       "   0.1897917240858078,\n",
       "   0.1680036187171936,\n",
       "   0.178926020860672,\n",
       "   0.19843029975891113,\n",
       "   0.1918793022632599,\n",
       "   0.1736644059419632,\n",
       "   0.17844778299331665,\n",
       "   0.1946702003479004,\n",
       "   0.19082701206207275,\n",
       "   0.17978276312351227,\n",
       "   0.18322418630123138,\n",
       "   0.16564451158046722,\n",
       "   0.19413992762565613,\n",
       "   0.17035777866840363,\n",
       "   0.17619441449642181,\n",
       "   0.1636294573545456,\n",
       "   0.17377419769763947,\n",
       "   0.17216064035892487,\n",
       "   0.17466282844543457,\n",
       "   0.17647160589694977,\n",
       "   0.1500343531370163,\n",
       "   0.19344530999660492,\n",
       "   0.17158246040344238,\n",
       "   0.15417878329753876,\n",
       "   0.18988920748233795,\n",
       "   0.1776891052722931,\n",
       "   0.16705726087093353,\n",
       "   0.15672217309474945,\n",
       "   0.16372182965278625,\n",
       "   0.15142272412776947,\n",
       "   0.1584739089012146,\n",
       "   0.148232102394104,\n",
       "   0.18434938788414001,\n",
       "   0.173932284116745,\n",
       "   0.16882647573947906,\n",
       "   0.17800529301166534,\n",
       "   0.15597061812877655,\n",
       "   0.18028753995895386,\n",
       "   0.1515074074268341,\n",
       "   0.17667990922927856,\n",
       "   0.16799834370613098,\n",
       "   0.16157880425453186,\n",
       "   0.14580827951431274,\n",
       "   0.16381384432315826,\n",
       "   0.1929338276386261,\n",
       "   0.14959822595119476,\n",
       "   0.15308259427547455,\n",
       "   0.1771824210882187,\n",
       "   0.14382079243659973,\n",
       "   0.14478346705436707,\n",
       "   0.16583415865898132,\n",
       "   0.16807608306407928,\n",
       "   0.14986498653888702,\n",
       "   0.1477334052324295,\n",
       "   0.1551637351512909,\n",
       "   0.16219253838062286,\n",
       "   0.15664681792259216,\n",
       "   0.1382310539484024,\n",
       "   0.14659185707569122,\n",
       "   0.16183477640151978,\n",
       "   0.1378389596939087,\n",
       "   0.13975392282009125,\n",
       "   0.13770760595798492,\n",
       "   0.15139922499656677,\n",
       "   0.13864366710186005,\n",
       "   0.14934657514095306,\n",
       "   0.15178297460079193,\n",
       "   0.15205734968185425,\n",
       "   0.15941929817199707,\n",
       "   0.14956073462963104,\n",
       "   0.1406203955411911,\n",
       "   0.14588479697704315,\n",
       "   0.1422097384929657,\n",
       "   0.12437143176794052,\n",
       "   0.14346668124198914,\n",
       "   0.15576478838920593,\n",
       "   0.13073775172233582,\n",
       "   0.15220198035240173,\n",
       "   0.15113498270511627,\n",
       "   0.1520555466413498,\n",
       "   0.15614525973796844,\n",
       "   0.12728892266750336,\n",
       "   0.15177127718925476,\n",
       "   0.1274544596672058,\n",
       "   0.13318170607089996,\n",
       "   0.13666807115077972,\n",
       "   0.1482934057712555,\n",
       "   0.13975989818572998,\n",
       "   0.133830264210701,\n",
       "   0.13283175230026245,\n",
       "   0.151712566614151,\n",
       "   0.1371287852525711,\n",
       "   0.15529990196228027,\n",
       "   0.1635771095752716,\n",
       "   0.12682335078716278,\n",
       "   0.14545853435993195,\n",
       "   0.133253812789917,\n",
       "   0.12560808658599854,\n",
       "   0.14689581096172333,\n",
       "   0.12820172309875488,\n",
       "   0.13880011439323425,\n",
       "   0.13369886577129364,\n",
       "   0.13489088416099548,\n",
       "   0.13480719923973083,\n",
       "   0.13951633870601654,\n",
       "   0.12877525389194489,\n",
       "   0.1357855349779129,\n",
       "   0.14226184785366058,\n",
       "   0.12518127262592316,\n",
       "   0.14878568053245544,\n",
       "   0.13787133991718292,\n",
       "   0.13453498482704163,\n",
       "   0.13558395206928253,\n",
       "   0.14207321405410767,\n",
       "   0.12569263577461243,\n",
       "   0.12919607758522034,\n",
       "   0.13977856934070587,\n",
       "   0.13577812910079956,\n",
       "   0.13541457056999207,\n",
       "   0.12755587697029114,\n",
       "   0.13348288834095,\n",
       "   0.30694323778152466,\n",
       "   0.1342434287071228,\n",
       "   0.13744331896305084,\n",
       "   0.1277133971452713,\n",
       "   0.1330261081457138,\n",
       "   0.1410529762506485,\n",
       "   0.13137561082839966,\n",
       "   0.24877218902111053,\n",
       "   0.1274169534444809,\n",
       "   0.13455137610435486,\n",
       "   0.13151289522647858,\n",
       "   0.14286479353904724,\n",
       "   0.11991366744041443,\n",
       "   0.12481191009283066,\n",
       "   0.125905379652977,\n",
       "   0.12561294436454773,\n",
       "   0.14414449036121368,\n",
       "   0.134029820561409,\n",
       "   0.1430172175168991,\n",
       "   0.1330898255109787,\n",
       "   0.12717492878437042,\n",
       "   0.13467605412006378,\n",
       "   0.12157435715198517,\n",
       "   0.13322633504867554,\n",
       "   0.11941755563020706,\n",
       "   0.12500357627868652,\n",
       "   0.1145416647195816,\n",
       "   0.1380617320537567,\n",
       "   0.1275869756937027,\n",
       "   0.1294267475605011,\n",
       "   0.1397576481103897,\n",
       "   0.1352234184741974,\n",
       "   0.13640739023685455,\n",
       "   0.1427023559808731,\n",
       "   0.12669508159160614,\n",
       "   0.13165152072906494,\n",
       "   0.1357365846633911,\n",
       "   0.13163422048091888,\n",
       "   0.11678764224052429,\n",
       "   0.1200161874294281,\n",
       "   0.1134941428899765,\n",
       "   0.12828578054904938,\n",
       "   0.12434234470129013,\n",
       "   0.11217088997364044,\n",
       "   0.11099596321582794,\n",
       "   0.1289965957403183,\n",
       "   0.12449287623167038,\n",
       "   0.12668482959270477,\n",
       "   0.12343017756938934,\n",
       "   0.11524936556816101,\n",
       "   0.11665895581245422,\n",
       "   0.12368511408567429,\n",
       "   0.11885961145162582,\n",
       "   0.12435687333345413,\n",
       "   0.11334516853094101,\n",
       "   0.11684190481901169,\n",
       "   0.13040195405483246,\n",
       "   0.11846886575222015,\n",
       "   0.11239009350538254,\n",
       "   0.12207111716270447,\n",
       "   0.11174319684505463,\n",
       "   0.11684602499008179,\n",
       "   0.11895564943552017,\n",
       "   0.1235441043972969,\n",
       "   0.12129046022891998,\n",
       "   0.12213356047868729,\n",
       "   0.114736407995224,\n",
       "   0.11843621730804443,\n",
       "   0.11987217515707016,\n",
       "   0.10867829620838165,\n",
       "   0.12454336881637573,\n",
       "   0.2885885238647461,\n",
       "   0.11634255945682526,\n",
       "   0.11761381477117538,\n",
       "   0.10596038401126862,\n",
       "   0.11530982702970505,\n",
       "   0.12098339945077896,\n",
       "   0.11617997288703918,\n",
       "   0.11977679282426834,\n",
       "   0.11862382292747498,\n",
       "   0.12390859425067902,\n",
       "   0.10485656559467316],\n",
       "  'train_jsc_loss': [0.7743474245071411,\n",
       "   0.7142750024795532,\n",
       "   0.7088783383369446,\n",
       "   0.7499030232429504,\n",
       "   0.6629866361618042,\n",
       "   0.6746817231178284,\n",
       "   0.6144158840179443,\n",
       "   0.6383076310157776,\n",
       "   0.6323339343070984,\n",
       "   0.6189935207366943,\n",
       "   0.6741653680801392,\n",
       "   0.6259266138076782,\n",
       "   0.6333724856376648,\n",
       "   0.6059337258338928,\n",
       "   0.7014649510383606,\n",
       "   0.596971869468689,\n",
       "   0.6053060293197632,\n",
       "   0.5783868432044983,\n",
       "   0.5607856512069702,\n",
       "   0.5539599061012268,\n",
       "   0.6570541262626648,\n",
       "   0.5582931041717529,\n",
       "   0.6386367082595825,\n",
       "   0.6582594513893127,\n",
       "   0.6163029074668884,\n",
       "   0.5916038751602173,\n",
       "   0.5640324354171753,\n",
       "   0.6152268648147583,\n",
       "   0.49333563446998596,\n",
       "   0.5727257132530212,\n",
       "   0.5913801789283752,\n",
       "   0.5896242260932922,\n",
       "   0.6108248829841614,\n",
       "   0.5431960821151733,\n",
       "   0.5949833989143372,\n",
       "   0.6454891562461853,\n",
       "   0.5927188396453857,\n",
       "   0.6014589667320251,\n",
       "   0.5718490481376648,\n",
       "   0.5873934626579285,\n",
       "   0.4912134110927582,\n",
       "   0.4879957139492035,\n",
       "   0.5068409442901611,\n",
       "   0.5452081561088562,\n",
       "   0.6772226691246033,\n",
       "   0.5227783918380737,\n",
       "   0.4956313669681549,\n",
       "   0.5735369324684143,\n",
       "   0.5536134839057922,\n",
       "   0.6137404441833496,\n",
       "   0.5900899171829224,\n",
       "   0.5438941121101379,\n",
       "   0.5800303816795349,\n",
       "   0.5825627446174622,\n",
       "   0.4521307945251465,\n",
       "   0.4640442132949829,\n",
       "   0.5470187664031982,\n",
       "   0.5203676819801331,\n",
       "   0.4523313045501709,\n",
       "   0.5430347919464111,\n",
       "   0.48177555203437805,\n",
       "   0.566628634929657,\n",
       "   0.5074746012687683,\n",
       "   0.5677145719528198,\n",
       "   0.4799385666847229,\n",
       "   0.5824540257453918,\n",
       "   0.5556119084358215,\n",
       "   0.5293604135513306,\n",
       "   0.5368795990943909,\n",
       "   0.5276565551757812,\n",
       "   0.5012998580932617,\n",
       "   0.5352447628974915,\n",
       "   0.47018885612487793,\n",
       "   0.5244295001029968,\n",
       "   0.47611549496650696,\n",
       "   0.5040656328201294,\n",
       "   0.4945868253707886,\n",
       "   0.5222314596176147,\n",
       "   0.5913991928100586,\n",
       "   0.452263206243515,\n",
       "   0.4722972512245178,\n",
       "   0.49052828550338745,\n",
       "   0.43631187081336975,\n",
       "   0.4715832471847534,\n",
       "   0.4838391840457916,\n",
       "   0.509475827217102,\n",
       "   0.48989567160606384,\n",
       "   0.4361218512058258,\n",
       "   0.5395793914794922,\n",
       "   0.5132503509521484,\n",
       "   0.4934149384498596,\n",
       "   0.46789494156837463,\n",
       "   0.49791857600212097,\n",
       "   0.4459414780139923,\n",
       "   0.5028079152107239,\n",
       "   0.46012207865715027,\n",
       "   0.5134115219116211,\n",
       "   0.48453861474990845,\n",
       "   0.5010688304901123,\n",
       "   0.4642007648944855,\n",
       "   0.517780601978302,\n",
       "   0.4675258696079254,\n",
       "   0.47340744733810425,\n",
       "   0.4684353470802307,\n",
       "   0.4455522894859314,\n",
       "   0.4278480112552643,\n",
       "   0.455978661775589,\n",
       "   0.466541588306427,\n",
       "   0.4516047537326813,\n",
       "   0.4344467520713806,\n",
       "   0.47579875588417053,\n",
       "   0.4548155665397644,\n",
       "   0.4587872326374054,\n",
       "   0.41631031036376953,\n",
       "   0.4227147698402405,\n",
       "   0.4533563256263733,\n",
       "   0.45348596572875977,\n",
       "   0.3954114317893982,\n",
       "   0.3884141445159912,\n",
       "   0.4280097782611847,\n",
       "   0.41510462760925293,\n",
       "   0.4096698760986328,\n",
       "   0.4504220485687256,\n",
       "   0.38900765776634216,\n",
       "   0.4542308747768402,\n",
       "   0.46985572576522827,\n",
       "   0.4057168960571289,\n",
       "   0.3836909532546997,\n",
       "   0.39591214060783386,\n",
       "   0.39530977606773376,\n",
       "   0.41155901551246643,\n",
       "   0.39937636256217957,\n",
       "   0.45059138536453247,\n",
       "   0.408931165933609,\n",
       "   0.4077135920524597,\n",
       "   0.41214537620544434,\n",
       "   0.4337622821331024,\n",
       "   0.4119715690612793,\n",
       "   0.41554346680641174,\n",
       "   0.4666144847869873,\n",
       "   0.3924696743488312,\n",
       "   0.41417330503463745,\n",
       "   0.4124312698841095,\n",
       "   0.42767757177352905,\n",
       "   0.40998297929763794,\n",
       "   0.393374502658844,\n",
       "   0.348313570022583,\n",
       "   0.3861977159976959,\n",
       "   0.3827502131462097,\n",
       "   0.43126994371414185,\n",
       "   0.36366525292396545,\n",
       "   0.39157944917678833,\n",
       "   0.36609363555908203,\n",
       "   0.38519394397735596,\n",
       "   0.41559675335884094,\n",
       "   0.38137930631637573,\n",
       "   0.40412330627441406,\n",
       "   0.41664546728134155,\n",
       "   0.394869327545166,\n",
       "   0.39534568786621094,\n",
       "   0.3735080659389496,\n",
       "   0.40214428305625916,\n",
       "   0.41745203733444214,\n",
       "   0.34935230016708374,\n",
       "   0.39056679606437683,\n",
       "   0.41297173500061035,\n",
       "   0.34411004185676575,\n",
       "   0.3827945590019226,\n",
       "   0.37569597363471985,\n",
       "   0.36379438638687134,\n",
       "   0.4129549562931061,\n",
       "   0.36643752455711365,\n",
       "   0.3595616817474365,\n",
       "   0.38137122988700867,\n",
       "   0.40674760937690735,\n",
       "   0.34723564982414246,\n",
       "   0.3633730411529541,\n",
       "   0.37306785583496094,\n",
       "   0.3513661324977875,\n",
       "   0.36788979172706604,\n",
       "   0.38532203435897827,\n",
       "   0.3508872389793396,\n",
       "   0.3704209625720978,\n",
       "   0.360178679227829,\n",
       "   0.3538299798965454,\n",
       "   0.3754443824291229,\n",
       "   0.33980244398117065,\n",
       "   0.39583078026771545,\n",
       "   0.3576195240020752,\n",
       "   0.3501269221305847,\n",
       "   0.40981438755989075,\n",
       "   0.3808819353580475,\n",
       "   0.3447290062904358,\n",
       "   0.4051145017147064,\n",
       "   0.34424299001693726,\n",
       "   0.4029441773891449,\n",
       "   0.3690757155418396,\n",
       "   0.3194008469581604,\n",
       "   0.34461691975593567,\n",
       "   0.347767174243927,\n",
       "   0.3375215530395508,\n",
       "   0.35113826394081116,\n",
       "   0.33441340923309326,\n",
       "   0.30188247561454773,\n",
       "   0.3545232117176056,\n",
       "   0.341340571641922,\n",
       "   0.37548714876174927,\n",
       "   0.34393832087516785,\n",
       "   0.32186344265937805,\n",
       "   0.34654828906059265,\n",
       "   0.2988044321537018,\n",
       "   0.3581490218639374,\n",
       "   0.3535574972629547,\n",
       "   0.3348042964935303,\n",
       "   0.38591524958610535,\n",
       "   0.3425365090370178,\n",
       "   0.36428332328796387,\n",
       "   0.37499141693115234,\n",
       "   0.33579021692276,\n",
       "   0.3544347286224365,\n",
       "   0.33608946204185486,\n",
       "   0.3530678153038025,\n",
       "   0.3532385528087616,\n",
       "   0.3524816632270813,\n",
       "   0.33035707473754883,\n",
       "   0.3219919502735138,\n",
       "   0.34186241030693054,\n",
       "   0.34396395087242126,\n",
       "   0.3431049883365631,\n",
       "   0.35976091027259827,\n",
       "   0.36879751086235046,\n",
       "   0.3331640660762787,\n",
       "   0.31459662318229675,\n",
       "   0.3712550699710846,\n",
       "   0.36900317668914795,\n",
       "   0.32276347279548645,\n",
       "   0.3310616612434387,\n",
       "   0.2997788190841675,\n",
       "   0.35402601957321167,\n",
       "   0.29544106125831604,\n",
       "   0.334358811378479,\n",
       "   0.3534955680370331,\n",
       "   0.32234495878219604,\n",
       "   0.334907203912735,\n",
       "   0.3139849305152893,\n",
       "   0.3257339596748352,\n",
       "   0.363557904958725,\n",
       "   0.33353087306022644,\n",
       "   0.3235921859741211,\n",
       "   0.3517630994319916,\n",
       "   0.3181079924106598,\n",
       "   0.36038869619369507,\n",
       "   0.35975557565689087,\n",
       "   0.32141464948654175,\n",
       "   0.31880372762680054,\n",
       "   0.29676660895347595,\n",
       "   0.3504444658756256,\n",
       "   0.313582181930542,\n",
       "   0.3430909216403961,\n",
       "   0.34301450848579407,\n",
       "   0.32802823185920715,\n",
       "   0.3261949419975281,\n",
       "   0.32082638144493103,\n",
       "   0.3117574155330658,\n",
       "   0.29344436526298523,\n",
       "   0.31468912959098816,\n",
       "   0.2948354482650757,\n",
       "   0.3010825514793396,\n",
       "   0.32197028398513794,\n",
       "   0.3259211778640747,\n",
       "   0.29636549949645996,\n",
       "   0.33169832825660706,\n",
       "   0.3520911931991577,\n",
       "   0.3063509166240692,\n",
       "   0.3090372383594513,\n",
       "   0.3263963758945465,\n",
       "   0.2933570444583893,\n",
       "   0.2983841896057129,\n",
       "   0.2904166877269745,\n",
       "   0.2926030158996582,\n",
       "   0.31464964151382446,\n",
       "   0.34596872329711914,\n",
       "   0.30861616134643555,\n",
       "   0.3076995313167572,\n",
       "   0.3161698877811432,\n",
       "   0.31304267048835754,\n",
       "   0.27135366201400757,\n",
       "   0.2957049310207367,\n",
       "   0.2969878017902374,\n",
       "   0.31653380393981934,\n",
       "   0.30675238370895386,\n",
       "   0.3138023316860199,\n",
       "   0.28793421387672424,\n",
       "   0.3277990520000458,\n",
       "   0.27168676257133484,\n",
       "   0.2969900667667389,\n",
       "   0.33282560110092163,\n",
       "   0.32564136385917664,\n",
       "   0.3074272871017456,\n",
       "   0.2977418899536133,\n",
       "   0.3127281665802002,\n",
       "   0.3054056763648987,\n",
       "   0.2746388018131256,\n",
       "   0.28440189361572266,\n",
       "   0.30629053711891174,\n",
       "   0.30597272515296936,\n",
       "   0.34556692838668823,\n",
       "   0.2992127537727356,\n",
       "   0.2885754704475403,\n",
       "   0.29089200496673584,\n",
       "   0.270821750164032,\n",
       "   0.28552716970443726,\n",
       "   0.30277010798454285,\n",
       "   0.30788716673851013,\n",
       "   0.2698860466480255,\n",
       "   0.26862433552742004,\n",
       "   0.2968517541885376,\n",
       "   0.2665064334869385,\n",
       "   0.26627013087272644,\n",
       "   0.25796446204185486,\n",
       "   0.3014410734176636,\n",
       "   0.2928003966808319,\n",
       "   0.26275205612182617,\n",
       "   0.2655040919780731,\n",
       "   0.2751453220844269,\n",
       "   0.2819409966468811,\n",
       "   0.26106977462768555,\n",
       "   0.2752412259578705,\n",
       "   0.3190937936306,\n",
       "   0.2870776057243347,\n",
       "   0.3033159375190735,\n",
       "   0.30544227361679077,\n",
       "   0.23776979744434357,\n",
       "   0.27272602915763855,\n",
       "   0.275034636259079,\n",
       "   0.2735699713230133,\n",
       "   0.25494542717933655,\n",
       "   0.2709956169128418,\n",
       "   0.27273160219192505,\n",
       "   0.29111698269844055,\n",
       "   0.2695336937904358,\n",
       "   0.2573715150356293,\n",
       "   0.2937098443508148,\n",
       "   0.2726823389530182,\n",
       "   0.29609036445617676,\n",
       "   0.29619184136390686,\n",
       "   0.28056055307388306,\n",
       "   0.24051329493522644,\n",
       "   0.3022499978542328,\n",
       "   0.2568579614162445,\n",
       "   0.2820473313331604,\n",
       "   0.2833760380744934,\n",
       "   0.2711416482925415,\n",
       "   0.2598176896572113,\n",
       "   0.2907656729221344,\n",
       "   0.25798285007476807,\n",
       "   0.2611105442047119,\n",
       "   0.25755006074905396,\n",
       "   0.26585784554481506,\n",
       "   0.26915183663368225,\n",
       "   0.2749984860420227,\n",
       "   0.30755123496055603,\n",
       "   0.24787674844264984,\n",
       "   0.26226329803466797,\n",
       "   0.2881268560886383,\n",
       "   0.2854297459125519,\n",
       "   0.26658135652542114,\n",
       "   0.2810837924480438,\n",
       "   0.2515625059604645,\n",
       "   0.3033067286014557,\n",
       "   0.26155734062194824,\n",
       "   0.2767949104309082,\n",
       "   0.2411075085401535,\n",
       "   0.2917267084121704,\n",
       "   0.2848125994205475,\n",
       "   0.27935990691185,\n",
       "   0.2360180765390396,\n",
       "   0.3020164370536804,\n",
       "   0.2519986927509308,\n",
       "   0.2485228031873703,\n",
       "   0.257987916469574,\n",
       "   0.25632840394973755,\n",
       "   0.25357717275619507,\n",
       "   0.24583657085895538,\n",
       "   0.27303266525268555,\n",
       "   0.28244397044181824,\n",
       "   0.24770745635032654,\n",
       "   0.265325129032135,\n",
       "   0.24021205306053162,\n",
       "   0.2526595890522003,\n",
       "   0.27046850323677063,\n",
       "   0.25873318314552307,\n",
       "   0.24555182456970215,\n",
       "   0.24361255764961243,\n",
       "   0.24722379446029663,\n",
       "   0.264068603515625,\n",
       "   0.2715553343296051,\n",
       "   0.2674020528793335,\n",
       "   0.2546297311782837,\n",
       "   0.2322845309972763,\n",
       "   0.2658056318759918,\n",
       "   0.24978168308734894,\n",
       "   0.2563132345676422,\n",
       "   0.2745973467826843,\n",
       "   0.27132555842399597,\n",
       "   0.2731427848339081,\n",
       "   0.25366446375846863,\n",
       "   0.23467372357845306,\n",
       "   0.26335981488227844,\n",
       "   0.24251116812229156,\n",
       "   0.2541031241416931,\n",
       "   0.26361212134361267,\n",
       "   0.24264772236347198,\n",
       "   0.2665880620479584,\n",
       "   0.27872779965400696,\n",
       "   0.2614890933036804,\n",
       "   0.23016409575939178,\n",
       "   0.2629396617412567,\n",
       "   0.23698538541793823,\n",
       "   0.22169318795204163,\n",
       "   0.2531633675098419,\n",
       "   0.2515294849872589,\n",
       "   0.24609880149364471,\n",
       "   0.23127494752407074,\n",
       "   0.24568210542201996,\n",
       "   0.23116619884967804,\n",
       "   0.26022008061408997,\n",
       "   0.23075531423091888,\n",
       "   0.262736976146698,\n",
       "   0.220915749669075,\n",
       "   0.24314400553703308,\n",
       "   0.23845213651657104,\n",
       "   0.2616446614265442,\n",
       "   0.21400372684001923,\n",
       "   0.24324235320091248,\n",
       "   0.22818173468112946,\n",
       "   0.24976050853729248,\n",
       "   0.22994163632392883,\n",
       "   0.2424287497997284,\n",
       "   0.254843145608902,\n",
       "   0.2286875694990158,\n",
       "   0.2510679066181183,\n",
       "   0.24532581865787506,\n",
       "   0.22390025854110718,\n",
       "   0.26237422227859497,\n",
       "   0.24575956165790558,\n",
       "   0.24238544702529907,\n",
       "   0.25844600796699524,\n",
       "   0.2482050359249115,\n",
       "   0.24312818050384521,\n",
       "   0.2585425078868866,\n",
       "   0.25483888387680054,\n",
       "   0.2502638101577759,\n",
       "   0.26293280720710754,\n",
       "   0.23730657994747162,\n",
       "   0.23853282630443573,\n",
       "   0.22528670728206635,\n",
       "   0.2526382505893707,\n",
       "   0.22428061068058014,\n",
       "   0.23361383378505707,\n",
       "   0.25461849570274353,\n",
       "   0.25999170541763306,\n",
       "   0.24158167839050293,\n",
       "   0.22500917315483093,\n",
       "   0.23546110093593597,\n",
       "   0.2169402539730072,\n",
       "   0.22573146224021912,\n",
       "   0.2394782304763794,\n",
       "   0.24724912643432617,\n",
       "   0.2354261726140976,\n",
       "   0.23443906009197235,\n",
       "   0.22681300342082977,\n",
       "   0.24667195975780487,\n",
       "   0.24028247594833374,\n",
       "   0.23273275792598724,\n",
       "   0.23156596720218658,\n",
       "   0.23325593769550323,\n",
       "   0.23427285254001617,\n",
       "   0.21873000264167786,\n",
       "   0.2321908175945282,\n",
       "   0.244790181517601,\n",
       "   0.2571638524532318,\n",
       "   0.23342947661876678,\n",
       "   0.2432469129562378,\n",
       "   0.24287191033363342,\n",
       "   0.228952556848526,\n",
       "   0.2222537398338318,\n",
       "   0.20500026643276215,\n",
       "   0.22247733175754547,\n",
       "   0.23168720304965973,\n",
       "   0.2430362105369568,\n",
       "   0.22344426810741425,\n",
       "   0.23613391816616058,\n",
       "   0.23076999187469482,\n",
       "   0.2262992560863495,\n",
       "   0.20249754190444946,\n",
       "   0.23347873985767365,\n",
       "   0.21395014226436615,\n",
       "   0.25521624088287354,\n",
       "   0.21354246139526367],\n",
       "  'train_ff_loss': [0.8230463862419128,\n",
       "   0.8183382749557495,\n",
       "   0.713721513748169,\n",
       "   0.8705134987831116,\n",
       "   0.844068706035614,\n",
       "   0.7211213707923889,\n",
       "   0.8329756259918213,\n",
       "   0.7850827574729919,\n",
       "   0.7838881611824036,\n",
       "   0.8567049503326416,\n",
       "   0.7890573143959045,\n",
       "   0.7421520352363586,\n",
       "   0.7451143860816956,\n",
       "   0.7823214530944824,\n",
       "   0.7485763430595398,\n",
       "   0.6822729110717773,\n",
       "   0.7871314883232117,\n",
       "   0.7367175817489624,\n",
       "   0.7848358750343323,\n",
       "   0.7987601161003113,\n",
       "   0.7056185007095337,\n",
       "   0.7565408945083618,\n",
       "   0.7656417489051819,\n",
       "   0.7818285822868347,\n",
       "   0.7642199993133545,\n",
       "   0.7649680376052856,\n",
       "   0.6549549102783203,\n",
       "   0.6986569166183472,\n",
       "   0.6961244344711304,\n",
       "   0.7319350242614746,\n",
       "   0.6770836710929871,\n",
       "   0.7441658973693848,\n",
       "   0.7055125832557678,\n",
       "   0.7082760334014893,\n",
       "   0.708438515663147,\n",
       "   0.7688565254211426,\n",
       "   0.7371664047241211,\n",
       "   0.6521443128585815,\n",
       "   0.6834250688552856,\n",
       "   0.7764279246330261,\n",
       "   0.6561077237129211,\n",
       "   0.6888208389282227,\n",
       "   0.6715306639671326,\n",
       "   0.7720604538917542,\n",
       "   0.7360047101974487,\n",
       "   0.7064597010612488,\n",
       "   0.6758952140808105,\n",
       "   0.6470698714256287,\n",
       "   0.6320164203643799,\n",
       "   0.5900922417640686,\n",
       "   0.6988398432731628,\n",
       "   0.6799256801605225,\n",
       "   0.6641362309455872,\n",
       "   0.6740142107009888,\n",
       "   0.6747081279754639,\n",
       "   0.657992959022522,\n",
       "   0.675645649433136,\n",
       "   0.6858909726142883,\n",
       "   0.6559354662895203,\n",
       "   0.708943784236908,\n",
       "   0.5908403992652893,\n",
       "   0.6694653630256653,\n",
       "   0.6627485752105713,\n",
       "   0.6401668190956116,\n",
       "   0.6531357169151306,\n",
       "   0.6704990863800049,\n",
       "   0.7096852660179138,\n",
       "   0.6513524055480957,\n",
       "   0.705712616443634,\n",
       "   0.668409526348114,\n",
       "   0.6644077301025391,\n",
       "   0.6411675810813904,\n",
       "   0.6355013251304626,\n",
       "   0.6377482414245605,\n",
       "   0.6617157459259033,\n",
       "   0.5963159799575806,\n",
       "   0.636536717414856,\n",
       "   0.6350971460342407,\n",
       "   0.6009751558303833,\n",
       "   0.6187984943389893,\n",
       "   0.6228742599487305,\n",
       "   0.6295987963676453,\n",
       "   0.5844513177871704,\n",
       "   0.5949983596801758,\n",
       "   0.6268304586410522,\n",
       "   0.605757474899292,\n",
       "   0.617730438709259,\n",
       "   0.6068755388259888,\n",
       "   0.6022316217422485,\n",
       "   0.6191593408584595,\n",
       "   0.6375907063484192,\n",
       "   0.6099168658256531,\n",
       "   0.6209047436714172,\n",
       "   0.6384268999099731,\n",
       "   0.6425424814224243,\n",
       "   0.6174215078353882,\n",
       "   0.6270583868026733,\n",
       "   0.6171844005584717,\n",
       "   0.5696530938148499,\n",
       "   0.638469934463501,\n",
       "   0.583640456199646,\n",
       "   0.6292691230773926,\n",
       "   0.6253021955490112,\n",
       "   0.6359534859657288,\n",
       "   0.681050181388855,\n",
       "   0.5865913033485413,\n",
       "   0.5814230442047119,\n",
       "   0.5690761208534241,\n",
       "   0.5605530738830566,\n",
       "   0.5776423811912537,\n",
       "   0.6051785349845886,\n",
       "   0.586656928062439,\n",
       "   0.6071088314056396,\n",
       "   0.5400629639625549,\n",
       "   0.5784937143325806,\n",
       "   0.5978364944458008,\n",
       "   0.5835791230201721,\n",
       "   0.6190031170845032,\n",
       "   0.6093729734420776,\n",
       "   0.5795852541923523,\n",
       "   0.5896686315536499,\n",
       "   0.6302169561386108,\n",
       "   0.6164882183074951,\n",
       "   0.5762185454368591,\n",
       "   0.6471816897392273,\n",
       "   0.597776472568512,\n",
       "   0.6088945269584656,\n",
       "   0.6232921481132507,\n",
       "   0.5961973667144775,\n",
       "   0.6194095015525818,\n",
       "   0.5885041952133179,\n",
       "   0.4907795488834381,\n",
       "   0.5715495944023132,\n",
       "   0.5624733567237854,\n",
       "   0.6350516080856323,\n",
       "   0.5758710503578186,\n",
       "   0.5414396524429321,\n",
       "   0.5899880528450012,\n",
       "   0.5289204716682434,\n",
       "   0.5618292689323425,\n",
       "   0.5353600382804871,\n",
       "   0.538214921951294,\n",
       "   0.6002193689346313,\n",
       "   0.5253098011016846,\n",
       "   0.583454430103302,\n",
       "   0.5402231216430664,\n",
       "   0.5946564078330994,\n",
       "   0.5515507459640503,\n",
       "   0.6053229570388794,\n",
       "   0.5379941463470459,\n",
       "   0.5882399678230286,\n",
       "   0.5602062940597534,\n",
       "   0.5459368824958801,\n",
       "   0.5641621947288513,\n",
       "   0.5426529049873352,\n",
       "   0.608418345451355,\n",
       "   0.5537643432617188,\n",
       "   0.5201374888420105,\n",
       "   0.528554379940033,\n",
       "   0.5485615730285645,\n",
       "   0.5578619837760925,\n",
       "   0.5242103934288025,\n",
       "   0.5337762832641602,\n",
       "   0.5065382719039917,\n",
       "   0.501028299331665,\n",
       "   0.5297343730926514,\n",
       "   0.5801354646682739,\n",
       "   0.5273970365524292,\n",
       "   0.5310043692588806,\n",
       "   0.5302573442459106,\n",
       "   0.5608470439910889,\n",
       "   0.5869085788726807,\n",
       "   0.5142753720283508,\n",
       "   0.5107125043869019,\n",
       "   0.5430322885513306,\n",
       "   0.5355939269065857,\n",
       "   0.5667023062705994,\n",
       "   0.5594159960746765,\n",
       "   0.5399613976478577,\n",
       "   0.49014344811439514,\n",
       "   0.5573448538780212,\n",
       "   0.5775651335716248,\n",
       "   0.5393848419189453,\n",
       "   0.5181260704994202,\n",
       "   0.5226495265960693,\n",
       "   0.5097547173500061,\n",
       "   0.5579320192337036,\n",
       "   0.5813468098640442,\n",
       "   0.5152408480644226,\n",
       "   0.4948403537273407,\n",
       "   0.5351744890213013,\n",
       "   0.5042657256126404,\n",
       "   0.5424638390541077,\n",
       "   0.5523971319198608,\n",
       "   0.519845187664032,\n",
       "   0.5273212790489197,\n",
       "   0.5490588545799255,\n",
       "   0.5321730375289917,\n",
       "   0.4986068308353424,\n",
       "   0.5320900678634644,\n",
       "   0.5627368092536926,\n",
       "   0.5412193536758423,\n",
       "   0.5208677649497986,\n",
       "   0.5638685822486877,\n",
       "   0.5297631025314331,\n",
       "   0.512691080570221,\n",
       "   0.5464060306549072,\n",
       "   0.4886217713356018,\n",
       "   0.5126332640647888,\n",
       "   0.5347791314125061,\n",
       "   0.5414514541625977,\n",
       "   0.5049782991409302,\n",
       "   0.49336618185043335,\n",
       "   0.5065542459487915,\n",
       "   0.49794378876686096,\n",
       "   0.5429061651229858,\n",
       "   0.4939550459384918,\n",
       "   0.4695732891559601,\n",
       "   0.49639543890953064,\n",
       "   0.5132113099098206,\n",
       "   0.5376420617103577,\n",
       "   0.533219575881958,\n",
       "   0.5235361456871033,\n",
       "   0.5004832148551941,\n",
       "   0.5125089883804321,\n",
       "   0.5190274119377136,\n",
       "   0.5128078460693359,\n",
       "   0.49678003787994385,\n",
       "   0.5154960751533508,\n",
       "   0.47493210434913635,\n",
       "   0.5186938047409058,\n",
       "   0.5367756485939026,\n",
       "   0.48665758967399597,\n",
       "   0.48927435278892517,\n",
       "   0.48755699396133423,\n",
       "   0.48751696944236755,\n",
       "   0.4995048940181732,\n",
       "   0.53863126039505,\n",
       "   0.5195958018302917,\n",
       "   0.527742326259613,\n",
       "   0.47632265090942383,\n",
       "   0.5059837102890015,\n",
       "   0.4961544871330261,\n",
       "   0.4768275320529938,\n",
       "   0.45450636744499207,\n",
       "   0.5258409976959229,\n",
       "   0.559030294418335,\n",
       "   0.4748319089412689,\n",
       "   0.4728945791721344,\n",
       "   0.4545949101448059,\n",
       "   0.4763087034225464,\n",
       "   0.4335041046142578,\n",
       "   0.4935937225818634,\n",
       "   0.47429192066192627,\n",
       "   0.4909968674182892,\n",
       "   0.4702056348323822,\n",
       "   0.49118074774742126,\n",
       "   0.5166541337966919,\n",
       "   0.4696793556213379,\n",
       "   0.43828511238098145,\n",
       "   0.4996863901615143,\n",
       "   0.476945698261261,\n",
       "   0.48226049542427063,\n",
       "   0.49846068024635315,\n",
       "   0.4529978334903717,\n",
       "   0.5061505436897278,\n",
       "   0.48523446917533875,\n",
       "   0.48276442289352417,\n",
       "   0.47059813141822815,\n",
       "   0.4600181579589844,\n",
       "   0.5026077032089233,\n",
       "   0.48535773158073425,\n",
       "   0.4703652858734131,\n",
       "   0.5261580348014832,\n",
       "   0.49070337414741516,\n",
       "   0.4541059136390686,\n",
       "   0.4296402633190155,\n",
       "   0.48254749178886414,\n",
       "   0.43907684087753296,\n",
       "   0.448132187128067,\n",
       "   0.44867077469825745,\n",
       "   0.4563221037387848,\n",
       "   0.49515023827552795,\n",
       "   0.4897729158401489,\n",
       "   0.4707818329334259,\n",
       "   0.46392321586608887,\n",
       "   0.44636520743370056,\n",
       "   0.49054479598999023,\n",
       "   0.4851855933666229,\n",
       "   0.4484310746192932,\n",
       "   0.4638112187385559,\n",
       "   0.5005412697792053,\n",
       "   0.49921414256095886,\n",
       "   0.45946088433265686,\n",
       "   0.42738452553749084,\n",
       "   0.4655768573284149,\n",
       "   0.5055429339408875,\n",
       "   0.4733615815639496,\n",
       "   0.49199384450912476,\n",
       "   0.46144214272499084,\n",
       "   0.4582488536834717,\n",
       "   0.4473602771759033,\n",
       "   0.4892357289791107,\n",
       "   0.4715871512889862,\n",
       "   0.45058202743530273,\n",
       "   0.42224356532096863,\n",
       "   0.44224998354911804,\n",
       "   0.43375423550605774,\n",
       "   0.45948272943496704,\n",
       "   0.4261331260204315,\n",
       "   0.45535343885421753,\n",
       "   0.4558234214782715,\n",
       "   0.4818600118160248,\n",
       "   0.4237963855266571,\n",
       "   0.4312835931777954,\n",
       "   0.4448455274105072,\n",
       "   0.4533185064792633,\n",
       "   0.4553506672382355,\n",
       "   0.4474334716796875,\n",
       "   0.42831435799598694,\n",
       "   0.41478434205055237,\n",
       "   0.4452877342700958,\n",
       "   0.5173206925392151,\n",
       "   0.46766042709350586,\n",
       "   0.4166085720062256,\n",
       "   0.45121607184410095,\n",
       "   0.42025113105773926,\n",
       "   0.4354211688041687,\n",
       "   0.4242047667503357,\n",
       "   0.43523386120796204,\n",
       "   0.43015867471694946,\n",
       "   0.4523484408855438,\n",
       "   0.4306196868419647,\n",
       "   0.4295576512813568,\n",
       "   0.43738701939582825,\n",
       "   0.4185160994529724,\n",
       "   0.48084723949432373,\n",
       "   0.4601726233959198,\n",
       "   0.41147559881210327,\n",
       "   0.45247510075569153,\n",
       "   0.456585168838501,\n",
       "   0.47136133909225464,\n",
       "   0.4328456223011017,\n",
       "   0.43004873394966125,\n",
       "   0.431111603975296,\n",
       "   0.4537900984287262,\n",
       "   0.4819999039173126,\n",
       "   0.4623532295227051,\n",
       "   0.43022242188453674,\n",
       "   0.44979748129844666,\n",
       "   0.4364696145057678,\n",
       "   0.4410726726055145,\n",
       "   0.4256249666213989,\n",
       "   0.46882033348083496,\n",
       "   0.413594514131546,\n",
       "   0.4081374406814575,\n",
       "   0.4529707133769989,\n",
       "   0.41360926628112793,\n",
       "   0.4295963644981384,\n",
       "   0.4246174097061157,\n",
       "   0.39205724000930786,\n",
       "   0.4290390610694885,\n",
       "   0.4380524456501007,\n",
       "   0.44799286127090454,\n",
       "   0.4442291259765625,\n",
       "   0.4490029513835907,\n",
       "   0.4236997067928314,\n",
       "   0.404613733291626,\n",
       "   0.39499804377555847,\n",
       "   0.4104911983013153,\n",
       "   0.39013954997062683,\n",
       "   0.3905116617679596,\n",
       "   0.43164128065109253,\n",
       "   0.4450611174106598,\n",
       "   0.3950892388820648,\n",
       "   0.4133200943470001,\n",
       "   0.4256538152694702,\n",
       "   0.42916837334632874,\n",
       "   0.45365771651268005,\n",
       "   0.44111812114715576,\n",
       "   0.4279994070529938,\n",
       "   0.4125819206237793,\n",
       "   0.42298623919487,\n",
       "   0.40901312232017517,\n",
       "   0.42489659786224365,\n",
       "   0.448550283908844,\n",
       "   0.40643176436424255,\n",
       "   0.4352473020553589,\n",
       "   0.43028610944747925,\n",
       "   0.4218880534172058,\n",
       "   0.4142264425754547,\n",
       "   0.444170743227005,\n",
       "   0.4444800615310669,\n",
       "   0.398091197013855,\n",
       "   0.40666109323501587,\n",
       "   0.43793004751205444,\n",
       "   0.4666523337364197,\n",
       "   0.4170927107334137,\n",
       "   0.3877151906490326,\n",
       "   0.3810827434062958,\n",
       "   0.4268644154071808,\n",
       "   0.4104275405406952,\n",
       "   0.41223371028900146,\n",
       "   0.4188632071018219,\n",
       "   0.4161010682582855,\n",
       "   0.4297782778739929,\n",
       "   0.4043425917625427,\n",
       "   0.4247434437274933,\n",
       "   0.4075765013694763,\n",
       "   0.38764530420303345,\n",
       "   0.4261276423931122,\n",
       "   0.38360467553138733,\n",
       "   0.4089242219924927,\n",
       "   0.4116218090057373,\n",
       "   0.41279152035713196,\n",
       "   0.37856703996658325,\n",
       "   0.41497084498405457,\n",
       "   0.42070329189300537,\n",
       "   0.42482790350914,\n",
       "   0.421192467212677,\n",
       "   0.3797266483306885,\n",
       "   0.39881202578544617,\n",
       "   0.3937266170978546,\n",
       "   0.42674532532691956,\n",
       "   0.4028977155685425,\n",
       "   0.4061594605445862,\n",
       "   0.42641717195510864,\n",
       "   0.40620654821395874,\n",
       "   0.38634467124938965,\n",
       "   0.424837589263916,\n",
       "   0.41913866996765137,\n",
       "   0.38420283794403076,\n",
       "   0.38601526618003845,\n",
       "   0.38982653617858887,\n",
       "   0.4070965647697449,\n",
       "   0.39003244042396545,\n",
       "   0.3767610490322113,\n",
       "   0.4054693877696991,\n",
       "   0.3843117654323578,\n",
       "   0.4102003872394562,\n",
       "   0.4206865429878235,\n",
       "   0.36579087376594543,\n",
       "   0.3710014224052429,\n",
       "   0.3925512135028839,\n",
       "   0.39391401410102844,\n",
       "   0.4172729551792145,\n",
       "   0.4141359031200409,\n",
       "   0.3653806447982788,\n",
       "   0.3779149055480957,\n",
       "   0.38658320903778076,\n",
       "   0.3982977569103241,\n",
       "   0.37396901845932007,\n",
       "   0.3826194703578949,\n",
       "   0.3881835639476776,\n",
       "   0.36672645807266235,\n",
       "   0.39739638566970825,\n",
       "   0.38740426301956177,\n",
       "   0.3751656115055084,\n",
       "   0.39614081382751465,\n",
       "   0.36087626218795776,\n",
       "   0.35382241010665894,\n",
       "   0.37163621187210083,\n",
       "   0.3504478931427002,\n",
       "   0.3854701817035675,\n",
       "   0.4001028537750244,\n",
       "   0.39955905079841614,\n",
       "   0.39180439710617065,\n",
       "   0.39795854687690735,\n",
       "   0.38913586735725403,\n",
       "   0.3560805916786194,\n",
       "   0.39972183108329773,\n",
       "   0.3724697530269623,\n",
       "   0.39755046367645264,\n",
       "   0.399150550365448,\n",
       "   0.4013545513153076,\n",
       "   0.40440481901168823,\n",
       "   0.3816046416759491,\n",
       "   0.37597450613975525,\n",
       "   0.3876243531703949,\n",
       "   0.4225762188434601,\n",
       "   0.38069018721580505,\n",
       "   0.39748430252075195,\n",
       "   0.36771032214164734,\n",
       "   0.39543285965919495,\n",
       "   0.38776373863220215,\n",
       "   0.34557950496673584,\n",
       "   0.4070446491241455,\n",
       "   0.41799870133399963,\n",
       "   0.37470656633377075,\n",
       "   0.4059354364871979,\n",
       "   0.3664529621601105,\n",
       "   0.3905983865261078,\n",
       "   0.377569317817688,\n",
       "   0.3525235950946808,\n",
       "   0.34832650423049927,\n",
       "   0.38490989804267883,\n",
       "   0.3607533872127533,\n",
       "   0.39599743485450745,\n",
       "   0.3596439063549042,\n",
       "   0.3816235661506653]},\n",
       " 3: {'lr': 1e-05,\n",
       "  'best_loss_epoch': 499,\n",
       "  'best_acc_epoch': 433,\n",
       "  'best_r2_epoch': 11,\n",
       "  'pce_loss': [15.727108001708984,\n",
       "   8.637561798095703,\n",
       "   5.03402042388916,\n",
       "   2.9930953979492188,\n",
       "   1.8721213340759277,\n",
       "   1.143565058708191,\n",
       "   0.6399388313293457,\n",
       "   0.359045147895813,\n",
       "   0.20058268308639526,\n",
       "   0.12406522780656815,\n",
       "   0.09549210965633392,\n",
       "   0.09635841101408005,\n",
       "   0.11207138001918793,\n",
       "   0.1345817595720291,\n",
       "   0.15443430840969086,\n",
       "   0.16352790594100952,\n",
       "   0.1831681728363037,\n",
       "   0.20061853528022766,\n",
       "   0.21278612315654755,\n",
       "   0.22258824110031128,\n",
       "   0.22488324344158173,\n",
       "   0.23211923241615295,\n",
       "   0.23232200741767883,\n",
       "   0.2360733449459076,\n",
       "   0.2361813187599182,\n",
       "   0.22950297594070435,\n",
       "   0.22367650270462036,\n",
       "   0.21682365238666534,\n",
       "   0.2130679190158844,\n",
       "   0.21097543835639954,\n",
       "   0.20561844110488892,\n",
       "   0.2051299810409546,\n",
       "   0.19933098554611206,\n",
       "   0.19662567973136902,\n",
       "   0.19225412607192993,\n",
       "   0.1985769122838974,\n",
       "   0.20147472620010376,\n",
       "   0.2114570140838623,\n",
       "   0.21702465415000916,\n",
       "   0.21817897260189056,\n",
       "   0.2236272543668747,\n",
       "   0.2231435924768448,\n",
       "   0.22266338765621185,\n",
       "   0.22462382912635803,\n",
       "   0.22480663657188416,\n",
       "   0.22341516613960266,\n",
       "   0.22141796350479126,\n",
       "   0.22021286189556122,\n",
       "   0.21416544914245605,\n",
       "   0.21095965802669525,\n",
       "   0.20326457917690277,\n",
       "   0.19339105486869812,\n",
       "   0.1882053017616272,\n",
       "   0.18327634036540985,\n",
       "   0.1760558933019638,\n",
       "   0.16599437594413757,\n",
       "   0.15769456326961517,\n",
       "   0.15303796529769897,\n",
       "   0.1472546011209488,\n",
       "   0.13989686965942383,\n",
       "   0.13596086204051971,\n",
       "   0.13072040677070618,\n",
       "   0.12968102097511292,\n",
       "   0.1288386881351471,\n",
       "   0.12630166113376617,\n",
       "   0.12660449743270874,\n",
       "   0.1284569650888443,\n",
       "   0.12887120246887207,\n",
       "   0.12807099521160126,\n",
       "   0.12776072323322296,\n",
       "   0.1265929937362671,\n",
       "   0.12460822612047195,\n",
       "   0.12130741029977798,\n",
       "   0.12187688797712326,\n",
       "   0.12126436829566956,\n",
       "   0.12233893573284149,\n",
       "   0.11770683526992798,\n",
       "   0.11911492794752121,\n",
       "   0.11502061784267426,\n",
       "   0.11473067849874496,\n",
       "   0.11298934370279312,\n",
       "   0.10900228470563889,\n",
       "   0.10684196650981903,\n",
       "   0.10549623519182205,\n",
       "   0.10728825628757477,\n",
       "   0.10835980623960495,\n",
       "   0.10812825709581375,\n",
       "   0.10819884389638901,\n",
       "   0.1069578006863594,\n",
       "   0.11057595163583755,\n",
       "   0.1132051944732666,\n",
       "   0.11592183262109756,\n",
       "   0.11957898736000061,\n",
       "   0.1184675395488739,\n",
       "   0.12088177353143692,\n",
       "   0.12054216116666794,\n",
       "   0.11895100027322769,\n",
       "   0.120221346616745,\n",
       "   0.12178223580121994,\n",
       "   0.12095647305250168,\n",
       "   0.12086191773414612,\n",
       "   0.1204003095626831,\n",
       "   0.11899559944868088,\n",
       "   0.11491097509860992,\n",
       "   0.11357074975967407,\n",
       "   0.1118156760931015,\n",
       "   0.11191248148679733,\n",
       "   0.11253216862678528,\n",
       "   0.1097375676035881,\n",
       "   0.10800697654485703,\n",
       "   0.10687625408172607,\n",
       "   0.10757768899202347,\n",
       "   0.10814063996076584,\n",
       "   0.10516553372144699,\n",
       "   0.10362699627876282,\n",
       "   0.1057821661233902,\n",
       "   0.1061052531003952,\n",
       "   0.10643282532691956,\n",
       "   0.1052560955286026,\n",
       "   0.10497697442770004,\n",
       "   0.10608062148094177,\n",
       "   0.10500083118677139,\n",
       "   0.10486828535795212,\n",
       "   0.10565570741891861,\n",
       "   0.10724764317274094,\n",
       "   0.1067986786365509,\n",
       "   0.1106807217001915,\n",
       "   0.11440110951662064,\n",
       "   0.11955174803733826,\n",
       "   0.12469495832920074,\n",
       "   0.12947086989879608,\n",
       "   0.13180625438690186,\n",
       "   0.13528147339820862,\n",
       "   0.13839091360569,\n",
       "   0.14116767048835754,\n",
       "   0.1407792866230011,\n",
       "   0.1422092318534851,\n",
       "   0.14669647812843323,\n",
       "   0.14820496737957,\n",
       "   0.1520358771085739,\n",
       "   0.14961081743240356,\n",
       "   0.14773043990135193,\n",
       "   0.14651907980442047,\n",
       "   0.14279437065124512,\n",
       "   0.14360643923282623,\n",
       "   0.1420406997203827,\n",
       "   0.14081746339797974,\n",
       "   0.1392628252506256,\n",
       "   0.13652262091636658,\n",
       "   0.13145239651203156,\n",
       "   0.1303185671567917,\n",
       "   0.12920831143856049,\n",
       "   0.12839190661907196,\n",
       "   0.12818141281604767,\n",
       "   0.12811079621315002,\n",
       "   0.12786297500133514,\n",
       "   0.12861087918281555,\n",
       "   0.12829218804836273,\n",
       "   0.1291537582874298,\n",
       "   0.13305701315402985,\n",
       "   0.1304703950881958,\n",
       "   0.1307736188173294,\n",
       "   0.13249681890010834,\n",
       "   0.13302789628505707,\n",
       "   0.1353558599948883,\n",
       "   0.13764885067939758,\n",
       "   0.13623875379562378,\n",
       "   0.13840985298156738,\n",
       "   0.13704775273799896,\n",
       "   0.13762113451957703,\n",
       "   0.13507308065891266,\n",
       "   0.13110414147377014,\n",
       "   0.1303369253873825,\n",
       "   0.12932035326957703,\n",
       "   0.12939749658107758,\n",
       "   0.13077174127101898,\n",
       "   0.13046914339065552,\n",
       "   0.12882407009601593,\n",
       "   0.1274680346250534,\n",
       "   0.12704983353614807,\n",
       "   0.129081591963768,\n",
       "   0.12808416783809662,\n",
       "   0.1258382350206375,\n",
       "   0.12528115510940552,\n",
       "   0.12860850989818573,\n",
       "   0.12737488746643066,\n",
       "   0.12726260721683502,\n",
       "   0.12796998023986816,\n",
       "   0.1268654614686966,\n",
       "   0.12841537594795227,\n",
       "   0.1284073144197464,\n",
       "   0.12764433026313782,\n",
       "   0.1268237829208374,\n",
       "   0.12770766019821167,\n",
       "   0.12806463241577148,\n",
       "   0.128219872713089,\n",
       "   0.1280451864004135,\n",
       "   0.12936612963676453,\n",
       "   0.13033360242843628,\n",
       "   0.1300044059753418,\n",
       "   0.12864747643470764,\n",
       "   0.12816299498081207,\n",
       "   0.128471240401268,\n",
       "   0.12785522639751434,\n",
       "   0.12734895944595337,\n",
       "   0.12761817872524261,\n",
       "   0.1288759857416153,\n",
       "   0.1278018355369568,\n",
       "   0.12604402005672455,\n",
       "   0.12602946162223816,\n",
       "   0.12525981664657593,\n",
       "   0.1245807483792305,\n",
       "   0.12367990612983704,\n",
       "   0.12380748987197876,\n",
       "   0.12315427511930466,\n",
       "   0.12303175777196884,\n",
       "   0.12102427333593369,\n",
       "   0.12368205934762955,\n",
       "   0.12411472946405411,\n",
       "   0.1245909035205841,\n",
       "   0.12532645463943481,\n",
       "   0.1272665560245514,\n",
       "   0.12767185270786285,\n",
       "   0.12891080975532532,\n",
       "   0.1281447857618332,\n",
       "   0.12947791814804077,\n",
       "   0.1298990100622177,\n",
       "   0.13171900808811188,\n",
       "   0.1338074505329132,\n",
       "   0.13620910048484802,\n",
       "   0.135691300034523,\n",
       "   0.13524822890758514,\n",
       "   0.1357019543647766,\n",
       "   0.1360156387090683,\n",
       "   0.13406884670257568,\n",
       "   0.1353331208229065,\n",
       "   0.13299734890460968,\n",
       "   0.13332177698612213,\n",
       "   0.13140052556991577,\n",
       "   0.13132931292057037,\n",
       "   0.13100114464759827,\n",
       "   0.13370254635810852,\n",
       "   0.1345464140176773,\n",
       "   0.1334703266620636,\n",
       "   0.13109557330608368,\n",
       "   0.12948104739189148,\n",
       "   0.12896396219730377,\n",
       "   0.12727142870426178,\n",
       "   0.12679217755794525,\n",
       "   0.12633614242076874,\n",
       "   0.12464918941259384,\n",
       "   0.12370680272579193,\n",
       "   0.12483137100934982,\n",
       "   0.12550586462020874,\n",
       "   0.1271974742412567,\n",
       "   0.12712949514389038,\n",
       "   0.126533642411232,\n",
       "   0.12474465370178223,\n",
       "   0.12505558133125305,\n",
       "   0.12529103457927704,\n",
       "   0.12628419697284698,\n",
       "   0.12843142449855804,\n",
       "   0.1287803053855896,\n",
       "   0.12992431223392487,\n",
       "   0.13175468146800995,\n",
       "   0.13248386979103088,\n",
       "   0.13338381052017212,\n",
       "   0.1347021907567978,\n",
       "   0.13570304214954376,\n",
       "   0.13507944345474243,\n",
       "   0.13326814770698547,\n",
       "   0.13317450881004333,\n",
       "   0.13217949867248535,\n",
       "   0.12928839027881622,\n",
       "   0.12722930312156677,\n",
       "   0.12428946048021317,\n",
       "   0.1250414401292801,\n",
       "   0.12465532124042511,\n",
       "   0.1227966696023941,\n",
       "   0.12215935438871384,\n",
       "   0.12087883055210114,\n",
       "   0.11883222311735153,\n",
       "   0.11744261533021927,\n",
       "   0.1189340204000473,\n",
       "   0.12051267176866531,\n",
       "   0.11958929896354675,\n",
       "   0.12011652439832687,\n",
       "   0.11962728947401047,\n",
       "   0.11994802206754684,\n",
       "   0.12037719041109085,\n",
       "   0.12010312080383301,\n",
       "   0.12089087814092636,\n",
       "   0.12166900187730789,\n",
       "   0.12193785607814789,\n",
       "   0.12234228104352951,\n",
       "   0.12432441860437393,\n",
       "   0.12533284723758698,\n",
       "   0.12552934885025024,\n",
       "   0.1276322603225708,\n",
       "   0.12551827728748322,\n",
       "   0.1237511932849884,\n",
       "   0.1223200336098671,\n",
       "   0.12053485959768295,\n",
       "   0.11843313276767731,\n",
       "   0.11619549244642258,\n",
       "   0.11431039869785309,\n",
       "   0.11362462490797043,\n",
       "   0.11457418650388718,\n",
       "   0.11580586433410645,\n",
       "   0.11719844490289688,\n",
       "   0.11555773764848709,\n",
       "   0.11688485741615295,\n",
       "   0.1177188903093338,\n",
       "   0.11761736869812012,\n",
       "   0.11728009581565857,\n",
       "   0.1170022264122963,\n",
       "   0.11669620126485825,\n",
       "   0.11602011322975159,\n",
       "   0.11542312055826187,\n",
       "   0.11468371003866196,\n",
       "   0.11519766598939896,\n",
       "   0.11321863532066345,\n",
       "   0.11382978409528732,\n",
       "   0.11330745369195938,\n",
       "   0.11517897248268127,\n",
       "   0.11474531143903732,\n",
       "   0.11625482141971588,\n",
       "   0.11681867390871048,\n",
       "   0.11952528357505798,\n",
       "   0.12016753852367401,\n",
       "   0.11912588775157928,\n",
       "   0.11979571729898453,\n",
       "   0.11824116855859756,\n",
       "   0.11739540100097656,\n",
       "   0.11761004477739334,\n",
       "   0.1168002039194107,\n",
       "   0.11699626594781876,\n",
       "   0.11823143064975739,\n",
       "   0.11895421147346497,\n",
       "   0.12095878273248672,\n",
       "   0.12151816487312317,\n",
       "   0.12127821147441864,\n",
       "   0.12196952104568481,\n",
       "   0.12400005012750626,\n",
       "   0.12332737445831299,\n",
       "   0.12387256324291229,\n",
       "   0.12335668504238129,\n",
       "   0.12300579994916916,\n",
       "   0.12397419661283493,\n",
       "   0.12527824938297272,\n",
       "   0.12536337971687317,\n",
       "   0.12434661388397217,\n",
       "   0.12258566170930862,\n",
       "   0.12235020846128464,\n",
       "   0.12018527835607529,\n",
       "   0.11760486662387848,\n",
       "   0.11860029399394989,\n",
       "   0.11927352845668793,\n",
       "   0.11858959496021271,\n",
       "   0.11663830280303955,\n",
       "   0.11585089564323425,\n",
       "   0.11584299057722092,\n",
       "   0.11645998060703278,\n",
       "   0.11654499918222427,\n",
       "   0.11653990298509598,\n",
       "   0.11680963635444641,\n",
       "   0.1178460419178009,\n",
       "   0.11850617080926895,\n",
       "   0.11890792846679688,\n",
       "   0.11908721923828125,\n",
       "   0.11997127532958984,\n",
       "   0.1193532794713974,\n",
       "   0.11986902356147766,\n",
       "   0.1183137372136116,\n",
       "   0.11861533671617508,\n",
       "   0.11883227527141571,\n",
       "   0.1169598326086998,\n",
       "   0.11622750014066696,\n",
       "   0.1157224103808403,\n",
       "   0.11555768549442291,\n",
       "   0.11550003290176392,\n",
       "   0.11516697704792023,\n",
       "   0.11431387066841125,\n",
       "   0.11455786973237991,\n",
       "   0.1157141625881195,\n",
       "   0.11606521904468536,\n",
       "   0.11876068264245987,\n",
       "   0.12016177177429199,\n",
       "   0.12179532647132874,\n",
       "   0.12356485426425934,\n",
       "   0.12427075207233429,\n",
       "   0.12434487789869308,\n",
       "   0.12269418686628342,\n",
       "   0.12427408248186111,\n",
       "   0.1227250024676323,\n",
       "   0.12043077498674393,\n",
       "   0.1179315522313118,\n",
       "   0.1153881773352623,\n",
       "   0.11457545310258865,\n",
       "   0.11245250701904297,\n",
       "   0.11248290538787842,\n",
       "   0.11139757186174393,\n",
       "   0.11326313018798828,\n",
       "   0.1094285398721695,\n",
       "   0.11048232018947601,\n",
       "   0.11239873617887497,\n",
       "   0.11392407864332199,\n",
       "   0.11330843716859818,\n",
       "   0.11235670745372772,\n",
       "   0.11247322708368301,\n",
       "   0.1122557744383812,\n",
       "   0.11257478594779968,\n",
       "   0.11428267508745193,\n",
       "   0.11335213482379913,\n",
       "   0.11305999755859375,\n",
       "   0.11529234051704407,\n",
       "   0.1147884726524353,\n",
       "   0.11412036418914795,\n",
       "   0.11442504823207855,\n",
       "   0.11642619222402573,\n",
       "   0.11640512198209763,\n",
       "   0.11768977344036102,\n",
       "   0.11733218282461166,\n",
       "   0.11512444168329239,\n",
       "   0.11440634727478027,\n",
       "   0.11267177015542984,\n",
       "   0.10950259864330292,\n",
       "   0.10930021852254868,\n",
       "   0.10847598314285278,\n",
       "   0.10833137482404709,\n",
       "   0.10945338010787964,\n",
       "   0.10964686423540115,\n",
       "   0.11090623587369919,\n",
       "   0.1111888512969017,\n",
       "   0.10998240113258362,\n",
       "   0.10961220413446426,\n",
       "   0.10830309242010117,\n",
       "   0.10655522346496582,\n",
       "   0.10566280037164688,\n",
       "   0.10611361265182495,\n",
       "   0.10755854099988937,\n",
       "   0.10680435597896576,\n",
       "   0.10735224187374115,\n",
       "   0.10801264643669128,\n",
       "   0.10778742283582687,\n",
       "   0.10781574994325638,\n",
       "   0.11123385280370712,\n",
       "   0.11082833260297775,\n",
       "   0.1124691367149353,\n",
       "   0.11495723575353622,\n",
       "   0.11558075249195099,\n",
       "   0.11612407118082047,\n",
       "   0.1151094064116478,\n",
       "   0.1156144067645073,\n",
       "   0.11669426411390305,\n",
       "   0.11737962067127228,\n",
       "   0.11842211335897446,\n",
       "   0.11728260666131973,\n",
       "   0.1137518584728241,\n",
       "   0.11331894248723984,\n",
       "   0.11149389296770096,\n",
       "   0.10905884951353073,\n",
       "   0.11052225530147552,\n",
       "   0.1079983040690422,\n",
       "   0.10637909919023514,\n",
       "   0.10406237095594406,\n",
       "   0.10325737297534943,\n",
       "   0.10293426364660263,\n",
       "   0.10244691371917725,\n",
       "   0.10300488024950027,\n",
       "   0.10337890684604645,\n",
       "   0.10403995960950851,\n",
       "   0.10381561517715454,\n",
       "   0.10343688726425171,\n",
       "   0.10337868332862854,\n",
       "   0.10305401682853699,\n",
       "   0.10444995015859604,\n",
       "   0.10405278205871582,\n",
       "   0.10404013097286224,\n",
       "   0.10524514317512512,\n",
       "   0.10461083799600601,\n",
       "   0.10378425568342209,\n",
       "   0.10285674780607224,\n",
       "   0.10147065669298172,\n",
       "   0.10072781145572662,\n",
       "   0.10117442905902863,\n",
       "   0.10341742634773254,\n",
       "   0.10377518087625504,\n",
       "   0.10527494549751282,\n",
       "   0.10605400055646896,\n",
       "   0.10582069307565689,\n",
       "   0.10642727464437485,\n",
       "   0.1079067811369896,\n",
       "   0.10695221275091171,\n",
       "   0.10703329741954803,\n",
       "   0.10599710047245026,\n",
       "   0.10550729930400848,\n",
       "   0.10521303117275238,\n",
       "   0.10643065720796585,\n",
       "   0.10808254778385162],\n",
       "  'voc_loss': [1.1047250032424927,\n",
       "   0.38941892981529236,\n",
       "   0.1729310154914856,\n",
       "   0.06546693295240402,\n",
       "   0.03758988529443741,\n",
       "   0.03602573648095131,\n",
       "   0.04694904386997223,\n",
       "   0.055102817714214325,\n",
       "   0.07029233127832413,\n",
       "   0.09169430285692215,\n",
       "   0.10776679962873459,\n",
       "   0.1151777133345604,\n",
       "   0.14272481203079224,\n",
       "   0.14793580770492554,\n",
       "   0.15662793815135956,\n",
       "   0.16430051624774933,\n",
       "   0.15822935104370117,\n",
       "   0.15299268066883087,\n",
       "   0.15024542808532715,\n",
       "   0.1486000418663025,\n",
       "   0.14522086083889008,\n",
       "   0.14440633356571198,\n",
       "   0.1270478367805481,\n",
       "   0.12496352195739746,\n",
       "   0.12729662656784058,\n",
       "   0.11098428070545197,\n",
       "   0.10517898201942444,\n",
       "   0.09341755509376526,\n",
       "   0.08041125535964966,\n",
       "   0.07394269853830338,\n",
       "   0.06497617065906525,\n",
       "   0.0633666142821312,\n",
       "   0.059208568185567856,\n",
       "   0.05415758118033409,\n",
       "   0.048176318407058716,\n",
       "   0.04188896343111992,\n",
       "   0.03936886042356491,\n",
       "   0.03997024893760681,\n",
       "   0.04007145017385483,\n",
       "   0.0386519581079483,\n",
       "   0.03714664652943611,\n",
       "   0.03915988653898239,\n",
       "   0.041304972022771835,\n",
       "   0.046219900250434875,\n",
       "   0.04796398803591728,\n",
       "   0.049853064119815826,\n",
       "   0.04700155928730965,\n",
       "   0.04801059514284134,\n",
       "   0.051181063055992126,\n",
       "   0.05630826950073242,\n",
       "   0.061582937836647034,\n",
       "   0.06747833639383316,\n",
       "   0.07002435624599457,\n",
       "   0.07914241403341293,\n",
       "   0.08232792466878891,\n",
       "   0.09216609597206116,\n",
       "   0.09971606731414795,\n",
       "   0.1062721386551857,\n",
       "   0.1052650734782219,\n",
       "   0.10328042507171631,\n",
       "   0.10411250591278076,\n",
       "   0.1004951074719429,\n",
       "   0.102989062666893,\n",
       "   0.1034177765250206,\n",
       "   0.10047823190689087,\n",
       "   0.09601882100105286,\n",
       "   0.09230103343725204,\n",
       "   0.08981440216302872,\n",
       "   0.08664800226688385,\n",
       "   0.0885288193821907,\n",
       "   0.08946094661951065,\n",
       "   0.0881497859954834,\n",
       "   0.09203336387872696,\n",
       "   0.09499513357877731,\n",
       "   0.09861651808023453,\n",
       "   0.10386332869529724,\n",
       "   0.10266365855932236,\n",
       "   0.10129576176404953,\n",
       "   0.10617417097091675,\n",
       "   0.10981350392103195,\n",
       "   0.11652504652738571,\n",
       "   0.12217134982347488,\n",
       "   0.12015020102262497,\n",
       "   0.12776899337768555,\n",
       "   0.12877841293811798,\n",
       "   0.1363392323255539,\n",
       "   0.13440589606761932,\n",
       "   0.1412239968776703,\n",
       "   0.13933126628398895,\n",
       "   0.13803985714912415,\n",
       "   0.13067972660064697,\n",
       "   0.1275380551815033,\n",
       "   0.12124970555305481,\n",
       "   0.11307317763566971,\n",
       "   0.10862547904253006,\n",
       "   0.09793194383382797,\n",
       "   0.09324722737073898,\n",
       "   0.08813866972923279,\n",
       "   0.08019230514764786,\n",
       "   0.07321009784936905,\n",
       "   0.06859444081783295,\n",
       "   0.0711125060915947,\n",
       "   0.06943707913160324,\n",
       "   0.07307461649179459,\n",
       "   0.07329433411359787,\n",
       "   0.07408574968576431,\n",
       "   0.07195582985877991,\n",
       "   0.07206707447767258,\n",
       "   0.06987590342760086,\n",
       "   0.06804900616407394,\n",
       "   0.07113800197839737,\n",
       "   0.07828420400619507,\n",
       "   0.0816677063703537,\n",
       "   0.08488703519105911,\n",
       "   0.09112200886011124,\n",
       "   0.09515143185853958,\n",
       "   0.10005185753107071,\n",
       "   0.09822065383195877,\n",
       "   0.10096028447151184,\n",
       "   0.10403291136026382,\n",
       "   0.10911542922258377,\n",
       "   0.113396555185318,\n",
       "   0.1169886365532875,\n",
       "   0.12095583230257034,\n",
       "   0.12301989644765854,\n",
       "   0.12499141693115234,\n",
       "   0.12657888233661652,\n",
       "   0.12706048786640167,\n",
       "   0.12935155630111694,\n",
       "   0.12552013993263245,\n",
       "   0.12080635130405426,\n",
       "   0.11897510290145874,\n",
       "   0.12015698105096817,\n",
       "   0.11693791300058365,\n",
       "   0.11218103766441345,\n",
       "   0.10573814809322357,\n",
       "   0.09661830961704254,\n",
       "   0.09392755478620529,\n",
       "   0.08971412479877472,\n",
       "   0.08592074364423752,\n",
       "   0.0811316967010498,\n",
       "   0.07954300940036774,\n",
       "   0.07794398069381714,\n",
       "   0.07726524770259857,\n",
       "   0.07840299606323242,\n",
       "   0.0750914067029953,\n",
       "   0.07502640783786774,\n",
       "   0.07213572412729263,\n",
       "   0.07135693728923798,\n",
       "   0.06985490769147873,\n",
       "   0.06835626065731049,\n",
       "   0.06619511544704437,\n",
       "   0.07199490815401077,\n",
       "   0.07668378204107285,\n",
       "   0.08118496835231781,\n",
       "   0.08721709251403809,\n",
       "   0.0914192795753479,\n",
       "   0.09762062132358551,\n",
       "   0.09947390109300613,\n",
       "   0.09889187663793564,\n",
       "   0.1054321676492691,\n",
       "   0.10556918382644653,\n",
       "   0.10921210795640945,\n",
       "   0.10979755967855453,\n",
       "   0.11275400221347809,\n",
       "   0.11215610057115555,\n",
       "   0.1120956763625145,\n",
       "   0.11539080739021301,\n",
       "   0.11394545435905457,\n",
       "   0.1137860044836998,\n",
       "   0.11254028230905533,\n",
       "   0.10890237241983414,\n",
       "   0.1052166298031807,\n",
       "   0.10458295047283173,\n",
       "   0.10242629796266556,\n",
       "   0.09811556339263916,\n",
       "   0.09247048944234848,\n",
       "   0.08894150704145432,\n",
       "   0.08689041435718536,\n",
       "   0.08038340508937836,\n",
       "   0.08154745399951935,\n",
       "   0.08174190670251846,\n",
       "   0.08248130232095718,\n",
       "   0.07991936802864075,\n",
       "   0.0811883881688118,\n",
       "   0.07999028265476227,\n",
       "   0.07923481613397598,\n",
       "   0.07425530254840851,\n",
       "   0.07952772080898285,\n",
       "   0.07465752959251404,\n",
       "   0.07537367939949036,\n",
       "   0.07756690680980682,\n",
       "   0.07702076435089111,\n",
       "   0.07661427557468414,\n",
       "   0.08042406290769577,\n",
       "   0.08294522017240524,\n",
       "   0.0796569436788559,\n",
       "   0.07845614850521088,\n",
       "   0.07575670629739761,\n",
       "   0.07783114165067673,\n",
       "   0.07670417428016663,\n",
       "   0.07712045311927795,\n",
       "   0.07638664543628693,\n",
       "   0.07380493730306625,\n",
       "   0.06970302015542984,\n",
       "   0.07190487533807755,\n",
       "   0.07132157683372498,\n",
       "   0.07345167547464371,\n",
       "   0.0737137570977211,\n",
       "   0.07243648916482925,\n",
       "   0.07321842759847641,\n",
       "   0.07256699353456497,\n",
       "   0.07130780071020126,\n",
       "   0.06705863028764725,\n",
       "   0.06850910186767578,\n",
       "   0.0703764483332634,\n",
       "   0.07295208424329758,\n",
       "   0.07112967222929001,\n",
       "   0.06868785619735718,\n",
       "   0.06711149960756302,\n",
       "   0.06621388345956802,\n",
       "   0.06736396998167038,\n",
       "   0.06691577285528183,\n",
       "   0.06637837737798691,\n",
       "   0.06678257882595062,\n",
       "   0.06939739733934402,\n",
       "   0.06550171226263046,\n",
       "   0.06897120922803879,\n",
       "   0.07493265718221664,\n",
       "   0.07544171065092087,\n",
       "   0.07670333981513977,\n",
       "   0.07467877119779587,\n",
       "   0.07123004645109177,\n",
       "   0.0676165223121643,\n",
       "   0.06512977182865143,\n",
       "   0.06298631429672241,\n",
       "   0.06142875552177429,\n",
       "   0.05764717608690262,\n",
       "   0.051558591425418854,\n",
       "   0.04805047810077667,\n",
       "   0.046214595437049866,\n",
       "   0.04082952067255974,\n",
       "   0.03751768171787262,\n",
       "   0.039004553109407425,\n",
       "   0.03872509300708771,\n",
       "   0.03741714358329773,\n",
       "   0.03730838745832443,\n",
       "   0.03795435652136803,\n",
       "   0.03715227171778679,\n",
       "   0.039148494601249695,\n",
       "   0.04181832820177078,\n",
       "   0.045285072177648544,\n",
       "   0.05010288208723068,\n",
       "   0.05479273945093155,\n",
       "   0.056734926998615265,\n",
       "   0.05867185816168785,\n",
       "   0.05914045497775078,\n",
       "   0.058097291737794876,\n",
       "   0.058651890605688095,\n",
       "   0.05844220891594887,\n",
       "   0.05795476213097572,\n",
       "   0.06013079732656479,\n",
       "   0.062159694731235504,\n",
       "   0.06238515302538872,\n",
       "   0.06492161005735397,\n",
       "   0.06548796594142914,\n",
       "   0.06957018375396729,\n",
       "   0.07491029798984528,\n",
       "   0.07406690716743469,\n",
       "   0.07436200231313705,\n",
       "   0.07680099457502365,\n",
       "   0.07653564214706421,\n",
       "   0.07571138441562653,\n",
       "   0.07798981666564941,\n",
       "   0.07627987861633301,\n",
       "   0.07410147786140442,\n",
       "   0.06955622881650925,\n",
       "   0.06798169016838074,\n",
       "   0.06951382011175156,\n",
       "   0.06981176882982254,\n",
       "   0.06580859422683716,\n",
       "   0.06105446442961693,\n",
       "   0.060895148664712906,\n",
       "   0.058523811399936676,\n",
       "   0.05601397529244423,\n",
       "   0.05867118015885353,\n",
       "   0.058550454676151276,\n",
       "   0.058555882424116135,\n",
       "   0.05763634666800499,\n",
       "   0.05977140739560127,\n",
       "   0.06370826810598373,\n",
       "   0.06608227640390396,\n",
       "   0.07271790504455566,\n",
       "   0.07575054466724396,\n",
       "   0.0770188644528389,\n",
       "   0.07428429275751114,\n",
       "   0.07261805981397629,\n",
       "   0.06775996088981628,\n",
       "   0.06508684158325195,\n",
       "   0.06563872843980789,\n",
       "   0.0645371675491333,\n",
       "   0.0638522133231163,\n",
       "   0.0602523535490036,\n",
       "   0.059485793113708496,\n",
       "   0.061555713415145874,\n",
       "   0.06334403157234192,\n",
       "   0.06442724913358688,\n",
       "   0.06792835146188736,\n",
       "   0.06587649136781693,\n",
       "   0.06537345796823502,\n",
       "   0.06317935883998871,\n",
       "   0.06041460856795311,\n",
       "   0.06042438745498657,\n",
       "   0.06058500334620476,\n",
       "   0.06250235438346863,\n",
       "   0.06689965724945068,\n",
       "   0.06496071070432663,\n",
       "   0.06540179252624512,\n",
       "   0.0652330070734024,\n",
       "   0.06540486961603165,\n",
       "   0.06371799856424332,\n",
       "   0.061012208461761475,\n",
       "   0.057824041694402695,\n",
       "   0.05622733756899834,\n",
       "   0.05550850182771683,\n",
       "   0.05643106997013092,\n",
       "   0.059832703322172165,\n",
       "   0.05500932037830353,\n",
       "   0.052178751677274704,\n",
       "   0.049872659146785736,\n",
       "   0.04801410064101219,\n",
       "   0.04454515874385834,\n",
       "   0.04477958381175995,\n",
       "   0.04684799537062645,\n",
       "   0.04699220135807991,\n",
       "   0.04904933646321297,\n",
       "   0.0511263906955719,\n",
       "   0.05311829596757889,\n",
       "   0.05200856551527977,\n",
       "   0.051948901265859604,\n",
       "   0.049717292189598083,\n",
       "   0.04983912780880928,\n",
       "   0.05098458379507065,\n",
       "   0.04934300109744072,\n",
       "   0.05004701763391495,\n",
       "   0.04782911390066147,\n",
       "   0.04383957013487816,\n",
       "   0.04601506516337395,\n",
       "   0.04526444524526596,\n",
       "   0.04335550591349602,\n",
       "   0.04174688458442688,\n",
       "   0.04128066450357437,\n",
       "   0.042736250907182693,\n",
       "   0.04463419318199158,\n",
       "   0.04666997492313385,\n",
       "   0.049181852489709854,\n",
       "   0.050963278859853745,\n",
       "   0.05220459774136543,\n",
       "   0.05423865467309952,\n",
       "   0.05646970123052597,\n",
       "   0.05860389396548271,\n",
       "   0.0601932518184185,\n",
       "   0.06056167557835579,\n",
       "   0.06215917319059372,\n",
       "   0.061537519097328186,\n",
       "   0.06277457624673843,\n",
       "   0.060708045959472656,\n",
       "   0.06104828044772148,\n",
       "   0.05887971445918083,\n",
       "   0.05681980401277542,\n",
       "   0.051475170999765396,\n",
       "   0.04622731730341911,\n",
       "   0.04774904251098633,\n",
       "   0.05085088312625885,\n",
       "   0.05195917189121246,\n",
       "   0.05044255033135414,\n",
       "   0.04929850995540619,\n",
       "   0.04542364925146103,\n",
       "   0.04511796310544014,\n",
       "   0.04468727111816406,\n",
       "   0.044129516929388046,\n",
       "   0.043425969779491425,\n",
       "   0.04151123762130737,\n",
       "   0.04031124711036682,\n",
       "   0.037165261805057526,\n",
       "   0.03373770788311958,\n",
       "   0.032113492488861084,\n",
       "   0.03361976146697998,\n",
       "   0.03544406220316887,\n",
       "   0.0362524576485157,\n",
       "   0.037538792937994,\n",
       "   0.03825226053595543,\n",
       "   0.040139585733413696,\n",
       "   0.04027100279927254,\n",
       "   0.040228355675935745,\n",
       "   0.0400913767516613,\n",
       "   0.042663995176553726,\n",
       "   0.041518792510032654,\n",
       "   0.040169309824705124,\n",
       "   0.03940598666667938,\n",
       "   0.03717953711748123,\n",
       "   0.03438581898808479,\n",
       "   0.03383355960249901,\n",
       "   0.0369923897087574,\n",
       "   0.03907844424247742,\n",
       "   0.04117996245622635,\n",
       "   0.04626252129673958,\n",
       "   0.05370005592703819,\n",
       "   0.052869878709316254,\n",
       "   0.05722422897815704,\n",
       "   0.056026868522167206,\n",
       "   0.05840044096112251,\n",
       "   0.05985807254910469,\n",
       "   0.059814997017383575,\n",
       "   0.06032878905534744,\n",
       "   0.05970986187458038,\n",
       "   0.05816498026251793,\n",
       "   0.056367985904216766,\n",
       "   0.053786542266607285,\n",
       "   0.0543346107006073,\n",
       "   0.053874507546424866,\n",
       "   0.05300748348236084,\n",
       "   0.053710225969552994,\n",
       "   0.051973000168800354,\n",
       "   0.04900222644209862,\n",
       "   0.04961609095335007,\n",
       "   0.04567722976207733,\n",
       "   0.04567449912428856,\n",
       "   0.04478933662176132,\n",
       "   0.04318277910351753,\n",
       "   0.037659917026758194,\n",
       "   0.03746300935745239,\n",
       "   0.03444832190871239,\n",
       "   0.031065069139003754,\n",
       "   0.031390681862831116,\n",
       "   0.033062078058719635,\n",
       "   0.03458813205361366,\n",
       "   0.03376977518200874,\n",
       "   0.03533632308244705,\n",
       "   0.04033997282385826,\n",
       "   0.04331871122121811,\n",
       "   0.04643319919705391,\n",
       "   0.04899563267827034,\n",
       "   0.05072196200489998,\n",
       "   0.053189717233181,\n",
       "   0.05416034907102585,\n",
       "   0.05461113899946213,\n",
       "   0.05646171793341637,\n",
       "   0.058185186237096786,\n",
       "   0.056838832795619965,\n",
       "   0.056772325187921524,\n",
       "   0.05713019147515297,\n",
       "   0.05378106236457825,\n",
       "   0.053288672119379044,\n",
       "   0.05066504701972008,\n",
       "   0.04842250794172287,\n",
       "   0.04949311539530754,\n",
       "   0.04966725409030914,\n",
       "   0.051248446106910706,\n",
       "   0.05074420943856239,\n",
       "   0.053503915667533875,\n",
       "   0.05392773449420929,\n",
       "   0.05381730943918228,\n",
       "   0.055721040815114975,\n",
       "   0.057562947273254395,\n",
       "   0.05715612694621086,\n",
       "   0.056824520230293274,\n",
       "   0.056698139756917953,\n",
       "   0.055157385766506195,\n",
       "   0.05329381301999092,\n",
       "   0.05203502997756004,\n",
       "   0.0513022318482399,\n",
       "   0.0482841394841671,\n",
       "   0.04682330787181854,\n",
       "   0.043264951556921005,\n",
       "   0.043216899037361145,\n",
       "   0.042685918509960175,\n",
       "   0.0409637913107872,\n",
       "   0.038649722933769226,\n",
       "   0.036978378891944885,\n",
       "   0.037225041538476944,\n",
       "   0.03766799345612526,\n",
       "   0.03818139433860779,\n",
       "   0.04143577069044113,\n",
       "   0.04448368772864342,\n",
       "   0.04775138944387436,\n",
       "   0.04649092257022858,\n",
       "   0.04781009256839752,\n",
       "   0.05020206421613693,\n",
       "   0.05006048083305359,\n",
       "   0.05188119783997536,\n",
       "   0.05500464141368866,\n",
       "   0.05583867058157921,\n",
       "   0.05469381809234619,\n",
       "   0.05288108438253403,\n",
       "   0.0498083271086216,\n",
       "   0.04806039482355118,\n",
       "   0.04426678642630577,\n",
       "   0.04281176999211311,\n",
       "   0.040656376630067825],\n",
       "  'jsc_loss': [0.33084264397621155,\n",
       "   0.2870785593986511,\n",
       "   0.2845703363418579,\n",
       "   0.27568912506103516,\n",
       "   0.25437888503074646,\n",
       "   0.24745671451091766,\n",
       "   0.23613004386425018,\n",
       "   0.22420117259025574,\n",
       "   0.22628431022167206,\n",
       "   0.21880875527858734,\n",
       "   0.2180653214454651,\n",
       "   0.2190483957529068,\n",
       "   0.2282990962266922,\n",
       "   0.25023993849754333,\n",
       "   0.2785179316997528,\n",
       "   0.3052159547805786,\n",
       "   0.3337489366531372,\n",
       "   0.36670100688934326,\n",
       "   0.39616191387176514,\n",
       "   0.4099351763725281,\n",
       "   0.42703887820243835,\n",
       "   0.4534416198730469,\n",
       "   0.4705313444137573,\n",
       "   0.4948543310165405,\n",
       "   0.5069539546966553,\n",
       "   0.5210115909576416,\n",
       "   0.5300835371017456,\n",
       "   0.5373327732086182,\n",
       "   0.5444106459617615,\n",
       "   0.548435628414154,\n",
       "   0.5504059195518494,\n",
       "   0.5533347129821777,\n",
       "   0.5465477108955383,\n",
       "   0.5477294921875,\n",
       "   0.5514324903488159,\n",
       "   0.5486053228378296,\n",
       "   0.5512977838516235,\n",
       "   0.5528508424758911,\n",
       "   0.5480132699012756,\n",
       "   0.5493325591087341,\n",
       "   0.5471439957618713,\n",
       "   0.5458805561065674,\n",
       "   0.547044575214386,\n",
       "   0.5467520356178284,\n",
       "   0.5432605147361755,\n",
       "   0.5438493490219116,\n",
       "   0.5435164570808411,\n",
       "   0.5418408513069153,\n",
       "   0.5408170819282532,\n",
       "   0.5417181849479675,\n",
       "   0.5410000085830688,\n",
       "   0.545266330242157,\n",
       "   0.5516617894172668,\n",
       "   0.5530980229377747,\n",
       "   0.5523805022239685,\n",
       "   0.5578979849815369,\n",
       "   0.5572260022163391,\n",
       "   0.557624876499176,\n",
       "   0.5564309358596802,\n",
       "   0.5567924380302429,\n",
       "   0.5588752031326294,\n",
       "   0.5597483515739441,\n",
       "   0.5576827526092529,\n",
       "   0.5493512749671936,\n",
       "   0.5379500985145569,\n",
       "   0.5312855243682861,\n",
       "   0.5258967280387878,\n",
       "   0.5174713730812073,\n",
       "   0.504547655582428,\n",
       "   0.4640519618988037,\n",
       "   0.4259227216243744,\n",
       "   0.3998152017593384,\n",
       "   0.3672579526901245,\n",
       "   0.3433424234390259,\n",
       "   0.32482853531837463,\n",
       "   0.29935967922210693,\n",
       "   0.2698490023612976,\n",
       "   0.25145307183265686,\n",
       "   0.23333525657653809,\n",
       "   0.21370165050029755,\n",
       "   0.1948074996471405,\n",
       "   0.17701631784439087,\n",
       "   0.15971563756465912,\n",
       "   0.14893586933612823,\n",
       "   0.1437598317861557,\n",
       "   0.13813525438308716,\n",
       "   0.12770213186740875,\n",
       "   0.12360898405313492,\n",
       "   0.12836866080760956,\n",
       "   0.12614773213863373,\n",
       "   0.12808118760585785,\n",
       "   0.12717069685459137,\n",
       "   0.1357044279575348,\n",
       "   0.13544459640979767,\n",
       "   0.13650232553482056,\n",
       "   0.13224488496780396,\n",
       "   0.1290612369775772,\n",
       "   0.133116215467453,\n",
       "   0.13161440193653107,\n",
       "   0.13041967153549194,\n",
       "   0.13110007345676422,\n",
       "   0.13354656100273132,\n",
       "   0.142195463180542,\n",
       "   0.14255256950855255,\n",
       "   0.14546135067939758,\n",
       "   0.14586937427520752,\n",
       "   0.1480521857738495,\n",
       "   0.15170082449913025,\n",
       "   0.14988893270492554,\n",
       "   0.14260242879390717,\n",
       "   0.13882023096084595,\n",
       "   0.13755059242248535,\n",
       "   0.13961298763751984,\n",
       "   0.13204312324523926,\n",
       "   0.13245849311351776,\n",
       "   0.1316344439983368,\n",
       "   0.13350597023963928,\n",
       "   0.13458214700222015,\n",
       "   0.13489174842834473,\n",
       "   0.12667016685009003,\n",
       "   0.13194242119789124,\n",
       "   0.12962497770786285,\n",
       "   0.13126258552074432,\n",
       "   0.12944112718105316,\n",
       "   0.12015368789434433,\n",
       "   0.11558698862791061,\n",
       "   0.10727032274007797,\n",
       "   0.1087101399898529,\n",
       "   0.11151402443647385,\n",
       "   0.1090228408575058,\n",
       "   0.10783564299345016,\n",
       "   0.1104569360613823,\n",
       "   0.1110866367816925,\n",
       "   0.10788507014513016,\n",
       "   0.11071892082691193,\n",
       "   0.11104899644851685,\n",
       "   0.1126173883676529,\n",
       "   0.11450644582509995,\n",
       "   0.11505364626646042,\n",
       "   0.11899897456169128,\n",
       "   0.11662028729915619,\n",
       "   0.11956986039876938,\n",
       "   0.12343939393758774,\n",
       "   0.12277299910783768,\n",
       "   0.12189263850450516,\n",
       "   0.1250716894865036,\n",
       "   0.11768998205661774,\n",
       "   0.11562364548444748,\n",
       "   0.11525443941354752,\n",
       "   0.11408868432044983,\n",
       "   0.11660156399011612,\n",
       "   0.11894728988409042,\n",
       "   0.11657525599002838,\n",
       "   0.11566940695047379,\n",
       "   0.11106043308973312,\n",
       "   0.10809588432312012,\n",
       "   0.10960552841424942,\n",
       "   0.10803161561489105,\n",
       "   0.10364590585231781,\n",
       "   0.10705675184726715,\n",
       "   0.11006497591733932,\n",
       "   0.11141461133956909,\n",
       "   0.10821303725242615,\n",
       "   0.111750029027462,\n",
       "   0.11415191739797592,\n",
       "   0.11506902426481247,\n",
       "   0.11496934294700623,\n",
       "   0.11320839077234268,\n",
       "   0.10677579790353775,\n",
       "   0.09898936748504639,\n",
       "   0.09718997776508331,\n",
       "   0.09384191036224365,\n",
       "   0.0939633846282959,\n",
       "   0.08911316841840744,\n",
       "   0.08737701177597046,\n",
       "   0.09046462178230286,\n",
       "   0.08546581119298935,\n",
       "   0.09131654351949692,\n",
       "   0.08949866890907288,\n",
       "   0.09027603268623352,\n",
       "   0.0885719284415245,\n",
       "   0.08713224530220032,\n",
       "   0.08675593137741089,\n",
       "   0.09265431016683578,\n",
       "   0.09101474285125732,\n",
       "   0.0940265879034996,\n",
       "   0.09501667320728302,\n",
       "   0.09759566932916641,\n",
       "   0.09998323768377304,\n",
       "   0.09933967143297195,\n",
       "   0.09939354658126831,\n",
       "   0.10294446349143982,\n",
       "   0.10659661889076233,\n",
       "   0.10870859026908875,\n",
       "   0.11054062843322754,\n",
       "   0.10854799300432205,\n",
       "   0.10528842359781265,\n",
       "   0.10615171492099762,\n",
       "   0.10615482926368713,\n",
       "   0.10525441914796829,\n",
       "   0.10179590433835983,\n",
       "   0.10307762026786804,\n",
       "   0.10070641338825226,\n",
       "   0.09968053549528122,\n",
       "   0.09449446946382523,\n",
       "   0.08786109834909439,\n",
       "   0.09142470359802246,\n",
       "   0.08968140184879303,\n",
       "   0.08786065876483917,\n",
       "   0.08679789304733276,\n",
       "   0.08193684369325638,\n",
       "   0.08198445290327072,\n",
       "   0.07669828087091446,\n",
       "   0.07353563606739044,\n",
       "   0.07213679701089859,\n",
       "   0.07160598039627075,\n",
       "   0.06882628053426743,\n",
       "   0.07095018029212952,\n",
       "   0.07707957178354263,\n",
       "   0.076468825340271,\n",
       "   0.07776840776205063,\n",
       "   0.08340457081794739,\n",
       "   0.08523059636354446,\n",
       "   0.08696299046278,\n",
       "   0.0862559974193573,\n",
       "   0.08507641404867172,\n",
       "   0.08606415241956711,\n",
       "   0.0913398414850235,\n",
       "   0.09368545562028885,\n",
       "   0.09678389877080917,\n",
       "   0.10236763209104538,\n",
       "   0.10758744925260544,\n",
       "   0.11077442020177841,\n",
       "   0.10614527761936188,\n",
       "   0.10498844087123871,\n",
       "   0.10202614217996597,\n",
       "   0.09856432676315308,\n",
       "   0.1004682183265686,\n",
       "   0.09742792695760727,\n",
       "   0.09169664978981018,\n",
       "   0.08728134632110596,\n",
       "   0.0871039628982544,\n",
       "   0.08466668426990509,\n",
       "   0.08224251121282578,\n",
       "   0.08179514110088348,\n",
       "   0.08707144856452942,\n",
       "   0.09277845919132233,\n",
       "   0.09897597879171371,\n",
       "   0.0987754613161087,\n",
       "   0.10236958414316177,\n",
       "   0.10083624720573425,\n",
       "   0.0984136089682579,\n",
       "   0.09839335829019547,\n",
       "   0.09826988726854324,\n",
       "   0.09849448502063751,\n",
       "   0.0914902314543724,\n",
       "   0.0891784057021141,\n",
       "   0.08547475934028625,\n",
       "   0.08109568059444427,\n",
       "   0.07884161174297333,\n",
       "   0.07724884152412415,\n",
       "   0.0747780054807663,\n",
       "   0.07532522082328796,\n",
       "   0.0732741504907608,\n",
       "   0.0775221586227417,\n",
       "   0.08181627839803696,\n",
       "   0.0850311890244484,\n",
       "   0.08974069356918335,\n",
       "   0.09423316270112991,\n",
       "   0.09840498864650726,\n",
       "   0.10117968171834946,\n",
       "   0.10440272837877274,\n",
       "   0.11394985765218735,\n",
       "   0.11413412541151047,\n",
       "   0.11628450453281403,\n",
       "   0.11512742936611176,\n",
       "   0.11169518530368805,\n",
       "   0.11007073521614075,\n",
       "   0.1113848090171814,\n",
       "   0.11298526078462601,\n",
       "   0.11126416176557541,\n",
       "   0.11073049902915955,\n",
       "   0.1085318848490715,\n",
       "   0.10361373424530029,\n",
       "   0.10322543978691101,\n",
       "   0.10300527513027191,\n",
       "   0.10373514890670776,\n",
       "   0.10325329750776291,\n",
       "   0.10533639043569565,\n",
       "   0.10464590787887573,\n",
       "   0.1016460657119751,\n",
       "   0.10364139080047607,\n",
       "   0.09868471324443817,\n",
       "   0.10012155026197433,\n",
       "   0.0998225137591362,\n",
       "   0.09886092692613602,\n",
       "   0.09709737449884415,\n",
       "   0.09594213962554932,\n",
       "   0.09599070250988007,\n",
       "   0.095853291451931,\n",
       "   0.09179805964231491,\n",
       "   0.09438071399927139,\n",
       "   0.09840092062950134,\n",
       "   0.10008833557367325,\n",
       "   0.10423217713832855,\n",
       "   0.10475511103868484,\n",
       "   0.10756678134202957,\n",
       "   0.10757187753915787,\n",
       "   0.10275806486606598,\n",
       "   0.10057511180639267,\n",
       "   0.10108932107686996,\n",
       "   0.10296901315450668,\n",
       "   0.10319019854068756,\n",
       "   0.10595262050628662,\n",
       "   0.1042889803647995,\n",
       "   0.1085873693227768,\n",
       "   0.10936469584703445,\n",
       "   0.11129393428564072,\n",
       "   0.10959368944168091,\n",
       "   0.11043746024370193,\n",
       "   0.10634133219718933,\n",
       "   0.10218097269535065,\n",
       "   0.10289876163005829,\n",
       "   0.10527117550373077,\n",
       "   0.10400804132223129,\n",
       "   0.10208343714475632,\n",
       "   0.09977869689464569,\n",
       "   0.0961773693561554,\n",
       "   0.09295951575040817,\n",
       "   0.09422553330659866,\n",
       "   0.09411165118217468,\n",
       "   0.09360650926828384,\n",
       "   0.09796112775802612,\n",
       "   0.09597732126712799,\n",
       "   0.09730202704668045,\n",
       "   0.0932171642780304,\n",
       "   0.09243122488260269,\n",
       "   0.09381705522537231,\n",
       "   0.0931565910577774,\n",
       "   0.0925198346376419,\n",
       "   0.09072776883840561,\n",
       "   0.0937189981341362,\n",
       "   0.09231797605752945,\n",
       "   0.09238409996032715,\n",
       "   0.09422048181295395,\n",
       "   0.09248562902212143,\n",
       "   0.09105858951807022,\n",
       "   0.08783501386642456,\n",
       "   0.0874742716550827,\n",
       "   0.08759994804859161,\n",
       "   0.08658646047115326,\n",
       "   0.08802849054336548,\n",
       "   0.08961208909749985,\n",
       "   0.09021496772766113,\n",
       "   0.09413818269968033,\n",
       "   0.09217260032892227,\n",
       "   0.09501421451568604,\n",
       "   0.09300059825181961,\n",
       "   0.09114723652601242,\n",
       "   0.08955270797014236,\n",
       "   0.09262289106845856,\n",
       "   0.09309204667806625,\n",
       "   0.09146630764007568,\n",
       "   0.09034133702516556,\n",
       "   0.0844591036438942,\n",
       "   0.08679086714982986,\n",
       "   0.08233962953090668,\n",
       "   0.083542600274086,\n",
       "   0.08355744183063507,\n",
       "   0.08070339262485504,\n",
       "   0.08449071645736694,\n",
       "   0.08629046380519867,\n",
       "   0.08619408309459686,\n",
       "   0.08502715080976486,\n",
       "   0.08464032411575317,\n",
       "   0.0849018320441246,\n",
       "   0.08438350260257721,\n",
       "   0.08788127452135086,\n",
       "   0.0853584036231041,\n",
       "   0.08958470821380615,\n",
       "   0.09698052704334259,\n",
       "   0.10028674453496933,\n",
       "   0.10404201596975327,\n",
       "   0.1041732132434845,\n",
       "   0.10673138499259949,\n",
       "   0.10994666814804077,\n",
       "   0.11202999949455261,\n",
       "   0.10706782341003418,\n",
       "   0.10674354434013367,\n",
       "   0.10426492989063263,\n",
       "   0.10031041502952576,\n",
       "   0.09848450869321823,\n",
       "   0.09969275444746017,\n",
       "   0.10055655241012573,\n",
       "   0.10080806165933609,\n",
       "   0.1029689610004425,\n",
       "   0.10383915156126022,\n",
       "   0.1023857593536377,\n",
       "   0.10441423952579498,\n",
       "   0.10737428814172745,\n",
       "   0.11181217432022095,\n",
       "   0.1120835542678833,\n",
       "   0.11022179573774338,\n",
       "   0.10395927727222443,\n",
       "   0.1000623032450676,\n",
       "   0.09938252717256546,\n",
       "   0.09729988127946854,\n",
       "   0.0886799693107605,\n",
       "   0.08579877763986588,\n",
       "   0.08597417175769806,\n",
       "   0.08510131388902664,\n",
       "   0.08749838918447495,\n",
       "   0.08678523451089859,\n",
       "   0.08731921762228012,\n",
       "   0.08271490037441254,\n",
       "   0.08247875422239304,\n",
       "   0.0803665891289711,\n",
       "   0.08061161637306213,\n",
       "   0.07666415721178055,\n",
       "   0.07495690882205963,\n",
       "   0.07357445359230042,\n",
       "   0.07265212386846542,\n",
       "   0.07037074118852615,\n",
       "   0.07062286138534546,\n",
       "   0.07215388119220734,\n",
       "   0.07468622177839279,\n",
       "   0.07580440491437912,\n",
       "   0.07957181334495544,\n",
       "   0.07949631661176682,\n",
       "   0.08406128734350204,\n",
       "   0.08706314861774445,\n",
       "   0.08136168867349625,\n",
       "   0.08240386843681335,\n",
       "   0.08168775588274002,\n",
       "   0.08807998150587082,\n",
       "   0.08942229300737381,\n",
       "   0.0886111631989479,\n",
       "   0.09069130569696426,\n",
       "   0.09379405528306961,\n",
       "   0.09642994403839111,\n",
       "   0.1022440567612648,\n",
       "   0.1082448661327362,\n",
       "   0.11261225491762161,\n",
       "   0.11482833325862885,\n",
       "   0.11459507048130035,\n",
       "   0.11552907526493073,\n",
       "   0.11524748802185059,\n",
       "   0.1119924932718277,\n",
       "   0.111866295337677,\n",
       "   0.10766392946243286,\n",
       "   0.10351542383432388,\n",
       "   0.09869476407766342,\n",
       "   0.09830507636070251,\n",
       "   0.08867457509040833,\n",
       "   0.0804881826043129,\n",
       "   0.07807636260986328,\n",
       "   0.07985658198595047,\n",
       "   0.07518156617879868,\n",
       "   0.0716901645064354,\n",
       "   0.07234029471874237,\n",
       "   0.07761024683713913,\n",
       "   0.07611026614904404,\n",
       "   0.07732323557138443,\n",
       "   0.07805506139993668,\n",
       "   0.08123850077390671,\n",
       "   0.08749813586473465,\n",
       "   0.09147018194198608,\n",
       "   0.09501364827156067,\n",
       "   0.09553362429141998,\n",
       "   0.1011238694190979,\n",
       "   0.10163947194814682,\n",
       "   0.10185030102729797,\n",
       "   0.10476850718259811,\n",
       "   0.10188812017440796,\n",
       "   0.09968352317810059,\n",
       "   0.09848569333553314,\n",
       "   0.09770193696022034,\n",
       "   0.0951344296336174,\n",
       "   0.09272326529026031,\n",
       "   0.08901164680719376,\n",
       "   0.08736326545476913,\n",
       "   0.08653522282838821,\n",
       "   0.08403262495994568,\n",
       "   0.08302250504493713,\n",
       "   0.08251018077135086,\n",
       "   0.0781092494726181,\n",
       "   0.07776586711406708,\n",
       "   0.0757506713271141,\n",
       "   0.07447653263807297,\n",
       "   0.07253933697938919,\n",
       "   0.0746551901102066,\n",
       "   0.0749681368470192,\n",
       "   0.07619202136993408,\n",
       "   0.07949317246675491,\n",
       "   0.0781143382191658,\n",
       "   0.07900789380073547,\n",
       "   0.08193687349557877,\n",
       "   0.08182598650455475,\n",
       "   0.08397292345762253,\n",
       "   0.07851182669401169],\n",
       "  'ff_loss': [0.7805047035217285,\n",
       "   0.7846108675003052,\n",
       "   0.7811565399169922,\n",
       "   0.7762371301651001,\n",
       "   0.7436771392822266,\n",
       "   0.7139836549758911,\n",
       "   0.688866913318634,\n",
       "   0.674681544303894,\n",
       "   0.6581183075904846,\n",
       "   0.6464530229568481,\n",
       "   0.6390405893325806,\n",
       "   0.6308156251907349,\n",
       "   0.6238611936569214,\n",
       "   0.6252057552337646,\n",
       "   0.6276200413703918,\n",
       "   0.6312724351882935,\n",
       "   0.6322474479675293,\n",
       "   0.631596028804779,\n",
       "   0.6298075914382935,\n",
       "   0.6310058236122131,\n",
       "   0.637459397315979,\n",
       "   0.642841100692749,\n",
       "   0.6427084803581238,\n",
       "   0.6448560357093811,\n",
       "   0.647628128528595,\n",
       "   0.6486522555351257,\n",
       "   0.6585610508918762,\n",
       "   0.660835862159729,\n",
       "   0.6630337834358215,\n",
       "   0.6641362309455872,\n",
       "   0.6669003963470459,\n",
       "   0.6653659343719482,\n",
       "   0.6636439561843872,\n",
       "   0.670621931552887,\n",
       "   0.6806007027626038,\n",
       "   0.681398868560791,\n",
       "   0.6844246983528137,\n",
       "   0.6827255487442017,\n",
       "   0.6827483773231506,\n",
       "   0.6867467164993286,\n",
       "   0.6894144415855408,\n",
       "   0.6914299726486206,\n",
       "   0.6922714114189148,\n",
       "   0.6975193023681641,\n",
       "   0.6989625692367554,\n",
       "   0.7053240537643433,\n",
       "   0.7149336934089661,\n",
       "   0.7159639000892639,\n",
       "   0.7165677547454834,\n",
       "   0.7145320773124695,\n",
       "   0.720330536365509,\n",
       "   0.7200919985771179,\n",
       "   0.7240564823150635,\n",
       "   0.7286574840545654,\n",
       "   0.7313718795776367,\n",
       "   0.7326314449310303,\n",
       "   0.7327545285224915,\n",
       "   0.7356488704681396,\n",
       "   0.7273955941200256,\n",
       "   0.7257354855537415,\n",
       "   0.7240847945213318,\n",
       "   0.7224719524383545,\n",
       "   0.7264888286590576,\n",
       "   0.7254619002342224,\n",
       "   0.7258416414260864,\n",
       "   0.7246633172035217,\n",
       "   0.7234236598014832,\n",
       "   0.7184115648269653,\n",
       "   0.7174274325370789,\n",
       "   0.7185425162315369,\n",
       "   0.719768762588501,\n",
       "   0.7277954816818237,\n",
       "   0.7256306409835815,\n",
       "   0.7261260747909546,\n",
       "   0.7237696051597595,\n",
       "   0.7229381203651428,\n",
       "   0.7291028499603271,\n",
       "   0.7334240674972534,\n",
       "   0.7329069972038269,\n",
       "   0.7317870259284973,\n",
       "   0.728122889995575,\n",
       "   0.7223114371299744,\n",
       "   0.7283810377120972,\n",
       "   0.7292833924293518,\n",
       "   0.729895293712616,\n",
       "   0.7291460633277893,\n",
       "   0.7297670245170593,\n",
       "   0.7297656536102295,\n",
       "   0.7336994409561157,\n",
       "   0.7359626889228821,\n",
       "   0.7334443926811218,\n",
       "   0.7328439354896545,\n",
       "   0.7340723872184753,\n",
       "   0.7326775789260864,\n",
       "   0.7334874868392944,\n",
       "   0.7333129644393921,\n",
       "   0.7334787249565125,\n",
       "   0.733127772808075,\n",
       "   0.7343900203704834,\n",
       "   0.7344774603843689,\n",
       "   0.7363559007644653,\n",
       "   0.7353339195251465,\n",
       "   0.7342347502708435,\n",
       "   0.7313501834869385,\n",
       "   0.7294703722000122,\n",
       "   0.727782130241394,\n",
       "   0.7270821928977966,\n",
       "   0.727088987827301,\n",
       "   0.726423978805542,\n",
       "   0.728811502456665,\n",
       "   0.7291760444641113,\n",
       "   0.7288631200790405,\n",
       "   0.7281754016876221,\n",
       "   0.7284067273139954,\n",
       "   0.7287055850028992,\n",
       "   0.7307509183883667,\n",
       "   0.7328466773033142,\n",
       "   0.7333484888076782,\n",
       "   0.7332733869552612,\n",
       "   0.7314656972885132,\n",
       "   0.7297643423080444,\n",
       "   0.7289379835128784,\n",
       "   0.7294091582298279,\n",
       "   0.729569137096405,\n",
       "   0.731152355670929,\n",
       "   0.7321619987487793,\n",
       "   0.7322883605957031,\n",
       "   0.7327547073364258,\n",
       "   0.7336693406105042,\n",
       "   0.7351356148719788,\n",
       "   0.7355348467826843,\n",
       "   0.7342530488967896,\n",
       "   0.7350839376449585,\n",
       "   0.7349997162818909,\n",
       "   0.7364216446876526,\n",
       "   0.7358981966972351,\n",
       "   0.7374290823936462,\n",
       "   0.7360489368438721,\n",
       "   0.7360870838165283,\n",
       "   0.7359509468078613,\n",
       "   0.7357333302497864,\n",
       "   0.7357339859008789,\n",
       "   0.7364912629127502,\n",
       "   0.7351508736610413,\n",
       "   0.7351908683776855,\n",
       "   0.7351853251457214,\n",
       "   0.7345742583274841,\n",
       "   0.7350281476974487,\n",
       "   0.7352674603462219,\n",
       "   0.7368518114089966,\n",
       "   0.7376689314842224,\n",
       "   0.7369835376739502,\n",
       "   0.7363620400428772,\n",
       "   0.7361664772033691,\n",
       "   0.7331008315086365,\n",
       "   0.72996985912323,\n",
       "   0.7303684949874878,\n",
       "   0.7298950552940369,\n",
       "   0.7307829856872559,\n",
       "   0.7323920130729675,\n",
       "   0.7313424944877625,\n",
       "   0.7301775813102722,\n",
       "   0.7294347882270813,\n",
       "   0.729204535484314,\n",
       "   0.7292143106460571,\n",
       "   0.7298181056976318,\n",
       "   0.7285565733909607,\n",
       "   0.7294995188713074,\n",
       "   0.7293779850006104,\n",
       "   0.7300606966018677,\n",
       "   0.7309024930000305,\n",
       "   0.7320101261138916,\n",
       "   0.7303270697593689,\n",
       "   0.7315483689308167,\n",
       "   0.7295539379119873,\n",
       "   0.7285985946655273,\n",
       "   0.7303511500358582,\n",
       "   0.7305099368095398,\n",
       "   0.7300547361373901,\n",
       "   0.7289947271347046,\n",
       "   0.7266514301300049,\n",
       "   0.7268890738487244,\n",
       "   0.726870596408844,\n",
       "   0.7264919877052307,\n",
       "   0.7273947596549988,\n",
       "   0.7283632159233093,\n",
       "   0.7271842360496521,\n",
       "   0.7263681888580322,\n",
       "   0.725791871547699,\n",
       "   0.7259652018547058,\n",
       "   0.7259547114372253,\n",
       "   0.7253590822219849,\n",
       "   0.7260246276855469,\n",
       "   0.7247169017791748,\n",
       "   0.7237944006919861,\n",
       "   0.7235183119773865,\n",
       "   0.7238222360610962,\n",
       "   0.7236008644104004,\n",
       "   0.7246639728546143,\n",
       "   0.7233888506889343,\n",
       "   0.7242310047149658,\n",
       "   0.7243814468383789,\n",
       "   0.7253533005714417,\n",
       "   0.723923921585083,\n",
       "   0.7233172059059143,\n",
       "   0.7245408296585083,\n",
       "   0.7254965305328369,\n",
       "   0.7253057360649109,\n",
       "   0.7238156795501709,\n",
       "   0.723730206489563,\n",
       "   0.7244014739990234,\n",
       "   0.7238441705703735,\n",
       "   0.7241097688674927,\n",
       "   0.7228042483329773,\n",
       "   0.724334180355072,\n",
       "   0.7206478118896484,\n",
       "   0.7181793451309204,\n",
       "   0.7165455222129822,\n",
       "   0.7178440690040588,\n",
       "   0.7151414752006531,\n",
       "   0.7150980830192566,\n",
       "   0.719961404800415,\n",
       "   0.7204696536064148,\n",
       "   0.723147988319397,\n",
       "   0.7225556969642639,\n",
       "   0.7222117781639099,\n",
       "   0.7174602150917053,\n",
       "   0.7148045301437378,\n",
       "   0.7121530175209045,\n",
       "   0.7120827436447144,\n",
       "   0.7110624313354492,\n",
       "   0.7098493576049805,\n",
       "   0.7086411118507385,\n",
       "   0.7059863805770874,\n",
       "   0.7117542028427124,\n",
       "   0.7150260806083679,\n",
       "   0.7131040692329407,\n",
       "   0.7134974002838135,\n",
       "   0.714501678943634,\n",
       "   0.7143453359603882,\n",
       "   0.7156187891960144,\n",
       "   0.7187299132347107,\n",
       "   0.7183975577354431,\n",
       "   0.717788815498352,\n",
       "   0.7200703620910645,\n",
       "   0.7195233702659607,\n",
       "   0.7203065752983093,\n",
       "   0.7171542048454285,\n",
       "   0.7178006172180176,\n",
       "   0.7131915092468262,\n",
       "   0.7115882635116577,\n",
       "   0.7105737328529358,\n",
       "   0.7100545763969421,\n",
       "   0.7084699869155884,\n",
       "   0.7105178833007812,\n",
       "   0.7092541456222534,\n",
       "   0.7074018120765686,\n",
       "   0.7053164839744568,\n",
       "   0.7091633081436157,\n",
       "   0.7133534550666809,\n",
       "   0.7112939357757568,\n",
       "   0.7121498584747314,\n",
       "   0.7094180583953857,\n",
       "   0.7066335678100586,\n",
       "   0.7047351002693176,\n",
       "   0.7092307209968567,\n",
       "   0.7081453204154968,\n",
       "   0.708426296710968,\n",
       "   0.7116503715515137,\n",
       "   0.7140364646911621,\n",
       "   0.7134396433830261,\n",
       "   0.7134734988212585,\n",
       "   0.7099285125732422,\n",
       "   0.7114081382751465,\n",
       "   0.7101920247077942,\n",
       "   0.7055036425590515,\n",
       "   0.7058797478675842,\n",
       "   0.7066832780838013,\n",
       "   0.7055255770683289,\n",
       "   0.7038360238075256,\n",
       "   0.7033142447471619,\n",
       "   0.7042627930641174,\n",
       "   0.7038114666938782,\n",
       "   0.7011668086051941,\n",
       "   0.6999415159225464,\n",
       "   0.699401319026947,\n",
       "   0.6992668509483337,\n",
       "   0.699385941028595,\n",
       "   0.6967197060585022,\n",
       "   0.6994044184684753,\n",
       "   0.6965786814689636,\n",
       "   0.6974524259567261,\n",
       "   0.6957322955131531,\n",
       "   0.6946080327033997,\n",
       "   0.6927511096000671,\n",
       "   0.6921954154968262,\n",
       "   0.6909670233726501,\n",
       "   0.6930884718894958,\n",
       "   0.6928908824920654,\n",
       "   0.6924213171005249,\n",
       "   0.6925026774406433,\n",
       "   0.693516194820404,\n",
       "   0.6941997408866882,\n",
       "   0.6937412023544312,\n",
       "   0.6936115622520447,\n",
       "   0.6935627460479736,\n",
       "   0.6952715516090393,\n",
       "   0.6931992769241333,\n",
       "   0.6931264400482178,\n",
       "   0.689598560333252,\n",
       "   0.6896012425422668,\n",
       "   0.6858283877372742,\n",
       "   0.6893382668495178,\n",
       "   0.6885759234428406,\n",
       "   0.6902908682823181,\n",
       "   0.6892247200012207,\n",
       "   0.6882426738739014,\n",
       "   0.6889400482177734,\n",
       "   0.6860402226448059,\n",
       "   0.687736988067627,\n",
       "   0.6884090900421143,\n",
       "   0.6913236379623413,\n",
       "   0.6937682032585144,\n",
       "   0.6933954954147339,\n",
       "   0.695080578327179,\n",
       "   0.6940178275108337,\n",
       "   0.6956760287284851,\n",
       "   0.6982150673866272,\n",
       "   0.7013776302337646,\n",
       "   0.7007271647453308,\n",
       "   0.6994438767433167,\n",
       "   0.697623074054718,\n",
       "   0.6997615098953247,\n",
       "   0.7010749578475952,\n",
       "   0.6998140811920166,\n",
       "   0.7010171413421631,\n",
       "   0.6965497732162476,\n",
       "   0.6932359933853149,\n",
       "   0.6926940679550171,\n",
       "   0.693511426448822,\n",
       "   0.6909529566764832,\n",
       "   0.6875349283218384,\n",
       "   0.6874276995658875,\n",
       "   0.684455156326294,\n",
       "   0.6858400702476501,\n",
       "   0.6830152869224548,\n",
       "   0.6865956783294678,\n",
       "   0.6859709024429321,\n",
       "   0.6862141489982605,\n",
       "   0.6858035326004028,\n",
       "   0.6828492879867554,\n",
       "   0.6802122592926025,\n",
       "   0.6790333986282349,\n",
       "   0.6766294240951538,\n",
       "   0.6744521260261536,\n",
       "   0.6737596392631531,\n",
       "   0.6705275774002075,\n",
       "   0.6691632270812988,\n",
       "   0.668586015701294,\n",
       "   0.6686924695968628,\n",
       "   0.6659530997276306,\n",
       "   0.6657048463821411,\n",
       "   0.6687963008880615,\n",
       "   0.6692318916320801,\n",
       "   0.6681873798370361,\n",
       "   0.6671415567398071,\n",
       "   0.6676806211471558,\n",
       "   0.666999340057373,\n",
       "   0.6643319129943848,\n",
       "   0.6661739349365234,\n",
       "   0.6673126816749573,\n",
       "   0.6675599217414856,\n",
       "   0.6632636189460754,\n",
       "   0.6626540422439575,\n",
       "   0.6635156869888306,\n",
       "   0.6643030643463135,\n",
       "   0.6628316044807434,\n",
       "   0.659981369972229,\n",
       "   0.6604453921318054,\n",
       "   0.6602607369422913,\n",
       "   0.6579539179801941,\n",
       "   0.656143069267273,\n",
       "   0.6595811247825623,\n",
       "   0.6586582064628601,\n",
       "   0.6587045192718506,\n",
       "   0.6621055006980896,\n",
       "   0.6624157428741455,\n",
       "   0.6623644828796387,\n",
       "   0.6631743311882019,\n",
       "   0.664367139339447,\n",
       "   0.6646888852119446,\n",
       "   0.6659552454948425,\n",
       "   0.6655277013778687,\n",
       "   0.6650118827819824,\n",
       "   0.6643028259277344,\n",
       "   0.6632254719734192,\n",
       "   0.6633738279342651,\n",
       "   0.6629458069801331,\n",
       "   0.6644307374954224,\n",
       "   0.6641873121261597,\n",
       "   0.6617138981819153,\n",
       "   0.6643045544624329,\n",
       "   0.6649015545845032,\n",
       "   0.664916455745697,\n",
       "   0.6665704250335693,\n",
       "   0.6640841364860535,\n",
       "   0.6629084348678589,\n",
       "   0.6611584424972534,\n",
       "   0.6586052775382996,\n",
       "   0.6598741412162781,\n",
       "   0.6613782048225403,\n",
       "   0.6608821153640747,\n",
       "   0.6606842279434204,\n",
       "   0.6601556539535522,\n",
       "   0.6576133370399475,\n",
       "   0.6587228178977966,\n",
       "   0.6580933928489685,\n",
       "   0.6603938937187195,\n",
       "   0.6618329882621765,\n",
       "   0.6614972352981567,\n",
       "   0.6631590723991394,\n",
       "   0.6611800789833069,\n",
       "   0.6621676087379456,\n",
       "   0.6623666882514954,\n",
       "   0.6630812287330627,\n",
       "   0.6635949015617371,\n",
       "   0.6639239192008972,\n",
       "   0.6617740988731384,\n",
       "   0.6634548306465149,\n",
       "   0.6639403104782104,\n",
       "   0.6625219583511353,\n",
       "   0.6611678004264832,\n",
       "   0.6610139012336731,\n",
       "   0.6593196988105774,\n",
       "   0.6596531271934509,\n",
       "   0.659742534160614,\n",
       "   0.6598818302154541,\n",
       "   0.6592440009117126,\n",
       "   0.6610651612281799,\n",
       "   0.6608295440673828,\n",
       "   0.660510241985321,\n",
       "   0.6580284237861633,\n",
       "   0.658808171749115,\n",
       "   0.6580569744110107,\n",
       "   0.6582342982292175,\n",
       "   0.6591616868972778,\n",
       "   0.6579399704933167,\n",
       "   0.6571168899536133,\n",
       "   0.6547332406044006,\n",
       "   0.6532719135284424,\n",
       "   0.6531733274459839,\n",
       "   0.6542779207229614,\n",
       "   0.6527402997016907,\n",
       "   0.6515011787414551,\n",
       "   0.6526957750320435,\n",
       "   0.6534362435340881,\n",
       "   0.6519854664802551,\n",
       "   0.6500898003578186,\n",
       "   0.6508715748786926,\n",
       "   0.6527708172798157,\n",
       "   0.6519554853439331,\n",
       "   0.6520940065383911,\n",
       "   0.6522095799446106,\n",
       "   0.6521458625793457,\n",
       "   0.65085369348526,\n",
       "   0.6521527767181396,\n",
       "   0.6548973321914673,\n",
       "   0.6550519466400146,\n",
       "   0.6547783613204956,\n",
       "   0.6568120121955872,\n",
       "   0.6586365103721619,\n",
       "   0.657508909702301,\n",
       "   0.6576963067054749,\n",
       "   0.6576206088066101,\n",
       "   0.6584234237670898,\n",
       "   0.6577341556549072,\n",
       "   0.6586064696311951,\n",
       "   0.6585500836372375,\n",
       "   0.6585791110992432,\n",
       "   0.6588339805603027,\n",
       "   0.6590869426727295,\n",
       "   0.6571437120437622,\n",
       "   0.6555777192115784,\n",
       "   0.65324467420578,\n",
       "   0.6538606286048889,\n",
       "   0.6550545692443848,\n",
       "   0.6547683477401733,\n",
       "   0.6543169021606445,\n",
       "   0.6540562510490417,\n",
       "   0.6533987522125244,\n",
       "   0.6524136066436768,\n",
       "   0.6514672040939331,\n",
       "   0.6542876362800598,\n",
       "   0.6523631811141968,\n",
       "   0.65280681848526,\n",
       "   0.6525099873542786,\n",
       "   0.654184103012085,\n",
       "   0.6533220410346985,\n",
       "   0.6522237658500671,\n",
       "   0.6509626507759094],\n",
       "  'test_losses': [17.943180352449417,\n",
       "   10.098670154809952,\n",
       "   6.272678315639496,\n",
       "   4.110488586127758,\n",
       "   2.907767243683338,\n",
       "   2.141031164675951,\n",
       "   1.6118848323822021,\n",
       "   1.313030682504177,\n",
       "   1.155277632176876,\n",
       "   1.0810213088989258,\n",
       "   1.0603648200631142,\n",
       "   1.061400145292282,\n",
       "   1.1069564819335938,\n",
       "   1.1579632610082626,\n",
       "   1.217200219631195,\n",
       "   1.264316812157631,\n",
       "   1.3073939085006714,\n",
       "   1.3519082516431808,\n",
       "   1.3890010565519333,\n",
       "   1.412129282951355,\n",
       "   1.4346023797988892,\n",
       "   1.4728082865476608,\n",
       "   1.472609668970108,\n",
       "   1.5007472336292267,\n",
       "   1.518060028553009,\n",
       "   1.5101511031389236,\n",
       "   1.5175000727176666,\n",
       "   1.5084098428487778,\n",
       "   1.500923603773117,\n",
       "   1.4974899962544441,\n",
       "   1.4879009276628494,\n",
       "   1.4871972426772118,\n",
       "   1.4687312208116055,\n",
       "   1.46913468465209,\n",
       "   1.4724636375904083,\n",
       "   1.470470067113638,\n",
       "   1.476566068828106,\n",
       "   1.487003654241562,\n",
       "   1.4878577515482903,\n",
       "   1.4929102063179016,\n",
       "   1.497332338243723,\n",
       "   1.4996140077710152,\n",
       "   1.5032843463122845,\n",
       "   1.5151150673627853,\n",
       "   1.5149937085807323,\n",
       "   1.5224416330456734,\n",
       "   1.526869673281908,\n",
       "   1.5260282084345818,\n",
       "   1.5227313488721848,\n",
       "   1.5235181897878647,\n",
       "   1.5261780619621277,\n",
       "   1.5262277200818062,\n",
       "   1.533947929739952,\n",
       "   1.5441742613911629,\n",
       "   1.542136199772358,\n",
       "   1.5486899018287659,\n",
       "   1.5473911613225937,\n",
       "   1.5525838509202003,\n",
       "   1.5363462045788765,\n",
       "   1.5257052183151245,\n",
       "   1.5230333656072617,\n",
       "   1.5134358182549477,\n",
       "   1.5168416649103165,\n",
       "   1.5070696398615837,\n",
       "   1.4905716329813004,\n",
       "   1.4785721600055695,\n",
       "   1.4700783863663673,\n",
       "   1.4545685425400734,\n",
       "   1.436694085597992,\n",
       "   1.3988840207457542,\n",
       "   1.361745424568653,\n",
       "   1.3403686955571175,\n",
       "   1.306229367852211,\n",
       "   1.286340519785881,\n",
       "   1.2684790268540382,\n",
       "   1.2485000640153885,\n",
       "   1.219322346150875,\n",
       "   1.205287829041481,\n",
       "   1.187437042593956,\n",
       "   1.1700328588485718,\n",
       "   1.1524447798728943,\n",
       "   1.130501389503479,\n",
       "   1.1150888428092003,\n",
       "   1.1114844903349876,\n",
       "   1.1097217947244644,\n",
       "   1.1119803562760353,\n",
       "   1.1000033095479012,\n",
       "   1.1027974784374237,\n",
       "   1.1083571687340736,\n",
       "   1.1107262298464775,\n",
       "   1.1054105013608932,\n",
       "   1.1034745201468468,\n",
       "   1.1106055080890656,\n",
       "   1.0996628925204277,\n",
       "   1.099497064948082,\n",
       "   1.084031954407692,\n",
       "   1.0747381895780563,\n",
       "   1.0746040046215057,\n",
       "   1.0679789632558823,\n",
       "   1.0590637028217316,\n",
       "   1.0569123327732086,\n",
       "   1.0603932961821556,\n",
       "   1.0648628920316696,\n",
       "   1.0618883445858955,\n",
       "   1.0617968067526817,\n",
       "   1.0595529302954674,\n",
       "   1.0590026900172234,\n",
       "   1.0633890554308891,\n",
       "   1.0559263825416565,\n",
       "   1.0474699139595032,\n",
       "   1.0460105314850807,\n",
       "   1.0522756054997444,\n",
       "   1.0575967356562614,\n",
       "   1.0505024194717407,\n",
       "   1.055913083255291,\n",
       "   1.0633189603686333,\n",
       "   1.0725097581744194,\n",
       "   1.0725841149687767,\n",
       "   1.0743815153837204,\n",
       "   1.067145749926567,\n",
       "   1.0769028142094612,\n",
       "   1.0769603475928307,\n",
       "   1.0825286656618118,\n",
       "   1.0856218039989471,\n",
       "   1.0815735831856728,\n",
       "   1.0795390829443932,\n",
       "   1.0768182873725891,\n",
       "   1.082926444709301,\n",
       "   1.0940866693854332,\n",
       "   1.0943735539913177,\n",
       "   1.0936477109789848,\n",
       "   1.0954913422465324,\n",
       "   1.1016090288758278,\n",
       "   1.0982136130332947,\n",
       "   1.1004892736673355,\n",
       "   1.0934646278619766,\n",
       "   1.0888740122318268,\n",
       "   1.0911794155836105,\n",
       "   1.0890598222613335,\n",
       "   1.092906542122364,\n",
       "   1.083096131682396,\n",
       "   1.082577295601368,\n",
       "   1.0843937173485756,\n",
       "   1.0779834911227226,\n",
       "   1.0790929421782494,\n",
       "   1.077389121055603,\n",
       "   1.0681081116199493,\n",
       "   1.0620503425598145,\n",
       "   1.058401457965374,\n",
       "   1.0522477999329567,\n",
       "   1.0529453232884407,\n",
       "   1.0513342544436455,\n",
       "   1.0533241108059883,\n",
       "   1.0567010790109634,\n",
       "   1.0534570291638374,\n",
       "   1.0531458109617233,\n",
       "   1.0600041821599007,\n",
       "   1.0638394802808762,\n",
       "   1.0630565509200096,\n",
       "   1.0713976547122002,\n",
       "   1.0773100331425667,\n",
       "   1.0779349952936172,\n",
       "   1.0793567523360252,\n",
       "   1.0837800204753876,\n",
       "   1.0914760902523994,\n",
       "   1.0946920812129974,\n",
       "   1.0918603464961052,\n",
       "   1.0965085700154305,\n",
       "   1.0871469900012016,\n",
       "   1.0804572030901909,\n",
       "   1.0757058337330818,\n",
       "   1.0658585503697395,\n",
       "   1.059844009578228,\n",
       "   1.0545648410916328,\n",
       "   1.048754744231701,\n",
       "   1.0479505211114883,\n",
       "   1.0387565940618515,\n",
       "   1.039592057466507,\n",
       "   1.0339118540287018,\n",
       "   1.0267039984464645,\n",
       "   1.0258524045348167,\n",
       "   1.0238473936915398,\n",
       "   1.0219460651278496,\n",
       "   1.0243468210101128,\n",
       "   1.0282064005732536,\n",
       "   1.0297549739480019,\n",
       "   1.0286983326077461,\n",
       "   1.0261891409754753,\n",
       "   1.0321682915091515,\n",
       "   1.028377778828144,\n",
       "   1.0291292518377304,\n",
       "   1.0335147827863693,\n",
       "   1.0364657938480377,\n",
       "   1.0377474278211594,\n",
       "   1.0428237244486809,\n",
       "   1.0432313978672028,\n",
       "   1.0368127897381783,\n",
       "   1.0375748574733734,\n",
       "   1.0369091108441353,\n",
       "   1.0364788174629211,\n",
       "   1.0313785597682,\n",
       "   1.032742515206337,\n",
       "   1.0309175997972488,\n",
       "   1.0252646207809448,\n",
       "   1.0148636549711227,\n",
       "   1.0119249820709229,\n",
       "   1.0171187967061996,\n",
       "   1.0162406489253044,\n",
       "   1.0114341154694557,\n",
       "   1.0089940503239632,\n",
       "   1.0048165619373322,\n",
       "   1.0029763653874397,\n",
       "   0.9957957565784454,\n",
       "   0.9872060045599937,\n",
       "   0.988134354352951,\n",
       "   0.9856619983911514,\n",
       "   0.9809819832444191,\n",
       "   0.9823074340820312,\n",
       "   0.9877262264490128,\n",
       "   0.9833127036690712,\n",
       "   0.9844068288803101,\n",
       "   0.9979965016245842,\n",
       "   1.000287875533104,\n",
       "   1.0054001659154892,\n",
       "   1.003739058971405,\n",
       "   1.0061635076999664,\n",
       "   0.9989250898361206,\n",
       "   1.006834588944912,\n",
       "   1.0145785808563232,\n",
       "   1.0205174535512924,\n",
       "   1.0258247032761574,\n",
       "   1.027363806962967,\n",
       "   1.0263475328683853,\n",
       "   1.0157638192176819,\n",
       "   1.0159412622451782,\n",
       "   1.0153716579079628,\n",
       "   1.0060945004224777,\n",
       "   1.0049345716834068,\n",
       "   0.9948887228965759,\n",
       "   0.9854217767715454,\n",
       "   0.9801158756017685,\n",
       "   0.9803659431636333,\n",
       "   0.9751283377408981,\n",
       "   0.9725062064826488,\n",
       "   0.9716861695051193,\n",
       "   0.9734930098056793,\n",
       "   0.9793573841452599,\n",
       "   0.981355968862772,\n",
       "   0.9805205278098583,\n",
       "   0.9810457304120064,\n",
       "   0.9788920283317566,\n",
       "   0.9779792167246342,\n",
       "   0.9833821877837181,\n",
       "   0.9870384782552719,\n",
       "   0.9929447695612907,\n",
       "   0.9865457303822041,\n",
       "   0.9822543151676655,\n",
       "   0.9736331887543201,\n",
       "   0.9739664606750011,\n",
       "   0.9759283103048801,\n",
       "   0.9727817364037037,\n",
       "   0.9754900857806206,\n",
       "   0.9756832793354988,\n",
       "   0.972217183560133,\n",
       "   0.9789335504174232,\n",
       "   0.9890188351273537,\n",
       "   0.9961305037140846,\n",
       "   1.0077794790267944,\n",
       "   1.015653483569622,\n",
       "   1.0218828991055489,\n",
       "   1.0246884673833847,\n",
       "   1.0275863781571388,\n",
       "   1.0317692533135414,\n",
       "   1.0328204706311226,\n",
       "   1.029985710978508,\n",
       "   1.0190220102667809,\n",
       "   1.0121726021170616,\n",
       "   1.0093910247087479,\n",
       "   1.009220875799656,\n",
       "   1.008792407810688,\n",
       "   1.0012658312916756,\n",
       "   0.9948799796402454,\n",
       "   0.9906811155378819,\n",
       "   0.9822383746504784,\n",
       "   0.9796936027705669,\n",
       "   0.9806670732796192,\n",
       "   0.9816689789295197,\n",
       "   0.9808224104344845,\n",
       "   0.9796404652297497,\n",
       "   0.9841989241540432,\n",
       "   0.9820361360907555,\n",
       "   0.9880669713020325,\n",
       "   0.9888039156794548,\n",
       "   0.9924179837107658,\n",
       "   0.9919347688555717,\n",
       "   0.9896650537848473,\n",
       "   0.9860153049230576,\n",
       "   0.9823199212551117,\n",
       "   0.9816006869077682,\n",
       "   0.979431614279747,\n",
       "   0.9725890979170799,\n",
       "   0.9740691557526588,\n",
       "   0.9733878746628761,\n",
       "   0.9717484638094902,\n",
       "   0.9755949452519417,\n",
       "   0.9759722873568535,\n",
       "   0.9808902069926262,\n",
       "   0.9832736924290657,\n",
       "   0.9775668606162071,\n",
       "   0.9727455750107765,\n",
       "   0.9694276601076126,\n",
       "   0.9660968668758869,\n",
       "   0.9706717431545258,\n",
       "   0.9727309159934521,\n",
       "   0.9743622988462448,\n",
       "   0.9817139729857445,\n",
       "   0.9792642816901207,\n",
       "   0.9816558882594109,\n",
       "   0.9762900397181511,\n",
       "   0.9782630279660225,\n",
       "   0.9736660867929459,\n",
       "   0.9677354544401169,\n",
       "   0.9683207906782627,\n",
       "   0.9682014621794224,\n",
       "   0.9697760939598083,\n",
       "   0.9672776460647583,\n",
       "   0.9715422503650188,\n",
       "   0.9662204310297966,\n",
       "   0.9660411812365055,\n",
       "   0.9649928957223892,\n",
       "   0.9606955163180828,\n",
       "   0.9555704593658447,\n",
       "   0.9607433900237083,\n",
       "   0.9612956754863262,\n",
       "   0.9617183543741703,\n",
       "   0.9600838460028172,\n",
       "   0.9571036547422409,\n",
       "   0.9584027752280235,\n",
       "   0.9568134360015392,\n",
       "   0.9589389450848103,\n",
       "   0.95291618257761,\n",
       "   0.9523712657392025,\n",
       "   0.9526997804641724,\n",
       "   0.9501823075115681,\n",
       "   0.953434944152832,\n",
       "   0.94720259308815,\n",
       "   0.9448505230247974,\n",
       "   0.9428267814218998,\n",
       "   0.9429270625114441,\n",
       "   0.9420372359454632,\n",
       "   0.9365460127592087,\n",
       "   0.9338680282235146,\n",
       "   0.933967400342226,\n",
       "   0.9338287934660912,\n",
       "   0.935445562005043,\n",
       "   0.9327189587056637,\n",
       "   0.9351053647696972,\n",
       "   0.9336419515311718,\n",
       "   0.9325615018606186,\n",
       "   0.9313531816005707,\n",
       "   0.9330307804048061,\n",
       "   0.9348331354558468,\n",
       "   0.9372842647135258,\n",
       "   0.9382774010300636,\n",
       "   0.9307239055633545,\n",
       "   0.9335166364908218,\n",
       "   0.928574338555336,\n",
       "   0.9300963915884495,\n",
       "   0.9256769977509975,\n",
       "   0.9227843508124352,\n",
       "   0.9232498444616795,\n",
       "   0.9194309823215008,\n",
       "   0.9170757681131363,\n",
       "   0.9168458133935928,\n",
       "   0.9187305197119713,\n",
       "   0.9184797219932079,\n",
       "   0.9134734496474266,\n",
       "   0.9095137938857079,\n",
       "   0.90664416924119,\n",
       "   0.9100904017686844,\n",
       "   0.9145639948546886,\n",
       "   0.9150227606296539,\n",
       "   0.9194482490420341,\n",
       "   0.9177005365490913,\n",
       "   0.9183153286576271,\n",
       "   0.9218550957739353,\n",
       "   0.9253199175000191,\n",
       "   0.9232138395309448,\n",
       "   0.9271572642028332,\n",
       "   0.9284493811428547,\n",
       "   0.9268088452517986,\n",
       "   0.9270368926227093,\n",
       "   0.9280542284250259,\n",
       "   0.9301135204732418,\n",
       "   0.9280642457306385,\n",
       "   0.9267165847122669,\n",
       "   0.9278085269033909,\n",
       "   0.9222385361790657,\n",
       "   0.9235897399485111,\n",
       "   0.9234200939536095,\n",
       "   0.9231885150074959,\n",
       "   0.9221714995801449,\n",
       "   0.9222200401127338,\n",
       "   0.9152966625988483,\n",
       "   0.9161934927105904,\n",
       "   0.9170453622937202,\n",
       "   0.920394916087389,\n",
       "   0.9168469049036503,\n",
       "   0.9096306413412094,\n",
       "   0.9155457690358162,\n",
       "   0.9147621616721153,\n",
       "   0.9193557314574718,\n",
       "   0.9216102100908756,\n",
       "   0.9206420034170151,\n",
       "   0.9137170240283012,\n",
       "   0.9162037745118141,\n",
       "   0.9114134348928928,\n",
       "   0.9114938601851463,\n",
       "   0.9067087359726429,\n",
       "   0.9072149470448494,\n",
       "   0.9070131555199623,\n",
       "   0.9045294597744942,\n",
       "   0.9035807587206364,\n",
       "   0.9000869914889336,\n",
       "   0.898643683642149,\n",
       "   0.9005689844489098,\n",
       "   0.8949081525206566,\n",
       "   0.8963206298649311,\n",
       "   0.8962164670228958,\n",
       "   0.8995157517492771,\n",
       "   0.8966984041035175,\n",
       "   0.889639362692833,\n",
       "   0.888772327452898,\n",
       "   0.8832613751292229,\n",
       "   0.8891061916947365,\n",
       "   0.8918391093611717,\n",
       "   0.8913842178881168,\n",
       "   0.8902603052556515,\n",
       "   0.8958583399653435,\n",
       "   0.9037130735814571,\n",
       "   0.9136315509676933,\n",
       "   0.9195108450949192,\n",
       "   0.9277683012187481,\n",
       "   0.9316199161112309,\n",
       "   0.9338065087795258,\n",
       "   0.9366668611764908,\n",
       "   0.9390324503183365,\n",
       "   0.9363994337618351,\n",
       "   0.9372538588941097,\n",
       "   0.9327319115400314,\n",
       "   0.9290418289601803,\n",
       "   0.9262269474565983,\n",
       "   0.9199358448386192,\n",
       "   0.9090788327157497,\n",
       "   0.9005432687699795,\n",
       "   0.8973147347569466,\n",
       "   0.8997572772204876,\n",
       "   0.8922212272882462,\n",
       "   0.8875620439648628,\n",
       "   0.8891742639243603,\n",
       "   0.8945635408163071,\n",
       "   0.8911908566951752,\n",
       "   0.8938723802566528,\n",
       "   0.8939202688634396,\n",
       "   0.8960342407226562,\n",
       "   0.9008694104850292,\n",
       "   0.9064494073390961,\n",
       "   0.9096979983150959,\n",
       "   0.907916285097599,\n",
       "   0.9142345748841763,\n",
       "   0.9156899191439152,\n",
       "   0.9147014021873474,\n",
       "   0.9145645685493946,\n",
       "   0.9097689241170883,\n",
       "   0.90475058183074,\n",
       "   0.9024907648563385,\n",
       "   0.9034442752599716,\n",
       "   0.898701086640358,\n",
       "   0.893992230296135,\n",
       "   0.8900691494345665,\n",
       "   0.8882860876619816,\n",
       "   0.8851311840116978,\n",
       "   0.8806484863162041,\n",
       "   0.87917360663414,\n",
       "   0.8815823085606098,\n",
       "   0.8820896372199059,\n",
       "   0.8824425637722015,\n",
       "   0.8816528469324112,\n",
       "   0.8840097934007645,\n",
       "   0.8820525705814362,\n",
       "   0.8847706876695156,\n",
       "   0.8878672569990158,\n",
       "   0.8942251093685627,\n",
       "   0.8935023844242096,\n",
       "   0.8908355385065079,\n",
       "   0.8873233087360859,\n",
       "   0.8896886706352234,\n",
       "   0.8846278451383114,\n",
       "   0.8854391165077686,\n",
       "   0.8782134018838406],\n",
       "  'pce_acc': [475.586181640625,\n",
       "   351.2867431640625,\n",
       "   266.8139953613281,\n",
       "   204.10520935058594,\n",
       "   159.5399932861328,\n",
       "   122.23693084716797,\n",
       "   87.96698760986328,\n",
       "   62.960594177246094,\n",
       "   46.54521560668945,\n",
       "   34.51815414428711,\n",
       "   31.2613582611084,\n",
       "   30.17304801940918,\n",
       "   32.317840576171875,\n",
       "   35.686161041259766,\n",
       "   39.54208755493164,\n",
       "   41.09762191772461,\n",
       "   43.99374771118164,\n",
       "   46.45891571044922,\n",
       "   48.55097961425781,\n",
       "   50.44801330566406,\n",
       "   51.09847640991211,\n",
       "   52.2796630859375,\n",
       "   52.3881721496582,\n",
       "   52.87383270263672,\n",
       "   52.80411148071289,\n",
       "   51.9244270324707,\n",
       "   51.35032653808594,\n",
       "   50.37300109863281,\n",
       "   50.22937774658203,\n",
       "   50.126136779785156,\n",
       "   49.61858367919922,\n",
       "   50.11232376098633,\n",
       "   49.4554557800293,\n",
       "   49.323707580566406,\n",
       "   48.70090103149414,\n",
       "   49.81319046020508,\n",
       "   50.503421783447266,\n",
       "   52.1038932800293,\n",
       "   53.08354568481445,\n",
       "   53.32627868652344,\n",
       "   54.177921295166016,\n",
       "   54.040008544921875,\n",
       "   53.8386116027832,\n",
       "   54.23043441772461,\n",
       "   54.145538330078125,\n",
       "   53.93757629394531,\n",
       "   53.458290100097656,\n",
       "   53.157997131347656,\n",
       "   51.9740104675293,\n",
       "   51.1361198425293,\n",
       "   49.60459899902344,\n",
       "   47.63492202758789,\n",
       "   46.34634017944336,\n",
       "   45.189632415771484,\n",
       "   43.52416229248047,\n",
       "   41.264041900634766,\n",
       "   39.3255500793457,\n",
       "   37.95186996459961,\n",
       "   36.883724212646484,\n",
       "   35.96023941040039,\n",
       "   35.564170837402344,\n",
       "   34.895294189453125,\n",
       "   34.701473236083984,\n",
       "   34.274410247802734,\n",
       "   33.8322639465332,\n",
       "   33.767093658447266,\n",
       "   34.10018539428711,\n",
       "   34.642234802246094,\n",
       "   34.52887725830078,\n",
       "   34.841670989990234,\n",
       "   34.66707992553711,\n",
       "   34.599815368652344,\n",
       "   33.59541320800781,\n",
       "   33.76783752441406,\n",
       "   33.507633209228516,\n",
       "   33.43784713745117,\n",
       "   32.712860107421875,\n",
       "   32.85487365722656,\n",
       "   32.416744232177734,\n",
       "   32.488094329833984,\n",
       "   32.06878662109375,\n",
       "   31.588756561279297,\n",
       "   31.373607635498047,\n",
       "   31.346189498901367,\n",
       "   31.492801666259766,\n",
       "   31.777301788330078,\n",
       "   31.925573348999023,\n",
       "   32.1329460144043,\n",
       "   32.099239349365234,\n",
       "   32.538394927978516,\n",
       "   33.073760986328125,\n",
       "   33.597389221191406,\n",
       "   34.043548583984375,\n",
       "   34.15013885498047,\n",
       "   34.50170135498047,\n",
       "   34.502010345458984,\n",
       "   34.47974395751953,\n",
       "   34.58675765991211,\n",
       "   34.89326095581055,\n",
       "   34.99248504638672,\n",
       "   34.99435043334961,\n",
       "   34.98917770385742,\n",
       "   35.048946380615234,\n",
       "   34.73161315917969,\n",
       "   34.67244338989258,\n",
       "   34.61756896972656,\n",
       "   34.771080017089844,\n",
       "   34.925933837890625,\n",
       "   34.5623779296875,\n",
       "   34.358882904052734,\n",
       "   34.09355926513672,\n",
       "   34.08517837524414,\n",
       "   34.21199417114258,\n",
       "   33.75712203979492,\n",
       "   33.43763732910156,\n",
       "   33.798744201660156,\n",
       "   33.73188781738281,\n",
       "   33.841983795166016,\n",
       "   33.551170349121094,\n",
       "   33.44770812988281,\n",
       "   33.6811408996582,\n",
       "   33.55072021484375,\n",
       "   33.58442306518555,\n",
       "   33.689876556396484,\n",
       "   33.971275329589844,\n",
       "   33.996971130371094,\n",
       "   34.5711784362793,\n",
       "   35.11671447753906,\n",
       "   35.84891891479492,\n",
       "   36.64946365356445,\n",
       "   37.271934509277344,\n",
       "   37.718563079833984,\n",
       "   38.24455261230469,\n",
       "   38.58921432495117,\n",
       "   38.94056701660156,\n",
       "   39.01061248779297,\n",
       "   39.291351318359375,\n",
       "   39.69538497924805,\n",
       "   39.91676712036133,\n",
       "   40.565731048583984,\n",
       "   40.38380813598633,\n",
       "   40.130958557128906,\n",
       "   40.031288146972656,\n",
       "   39.45589065551758,\n",
       "   39.60214614868164,\n",
       "   39.31720733642578,\n",
       "   39.026947021484375,\n",
       "   38.788856506347656,\n",
       "   38.36058807373047,\n",
       "   37.7412223815918,\n",
       "   37.64906692504883,\n",
       "   37.65964126586914,\n",
       "   37.555320739746094,\n",
       "   37.53934097290039,\n",
       "   37.66407012939453,\n",
       "   37.69917297363281,\n",
       "   37.841270446777344,\n",
       "   37.853126525878906,\n",
       "   37.926429748535156,\n",
       "   38.50294494628906,\n",
       "   38.107975006103516,\n",
       "   38.300819396972656,\n",
       "   38.6688117980957,\n",
       "   38.6932258605957,\n",
       "   39.07160186767578,\n",
       "   39.469058990478516,\n",
       "   39.38758850097656,\n",
       "   39.670631408691406,\n",
       "   39.47999954223633,\n",
       "   39.65171432495117,\n",
       "   39.39585494995117,\n",
       "   38.93766403198242,\n",
       "   38.901092529296875,\n",
       "   38.80359649658203,\n",
       "   38.84346008300781,\n",
       "   39.125328063964844,\n",
       "   39.01295471191406,\n",
       "   38.631744384765625,\n",
       "   38.17415237426758,\n",
       "   37.85941696166992,\n",
       "   37.953861236572266,\n",
       "   37.79475402832031,\n",
       "   37.51773452758789,\n",
       "   37.45946502685547,\n",
       "   37.87026596069336,\n",
       "   37.775665283203125,\n",
       "   37.71672821044922,\n",
       "   37.803436279296875,\n",
       "   37.69377517700195,\n",
       "   37.90055465698242,\n",
       "   37.85282516479492,\n",
       "   37.552520751953125,\n",
       "   37.3028678894043,\n",
       "   37.411354064941406,\n",
       "   37.42646408081055,\n",
       "   37.22417449951172,\n",
       "   37.07545852661133,\n",
       "   37.13630294799805,\n",
       "   37.33495330810547,\n",
       "   37.53837966918945,\n",
       "   37.5384407043457,\n",
       "   37.68581008911133,\n",
       "   38.02627182006836,\n",
       "   38.04779815673828,\n",
       "   38.07978057861328,\n",
       "   38.195404052734375,\n",
       "   38.47208786010742,\n",
       "   38.50251770019531,\n",
       "   38.42620086669922,\n",
       "   38.34648513793945,\n",
       "   38.15046691894531,\n",
       "   38.040958404541016,\n",
       "   37.883480072021484,\n",
       "   37.892669677734375,\n",
       "   37.75535202026367,\n",
       "   37.60926818847656,\n",
       "   37.20489501953125,\n",
       "   37.44554901123047,\n",
       "   37.34913635253906,\n",
       "   37.17404556274414,\n",
       "   37.091087341308594,\n",
       "   37.19017028808594,\n",
       "   37.510963439941406,\n",
       "   37.759559631347656,\n",
       "   37.816009521484375,\n",
       "   37.793216705322266,\n",
       "   37.97922897338867,\n",
       "   38.03842544555664,\n",
       "   38.23994827270508,\n",
       "   38.54228210449219,\n",
       "   38.17601013183594,\n",
       "   37.66245651245117,\n",
       "   37.46614456176758,\n",
       "   37.29677200317383,\n",
       "   36.95402908325195,\n",
       "   36.86558532714844,\n",
       "   36.26471710205078,\n",
       "   36.34911346435547,\n",
       "   36.19063949584961,\n",
       "   36.302791595458984,\n",
       "   36.60621643066406,\n",
       "   37.01128387451172,\n",
       "   37.451717376708984,\n",
       "   37.54471206665039,\n",
       "   37.53078079223633,\n",
       "   37.472835540771484,\n",
       "   37.31647872924805,\n",
       "   37.13118362426758,\n",
       "   37.34286880493164,\n",
       "   37.33124542236328,\n",
       "   37.18095397949219,\n",
       "   37.221500396728516,\n",
       "   37.4110107421875,\n",
       "   37.37636947631836,\n",
       "   37.47682571411133,\n",
       "   37.327423095703125,\n",
       "   37.122169494628906,\n",
       "   36.74082565307617,\n",
       "   36.7317008972168,\n",
       "   36.92060852050781,\n",
       "   37.235496520996094,\n",
       "   37.328521728515625,\n",
       "   37.24723815917969,\n",
       "   37.24911117553711,\n",
       "   37.4279899597168,\n",
       "   37.06640625,\n",
       "   37.0539665222168,\n",
       "   37.3935546875,\n",
       "   37.504940032958984,\n",
       "   37.44484329223633,\n",
       "   37.0899658203125,\n",
       "   36.904178619384766,\n",
       "   36.499900817871094,\n",
       "   35.984352111816406,\n",
       "   36.03544235229492,\n",
       "   35.94264221191406,\n",
       "   36.22435760498047,\n",
       "   36.44157791137695,\n",
       "   36.63741683959961,\n",
       "   36.7083625793457,\n",
       "   36.61773681640625,\n",
       "   36.670169830322266,\n",
       "   36.70228576660156,\n",
       "   37.05868911743164,\n",
       "   37.34553909301758,\n",
       "   37.04666519165039,\n",
       "   36.689918518066406,\n",
       "   36.45344543457031,\n",
       "   36.19719696044922,\n",
       "   36.09696960449219,\n",
       "   35.883453369140625,\n",
       "   35.63088607788086,\n",
       "   35.493988037109375,\n",
       "   35.43577194213867,\n",
       "   35.272796630859375,\n",
       "   35.220054626464844,\n",
       "   35.362361907958984,\n",
       "   35.6455192565918,\n",
       "   35.70602035522461,\n",
       "   35.691864013671875,\n",
       "   35.501853942871094,\n",
       "   35.35901641845703,\n",
       "   35.14727020263672,\n",
       "   34.89006423950195,\n",
       "   34.715850830078125,\n",
       "   34.53188705444336,\n",
       "   34.64264678955078,\n",
       "   35.007686614990234,\n",
       "   35.41813659667969,\n",
       "   35.70909881591797,\n",
       "   35.82459259033203,\n",
       "   36.28528594970703,\n",
       "   36.408321380615234,\n",
       "   36.14583969116211,\n",
       "   35.9451789855957,\n",
       "   35.22520065307617,\n",
       "   34.8955078125,\n",
       "   34.248111724853516,\n",
       "   33.968116760253906,\n",
       "   33.578582763671875,\n",
       "   33.237701416015625,\n",
       "   32.836360931396484,\n",
       "   32.87946701049805,\n",
       "   32.81740188598633,\n",
       "   33.067996978759766,\n",
       "   33.09202194213867,\n",
       "   33.662261962890625,\n",
       "   34.347206115722656,\n",
       "   35.15745544433594,\n",
       "   35.61171340942383,\n",
       "   35.85430145263672,\n",
       "   36.09899139404297,\n",
       "   35.91007995605469,\n",
       "   36.030738830566406,\n",
       "   35.93662643432617,\n",
       "   35.497657775878906,\n",
       "   35.24101638793945,\n",
       "   35.237056732177734,\n",
       "   35.2270393371582,\n",
       "   35.268226623535156,\n",
       "   35.457576751708984,\n",
       "   35.57294845581055,\n",
       "   35.79753494262695,\n",
       "   35.944793701171875,\n",
       "   35.95820236206055,\n",
       "   35.6717643737793,\n",
       "   35.53274917602539,\n",
       "   35.38336181640625,\n",
       "   35.260868072509766,\n",
       "   35.41887283325195,\n",
       "   35.46617889404297,\n",
       "   35.622352600097656,\n",
       "   35.5375862121582,\n",
       "   35.50493240356445,\n",
       "   35.462100982666016,\n",
       "   35.34413146972656,\n",
       "   35.41387176513672,\n",
       "   35.2468376159668,\n",
       "   35.01299285888672,\n",
       "   34.180213928222656,\n",
       "   33.82225799560547,\n",
       "   33.721221923828125,\n",
       "   33.64334487915039,\n",
       "   33.435638427734375,\n",
       "   33.276405334472656,\n",
       "   33.324562072753906,\n",
       "   33.46050262451172,\n",
       "   33.58420944213867,\n",
       "   33.65155029296875,\n",
       "   33.698062896728516,\n",
       "   33.97270965576172,\n",
       "   34.171302795410156,\n",
       "   34.26972198486328,\n",
       "   34.363590240478516,\n",
       "   34.41908264160156,\n",
       "   34.663330078125,\n",
       "   34.74847412109375,\n",
       "   35.02916717529297,\n",
       "   35.323341369628906,\n",
       "   35.34807586669922,\n",
       "   35.29502868652344,\n",
       "   35.32538604736328,\n",
       "   35.12388229370117,\n",
       "   34.9252815246582,\n",
       "   34.876644134521484,\n",
       "   34.47616958618164,\n",
       "   34.1065788269043,\n",
       "   34.11956024169922,\n",
       "   33.97703170776367,\n",
       "   34.11933135986328,\n",
       "   34.150970458984375,\n",
       "   34.17878341674805,\n",
       "   33.84041976928711,\n",
       "   34.063934326171875,\n",
       "   33.913246154785156,\n",
       "   33.94428634643555,\n",
       "   33.768619537353516,\n",
       "   33.88389205932617,\n",
       "   33.86070251464844,\n",
       "   33.68558883666992,\n",
       "   33.65373229980469,\n",
       "   33.249290466308594,\n",
       "   33.54803466796875,\n",
       "   33.194766998291016,\n",
       "   33.36255645751953,\n",
       "   33.550445556640625,\n",
       "   33.87423324584961,\n",
       "   33.72483825683594,\n",
       "   33.43244552612305,\n",
       "   33.30900192260742,\n",
       "   33.1517219543457,\n",
       "   33.17154312133789,\n",
       "   33.41668701171875,\n",
       "   33.556427001953125,\n",
       "   33.41915512084961,\n",
       "   33.69744110107422,\n",
       "   33.68965148925781,\n",
       "   33.622650146484375,\n",
       "   33.767642974853516,\n",
       "   33.86711883544922,\n",
       "   33.649234771728516,\n",
       "   33.69751739501953,\n",
       "   33.674190521240234,\n",
       "   33.50211715698242,\n",
       "   33.59739685058594,\n",
       "   33.60772705078125,\n",
       "   33.542232513427734,\n",
       "   33.14545822143555,\n",
       "   33.033329010009766,\n",
       "   33.01896286010742,\n",
       "   32.772274017333984,\n",
       "   33.03380584716797,\n",
       "   33.319644927978516,\n",
       "   33.307533264160156,\n",
       "   33.05165100097656,\n",
       "   32.80504608154297,\n",
       "   32.09722137451172,\n",
       "   31.934648513793945,\n",
       "   31.7929630279541,\n",
       "   31.791778564453125,\n",
       "   31.846826553344727,\n",
       "   31.71025848388672,\n",
       "   31.664859771728516,\n",
       "   31.707181930541992,\n",
       "   31.780323028564453,\n",
       "   31.908512115478516,\n",
       "   32.436458587646484,\n",
       "   32.475791931152344,\n",
       "   32.71907424926758,\n",
       "   33.03331756591797,\n",
       "   33.09345245361328,\n",
       "   33.36841583251953,\n",
       "   33.17133331298828,\n",
       "   33.059043884277344,\n",
       "   33.07757568359375,\n",
       "   32.9932746887207,\n",
       "   33.19218063354492,\n",
       "   33.08769607543945,\n",
       "   32.59003829956055,\n",
       "   32.51186752319336,\n",
       "   32.186302185058594,\n",
       "   31.979965209960938,\n",
       "   32.25226974487305,\n",
       "   32.156288146972656,\n",
       "   32.296348571777344,\n",
       "   32.00958251953125,\n",
       "   31.829980850219727,\n",
       "   31.662309646606445,\n",
       "   31.549787521362305,\n",
       "   31.232982635498047,\n",
       "   31.221759796142578,\n",
       "   31.228805541992188,\n",
       "   31.337600708007812,\n",
       "   31.273042678833008,\n",
       "   31.22836685180664,\n",
       "   31.220230102539062,\n",
       "   31.336837768554688,\n",
       "   31.300565719604492,\n",
       "   31.299087524414062,\n",
       "   31.269332885742188,\n",
       "   31.224653244018555,\n",
       "   31.153362274169922,\n",
       "   31.029159545898438,\n",
       "   30.869314193725586,\n",
       "   30.82120704650879,\n",
       "   30.898574829101562,\n",
       "   31.075740814208984,\n",
       "   31.1678409576416,\n",
       "   31.597713470458984,\n",
       "   31.962818145751953,\n",
       "   31.904855728149414,\n",
       "   31.844324111938477,\n",
       "   31.855825424194336,\n",
       "   31.466516494750977,\n",
       "   31.44443702697754,\n",
       "   31.347972869873047,\n",
       "   31.382179260253906,\n",
       "   31.43400001525879,\n",
       "   31.63033103942871,\n",
       "   31.919475555419922],\n",
       "  'voc_acc': [128.43975830078125,\n",
       "   73.25273895263672,\n",
       "   45.194217681884766,\n",
       "   24.30377960205078,\n",
       "   21.49258041381836,\n",
       "   22.392620086669922,\n",
       "   24.088748931884766,\n",
       "   24.991107940673828,\n",
       "   29.414865493774414,\n",
       "   35.05177307128906,\n",
       "   38.94892883300781,\n",
       "   40.66779708862305,\n",
       "   46.01969909667969,\n",
       "   47.18941879272461,\n",
       "   48.787086486816406,\n",
       "   50.182334899902344,\n",
       "   49.42253875732422,\n",
       "   48.69483184814453,\n",
       "   48.215579986572266,\n",
       "   47.6401252746582,\n",
       "   46.369327545166016,\n",
       "   45.44565963745117,\n",
       "   41.50719451904297,\n",
       "   40.088871002197266,\n",
       "   39.060054779052734,\n",
       "   34.58012771606445,\n",
       "   33.159400939941406,\n",
       "   30.73064422607422,\n",
       "   27.81707000732422,\n",
       "   26.94131088256836,\n",
       "   25.602561950683594,\n",
       "   25.81390953063965,\n",
       "   25.41594886779785,\n",
       "   24.609222412109375,\n",
       "   23.558151245117188,\n",
       "   22.146888732910156,\n",
       "   21.554597854614258,\n",
       "   22.040721893310547,\n",
       "   22.218904495239258,\n",
       "   22.008089065551758,\n",
       "   21.673675537109375,\n",
       "   22.408597946166992,\n",
       "   23.092071533203125,\n",
       "   24.44234848022461,\n",
       "   24.891550064086914,\n",
       "   25.353374481201172,\n",
       "   24.69330596923828,\n",
       "   24.934555053710938,\n",
       "   25.67215347290039,\n",
       "   26.80499839782715,\n",
       "   28.063100814819336,\n",
       "   29.731645584106445,\n",
       "   30.47701072692871,\n",
       "   32.870731353759766,\n",
       "   33.595733642578125,\n",
       "   36.010616302490234,\n",
       "   37.68833541870117,\n",
       "   39.09743881225586,\n",
       "   38.847198486328125,\n",
       "   38.4704704284668,\n",
       "   38.68430709838867,\n",
       "   37.89564895629883,\n",
       "   38.38846206665039,\n",
       "   38.54154968261719,\n",
       "   37.868324279785156,\n",
       "   36.952781677246094,\n",
       "   36.162109375,\n",
       "   35.590248107910156,\n",
       "   34.92792510986328,\n",
       "   35.38380432128906,\n",
       "   35.70766830444336,\n",
       "   35.45978927612305,\n",
       "   36.444862365722656,\n",
       "   37.05884552001953,\n",
       "   37.90480041503906,\n",
       "   39.00474166870117,\n",
       "   38.809391021728516,\n",
       "   38.49248123168945,\n",
       "   39.49026870727539,\n",
       "   40.25774002075195,\n",
       "   41.65001678466797,\n",
       "   42.79652786254883,\n",
       "   42.501373291015625,\n",
       "   43.98320770263672,\n",
       "   44.15682601928711,\n",
       "   45.51420974731445,\n",
       "   45.15873336791992,\n",
       "   46.38788986206055,\n",
       "   46.063682556152344,\n",
       "   45.84392166137695,\n",
       "   44.53738784790039,\n",
       "   43.978729248046875,\n",
       "   42.79879379272461,\n",
       "   41.15990447998047,\n",
       "   40.23413848876953,\n",
       "   38.00052261352539,\n",
       "   36.98655700683594,\n",
       "   35.79876708984375,\n",
       "   33.905757904052734,\n",
       "   32.086788177490234,\n",
       "   30.83709716796875,\n",
       "   31.49167251586914,\n",
       "   31.02017593383789,\n",
       "   31.930065155029297,\n",
       "   32.07875442504883,\n",
       "   32.36318588256836,\n",
       "   31.826383590698242,\n",
       "   31.868282318115234,\n",
       "   31.361848831176758,\n",
       "   31.0028018951416,\n",
       "   31.9055233001709,\n",
       "   33.728816986083984,\n",
       "   34.66782760620117,\n",
       "   35.42607116699219,\n",
       "   36.877323150634766,\n",
       "   37.7819709777832,\n",
       "   38.80625915527344,\n",
       "   38.46171569824219,\n",
       "   39.04499435424805,\n",
       "   39.70053482055664,\n",
       "   40.72943878173828,\n",
       "   41.60372543334961,\n",
       "   42.30210876464844,\n",
       "   43.04963302612305,\n",
       "   43.46549987792969,\n",
       "   43.830806732177734,\n",
       "   44.154232025146484,\n",
       "   44.24134826660156,\n",
       "   44.69722366333008,\n",
       "   43.99758529663086,\n",
       "   43.11336135864258,\n",
       "   42.815582275390625,\n",
       "   43.07721710205078,\n",
       "   42.45314025878906,\n",
       "   41.55573272705078,\n",
       "   40.29832458496094,\n",
       "   38.350711822509766,\n",
       "   37.783302307128906,\n",
       "   36.87953567504883,\n",
       "   35.976524353027344,\n",
       "   34.90069580078125,\n",
       "   34.55442810058594,\n",
       "   34.15296173095703,\n",
       "   33.92390823364258,\n",
       "   34.171409606933594,\n",
       "   33.33626937866211,\n",
       "   33.284671783447266,\n",
       "   32.551795959472656,\n",
       "   32.32924270629883,\n",
       "   31.90055274963379,\n",
       "   31.517702102661133,\n",
       "   30.945709228515625,\n",
       "   32.417659759521484,\n",
       "   33.614234924316406,\n",
       "   34.78748321533203,\n",
       "   36.260860443115234,\n",
       "   37.201202392578125,\n",
       "   38.5793571472168,\n",
       "   39.05789566040039,\n",
       "   38.94853210449219,\n",
       "   40.31999969482422,\n",
       "   40.38412094116211,\n",
       "   41.15711212158203,\n",
       "   41.321075439453125,\n",
       "   41.93117141723633,\n",
       "   41.823333740234375,\n",
       "   41.85116195678711,\n",
       "   42.524070739746094,\n",
       "   42.27598571777344,\n",
       "   42.27880859375,\n",
       "   42.04873275756836,\n",
       "   41.33833694458008,\n",
       "   40.606204986572266,\n",
       "   40.48882293701172,\n",
       "   40.063018798828125,\n",
       "   39.166385650634766,\n",
       "   37.92551803588867,\n",
       "   37.14625549316406,\n",
       "   36.64454650878906,\n",
       "   35.11947250366211,\n",
       "   35.38325881958008,\n",
       "   35.44440841674805,\n",
       "   35.62249755859375,\n",
       "   35.00411605834961,\n",
       "   35.26445770263672,\n",
       "   34.96503448486328,\n",
       "   34.774879455566406,\n",
       "   33.5766487121582,\n",
       "   34.83692169189453,\n",
       "   33.69126510620117,\n",
       "   33.85979080200195,\n",
       "   34.34880447387695,\n",
       "   34.214969635009766,\n",
       "   34.092281341552734,\n",
       "   34.97128677368164,\n",
       "   35.56959533691406,\n",
       "   34.789283752441406,\n",
       "   34.4796257019043,\n",
       "   33.83518981933594,\n",
       "   34.37900161743164,\n",
       "   34.071624755859375,\n",
       "   34.20218276977539,\n",
       "   34.00992965698242,\n",
       "   33.40583801269531,\n",
       "   32.3320198059082,\n",
       "   32.908260345458984,\n",
       "   32.73948287963867,\n",
       "   33.237335205078125,\n",
       "   33.29468536376953,\n",
       "   32.94829559326172,\n",
       "   33.103485107421875,\n",
       "   32.90105438232422,\n",
       "   32.62342071533203,\n",
       "   31.523704528808594,\n",
       "   31.82788848876953,\n",
       "   32.35832214355469,\n",
       "   33.06536102294922,\n",
       "   32.624290466308594,\n",
       "   32.029571533203125,\n",
       "   31.62052345275879,\n",
       "   31.420469284057617,\n",
       "   31.732580184936523,\n",
       "   31.64116096496582,\n",
       "   31.553173065185547,\n",
       "   31.66912841796875,\n",
       "   32.349037170410156,\n",
       "   31.330507278442383,\n",
       "   32.251121520996094,\n",
       "   33.73652648925781,\n",
       "   33.87649917602539,\n",
       "   34.180137634277344,\n",
       "   33.72661590576172,\n",
       "   32.835201263427734,\n",
       "   31.969865798950195,\n",
       "   31.322830200195312,\n",
       "   30.79255485534668,\n",
       "   30.34827995300293,\n",
       "   29.329002380371094,\n",
       "   27.662458419799805,\n",
       "   26.630428314208984,\n",
       "   26.034822463989258,\n",
       "   24.275489807128906,\n",
       "   23.0344295501709,\n",
       "   23.56377410888672,\n",
       "   23.464536666870117,\n",
       "   23.03240203857422,\n",
       "   22.92003059387207,\n",
       "   23.105770111083984,\n",
       "   22.846435546875,\n",
       "   23.538612365722656,\n",
       "   24.431533813476562,\n",
       "   25.60690689086914,\n",
       "   27.070594787597656,\n",
       "   28.473018646240234,\n",
       "   29.089344024658203,\n",
       "   29.6697998046875,\n",
       "   29.82856559753418,\n",
       "   29.532182693481445,\n",
       "   29.719961166381836,\n",
       "   29.666698455810547,\n",
       "   29.52400779724121,\n",
       "   30.130443572998047,\n",
       "   30.704286575317383,\n",
       "   30.832862854003906,\n",
       "   31.488901138305664,\n",
       "   31.658100128173828,\n",
       "   32.7136116027832,\n",
       "   34.04692077636719,\n",
       "   33.91580581665039,\n",
       "   34.05992889404297,\n",
       "   34.64939498901367,\n",
       "   34.59414291381836,\n",
       "   34.41184616088867,\n",
       "   34.94050216674805,\n",
       "   34.577247619628906,\n",
       "   34.067081451416016,\n",
       "   32.9487190246582,\n",
       "   32.549251556396484,\n",
       "   32.92987060546875,\n",
       "   32.95929718017578,\n",
       "   31.978702545166016,\n",
       "   30.762300491333008,\n",
       "   30.694448471069336,\n",
       "   29.997392654418945,\n",
       "   29.289615631103516,\n",
       "   30.009052276611328,\n",
       "   29.967100143432617,\n",
       "   29.939794540405273,\n",
       "   29.685157775878906,\n",
       "   30.29121208190918,\n",
       "   31.34041404724121,\n",
       "   31.962934494018555,\n",
       "   33.64546585083008,\n",
       "   34.37611389160156,\n",
       "   34.690494537353516,\n",
       "   34.06620788574219,\n",
       "   33.66499328613281,\n",
       "   32.49627685546875,\n",
       "   31.84501838684082,\n",
       "   31.983253479003906,\n",
       "   31.72671890258789,\n",
       "   31.546628952026367,\n",
       "   30.59023666381836,\n",
       "   30.398706436157227,\n",
       "   30.96002960205078,\n",
       "   31.43020248413086,\n",
       "   31.733903884887695,\n",
       "   32.59532165527344,\n",
       "   32.06180191040039,\n",
       "   31.86945915222168,\n",
       "   31.332735061645508,\n",
       "   30.57245635986328,\n",
       "   30.57069206237793,\n",
       "   30.625877380371094,\n",
       "   31.125349044799805,\n",
       "   32.23585510253906,\n",
       "   31.73583221435547,\n",
       "   31.815284729003906,\n",
       "   31.82952117919922,\n",
       "   31.882164001464844,\n",
       "   31.466415405273438,\n",
       "   30.744033813476562,\n",
       "   29.890178680419922,\n",
       "   29.48185157775879,\n",
       "   29.281166076660156,\n",
       "   29.526391983032227,\n",
       "   30.49131965637207,\n",
       "   29.16518211364746,\n",
       "   28.329627990722656,\n",
       "   27.652498245239258,\n",
       "   27.070554733276367,\n",
       "   25.994565963745117,\n",
       "   26.086055755615234,\n",
       "   26.698928833007812,\n",
       "   26.752927780151367,\n",
       "   27.355743408203125,\n",
       "   27.989912033081055,\n",
       "   28.522762298583984,\n",
       "   28.166383743286133,\n",
       "   28.09603500366211,\n",
       "   27.392274856567383,\n",
       "   27.38880157470703,\n",
       "   27.637895584106445,\n",
       "   27.093448638916016,\n",
       "   27.350940704345703,\n",
       "   26.642349243164062,\n",
       "   25.446584701538086,\n",
       "   26.116809844970703,\n",
       "   25.898239135742188,\n",
       "   25.251861572265625,\n",
       "   24.730003356933594,\n",
       "   24.55768585205078,\n",
       "   25.033573150634766,\n",
       "   25.64236831665039,\n",
       "   26.314470291137695,\n",
       "   27.13485336303711,\n",
       "   27.645687103271484,\n",
       "   27.980127334594727,\n",
       "   28.592926025390625,\n",
       "   29.29350471496582,\n",
       "   29.904142379760742,\n",
       "   30.295791625976562,\n",
       "   30.411602020263672,\n",
       "   30.87680435180664,\n",
       "   30.774837493896484,\n",
       "   31.145719528198242,\n",
       "   30.601152420043945,\n",
       "   30.758445739746094,\n",
       "   30.206315994262695,\n",
       "   29.591279983520508,\n",
       "   28.124156951904297,\n",
       "   26.596054077148438,\n",
       "   27.091577529907227,\n",
       "   28.074338912963867,\n",
       "   28.439044952392578,\n",
       "   27.954504013061523,\n",
       "   27.629756927490234,\n",
       "   26.476043701171875,\n",
       "   26.38473129272461,\n",
       "   26.274755477905273,\n",
       "   25.988174438476562,\n",
       "   25.71730613708496,\n",
       "   25.17351531982422,\n",
       "   24.742347717285156,\n",
       "   23.634300231933594,\n",
       "   22.38083839416504,\n",
       "   21.775196075439453,\n",
       "   22.403032302856445,\n",
       "   23.08366584777832,\n",
       "   23.361591339111328,\n",
       "   23.808025360107422,\n",
       "   24.064876556396484,\n",
       "   24.70793342590332,\n",
       "   24.78705406188965,\n",
       "   24.819412231445312,\n",
       "   24.772926330566406,\n",
       "   25.658065795898438,\n",
       "   25.29684066772461,\n",
       "   24.858638763427734,\n",
       "   24.644479751586914,\n",
       "   23.855070114135742,\n",
       "   22.826826095581055,\n",
       "   22.653593063354492,\n",
       "   23.77975845336914,\n",
       "   24.466201782226562,\n",
       "   25.15595245361328,\n",
       "   26.74065589904785,\n",
       "   28.924461364746094,\n",
       "   28.738075256347656,\n",
       "   29.94213104248047,\n",
       "   29.616943359375,\n",
       "   30.27349090576172,\n",
       "   30.657255172729492,\n",
       "   30.699026107788086,\n",
       "   30.854711532592773,\n",
       "   30.694263458251953,\n",
       "   30.273479461669922,\n",
       "   29.78574562072754,\n",
       "   29.079692840576172,\n",
       "   29.227861404418945,\n",
       "   29.117849349975586,\n",
       "   28.881376266479492,\n",
       "   29.04456901550293,\n",
       "   28.54828453063965,\n",
       "   27.66492462158203,\n",
       "   27.81536293029785,\n",
       "   26.649703979492188,\n",
       "   26.626375198364258,\n",
       "   26.322608947753906,\n",
       "   25.79543685913086,\n",
       "   23.971874237060547,\n",
       "   23.87460708618164,\n",
       "   22.800376892089844,\n",
       "   21.56732177734375,\n",
       "   21.679759979248047,\n",
       "   22.3067684173584,\n",
       "   22.860939025878906,\n",
       "   22.555620193481445,\n",
       "   23.059782028198242,\n",
       "   24.7582950592041,\n",
       "   25.72238540649414,\n",
       "   26.70225715637207,\n",
       "   27.48648452758789,\n",
       "   27.99980926513672,\n",
       "   28.716054916381836,\n",
       "   28.98271942138672,\n",
       "   29.11890411376953,\n",
       "   29.675308227539062,\n",
       "   30.116668701171875,\n",
       "   29.761119842529297,\n",
       "   29.75510025024414,\n",
       "   29.87586212158203,\n",
       "   28.98917579650879,\n",
       "   28.856971740722656,\n",
       "   28.123062133789062,\n",
       "   27.48359489440918,\n",
       "   27.80953598022461,\n",
       "   27.862844467163086,\n",
       "   28.30563735961914,\n",
       "   28.177330017089844,\n",
       "   28.968503952026367,\n",
       "   29.11541748046875,\n",
       "   29.081226348876953,\n",
       "   29.608110427856445,\n",
       "   30.112098693847656,\n",
       "   30.010587692260742,\n",
       "   29.919645309448242,\n",
       "   29.901012420654297,\n",
       "   29.49049186706543,\n",
       "   28.973726272583008,\n",
       "   28.629762649536133,\n",
       "   28.415945053100586,\n",
       "   27.526336669921875,\n",
       "   27.10054588317871,\n",
       "   26.003820419311523,\n",
       "   25.972042083740234,\n",
       "   25.7989501953125,\n",
       "   25.24786949157715,\n",
       "   24.515527725219727,\n",
       "   23.916826248168945,\n",
       "   23.99127960205078,\n",
       "   24.177780151367188,\n",
       "   24.3704891204834,\n",
       "   25.438100814819336,\n",
       "   26.382919311523438,\n",
       "   27.398658752441406,\n",
       "   27.060779571533203,\n",
       "   27.455446243286133,\n",
       "   28.1748046875,\n",
       "   28.154558181762695,\n",
       "   28.672334671020508,\n",
       "   29.523670196533203,\n",
       "   29.74162483215332,\n",
       "   29.43098258972168,\n",
       "   28.945484161376953,\n",
       "   28.077178955078125,\n",
       "   27.55032730102539,\n",
       "   26.415916442871094,\n",
       "   25.96595001220703,\n",
       "   25.28863525390625],\n",
       "  'jsc_acc': [52.48196029663086,\n",
       "   47.771183013916016,\n",
       "   47.35020065307617,\n",
       "   46.312828063964844,\n",
       "   44.24689483642578,\n",
       "   43.46235275268555,\n",
       "   42.37324142456055,\n",
       "   41.21859359741211,\n",
       "   41.54716873168945,\n",
       "   40.937015533447266,\n",
       "   40.4156379699707,\n",
       "   42.46810531616211,\n",
       "   45.34489059448242,\n",
       "   48.768516540527344,\n",
       "   52.23859405517578,\n",
       "   54.86815643310547,\n",
       "   57.34017562866211,\n",
       "   60.01112365722656,\n",
       "   62.556827545166016,\n",
       "   64.00924682617188,\n",
       "   65.580078125,\n",
       "   67.85640716552734,\n",
       "   69.2263412475586,\n",
       "   71.18939208984375,\n",
       "   72.16181945800781,\n",
       "   73.2955551147461,\n",
       "   74.04849243164062,\n",
       "   74.67593383789062,\n",
       "   75.17769622802734,\n",
       "   75.41126251220703,\n",
       "   75.54705810546875,\n",
       "   75.7828140258789,\n",
       "   75.17321014404297,\n",
       "   75.1328125,\n",
       "   75.32010650634766,\n",
       "   75.11421203613281,\n",
       "   75.32948303222656,\n",
       "   75.65740966796875,\n",
       "   75.37754821777344,\n",
       "   75.50491333007812,\n",
       "   75.32379913330078,\n",
       "   75.19227600097656,\n",
       "   75.28358459472656,\n",
       "   75.20893859863281,\n",
       "   75.06774139404297,\n",
       "   75.11282348632812,\n",
       "   74.98318481445312,\n",
       "   74.7802505493164,\n",
       "   74.75505828857422,\n",
       "   74.82061004638672,\n",
       "   74.89313507080078,\n",
       "   75.2062759399414,\n",
       "   75.68453216552734,\n",
       "   75.81180572509766,\n",
       "   75.71043395996094,\n",
       "   76.28319549560547,\n",
       "   76.2306900024414,\n",
       "   76.31501007080078,\n",
       "   76.1250228881836,\n",
       "   76.18577575683594,\n",
       "   76.46005249023438,\n",
       "   76.60363006591797,\n",
       "   76.40681457519531,\n",
       "   75.77251434326172,\n",
       "   74.88782501220703,\n",
       "   74.33045959472656,\n",
       "   73.99053955078125,\n",
       "   73.47132873535156,\n",
       "   72.42613220214844,\n",
       "   69.68934631347656,\n",
       "   66.80496215820312,\n",
       "   64.88511657714844,\n",
       "   62.281333923339844,\n",
       "   60.336761474609375,\n",
       "   58.71099090576172,\n",
       "   56.434852600097656,\n",
       "   53.572750091552734,\n",
       "   51.75839614868164,\n",
       "   49.87503433227539,\n",
       "   47.8336067199707,\n",
       "   45.85783386230469,\n",
       "   43.73602294921875,\n",
       "   41.50367736816406,\n",
       "   39.804840087890625,\n",
       "   39.07603073120117,\n",
       "   38.23844528198242,\n",
       "   36.678096771240234,\n",
       "   35.914039611816406,\n",
       "   36.71149444580078,\n",
       "   36.39830017089844,\n",
       "   36.65260696411133,\n",
       "   36.49932861328125,\n",
       "   37.79052734375,\n",
       "   37.87388610839844,\n",
       "   38.07239532470703,\n",
       "   37.451210021972656,\n",
       "   37.05207824707031,\n",
       "   37.694339752197266,\n",
       "   37.410919189453125,\n",
       "   37.27849197387695,\n",
       "   37.46493148803711,\n",
       "   37.90300369262695,\n",
       "   39.37758255004883,\n",
       "   39.54399490356445,\n",
       "   40.09198760986328,\n",
       "   40.15388870239258,\n",
       "   40.422969818115234,\n",
       "   40.94546890258789,\n",
       "   40.69724655151367,\n",
       "   39.4467658996582,\n",
       "   39.03847122192383,\n",
       "   38.78282165527344,\n",
       "   39.0405387878418,\n",
       "   37.792236328125,\n",
       "   37.93326950073242,\n",
       "   37.9167366027832,\n",
       "   38.11044692993164,\n",
       "   38.210113525390625,\n",
       "   38.08396530151367,\n",
       "   36.6762580871582,\n",
       "   37.40596389770508,\n",
       "   37.068260192871094,\n",
       "   37.24563217163086,\n",
       "   37.03194046020508,\n",
       "   35.524810791015625,\n",
       "   34.833473205566406,\n",
       "   33.519412994384766,\n",
       "   33.727455139160156,\n",
       "   34.29072952270508,\n",
       "   33.81303405761719,\n",
       "   33.60787582397461,\n",
       "   34.179229736328125,\n",
       "   34.305904388427734,\n",
       "   33.79262161254883,\n",
       "   34.37892532348633,\n",
       "   34.476505279541016,\n",
       "   34.93117141723633,\n",
       "   35.327430725097656,\n",
       "   35.38978958129883,\n",
       "   36.016361236572266,\n",
       "   35.625667572021484,\n",
       "   36.23970031738281,\n",
       "   36.84685516357422,\n",
       "   36.76122283935547,\n",
       "   36.48273468017578,\n",
       "   36.977882385253906,\n",
       "   35.81380844116211,\n",
       "   35.430397033691406,\n",
       "   35.34281539916992,\n",
       "   35.0980224609375,\n",
       "   35.530845642089844,\n",
       "   35.869361877441406,\n",
       "   35.4869384765625,\n",
       "   35.37364196777344,\n",
       "   34.60285186767578,\n",
       "   34.10974884033203,\n",
       "   34.33742141723633,\n",
       "   34.069149017333984,\n",
       "   33.368629455566406,\n",
       "   33.95246887207031,\n",
       "   34.52621841430664,\n",
       "   34.85693359375,\n",
       "   34.34016799926758,\n",
       "   34.97804641723633,\n",
       "   35.2725944519043,\n",
       "   35.418296813964844,\n",
       "   35.34824752807617,\n",
       "   35.0850715637207,\n",
       "   34.14608383178711,\n",
       "   32.724037170410156,\n",
       "   32.36841583251953,\n",
       "   31.77582359313965,\n",
       "   31.855356216430664,\n",
       "   30.936885833740234,\n",
       "   30.6951961517334,\n",
       "   31.20635414123535,\n",
       "   30.40376853942871,\n",
       "   31.51906394958496,\n",
       "   31.1894474029541,\n",
       "   31.423446655273438,\n",
       "   31.132816314697266,\n",
       "   30.908754348754883,\n",
       "   30.834880828857422,\n",
       "   31.86115264892578,\n",
       "   31.478830337524414,\n",
       "   32.055301666259766,\n",
       "   32.122344970703125,\n",
       "   32.628971099853516,\n",
       "   33.053531646728516,\n",
       "   32.96259689331055,\n",
       "   32.85814666748047,\n",
       "   33.5308837890625,\n",
       "   34.28054428100586,\n",
       "   34.584781646728516,\n",
       "   34.95454025268555,\n",
       "   34.61541748046875,\n",
       "   34.040279388427734,\n",
       "   34.20936584472656,\n",
       "   34.065635681152344,\n",
       "   33.810752868652344,\n",
       "   33.148902893066406,\n",
       "   33.26484298706055,\n",
       "   32.854454040527344,\n",
       "   32.69581604003906,\n",
       "   31.816287994384766,\n",
       "   30.679325103759766,\n",
       "   31.352237701416016,\n",
       "   30.99342918395996,\n",
       "   30.83793830871582,\n",
       "   30.726850509643555,\n",
       "   29.827362060546875,\n",
       "   29.95499610900879,\n",
       "   28.91600799560547,\n",
       "   28.276935577392578,\n",
       "   27.99896812438965,\n",
       "   27.926549911499023,\n",
       "   27.36605453491211,\n",
       "   27.970096588134766,\n",
       "   29.38542938232422,\n",
       "   29.208904266357422,\n",
       "   29.532161712646484,\n",
       "   30.66926383972168,\n",
       "   31.047883987426758,\n",
       "   31.340349197387695,\n",
       "   31.254493713378906,\n",
       "   31.01387596130371,\n",
       "   31.2841739654541,\n",
       "   32.328277587890625,\n",
       "   32.754764556884766,\n",
       "   33.349830627441406,\n",
       "   34.290679931640625,\n",
       "   35.174495697021484,\n",
       "   35.608455657958984,\n",
       "   34.842735290527344,\n",
       "   34.61922836303711,\n",
       "   34.115867614746094,\n",
       "   33.38417053222656,\n",
       "   33.74173355102539,\n",
       "   33.21017074584961,\n",
       "   32.16646194458008,\n",
       "   31.35574722290039,\n",
       "   31.30486297607422,\n",
       "   30.93336296081543,\n",
       "   30.444162368774414,\n",
       "   30.352384567260742,\n",
       "   31.36860466003418,\n",
       "   32.508583068847656,\n",
       "   33.715675354003906,\n",
       "   33.68930435180664,\n",
       "   34.23947525024414,\n",
       "   34.04743576049805,\n",
       "   33.63014602661133,\n",
       "   33.67497253417969,\n",
       "   33.6303825378418,\n",
       "   33.70842361450195,\n",
       "   32.493858337402344,\n",
       "   32.127586364746094,\n",
       "   31.455167770385742,\n",
       "   30.673837661743164,\n",
       "   30.149003982543945,\n",
       "   29.79978370666504,\n",
       "   29.244665145874023,\n",
       "   29.318639755249023,\n",
       "   28.90506935119629,\n",
       "   29.77338409423828,\n",
       "   30.664512634277344,\n",
       "   31.257152557373047,\n",
       "   32.23783493041992,\n",
       "   33.01905822753906,\n",
       "   33.7962760925293,\n",
       "   34.27186965942383,\n",
       "   34.83896255493164,\n",
       "   36.40102005004883,\n",
       "   36.45980453491211,\n",
       "   36.77340316772461,\n",
       "   36.64543151855469,\n",
       "   36.090728759765625,\n",
       "   35.758235931396484,\n",
       "   36.03690719604492,\n",
       "   36.301055908203125,\n",
       "   36.01802444458008,\n",
       "   36.042232513427734,\n",
       "   35.72369384765625,\n",
       "   34.95011901855469,\n",
       "   34.88626480102539,\n",
       "   34.90681457519531,\n",
       "   35.12450408935547,\n",
       "   35.0203971862793,\n",
       "   35.323760986328125,\n",
       "   35.27617263793945,\n",
       "   34.77214431762695,\n",
       "   35.15424346923828,\n",
       "   34.34465789794922,\n",
       "   34.60391616821289,\n",
       "   34.52885437011719,\n",
       "   34.3815803527832,\n",
       "   34.065433502197266,\n",
       "   33.87491226196289,\n",
       "   33.867801666259766,\n",
       "   33.8387565612793,\n",
       "   33.08293914794922,\n",
       "   33.571659088134766,\n",
       "   34.25550079345703,\n",
       "   34.580665588378906,\n",
       "   35.217002868652344,\n",
       "   35.240577697753906,\n",
       "   35.646766662597656,\n",
       "   35.59991455078125,\n",
       "   34.78197479248047,\n",
       "   34.391780853271484,\n",
       "   34.44910430908203,\n",
       "   34.78388595581055,\n",
       "   34.81864929199219,\n",
       "   35.30769729614258,\n",
       "   35.06608581542969,\n",
       "   35.75730895996094,\n",
       "   35.84627151489258,\n",
       "   36.19071960449219,\n",
       "   35.923160552978516,\n",
       "   36.121097564697266,\n",
       "   35.44136428833008,\n",
       "   34.85137939453125,\n",
       "   35.05116271972656,\n",
       "   35.452110290527344,\n",
       "   35.279815673828125,\n",
       "   34.9586296081543,\n",
       "   34.52150344848633,\n",
       "   33.86510467529297,\n",
       "   33.244903564453125,\n",
       "   33.457420349121094,\n",
       "   33.452056884765625,\n",
       "   33.35890197753906,\n",
       "   34.15729904174805,\n",
       "   33.88390350341797,\n",
       "   34.066951751708984,\n",
       "   33.32785415649414,\n",
       "   33.21847915649414,\n",
       "   33.472572326660156,\n",
       "   33.34564971923828,\n",
       "   33.27906799316406,\n",
       "   32.995079040527344,\n",
       "   33.522682189941406,\n",
       "   33.32041931152344,\n",
       "   33.317298889160156,\n",
       "   33.61658477783203,\n",
       "   33.3389892578125,\n",
       "   33.127647399902344,\n",
       "   32.51523208618164,\n",
       "   32.45429229736328,\n",
       "   32.490028381347656,\n",
       "   32.30718231201172,\n",
       "   32.56829071044922,\n",
       "   32.86308288574219,\n",
       "   32.94617462158203,\n",
       "   33.658851623535156,\n",
       "   33.318729400634766,\n",
       "   33.85542678833008,\n",
       "   33.45645523071289,\n",
       "   33.16259002685547,\n",
       "   32.883846282958984,\n",
       "   33.41989517211914,\n",
       "   33.53452682495117,\n",
       "   33.248756408691406,\n",
       "   33.06560516357422,\n",
       "   31.930330276489258,\n",
       "   32.38362503051758,\n",
       "   31.499174118041992,\n",
       "   31.750032424926758,\n",
       "   31.75140953063965,\n",
       "   31.20463752746582,\n",
       "   31.944719314575195,\n",
       "   32.28944778442383,\n",
       "   32.30366897583008,\n",
       "   32.06694030761719,\n",
       "   32.013282775878906,\n",
       "   32.08594512939453,\n",
       "   31.998720169067383,\n",
       "   32.700687408447266,\n",
       "   32.19001007080078,\n",
       "   33.01483917236328,\n",
       "   34.365196228027344,\n",
       "   34.96879196166992,\n",
       "   35.63413619995117,\n",
       "   35.65335464477539,\n",
       "   36.08884048461914,\n",
       "   36.61893081665039,\n",
       "   37.00521469116211,\n",
       "   36.214962005615234,\n",
       "   36.20148849487305,\n",
       "   35.823402404785156,\n",
       "   35.11106872558594,\n",
       "   34.75753402709961,\n",
       "   34.955078125,\n",
       "   35.06393814086914,\n",
       "   35.120506286621094,\n",
       "   35.460750579833984,\n",
       "   35.585506439208984,\n",
       "   35.34516525268555,\n",
       "   35.70528030395508,\n",
       "   36.23550033569336,\n",
       "   36.94645309448242,\n",
       "   36.97283935546875,\n",
       "   36.70280456542969,\n",
       "   35.65985107421875,\n",
       "   34.958892822265625,\n",
       "   34.83451461791992,\n",
       "   34.46876907348633,\n",
       "   32.9067497253418,\n",
       "   32.39420700073242,\n",
       "   32.42369842529297,\n",
       "   32.25703811645508,\n",
       "   32.71985626220703,\n",
       "   32.56007766723633,\n",
       "   32.670963287353516,\n",
       "   31.757673263549805,\n",
       "   31.70659828186035,\n",
       "   31.293970108032227,\n",
       "   31.347366333007812,\n",
       "   30.553382873535156,\n",
       "   30.194293975830078,\n",
       "   29.88502311706543,\n",
       "   29.668123245239258,\n",
       "   29.15554428100586,\n",
       "   29.20087242126465,\n",
       "   29.531639099121094,\n",
       "   30.033985137939453,\n",
       "   30.283370971679688,\n",
       "   31.07309913635254,\n",
       "   31.05699348449707,\n",
       "   31.958234786987305,\n",
       "   32.501121520996094,\n",
       "   31.416793823242188,\n",
       "   31.65871810913086,\n",
       "   31.543251037597656,\n",
       "   32.79741668701172,\n",
       "   33.06484603881836,\n",
       "   32.91889953613281,\n",
       "   33.328792572021484,\n",
       "   33.883506774902344,\n",
       "   34.32888412475586,\n",
       "   35.32136535644531,\n",
       "   36.317081451416016,\n",
       "   37.03841781616211,\n",
       "   37.40242385864258,\n",
       "   37.39154815673828,\n",
       "   37.552242279052734,\n",
       "   37.474857330322266,\n",
       "   36.9727668762207,\n",
       "   36.96343994140625,\n",
       "   36.27355194091797,\n",
       "   35.590232849121094,\n",
       "   34.75642395019531,\n",
       "   34.700740814208984,\n",
       "   32.9416618347168,\n",
       "   31.28061866760254,\n",
       "   30.787626266479492,\n",
       "   31.181198120117188,\n",
       "   30.213329315185547,\n",
       "   29.49395751953125,\n",
       "   29.65923309326172,\n",
       "   30.751949310302734,\n",
       "   30.409774780273438,\n",
       "   30.689350128173828,\n",
       "   30.866518020629883,\n",
       "   31.512563705444336,\n",
       "   32.75339889526367,\n",
       "   33.514892578125,\n",
       "   34.19181823730469,\n",
       "   34.29460906982422,\n",
       "   35.33027648925781,\n",
       "   35.42515182495117,\n",
       "   35.45827102661133,\n",
       "   35.936031341552734,\n",
       "   35.42824935913086,\n",
       "   35.01456832885742,\n",
       "   34.78303146362305,\n",
       "   34.64404296875,\n",
       "   34.1594123840332,\n",
       "   33.675743103027344,\n",
       "   32.93899154663086,\n",
       "   32.619686126708984,\n",
       "   32.46509552001953,\n",
       "   31.966014862060547,\n",
       "   31.760961532592773,\n",
       "   31.652271270751953,\n",
       "   30.738603591918945,\n",
       "   30.654720306396484,\n",
       "   30.2178897857666,\n",
       "   29.939434051513672,\n",
       "   29.52428436279297,\n",
       "   30.026241302490234,\n",
       "   30.17401695251465,\n",
       "   30.484527587890625,\n",
       "   31.24207305908203,\n",
       "   30.964914321899414,\n",
       "   31.163331985473633,\n",
       "   31.804901123046875,\n",
       "   31.76053237915039,\n",
       "   32.201499938964844,\n",
       "   31.079191207885742],\n",
       "  'ff_acc': [99.54564666748047,\n",
       "   99.80731964111328,\n",
       "   99.58806610107422,\n",
       "   99.27027130126953,\n",
       "   97.1864013671875,\n",
       "   95.11009979248047,\n",
       "   93.20613861083984,\n",
       "   92.05908966064453,\n",
       "   90.6398696899414,\n",
       "   89.58158111572266,\n",
       "   88.88027954101562,\n",
       "   88.06674194335938,\n",
       "   87.3360366821289,\n",
       "   87.42821502685547,\n",
       "   87.61488342285156,\n",
       "   87.93956756591797,\n",
       "   87.98511505126953,\n",
       "   87.93014526367188,\n",
       "   87.83238983154297,\n",
       "   88.0386734008789,\n",
       "   88.6844482421875,\n",
       "   89.22518157958984,\n",
       "   89.2774429321289,\n",
       "   89.53373718261719,\n",
       "   89.80370330810547,\n",
       "   89.91629791259766,\n",
       "   90.75746154785156,\n",
       "   90.95328521728516,\n",
       "   91.12716674804688,\n",
       "   91.24569702148438,\n",
       "   91.48294067382812,\n",
       "   91.38582611083984,\n",
       "   91.2648696899414,\n",
       "   91.82830810546875,\n",
       "   92.60718536376953,\n",
       "   92.67168426513672,\n",
       "   92.9089584350586,\n",
       "   92.78140258789062,\n",
       "   92.7830810546875,\n",
       "   93.08181762695312,\n",
       "   93.25940704345703,\n",
       "   93.38394165039062,\n",
       "   93.4498291015625,\n",
       "   93.81925964355469,\n",
       "   93.92157745361328,\n",
       "   94.37741088867188,\n",
       "   95.04690551757812,\n",
       "   95.120849609375,\n",
       "   95.1622314453125,\n",
       "   95.01944732666016,\n",
       "   95.41767120361328,\n",
       "   95.41539001464844,\n",
       "   95.6960678100586,\n",
       "   95.99504852294922,\n",
       "   96.17706298828125,\n",
       "   96.26468658447266,\n",
       "   96.26454162597656,\n",
       "   96.4532470703125,\n",
       "   95.89603424072266,\n",
       "   95.79207611083984,\n",
       "   95.67054748535156,\n",
       "   95.56204986572266,\n",
       "   95.83514404296875,\n",
       "   95.76737976074219,\n",
       "   95.7782974243164,\n",
       "   95.69857025146484,\n",
       "   95.61945343017578,\n",
       "   95.28260803222656,\n",
       "   95.2068099975586,\n",
       "   95.27472686767578,\n",
       "   95.35749816894531,\n",
       "   95.90911865234375,\n",
       "   95.75291442871094,\n",
       "   95.79229736328125,\n",
       "   95.61406707763672,\n",
       "   95.5566635131836,\n",
       "   95.96376037597656,\n",
       "   96.24396514892578,\n",
       "   96.18230438232422,\n",
       "   96.0937728881836,\n",
       "   95.82655334472656,\n",
       "   95.43026733398438,\n",
       "   95.84002685546875,\n",
       "   95.8923110961914,\n",
       "   95.91981506347656,\n",
       "   95.87627410888672,\n",
       "   95.90138244628906,\n",
       "   95.9041748046875,\n",
       "   96.16001892089844,\n",
       "   96.33899688720703,\n",
       "   96.13542175292969,\n",
       "   96.08685302734375,\n",
       "   96.19305419921875,\n",
       "   96.08100128173828,\n",
       "   96.143798828125,\n",
       "   96.12662506103516,\n",
       "   96.14086151123047,\n",
       "   96.11479187011719,\n",
       "   96.2209701538086,\n",
       "   96.22681427001953,\n",
       "   96.37967681884766,\n",
       "   96.29615783691406,\n",
       "   96.2115478515625,\n",
       "   95.97348022460938,\n",
       "   95.81961822509766,\n",
       "   95.67271423339844,\n",
       "   95.6088638305664,\n",
       "   95.6104736328125,\n",
       "   95.55450439453125,\n",
       "   95.76043701171875,\n",
       "   95.79402923583984,\n",
       "   95.77034759521484,\n",
       "   95.70915985107422,\n",
       "   95.73123931884766,\n",
       "   95.75476837158203,\n",
       "   95.92816925048828,\n",
       "   96.1032485961914,\n",
       "   96.14539337158203,\n",
       "   96.13687133789062,\n",
       "   95.98170471191406,\n",
       "   95.83865356445312,\n",
       "   95.76820373535156,\n",
       "   95.81090545654297,\n",
       "   95.82743072509766,\n",
       "   95.96224212646484,\n",
       "   96.04956817626953,\n",
       "   96.0623550415039,\n",
       "   96.10216522216797,\n",
       "   96.1787109375,\n",
       "   96.29875946044922,\n",
       "   96.33206176757812,\n",
       "   96.2243881225586,\n",
       "   96.29335021972656,\n",
       "   96.28103637695312,\n",
       "   96.39404296875,\n",
       "   96.34884643554688,\n",
       "   96.47282409667969,\n",
       "   96.36297607421875,\n",
       "   96.36724853515625,\n",
       "   96.35563659667969,\n",
       "   96.33232116699219,\n",
       "   96.33473205566406,\n",
       "   96.39204406738281,\n",
       "   96.28450775146484,\n",
       "   96.28722381591797,\n",
       "   96.2850570678711,\n",
       "   96.23609924316406,\n",
       "   96.27922821044922,\n",
       "   96.3018798828125,\n",
       "   96.4363784790039,\n",
       "   96.50397491455078,\n",
       "   96.45553588867188,\n",
       "   96.4078598022461,\n",
       "   96.3924789428711,\n",
       "   96.13978576660156,\n",
       "   95.87467193603516,\n",
       "   95.90814971923828,\n",
       "   95.86825561523438,\n",
       "   95.94611358642578,\n",
       "   96.084228515625,\n",
       "   95.99617767333984,\n",
       "   95.89741516113281,\n",
       "   95.83355712890625,\n",
       "   95.81253814697266,\n",
       "   95.81281280517578,\n",
       "   95.86395263671875,\n",
       "   95.75342559814453,\n",
       "   95.83502960205078,\n",
       "   95.82546997070312,\n",
       "   95.88473510742188,\n",
       "   95.95716094970703,\n",
       "   96.0515365600586,\n",
       "   95.90706634521484,\n",
       "   96.01467895507812,\n",
       "   95.8436279296875,\n",
       "   95.76141357421875,\n",
       "   95.9139175415039,\n",
       "   95.9275131225586,\n",
       "   95.88868713378906,\n",
       "   95.79700469970703,\n",
       "   95.59052276611328,\n",
       "   95.61011505126953,\n",
       "   95.60975646972656,\n",
       "   95.57594299316406,\n",
       "   95.65621948242188,\n",
       "   95.7413101196289,\n",
       "   95.63717651367188,\n",
       "   95.56438446044922,\n",
       "   95.51213836669922,\n",
       "   95.52778625488281,\n",
       "   95.5257797241211,\n",
       "   95.47183990478516,\n",
       "   95.53104400634766,\n",
       "   95.41224670410156,\n",
       "   95.33000946044922,\n",
       "   95.30467224121094,\n",
       "   95.33207702636719,\n",
       "   95.31243133544922,\n",
       "   95.4110336303711,\n",
       "   95.2935791015625,\n",
       "   95.37042999267578,\n",
       "   95.38461303710938,\n",
       "   95.47285461425781,\n",
       "   95.3426513671875,\n",
       "   95.28639221191406,\n",
       "   95.39876556396484,\n",
       "   95.48594665527344,\n",
       "   95.46937561035156,\n",
       "   95.33365631103516,\n",
       "   95.32625579833984,\n",
       "   95.38763427734375,\n",
       "   95.3365707397461,\n",
       "   95.36087799072266,\n",
       "   95.24115753173828,\n",
       "   95.3818588256836,\n",
       "   95.12724304199219,\n",
       "   94.97856140136719,\n",
       "   94.87764739990234,\n",
       "   94.9808578491211,\n",
       "   94.79486083984375,\n",
       "   94.79976654052734,\n",
       "   95.13190460205078,\n",
       "   95.16535949707031,\n",
       "   95.35929870605469,\n",
       "   95.3365249633789,\n",
       "   95.32167053222656,\n",
       "   94.9942626953125,\n",
       "   94.8083724975586,\n",
       "   94.65072631835938,\n",
       "   94.6402359008789,\n",
       "   94.57260131835938,\n",
       "   94.48918151855469,\n",
       "   94.43191528320312,\n",
       "   94.25856018066406,\n",
       "   94.66146087646484,\n",
       "   94.88497924804688,\n",
       "   94.73970794677734,\n",
       "   94.76895141601562,\n",
       "   94.82709503173828,\n",
       "   94.83025360107422,\n",
       "   94.92091369628906,\n",
       "   95.12519073486328,\n",
       "   95.10619354248047,\n",
       "   95.06981658935547,\n",
       "   95.19861602783203,\n",
       "   95.17243957519531,\n",
       "   95.21770477294922,\n",
       "   95.00774383544922,\n",
       "   95.04349517822266,\n",
       "   94.75202178955078,\n",
       "   94.62886810302734,\n",
       "   94.56221008300781,\n",
       "   94.52647399902344,\n",
       "   94.42318725585938,\n",
       "   94.583740234375,\n",
       "   94.50640869140625,\n",
       "   94.36630249023438,\n",
       "   94.18865966796875,\n",
       "   94.44224548339844,\n",
       "   94.75592041015625,\n",
       "   94.58043670654297,\n",
       "   94.62681579589844,\n",
       "   94.44667053222656,\n",
       "   94.24365997314453,\n",
       "   94.11902618408203,\n",
       "   94.43338775634766,\n",
       "   94.37535858154297,\n",
       "   94.40441131591797,\n",
       "   94.6109390258789,\n",
       "   94.776611328125,\n",
       "   94.74263763427734,\n",
       "   94.73262786865234,\n",
       "   94.46574401855469,\n",
       "   94.54559326171875,\n",
       "   94.44015502929688,\n",
       "   94.11995697021484,\n",
       "   94.15218353271484,\n",
       "   94.22051239013672,\n",
       "   94.1604232788086,\n",
       "   94.05352783203125,\n",
       "   94.03175354003906,\n",
       "   94.11152648925781,\n",
       "   94.1065444946289,\n",
       "   93.92976379394531,\n",
       "   93.84963989257812,\n",
       "   93.83688354492188,\n",
       "   93.8433609008789,\n",
       "   93.8376235961914,\n",
       "   93.64900207519531,\n",
       "   93.86969757080078,\n",
       "   93.65802764892578,\n",
       "   93.71527099609375,\n",
       "   93.61587524414062,\n",
       "   93.55131530761719,\n",
       "   93.42655181884766,\n",
       "   93.39214324951172,\n",
       "   93.31625366210938,\n",
       "   93.47681427001953,\n",
       "   93.4627685546875,\n",
       "   93.43535614013672,\n",
       "   93.43529510498047,\n",
       "   93.51301574707031,\n",
       "   93.56360626220703,\n",
       "   93.52001953125,\n",
       "   93.51050567626953,\n",
       "   93.50105285644531,\n",
       "   93.61372375488281,\n",
       "   93.47040557861328,\n",
       "   93.46167755126953,\n",
       "   93.18534088134766,\n",
       "   93.1797103881836,\n",
       "   92.8949203491211,\n",
       "   93.17237091064453,\n",
       "   93.10943603515625,\n",
       "   93.23401641845703,\n",
       "   93.14834594726562,\n",
       "   93.07762908935547,\n",
       "   93.11656951904297,\n",
       "   92.90652465820312,\n",
       "   93.0286865234375,\n",
       "   93.0850830078125,\n",
       "   93.28062438964844,\n",
       "   93.460693359375,\n",
       "   93.42479705810547,\n",
       "   93.53741455078125,\n",
       "   93.47239685058594,\n",
       "   93.58000183105469,\n",
       "   93.77113342285156,\n",
       "   94.0262222290039,\n",
       "   93.99758911132812,\n",
       "   93.92406463623047,\n",
       "   93.79405975341797,\n",
       "   93.95538330078125,\n",
       "   94.0511245727539,\n",
       "   93.96701049804688,\n",
       "   94.04721069335938,\n",
       "   93.72369384765625,\n",
       "   93.48938751220703,\n",
       "   93.45899963378906,\n",
       "   93.52131652832031,\n",
       "   93.32289123535156,\n",
       "   93.06742095947266,\n",
       "   93.0644760131836,\n",
       "   92.8470458984375,\n",
       "   92.94976806640625,\n",
       "   92.73662567138672,\n",
       "   93.00846099853516,\n",
       "   92.96099853515625,\n",
       "   92.96868133544922,\n",
       "   92.94611358642578,\n",
       "   92.71865844726562,\n",
       "   92.51424407958984,\n",
       "   92.42473602294922,\n",
       "   92.24412536621094,\n",
       "   92.07551574707031,\n",
       "   92.03081512451172,\n",
       "   91.77490234375,\n",
       "   91.67228698730469,\n",
       "   91.62811279296875,\n",
       "   91.63684844970703,\n",
       "   91.4114761352539,\n",
       "   91.38641357421875,\n",
       "   91.6339111328125,\n",
       "   91.67543029785156,\n",
       "   91.59081268310547,\n",
       "   91.51307678222656,\n",
       "   91.55367279052734,\n",
       "   91.50392150878906,\n",
       "   91.28736877441406,\n",
       "   91.43827819824219,\n",
       "   91.53667449951172,\n",
       "   91.56230926513672,\n",
       "   91.20622253417969,\n",
       "   91.15409088134766,\n",
       "   91.23086547851562,\n",
       "   91.29444122314453,\n",
       "   91.1672592163086,\n",
       "   90.92678833007812,\n",
       "   90.96470642089844,\n",
       "   90.95313262939453,\n",
       "   90.7538833618164,\n",
       "   90.59595489501953,\n",
       "   90.89089965820312,\n",
       "   90.81233978271484,\n",
       "   90.81914520263672,\n",
       "   91.1138916015625,\n",
       "   91.14010620117188,\n",
       "   91.13784790039062,\n",
       "   91.20536804199219,\n",
       "   91.30401611328125,\n",
       "   91.33560943603516,\n",
       "   91.44341278076172,\n",
       "   91.40921020507812,\n",
       "   91.36582946777344,\n",
       "   91.3061294555664,\n",
       "   91.21829223632812,\n",
       "   91.22828674316406,\n",
       "   91.18975067138672,\n",
       "   91.3138427734375,\n",
       "   91.29106140136719,\n",
       "   91.08007049560547,\n",
       "   91.29754638671875,\n",
       "   91.34738159179688,\n",
       "   91.34918212890625,\n",
       "   91.48731231689453,\n",
       "   91.27909088134766,\n",
       "   91.18035125732422,\n",
       "   91.03206634521484,\n",
       "   90.81082153320312,\n",
       "   90.91766357421875,\n",
       "   91.04244995117188,\n",
       "   90.9962387084961,\n",
       "   90.98247528076172,\n",
       "   90.93704223632812,\n",
       "   90.7217025756836,\n",
       "   90.81906127929688,\n",
       "   90.7623291015625,\n",
       "   90.96493530273438,\n",
       "   91.0908432006836,\n",
       "   91.06220245361328,\n",
       "   91.19819641113281,\n",
       "   91.02983856201172,\n",
       "   91.11707305908203,\n",
       "   91.13533782958984,\n",
       "   91.19850158691406,\n",
       "   91.23986053466797,\n",
       "   91.27108764648438,\n",
       "   91.08473205566406,\n",
       "   91.22577667236328,\n",
       "   91.26824951171875,\n",
       "   91.150634765625,\n",
       "   91.03636169433594,\n",
       "   91.02438354492188,\n",
       "   90.87715911865234,\n",
       "   90.9066162109375,\n",
       "   90.9171371459961,\n",
       "   90.92853546142578,\n",
       "   90.87122344970703,\n",
       "   91.03031158447266,\n",
       "   91.00968933105469,\n",
       "   90.98149108886719,\n",
       "   90.76839447021484,\n",
       "   90.8375244140625,\n",
       "   90.77079010009766,\n",
       "   90.7845230102539,\n",
       "   90.86798858642578,\n",
       "   90.76236724853516,\n",
       "   90.69160461425781,\n",
       "   90.48080444335938,\n",
       "   90.348388671875,\n",
       "   90.34031677246094,\n",
       "   90.43962860107422,\n",
       "   90.30059814453125,\n",
       "   90.18832397460938,\n",
       "   90.29708862304688,\n",
       "   90.36430358886719,\n",
       "   90.23456573486328,\n",
       "   90.06089782714844,\n",
       "   90.13436126708984,\n",
       "   90.30643463134766,\n",
       "   90.2303237915039,\n",
       "   90.24266052246094,\n",
       "   90.25397491455078,\n",
       "   90.24665069580078,\n",
       "   90.12864685058594,\n",
       "   90.24675750732422,\n",
       "   90.49513244628906,\n",
       "   90.50894165039062,\n",
       "   90.4852294921875,\n",
       "   90.66574096679688,\n",
       "   90.82550048828125,\n",
       "   90.72593688964844,\n",
       "   90.74391174316406,\n",
       "   90.73670959472656,\n",
       "   90.80923461914062,\n",
       "   90.7490463256836,\n",
       "   90.82652282714844,\n",
       "   90.82427978515625,\n",
       "   90.82624816894531,\n",
       "   90.84896850585938,\n",
       "   90.87179565429688,\n",
       "   90.70050048828125,\n",
       "   90.56118774414062,\n",
       "   90.3519515991211,\n",
       "   90.40739440917969,\n",
       "   90.51380920410156,\n",
       "   90.4876937866211,\n",
       "   90.44847869873047,\n",
       "   90.426025390625,\n",
       "   90.36641693115234,\n",
       "   90.27576446533203,\n",
       "   90.18766021728516,\n",
       "   90.44373321533203,\n",
       "   90.2681655883789,\n",
       "   90.30814361572266,\n",
       "   90.28054809570312,\n",
       "   90.43126678466797,\n",
       "   90.35398864746094,\n",
       "   90.25232696533203,\n",
       "   90.1371078491211],\n",
       "  'test_accs': [756.0535469055176,\n",
       "   572.1179847717285,\n",
       "   458.9464797973633,\n",
       "   373.9920883178711,\n",
       "   322.46586990356445,\n",
       "   283.2020034790039,\n",
       "   247.63511657714844,\n",
       "   221.22938537597656,\n",
       "   208.14711952209473,\n",
       "   200.0885238647461,\n",
       "   199.50620460510254,\n",
       "   201.3756923675537,\n",
       "   211.0184669494629,\n",
       "   219.0723114013672,\n",
       "   228.1826515197754,\n",
       "   234.0876808166504,\n",
       "   238.7415771484375,\n",
       "   243.0950164794922,\n",
       "   247.15577697753906,\n",
       "   250.13605880737305,\n",
       "   251.73233032226562,\n",
       "   254.80691146850586,\n",
       "   252.39915084838867,\n",
       "   253.68583297729492,\n",
       "   253.8296890258789,\n",
       "   249.7164077758789,\n",
       "   249.31568145751953,\n",
       "   246.7328643798828,\n",
       "   244.35131072998047,\n",
       "   243.72440719604492,\n",
       "   242.2511444091797,\n",
       "   243.09487342834473,\n",
       "   241.30948448181152,\n",
       "   240.89405059814453,\n",
       "   240.18634414672852,\n",
       "   239.74597549438477,\n",
       "   240.29646110534668,\n",
       "   242.58342742919922,\n",
       "   243.46307945251465,\n",
       "   243.92109870910645,\n",
       "   244.4348030090332,\n",
       "   245.02482414245605,\n",
       "   245.6640968322754,\n",
       "   247.70098114013672,\n",
       "   248.0264072418213,\n",
       "   248.78118515014648,\n",
       "   248.1816864013672,\n",
       "   247.99365234375,\n",
       "   247.5634536743164,\n",
       "   247.78117561340332,\n",
       "   247.97850608825684,\n",
       "   247.98823356628418,\n",
       "   248.203950881958,\n",
       "   249.86721801757812,\n",
       "   249.00739288330078,\n",
       "   249.82254028320312,\n",
       "   249.50911712646484,\n",
       "   249.81756591796875,\n",
       "   247.75197982788086,\n",
       "   246.40856170654297,\n",
       "   246.37907791137695,\n",
       "   244.95662307739258,\n",
       "   245.33189392089844,\n",
       "   244.35585403442383,\n",
       "   242.3667106628418,\n",
       "   240.74890518188477,\n",
       "   239.87228775024414,\n",
       "   238.98641967773438,\n",
       "   237.0897445678711,\n",
       "   235.18954849243164,\n",
       "   232.5372085571289,\n",
       "   230.85383987426758,\n",
       "   228.07452392578125,\n",
       "   226.95574188232422,\n",
       "   225.73749160766602,\n",
       "   224.4341049194336,\n",
       "   221.0587615966797,\n",
       "   219.34971618652344,\n",
       "   217.96435165405273,\n",
       "   216.67321395874023,\n",
       "   215.40319061279297,\n",
       "   213.55157470703125,\n",
       "   211.21868515014648,\n",
       "   211.02654838562012,\n",
       "   210.6454734802246,\n",
       "   211.40623092651367,\n",
       "   209.66378593444824,\n",
       "   210.33905029296875,\n",
       "   211.0344352722168,\n",
       "   211.11961364746094,\n",
       "   210.39917755126953,\n",
       "   210.16230010986328,\n",
       "   210.82592391967773,\n",
       "   209.26493072509766,\n",
       "   208.95203399658203,\n",
       "   206.0803680419922,\n",
       "   204.65924072265625,\n",
       "   204.1946563720703,\n",
       "   202.430908203125,\n",
       "   200.58457946777344,\n",
       "   199.67605590820312,\n",
       "   200.68001174926758,\n",
       "   201.65825271606445,\n",
       "   202.1791534423828,\n",
       "   202.66280364990234,\n",
       "   202.80735778808594,\n",
       "   202.62929725646973,\n",
       "   203.35015869140625,\n",
       "   202.17597770690918,\n",
       "   200.5688877105713,\n",
       "   200.8315830230713,\n",
       "   202.3671646118164,\n",
       "   203.62952041625977,\n",
       "   202.70666885375977,\n",
       "   204.00299835205078,\n",
       "   205.42562103271484,\n",
       "   206.7518424987793,\n",
       "   206.65920639038086,\n",
       "   206.81700134277344,\n",
       "   205.80620574951172,\n",
       "   207.6551971435547,\n",
       "   207.99090957641602,\n",
       "   208.9430694580078,\n",
       "   209.59888076782227,\n",
       "   208.923828125,\n",
       "   208.71081924438477,\n",
       "   208.30717849731445,\n",
       "   209.18768310546875,\n",
       "   211.01558303833008,\n",
       "   210.75884246826172,\n",
       "   210.32523345947266,\n",
       "   210.93776321411133,\n",
       "   211.92102432250977,\n",
       "   211.1160125732422,\n",
       "   211.26926803588867,\n",
       "   210.1342887878418,\n",
       "   209.04605865478516,\n",
       "   209.16909408569336,\n",
       "   208.55334091186523,\n",
       "   208.91425323486328,\n",
       "   207.24249267578125,\n",
       "   207.25981903076172,\n",
       "   207.42314910888672,\n",
       "   206.42552947998047,\n",
       "   206.54351425170898,\n",
       "   205.9164161682129,\n",
       "   204.3615264892578,\n",
       "   203.05027770996094,\n",
       "   202.33452606201172,\n",
       "   201.176176071167,\n",
       "   201.2015895843506,\n",
       "   200.93024826049805,\n",
       "   201.86777877807617,\n",
       "   202.91969680786133,\n",
       "   203.1941909790039,\n",
       "   203.94445419311523,\n",
       "   205.28804397583008,\n",
       "   206.36988830566406,\n",
       "   206.29906845092773,\n",
       "   207.48817443847656,\n",
       "   208.95037078857422,\n",
       "   209.43928909301758,\n",
       "   209.99964904785156,\n",
       "   210.8048858642578,\n",
       "   212.0881805419922,\n",
       "   212.57464218139648,\n",
       "   212.34042358398438,\n",
       "   213.11480331420898,\n",
       "   211.7275390625,\n",
       "   210.5392951965332,\n",
       "   209.7701644897461,\n",
       "   208.10336112976074,\n",
       "   207.26972007751465,\n",
       "   206.2439842224121,\n",
       "   205.44530296325684,\n",
       "   205.2594814300537,\n",
       "   203.25615882873535,\n",
       "   203.22457695007324,\n",
       "   201.8968334197998,\n",
       "   200.1993408203125,\n",
       "   200.0604591369629,\n",
       "   199.75803184509277,\n",
       "   199.58486938476562,\n",
       "   199.90067672729492,\n",
       "   200.26977348327637,\n",
       "   200.53731155395508,\n",
       "   200.25112915039062,\n",
       "   199.5734405517578,\n",
       "   201.09636688232422,\n",
       "   200.08220291137695,\n",
       "   200.09654235839844,\n",
       "   200.90404891967773,\n",
       "   201.32942581176758,\n",
       "   201.50066375732422,\n",
       "   202.68230056762695,\n",
       "   202.71385955810547,\n",
       "   201.23709869384766,\n",
       "   201.13772583007812,\n",
       "   200.64681243896484,\n",
       "   201.02171325683594,\n",
       "   200.12939834594727,\n",
       "   200.53744888305664,\n",
       "   200.36351013183594,\n",
       "   199.49210357666016,\n",
       "   197.5144805908203,\n",
       "   197.18175506591797,\n",
       "   198.04975509643555,\n",
       "   198.20265769958496,\n",
       "   197.89248085021973,\n",
       "   197.34788703918457,\n",
       "   196.4689483642578,\n",
       "   196.23357963562012,\n",
       "   194.78378677368164,\n",
       "   192.93446731567383,\n",
       "   192.96406745910645,\n",
       "   193.02138328552246,\n",
       "   192.61487197875977,\n",
       "   192.91758346557617,\n",
       "   193.7449951171875,\n",
       "   192.7983341217041,\n",
       "   192.84348487854004,\n",
       "   194.72391891479492,\n",
       "   195.3653678894043,\n",
       "   196.0123805999756,\n",
       "   196.07615661621094,\n",
       "   196.4778003692627,\n",
       "   195.58817291259766,\n",
       "   197.42619705200195,\n",
       "   199.38196563720703,\n",
       "   200.4088478088379,\n",
       "   201.21942901611328,\n",
       "   201.05274963378906,\n",
       "   200.34171676635742,\n",
       "   198.36793327331543,\n",
       "   197.55754852294922,\n",
       "   196.6589870452881,\n",
       "   194.73687553405762,\n",
       "   194.18880081176758,\n",
       "   191.8903636932373,\n",
       "   189.92993545532227,\n",
       "   188.91769981384277,\n",
       "   187.71682739257812,\n",
       "   186.52570343017578,\n",
       "   186.622465133667,\n",
       "   186.54631805419922,\n",
       "   187.0462818145752,\n",
       "   187.962797164917,\n",
       "   188.9603729248047,\n",
       "   188.92210388183594,\n",
       "   189.86135482788086,\n",
       "   190.28879165649414,\n",
       "   191.0207633972168,\n",
       "   192.68305206298828,\n",
       "   193.90295791625977,\n",
       "   194.85833358764648,\n",
       "   193.99748992919922,\n",
       "   193.44462394714355,\n",
       "   191.9168357849121,\n",
       "   191.56774520874023,\n",
       "   191.49223136901855,\n",
       "   191.1397247314453,\n",
       "   191.33044624328613,\n",
       "   191.71683502197266,\n",
       "   191.23070335388184,\n",
       "   192.80930137634277,\n",
       "   193.82240676879883,\n",
       "   195.40008926391602,\n",
       "   198.08272171020508,\n",
       "   199.05074310302734,\n",
       "   200.0776596069336,\n",
       "   200.75386810302734,\n",
       "   201.0699119567871,\n",
       "   201.77851104736328,\n",
       "   201.9302520751953,\n",
       "   201.8262481689453,\n",
       "   200.7751121520996,\n",
       "   199.41598892211914,\n",
       "   198.96957778930664,\n",
       "   199.76461791992188,\n",
       "   200.02224349975586,\n",
       "   198.6462173461914,\n",
       "   197.58622932434082,\n",
       "   197.22697257995605,\n",
       "   195.9359645843506,\n",
       "   195.3710594177246,\n",
       "   195.7994155883789,\n",
       "   195.6248836517334,\n",
       "   195.2512607574463,\n",
       "   194.85511779785156,\n",
       "   195.5340518951416,\n",
       "   195.65403938293457,\n",
       "   196.46333503723145,\n",
       "   197.0999870300293,\n",
       "   197.9671173095703,\n",
       "   197.91869735717773,\n",
       "   197.05998611450195,\n",
       "   196.40904235839844,\n",
       "   195.49352264404297,\n",
       "   194.8816089630127,\n",
       "   194.9492301940918,\n",
       "   193.74680709838867,\n",
       "   193.99032020568848,\n",
       "   193.55661392211914,\n",
       "   193.3894557952881,\n",
       "   194.40338897705078,\n",
       "   194.70372009277344,\n",
       "   195.63704109191895,\n",
       "   196.6733283996582,\n",
       "   195.72359085083008,\n",
       "   195.1556797027588,\n",
       "   194.78614234924316,\n",
       "   194.53654861450195,\n",
       "   194.97003364562988,\n",
       "   195.18885040283203,\n",
       "   195.37063026428223,\n",
       "   196.3667106628418,\n",
       "   195.55524063110352,\n",
       "   195.37068557739258,\n",
       "   194.62732315063477,\n",
       "   194.61053085327148,\n",
       "   193.23056411743164,\n",
       "   191.71239852905273,\n",
       "   191.28150177001953,\n",
       "   191.17616081237793,\n",
       "   191.1663932800293,\n",
       "   191.04944038391113,\n",
       "   192.2550868988037,\n",
       "   191.14862632751465,\n",
       "   190.75820922851562,\n",
       "   190.7192211151123,\n",
       "   190.30097770690918,\n",
       "   189.24651908874512,\n",
       "   190.10881805419922,\n",
       "   190.6646957397461,\n",
       "   190.7235164642334,\n",
       "   190.22846603393555,\n",
       "   190.1731014251709,\n",
       "   190.7217788696289,\n",
       "   190.19807243347168,\n",
       "   190.16464614868164,\n",
       "   189.16782188415527,\n",
       "   189.55185317993164,\n",
       "   189.82032585144043,\n",
       "   189.20258712768555,\n",
       "   189.87549591064453,\n",
       "   188.38972854614258,\n",
       "   187.11544227600098,\n",
       "   186.97640228271484,\n",
       "   186.58208084106445,\n",
       "   186.10687637329102,\n",
       "   185.2220230102539,\n",
       "   185.2625732421875,\n",
       "   185.85897827148438,\n",
       "   186.3376007080078,\n",
       "   187.51093864440918,\n",
       "   187.82852935791016,\n",
       "   188.68988800048828,\n",
       "   188.3557071685791,\n",
       "   188.39662170410156,\n",
       "   187.9944133758545,\n",
       "   188.55777168273926,\n",
       "   188.9379539489746,\n",
       "   188.93761444091797,\n",
       "   189.0534782409668,\n",
       "   187.57238578796387,\n",
       "   188.3669834136963,\n",
       "   187.114501953125,\n",
       "   187.5966091156006,\n",
       "   186.89664459228516,\n",
       "   185.93225860595703,\n",
       "   185.57826042175293,\n",
       "   184.61911392211914,\n",
       "   184.87119102478027,\n",
       "   185.65896034240723,\n",
       "   186.10227584838867,\n",
       "   185.9982204437256,\n",
       "   185.54421043395996,\n",
       "   185.13268661499023,\n",
       "   184.86278915405273,\n",
       "   185.5908031463623,\n",
       "   186.40228271484375,\n",
       "   186.6074390411377,\n",
       "   186.8224334716797,\n",
       "   186.1333236694336,\n",
       "   185.41893005371094,\n",
       "   184.58983039855957,\n",
       "   184.02709579467773,\n",
       "   183.87540245056152,\n",
       "   184.46755409240723,\n",
       "   184.60834121704102,\n",
       "   184.4056739807129,\n",
       "   184.44460678100586,\n",
       "   184.91264152526855,\n",
       "   185.2807559967041,\n",
       "   185.15929412841797,\n",
       "   185.39625549316406,\n",
       "   186.240478515625,\n",
       "   185.71564865112305,\n",
       "   185.73846435546875,\n",
       "   185.85663032531738,\n",
       "   185.53532600402832,\n",
       "   184.34650230407715,\n",
       "   184.2518138885498,\n",
       "   183.98355865478516,\n",
       "   184.27496337890625,\n",
       "   184.82000350952148,\n",
       "   186.264009475708,\n",
       "   186.58811569213867,\n",
       "   185.37554931640625,\n",
       "   186.5924949645996,\n",
       "   186.06815338134766,\n",
       "   187.16112899780273,\n",
       "   187.6164951324463,\n",
       "   187.86345863342285,\n",
       "   186.75324249267578,\n",
       "   186.9173641204834,\n",
       "   186.01943016052246,\n",
       "   185.7206974029541,\n",
       "   184.49156188964844,\n",
       "   184.35147666931152,\n",
       "   183.85030364990234,\n",
       "   183.27685546875,\n",
       "   182.99137687683105,\n",
       "   182.38661193847656,\n",
       "   181.99246215820312,\n",
       "   182.69693565368652,\n",
       "   181.74639511108398,\n",
       "   181.9296646118164,\n",
       "   181.63870811462402,\n",
       "   182.04088401794434,\n",
       "   180.39590454101562,\n",
       "   179.36156845092773,\n",
       "   178.8031234741211,\n",
       "   177.2952651977539,\n",
       "   178.43544387817383,\n",
       "   179.09379768371582,\n",
       "   178.80559539794922,\n",
       "   178.6902847290039,\n",
       "   179.76656341552734,\n",
       "   181.88864707946777,\n",
       "   183.87206840515137,\n",
       "   185.49799156188965,\n",
       "   187.02728652954102,\n",
       "   187.88020515441895,\n",
       "   188.67244911193848,\n",
       "   189.31146240234375,\n",
       "   189.79258728027344,\n",
       "   189.81547164916992,\n",
       "   190.27998733520508,\n",
       "   189.41637802124023,\n",
       "   188.77910232543945,\n",
       "   188.4403305053711,\n",
       "   187.1618480682373,\n",
       "   185.04600143432617,\n",
       "   182.77834510803223,\n",
       "   181.62879943847656,\n",
       "   182.41748046875,\n",
       "   181.22476768493652,\n",
       "   180.52399444580078,\n",
       "   180.65486526489258,\n",
       "   182.1370792388916,\n",
       "   181.74781799316406,\n",
       "   182.2768211364746,\n",
       "   182.87756729125977,\n",
       "   184.04965782165527,\n",
       "   185.02032661437988,\n",
       "   185.75965118408203,\n",
       "   186.26408195495605,\n",
       "   185.82011795043945,\n",
       "   186.20272636413574,\n",
       "   186.10217475891113,\n",
       "   185.82895851135254,\n",
       "   185.54388046264648,\n",
       "   184.53854751586914,\n",
       "   183.0559902191162,\n",
       "   182.72434997558594,\n",
       "   182.60635375976562,\n",
       "   181.5321273803711,\n",
       "   180.31660652160645,\n",
       "   178.97411918640137,\n",
       "   178.7074146270752,\n",
       "   178.4967384338379,\n",
       "   177.926851272583,\n",
       "   178.4203281402588,\n",
       "   179.26379203796387,\n",
       "   179.54964637756348,\n",
       "   179.27893447875977,\n",
       "   179.2896556854248,\n",
       "   180.13797760009766,\n",
       "   180.00807762145996,\n",
       "   180.8791961669922,\n",
       "   181.72967147827148,\n",
       "   182.5257110595703,\n",
       "   182.4077377319336,\n",
       "   181.66297912597656,\n",
       "   180.86903190612793,\n",
       "   181.16867446899414,\n",
       "   179.9644374847412,\n",
       "   180.05010795593262,\n",
       "   178.424409866333],\n",
       "  'pce_r2': [-55.305549303067615,\n",
       "   -40.269272015570294,\n",
       "   -28.292090158479684,\n",
       "   -19.257826057155764,\n",
       "   -13.008200697129102,\n",
       "   -8.293333916975287,\n",
       "   -4.73469687505983,\n",
       "   -2.390549650015284,\n",
       "   -0.9997856538325871,\n",
       "   -0.2697756536889542,\n",
       "   -0.013251968193754404,\n",
       "   -0.026065245927974434,\n",
       "   -0.1984826797720709,\n",
       "   -0.5448361928790677,\n",
       "   -0.9051436258349479,\n",
       "   -1.2619586799778233,\n",
       "   -1.8901334611199863,\n",
       "   -2.574997352204781,\n",
       "   -3.04025172925936,\n",
       "   -3.4915087638063316,\n",
       "   -3.7651934823918376,\n",
       "   -4.046427691314796,\n",
       "   -4.122771458462589,\n",
       "   -4.180810030595213,\n",
       "   -4.10827233342555,\n",
       "   -4.005056485734514,\n",
       "   -4.074380608374861,\n",
       "   -3.9170337203495915,\n",
       "   -4.157971866881061,\n",
       "   -4.289502929328393,\n",
       "   -4.414747864562171,\n",
       "   -5.025012777723654,\n",
       "   -5.077666599844532,\n",
       "   -5.351730713520152,\n",
       "   -5.234196696163491,\n",
       "   -5.688154773450814,\n",
       "   -6.233420842307116,\n",
       "   -6.951140459775524,\n",
       "   -7.631591535488537,\n",
       "   -7.874390332006543,\n",
       "   -8.37924830613378,\n",
       "   -8.146292154670432,\n",
       "   -7.717959032936754,\n",
       "   -8.125934266087372,\n",
       "   -7.813822243826488,\n",
       "   -7.687073473128136,\n",
       "   -7.092565346768421,\n",
       "   -6.747744942223285,\n",
       "   -5.905845180448716,\n",
       "   -5.227984761360738,\n",
       "   -4.513812450312032,\n",
       "   -3.812239279110143,\n",
       "   -3.336623489846982,\n",
       "   -3.0042527141342514,\n",
       "   -2.6135587178914057,\n",
       "   -2.203952128296869,\n",
       "   -1.9097652418806916,\n",
       "   -1.6721494946228672,\n",
       "   -1.5416918594726203,\n",
       "   -1.3802256145422716,\n",
       "   -1.2664906525478994,\n",
       "   -1.207174555711679,\n",
       "   -1.21069011164324,\n",
       "   -1.2748483428537054,\n",
       "   -1.2408740115492254,\n",
       "   -1.3253343626222351,\n",
       "   -1.4541619959598107,\n",
       "   -1.6431856737456974,\n",
       "   -1.6389349514383817,\n",
       "   -1.741905391290262,\n",
       "   -1.7368197241659096,\n",
       "   -1.7958757022524061,\n",
       "   -1.6254357305905862,\n",
       "   -1.6635905885098667,\n",
       "   -1.6083874467711272,\n",
       "   -1.5160160578708002,\n",
       "   -1.294322257282054,\n",
       "   -1.2864262398630957,\n",
       "   -1.1702851906469829,\n",
       "   -1.1110442289464495,\n",
       "   -1.0580426854094092,\n",
       "   -0.8797143106390186,\n",
       "   -0.8257771448856692,\n",
       "   -0.7485591563532852,\n",
       "   -0.7778398428768889,\n",
       "   -0.7996381355474991,\n",
       "   -0.7777385812680739,\n",
       "   -0.7543675782469752,\n",
       "   -0.7109587137792484,\n",
       "   -0.7719467182517732,\n",
       "   -0.8264697448430034,\n",
       "   -0.8950099138247904,\n",
       "   -1.0307331371712043,\n",
       "   -0.9701156763369438,\n",
       "   -0.9971077040043557,\n",
       "   -0.953436482113267,\n",
       "   -0.9349327302188755,\n",
       "   -0.9677568448726162,\n",
       "   -1.052841952774831,\n",
       "   -1.0457017434425362,\n",
       "   -1.0550517812055369,\n",
       "   -1.0178224188960394,\n",
       "   -0.9516101695382617,\n",
       "   -0.8813055292792491,\n",
       "   -0.8020513988172311,\n",
       "   -0.7124346324294004,\n",
       "   -0.7268195033786771,\n",
       "   -0.7149567567103552,\n",
       "   -0.6473620203137169,\n",
       "   -0.5930445182896806,\n",
       "   -0.5202442518582238,\n",
       "   -0.5056117965450964,\n",
       "   -0.49794736263659134,\n",
       "   -0.4412693878851264,\n",
       "   -0.42153400843786804,\n",
       "   -0.464756410670931,\n",
       "   -0.478355199440045,\n",
       "   -0.49913175055203096,\n",
       "   -0.4567763483358196,\n",
       "   -0.43847280441352776,\n",
       "   -0.4862344992665153,\n",
       "   -0.4391658610098692,\n",
       "   -0.4931036517606415,\n",
       "   -0.5044205480335535,\n",
       "   -0.5836834683230683,\n",
       "   -0.5966353503397905,\n",
       "   -0.7588345797005045,\n",
       "   -0.8657328601082339,\n",
       "   -0.9441200834604446,\n",
       "   -0.9948471300253452,\n",
       "   -1.0203944182527236,\n",
       "   -1.0223764279957352,\n",
       "   -1.019552299309983,\n",
       "   -1.0902310973868317,\n",
       "   -1.140374617805262,\n",
       "   -1.0389520048686207,\n",
       "   -0.9859799690334428,\n",
       "   -1.0762340505790498,\n",
       "   -1.062243805167626,\n",
       "   -1.005240866340035,\n",
       "   -0.8599936994815314,\n",
       "   -0.8362829656666884,\n",
       "   -0.7765134685579265,\n",
       "   -0.7112629302431965,\n",
       "   -0.6738142648131791,\n",
       "   -0.6222470846670198,\n",
       "   -0.5910431833104852,\n",
       "   -0.565258574127943,\n",
       "   -0.4904623739951055,\n",
       "   -0.4431906681748323,\n",
       "   -0.45595913841092983,\n",
       "   -0.4761532510964941,\n",
       "   -0.47011837910108367,\n",
       "   -0.4989752660855331,\n",
       "   -0.4741410754786133,\n",
       "   -0.5320329273698423,\n",
       "   -0.5439440768390031,\n",
       "   -0.49860678881692055,\n",
       "   -0.46503611166806236,\n",
       "   -0.46066245436812525,\n",
       "   -0.4763769492451888,\n",
       "   -0.5038332243432666,\n",
       "   -0.5468859149671943,\n",
       "   -0.5338163532698736,\n",
       "   -0.6161041570654524,\n",
       "   -0.6331478259700798,\n",
       "   -0.5947818894157648,\n",
       "   -0.6415619582547198,\n",
       "   -0.5847970543500804,\n",
       "   -0.5815101953792736,\n",
       "   -0.5553137005531206,\n",
       "   -0.5125601871035963,\n",
       "   -0.5105684437113529,\n",
       "   -0.513314939617276,\n",
       "   -0.49275979064608055,\n",
       "   -0.462046421048673,\n",
       "   -0.4178203548727051,\n",
       "   -0.36244991665672077,\n",
       "   -0.3083471209038413,\n",
       "   -0.2626201781361954,\n",
       "   -0.25179795809462435,\n",
       "   -0.2573053437018953,\n",
       "   -0.2511269468304216,\n",
       "   -0.2534481529120127,\n",
       "   -0.24794754703423738,\n",
       "   -0.2587573013993998,\n",
       "   -0.2590357061954949,\n",
       "   -0.2621874406108802,\n",
       "   -0.2681004315978208,\n",
       "   -0.2631409084542866,\n",
       "   -0.26080243519875346,\n",
       "   -0.2370458414485812,\n",
       "   -0.22116754904349634,\n",
       "   -0.2283147495818858,\n",
       "   -0.2192053106581089,\n",
       "   -0.19104796914395283,\n",
       "   -0.18108556764041062,\n",
       "   -0.1664286609700838,\n",
       "   -0.1719124318983849,\n",
       "   -0.1941612103405148,\n",
       "   -0.20257239401790295,\n",
       "   -0.22134533100945752,\n",
       "   -0.24495464103406306,\n",
       "   -0.2518250973112959,\n",
       "   -0.2598758296825332,\n",
       "   -0.2663107309771069,\n",
       "   -0.2658463604311512,\n",
       "   -0.2977100326903981,\n",
       "   -0.3300389807708304,\n",
       "   -0.30587353180210797,\n",
       "   -0.27939610057338693,\n",
       "   -0.2928624509783584,\n",
       "   -0.2870053282624585,\n",
       "   -0.28656899828895144,\n",
       "   -0.27800040657524416,\n",
       "   -0.2500672833242399,\n",
       "   -0.23314308874703582,\n",
       "   -0.2108173979173653,\n",
       "   -0.1889727235620864,\n",
       "   -0.1637113950152802,\n",
       "   -0.14666561498769437,\n",
       "   -0.13938501523987679,\n",
       "   -0.1614415062159278,\n",
       "   -0.16618356386400834,\n",
       "   -0.17368828782804124,\n",
       "   -0.14819434987177815,\n",
       "   -0.16208262494047787,\n",
       "   -0.14429650378798642,\n",
       "   -0.13728502021814148,\n",
       "   -0.13918767654342368,\n",
       "   -0.11799606212251401,\n",
       "   -0.09362085359685968,\n",
       "   -0.08484264275314746,\n",
       "   -0.08144743530320309,\n",
       "   -0.077233994679611,\n",
       "   -0.0676362009461815,\n",
       "   -0.056496052842776256,\n",
       "   -0.05925070667009291,\n",
       "   -0.06923499942319444,\n",
       "   -0.07420410144825507,\n",
       "   -0.09428513967783503,\n",
       "   -0.09671128483516012,\n",
       "   -0.11115832348624322,\n",
       "   -0.12095324817218711,\n",
       "   -0.14403058973155103,\n",
       "   -0.15358525306740023,\n",
       "   -0.14361190806976665,\n",
       "   -0.14263493775302982,\n",
       "   -0.16865562462147032,\n",
       "   -0.16845353351187287,\n",
       "   -0.1696401969026906,\n",
       "   -0.17885259360783223,\n",
       "   -0.17979769412200053,\n",
       "   -0.16928200909119662,\n",
       "   -0.15884670971767134,\n",
       "   -0.14643510688047612,\n",
       "   -0.13904273539056677,\n",
       "   -0.1274597979208274,\n",
       "   -0.12435312028772816,\n",
       "   -0.13598734440657934,\n",
       "   -0.15327947878779713,\n",
       "   -0.13676785414213333,\n",
       "   -0.1316426109186879,\n",
       "   -0.11931370561271537,\n",
       "   -0.1161806171165467,\n",
       "   -0.08785975063924756,\n",
       "   -0.08033790391525986,\n",
       "   -0.08638980150675657,\n",
       "   -0.08690325029658852,\n",
       "   -0.09026832935511608,\n",
       "   -0.0839278221217068,\n",
       "   -0.0745316800816338,\n",
       "   -0.05988044198990705,\n",
       "   -0.05150095654112907,\n",
       "   -0.06775645112111484,\n",
       "   -0.0844518995723802,\n",
       "   -0.09050581993420326,\n",
       "   -0.10484216790045187,\n",
       "   -0.13494654411439644,\n",
       "   -0.14318013083889825,\n",
       "   -0.15192528792610394,\n",
       "   -0.18026331874782597,\n",
       "   -0.2114920143714747,\n",
       "   -0.22626558738766245,\n",
       "   -0.22756107829194105,\n",
       "   -0.1979093197866879,\n",
       "   -0.15336080268112395,\n",
       "   -0.14129710827472763,\n",
       "   -0.1166252707211779,\n",
       "   -0.10638822295719064,\n",
       "   -0.10077688520246864,\n",
       "   -0.08516343331203302,\n",
       "   -0.0746238706347715,\n",
       "   -0.07479955553973872,\n",
       "   -0.06569976811058176,\n",
       "   -0.05678151390233577,\n",
       "   -0.05919477527450501,\n",
       "   -0.0724038325896772,\n",
       "   -0.06467381470861322,\n",
       "   -0.07567919131013401,\n",
       "   -0.07656775823788697,\n",
       "   -0.08371302844873707,\n",
       "   -0.08421240580413691,\n",
       "   -0.08176684667771328,\n",
       "   -0.08878999362234885,\n",
       "   -0.09348361460437293,\n",
       "   -0.10948601878819986,\n",
       "   -0.12595335472563618,\n",
       "   -0.14284681018432321,\n",
       "   -0.15122175032557195,\n",
       "   -0.19022384824949734,\n",
       "   -0.20756452702725303,\n",
       "   -0.20678217170591462,\n",
       "   -0.170751766290272,\n",
       "   -0.15496009713865733,\n",
       "   -0.10035930773765989,\n",
       "   -0.08374486757117738,\n",
       "   -0.058049449020251,\n",
       "   -0.051880972051643104,\n",
       "   -0.044635092668980025,\n",
       "   -0.02910930809572454,\n",
       "   -0.021673563456997558,\n",
       "   -0.016582772354446496,\n",
       "   -0.011922090810564834,\n",
       "   -0.015797511934073194,\n",
       "   -0.026186555706028347,\n",
       "   -0.04149044289457082,\n",
       "   -0.07366187804828739,\n",
       "   -0.09095681921878729,\n",
       "   -0.1096109329284074,\n",
       "   -0.13692707438973928,\n",
       "   -0.14606693452637254,\n",
       "   -0.14679554946791984,\n",
       "   -0.16516794561148895,\n",
       "   -0.15418548730461223,\n",
       "   -0.12961413367617514,\n",
       "   -0.10787348930284635,\n",
       "   -0.10409826294689895,\n",
       "   -0.10142179245501914,\n",
       "   -0.0908779170873979,\n",
       "   -0.09975903363167027,\n",
       "   -0.10986755055782793,\n",
       "   -0.1189818872612507,\n",
       "   -0.11398127648634593,\n",
       "   -0.12145569419503777,\n",
       "   -0.09481158960550817,\n",
       "   -0.08605927525700108,\n",
       "   -0.07959043163643487,\n",
       "   -0.07151753445828346,\n",
       "   -0.0741098321979814,\n",
       "   -0.08007534694672769,\n",
       "   -0.09667511584532007,\n",
       "   -0.09673419309037046,\n",
       "   -0.09290882420950664,\n",
       "   -0.10123762481588483,\n",
       "   -0.11612051866013062,\n",
       "   -0.10364949697978032,\n",
       "   -0.08581892879545538,\n",
       "   -0.07698364691039239,\n",
       "   -0.0511160033911604,\n",
       "   -0.044539636956617557,\n",
       "   -0.04027565916795561,\n",
       "   -0.03188306780140726,\n",
       "   -0.01968321510678983,\n",
       "   -0.011805806296409038,\n",
       "   -0.01303074097832102,\n",
       "   -0.011899302279877455,\n",
       "   -0.018906111629462874,\n",
       "   -0.020088509666413623,\n",
       "   -0.024686375610788014,\n",
       "   -0.02906955438122738,\n",
       "   -0.04390320934188097,\n",
       "   -0.048150026547152835,\n",
       "   -0.06202395611721556,\n",
       "   -0.05881207423850143,\n",
       "   -0.06763956370376323,\n",
       "   -0.07952376111996617,\n",
       "   -0.10884999794025307,\n",
       "   -0.13220912784791916,\n",
       "   -0.1340665354666677,\n",
       "   -0.12613895095759564,\n",
       "   -0.13437439314487154,\n",
       "   -0.13044540890348522,\n",
       "   -0.11661564723196549,\n",
       "   -0.09855641605899201,\n",
       "   -0.07396592200863394,\n",
       "   -0.04727591935847464,\n",
       "   -0.042013984201707766,\n",
       "   -0.026546003234495075,\n",
       "   -0.018703543287533098,\n",
       "   -0.015203643107844167,\n",
       "   -0.012807526470146824,\n",
       "   -0.017814979889406724,\n",
       "   -0.01788166387298129,\n",
       "   -0.027738475064283286,\n",
       "   -0.04747500482296063,\n",
       "   -0.05459416714635701,\n",
       "   -0.0775894259704839,\n",
       "   -0.08341100153558068,\n",
       "   -0.0876002712718702,\n",
       "   -0.09105113870565584,\n",
       "   -0.07613083992848302,\n",
       "   -0.07490615593760408,\n",
       "   -0.0821211932942254,\n",
       "   -0.08366178544803637,\n",
       "   -0.07443434816288375,\n",
       "   -0.07990015462544986,\n",
       "   -0.07806764428851065,\n",
       "   -0.07100538832706049,\n",
       "   -0.0691282600428269,\n",
       "   -0.0631802465550857,\n",
       "   -0.06272022140300493,\n",
       "   -0.06572456165470975,\n",
       "   -0.08168674821218058,\n",
       "   -0.07385519938200358,\n",
       "   -0.07260177675145907,\n",
       "   -0.077303587692225,\n",
       "   -0.07882306978137943,\n",
       "   -0.07920822547265649,\n",
       "   -0.07357527743518766,\n",
       "   -0.06954223261680492,\n",
       "   -0.057986962142744014,\n",
       "   -0.05798577979651953,\n",
       "   -0.059630334894135606,\n",
       "   -0.06584369083317276,\n",
       "   -0.08165277576922603,\n",
       "   -0.10247133741486358,\n",
       "   -0.08130992420230942,\n",
       "   -0.08252997845993293,\n",
       "   -0.08965227699336609,\n",
       "   -0.0704481221529627,\n",
       "   -0.08354566517609041,\n",
       "   -0.08745375672742517,\n",
       "   -0.08622954908217961,\n",
       "   -0.0794707713626519,\n",
       "   -0.07056110719469122,\n",
       "   -0.04223300424660703,\n",
       "   -0.04358205186555719,\n",
       "   -0.03999013631831483,\n",
       "   -0.034977965755564666,\n",
       "   -0.028978265655764446,\n",
       "   -0.026305783672923022,\n",
       "   -0.018083391549022387,\n",
       "   -0.014950614282808417,\n",
       "   -0.028201286441747797,\n",
       "   -0.039017804803424205,\n",
       "   -0.04562513385391198,\n",
       "   -0.057942999121101346,\n",
       "   -0.0624689906179523,\n",
       "   -0.05831617123443378,\n",
       "   -0.05826739507054701,\n",
       "   -0.06745936862065549,\n",
       "   -0.05798273100130147,\n",
       "   -0.04662399535269701,\n",
       "   -0.04019472636704635,\n",
       "   -0.02890474890725936,\n",
       "   -0.026876610413182345,\n",
       "   -0.021735488259779334,\n",
       "   -0.023369874303421412,\n",
       "   -0.020406764682392664,\n",
       "   -0.021502205073376857,\n",
       "   -0.0401618724957038,\n",
       "   -0.044762409661899705,\n",
       "   -0.06314939375085404,\n",
       "   -0.08334798635526419,\n",
       "   -0.08039820670987141,\n",
       "   -0.07148243326811854,\n",
       "   -0.06563507582014738,\n",
       "   -0.06688194544793413,\n",
       "   -0.04981849123737048,\n",
       "   -0.04949887741268988,\n",
       "   -0.04206350289233174,\n",
       "   -0.05542691845157899,\n",
       "   -0.049776662357450485,\n",
       "   -0.04774671361870508,\n",
       "   -0.06008099599689176,\n",
       "   -0.04910178667855014,\n",
       "   -0.05306227224827076,\n",
       "   -0.04739729706486506,\n",
       "   -0.03131162202452131,\n",
       "   -0.02938537649283668,\n",
       "   -0.03171067592074661,\n",
       "   -0.03429421992775028,\n",
       "   -0.04575849205058358,\n",
       "   -0.05423383260857495,\n",
       "   -0.057459126503390356,\n",
       "   -0.03897385974761147,\n",
       "   -0.034605170616121894,\n",
       "   -0.011193375969940966,\n",
       "   -0.00278137095469555,\n",
       "   -0.003595915145239381,\n",
       "   -0.004467927639542291,\n",
       "   -0.01187617706988009,\n",
       "   -0.02296368687882322,\n",
       "   -0.03596341308270734,\n",
       "   -0.045505403845737646,\n",
       "   -0.059659296590567346,\n",
       "   -0.06704141839033184,\n",
       "   -0.07054601382647019,\n",
       "   -0.07385533960433022],\n",
       "  'voc_r2': [-12.203498704145082,\n",
       "   -5.727005622735291,\n",
       "   -2.6588004090461395,\n",
       "   -0.5931857511709862,\n",
       "   -0.02298749501929387,\n",
       "   -0.10848663384062851,\n",
       "   -0.5750388440031406,\n",
       "   -1.2184264542072918,\n",
       "   -2.4787342983922516,\n",
       "   -4.7437379311553505,\n",
       "   -6.666872796641439,\n",
       "   -7.953795631730284,\n",
       "   -11.278098681771787,\n",
       "   -13.637173633412846,\n",
       "   -15.756370468939341,\n",
       "   -18.348528816007224,\n",
       "   -21.39189399551841,\n",
       "   -23.425603731841377,\n",
       "   -22.529610362094402,\n",
       "   -17.282013094772477,\n",
       "   -11.054659633830626,\n",
       "   -7.830200153187896,\n",
       "   -5.331290258436973,\n",
       "   -3.978508392216761,\n",
       "   -2.913247647554151,\n",
       "   -2.013218856764311,\n",
       "   -1.831866982292615,\n",
       "   -1.6629545874645624,\n",
       "   -1.4565794563359398,\n",
       "   -1.5054787917033057,\n",
       "   -1.5089886005920041,\n",
       "   -1.6841194249955658,\n",
       "   -1.7275553727959978,\n",
       "   -1.6793202197915869,\n",
       "   -1.4998288125785733,\n",
       "   -1.248416973769106,\n",
       "   -1.1011871372388056,\n",
       "   -1.2953923228476079,\n",
       "   -1.315088723624101,\n",
       "   -1.2771325346457356,\n",
       "   -1.1735368227077503,\n",
       "   -1.3891152063802497,\n",
       "   -1.5435757444921543,\n",
       "   -1.9799011635809416,\n",
       "   -2.184149899385039,\n",
       "   -2.37681705145695,\n",
       "   -2.1652876719915253,\n",
       "   -2.2669543288344296,\n",
       "   -2.4944714585358603,\n",
       "   -2.831869388252947,\n",
       "   -3.288759034740738,\n",
       "   -3.773214311569613,\n",
       "   -4.0425517053905144,\n",
       "   -4.81272152209344,\n",
       "   -4.957619269373655,\n",
       "   -5.961364249481969,\n",
       "   -6.593280725831143,\n",
       "   -7.189330815804338,\n",
       "   -6.994523110390213,\n",
       "   -6.982662598361751,\n",
       "   -7.189240409104356,\n",
       "   -6.835878926045418,\n",
       "   -6.915590369499459,\n",
       "   -7.1381467701678325,\n",
       "   -6.7450418009344055,\n",
       "   -6.534886033711596,\n",
       "   -6.331239626222951,\n",
       "   -6.106258391496181,\n",
       "   -6.0186491801295325,\n",
       "   -6.231693478183347,\n",
       "   -6.624137393956547,\n",
       "   -6.6654992315474395,\n",
       "   -7.385410675703808,\n",
       "   -7.509856151141941,\n",
       "   -8.094438905941857,\n",
       "   -8.570615376605502,\n",
       "   -8.716851870257221,\n",
       "   -8.452970119323021,\n",
       "   -8.847797649023136,\n",
       "   -9.326816234496706,\n",
       "   -10.346451384584494,\n",
       "   -11.34343761664388,\n",
       "   -11.816021399827177,\n",
       "   -13.117010959042277,\n",
       "   -13.138267007519744,\n",
       "   -13.939666097724631,\n",
       "   -13.6498322040887,\n",
       "   -14.68485469852437,\n",
       "   -14.542801365663015,\n",
       "   -14.479338012025357,\n",
       "   -13.770896717861362,\n",
       "   -13.612936206976656,\n",
       "   -12.835160605814302,\n",
       "   -11.442227294944935,\n",
       "   -10.680416536739271,\n",
       "   -9.388490259450203,\n",
       "   -8.860839189745427,\n",
       "   -8.082146954187019,\n",
       "   -7.090018200000719,\n",
       "   -6.054632746926597,\n",
       "   -5.443819604737147,\n",
       "   -5.671669408633658,\n",
       "   -5.418047814311138,\n",
       "   -5.681795291968917,\n",
       "   -5.9399752775522705,\n",
       "   -6.25434700005515,\n",
       "   -6.049417592542023,\n",
       "   -6.1061373955756775,\n",
       "   -6.046233638120052,\n",
       "   -6.1857614970421135,\n",
       "   -6.855215665516501,\n",
       "   -7.899237273601786,\n",
       "   -8.97829294730376,\n",
       "   -9.47338741374002,\n",
       "   -10.608574690383408,\n",
       "   -11.368679202568867,\n",
       "   -11.893262882973143,\n",
       "   -11.960441041508872,\n",
       "   -12.408919049597724,\n",
       "   -13.04557323978805,\n",
       "   -13.79610611671343,\n",
       "   -14.770736378340946,\n",
       "   -15.340645086364976,\n",
       "   -15.836361026200251,\n",
       "   -16.62383303113768,\n",
       "   -16.913145046751655,\n",
       "   -17.65940502056588,\n",
       "   -17.72817068877383,\n",
       "   -18.71809297598056,\n",
       "   -18.13892599487928,\n",
       "   -17.287721795619586,\n",
       "   -17.747551209633524,\n",
       "   -18.52739514419251,\n",
       "   -17.759738159463332,\n",
       "   -17.291867611167326,\n",
       "   -16.445246503226848,\n",
       "   -13.971511182621022,\n",
       "   -13.591477676268983,\n",
       "   -13.002664974873262,\n",
       "   -11.795363897529429,\n",
       "   -11.206190411195388,\n",
       "   -11.121051759581695,\n",
       "   -10.65467553248053,\n",
       "   -10.0483724782029,\n",
       "   -10.030051674697967,\n",
       "   -9.288378073080393,\n",
       "   -9.052017059692192,\n",
       "   -8.542989218414379,\n",
       "   -8.283256050725521,\n",
       "   -7.833328571630908,\n",
       "   -7.646413204352715,\n",
       "   -7.322590679856736,\n",
       "   -7.999421143826842,\n",
       "   -8.854130743743811,\n",
       "   -10.139244759627738,\n",
       "   -11.785031964115554,\n",
       "   -12.537227807283704,\n",
       "   -14.060441479305391,\n",
       "   -15.617270478664786,\n",
       "   -15.698754664373851,\n",
       "   -17.430949024181484,\n",
       "   -18.189576422318694,\n",
       "   -19.938606129342883,\n",
       "   -21.220033659923565,\n",
       "   -22.906833416440637,\n",
       "   -23.013662234163476,\n",
       "   -24.199446586704997,\n",
       "   -26.437257305450206,\n",
       "   -27.21526894148886,\n",
       "   -28.631105099260665,\n",
       "   -28.823486714908757,\n",
       "   -27.616270525545364,\n",
       "   -26.50490433765678,\n",
       "   -26.73316991260964,\n",
       "   -26.48725106514821,\n",
       "   -24.725357778184687,\n",
       "   -21.426885701917907,\n",
       "   -20.067421452377747,\n",
       "   -18.3463452867916,\n",
       "   -15.815795702189604,\n",
       "   -16.003446823763646,\n",
       "   -16.335505246913193,\n",
       "   -16.66335912925581,\n",
       "   -15.568112741457178,\n",
       "   -15.31405700319435,\n",
       "   -14.719635757187323,\n",
       "   -14.352652197253548,\n",
       "   -13.137584175558514,\n",
       "   -14.312508940273212,\n",
       "   -13.437417814352822,\n",
       "   -13.539187096781136,\n",
       "   -13.546134942308344,\n",
       "   -13.396080969919113,\n",
       "   -13.010965360832989,\n",
       "   -13.547063103147241,\n",
       "   -14.26096155917046,\n",
       "   -13.38488389565434,\n",
       "   -12.865099260491679,\n",
       "   -12.348802656105947,\n",
       "   -13.315972163045805,\n",
       "   -12.658770399762613,\n",
       "   -13.120778210317242,\n",
       "   -12.81344706601957,\n",
       "   -12.534487488739664,\n",
       "   -11.196944947474135,\n",
       "   -11.928348335692865,\n",
       "   -11.576181980644709,\n",
       "   -11.732010507287141,\n",
       "   -11.765069273251799,\n",
       "   -11.265367879903668,\n",
       "   -11.093739722802576,\n",
       "   -10.684638350191783,\n",
       "   -10.715930808744158,\n",
       "   -9.793782424636321,\n",
       "   -9.583253926841754,\n",
       "   -10.302434334015205,\n",
       "   -11.267153337204228,\n",
       "   -11.035928856846942,\n",
       "   -10.73567501489648,\n",
       "   -10.38290104232565,\n",
       "   -10.478545668483495,\n",
       "   -10.809096308498436,\n",
       "   -10.922493505805352,\n",
       "   -11.29118523979336,\n",
       "   -11.499353288235744,\n",
       "   -12.16873353826007,\n",
       "   -11.166988849896903,\n",
       "   -12.173848245645543,\n",
       "   -13.60907001750779,\n",
       "   -13.967854641603628,\n",
       "   -14.369601095932277,\n",
       "   -14.342232545672719,\n",
       "   -12.94999870716761,\n",
       "   -12.667702986976282,\n",
       "   -11.967637199852913,\n",
       "   -11.755135231754053,\n",
       "   -11.093367318138391,\n",
       "   -10.383497633813581,\n",
       "   -9.624070794913019,\n",
       "   -8.941720430411527,\n",
       "   -8.347519136949696,\n",
       "   -7.0930384352942255,\n",
       "   -5.973541857629943,\n",
       "   -6.3002873392648695,\n",
       "   -6.2050973187099165,\n",
       "   -6.028880134958997,\n",
       "   -5.722271437523898,\n",
       "   -5.693927198509092,\n",
       "   -5.6300420437948215,\n",
       "   -5.949216870194534,\n",
       "   -6.387368111214643,\n",
       "   -7.261503435768256,\n",
       "   -8.055433759107537,\n",
       "   -9.14670071405662,\n",
       "   -10.052160217472577,\n",
       "   -10.826470030077944,\n",
       "   -11.239375778966796,\n",
       "   -10.901327099268048,\n",
       "   -11.395056313207107,\n",
       "   -11.377323197558445,\n",
       "   -11.171538660629857,\n",
       "   -11.801354101013194,\n",
       "   -12.639386167673129,\n",
       "   -13.595059848682334,\n",
       "   -14.146436017312872,\n",
       "   -14.68308658997461,\n",
       "   -16.14600274236873,\n",
       "   -18.31346313001701,\n",
       "   -19.89815089734104,\n",
       "   -22.27073963395031,\n",
       "   -23.53117979911997,\n",
       "   -23.56719953550829,\n",
       "   -23.613083324853225,\n",
       "   -24.2795659235455,\n",
       "   -25.209037385861308,\n",
       "   -24.625199243888346,\n",
       "   -22.31996740613753,\n",
       "   -21.48174391405825,\n",
       "   -22.14739172225794,\n",
       "   -20.72759879797862,\n",
       "   -20.027178413457303,\n",
       "   -18.80944223773343,\n",
       "   -18.187286974114702,\n",
       "   -16.0491520559046,\n",
       "   -14.926869390807385,\n",
       "   -15.455751918885628,\n",
       "   -15.351185192738601,\n",
       "   -14.823237772577379,\n",
       "   -14.518377383098215,\n",
       "   -15.592035213602205,\n",
       "   -16.957137983250732,\n",
       "   -18.05757222237062,\n",
       "   -21.371549354484458,\n",
       "   -22.784529518579372,\n",
       "   -23.92477224666044,\n",
       "   -23.857290383611645,\n",
       "   -23.340094128740834,\n",
       "   -22.282211773553023,\n",
       "   -22.08082189357235,\n",
       "   -22.167683888674652,\n",
       "   -22.543232921784945,\n",
       "   -22.21278230830368,\n",
       "   -20.244342012797492,\n",
       "   -20.297421355228703,\n",
       "   -21.599673410569572,\n",
       "   -22.509925007615283,\n",
       "   -24.153557587520872,\n",
       "   -25.080631642670465,\n",
       "   -23.685549190833715,\n",
       "   -21.473723116277284,\n",
       "   -21.503496408577682,\n",
       "   -19.558277962638,\n",
       "   -19.343371980863484,\n",
       "   -19.85814953002842,\n",
       "   -20.3351272504598,\n",
       "   -21.357169174745227,\n",
       "   -20.276389312143372,\n",
       "   -19.520015935109964,\n",
       "   -21.076458584958342,\n",
       "   -21.219227392908213,\n",
       "   -21.18375073169101,\n",
       "   -19.73881894187502,\n",
       "   -18.509226718122438,\n",
       "   -18.74076853165725,\n",
       "   -18.4167633154805,\n",
       "   -18.525122800532305,\n",
       "   -21.059678731883523,\n",
       "   -18.77421631114674,\n",
       "   -16.80525626062259,\n",
       "   -15.698995053554935,\n",
       "   -14.438164059071097,\n",
       "   -12.979633787422433,\n",
       "   -13.290456053009537,\n",
       "   -13.656645421877991,\n",
       "   -13.805584229355821,\n",
       "   -14.248253521078919,\n",
       "   -15.41424138671604,\n",
       "   -15.345953521278012,\n",
       "   -14.399900317816668,\n",
       "   -13.595622750377043,\n",
       "   -12.324215406604475,\n",
       "   -11.911035471787486,\n",
       "   -11.281314670829703,\n",
       "   -10.336486159668455,\n",
       "   -10.913388774500092,\n",
       "   -9.994797249553395,\n",
       "   -9.43384886770799,\n",
       "   -9.808549679878334,\n",
       "   -9.763286291841462,\n",
       "   -8.95494191212289,\n",
       "   -8.540243566733698,\n",
       "   -8.303253106226315,\n",
       "   -8.640280696137259,\n",
       "   -9.094636080970908,\n",
       "   -9.85286556558976,\n",
       "   -11.03388268949898,\n",
       "   -11.340718362425541,\n",
       "   -11.405975467982522,\n",
       "   -12.256537055587952,\n",
       "   -13.924702938192372,\n",
       "   -14.99656926974203,\n",
       "   -14.83433032394779,\n",
       "   -15.24275101217749,\n",
       "   -16.445353817694073,\n",
       "   -17.728874474527014,\n",
       "   -19.363736005670294,\n",
       "   -18.707902808263157,\n",
       "   -20.723599373658182,\n",
       "   -20.467087132560984,\n",
       "   -18.326395563415478,\n",
       "   -17.239853230281923,\n",
       "   -15.800856355873616,\n",
       "   -17.064850788512356,\n",
       "   -20.22759684805623,\n",
       "   -22.279386593270022,\n",
       "   -20.04934816756138,\n",
       "   -19.876423128420694,\n",
       "   -18.34882577187177,\n",
       "   -18.12703417273237,\n",
       "   -18.411129338988648,\n",
       "   -15.594542479024419,\n",
       "   -14.326619697412632,\n",
       "   -14.645150431198731,\n",
       "   -13.363084133435136,\n",
       "   -11.485829878133744,\n",
       "   -9.756621844540609,\n",
       "   -9.093951674895077,\n",
       "   -10.260511315508797,\n",
       "   -11.280630476303019,\n",
       "   -11.557568735774229,\n",
       "   -12.008504473623503,\n",
       "   -12.464648451988719,\n",
       "   -13.394603827430329,\n",
       "   -13.987730453071777,\n",
       "   -14.834080600949122,\n",
       "   -14.732026890261837,\n",
       "   -16.949136886218756,\n",
       "   -16.54410042644902,\n",
       "   -15.839238895769576,\n",
       "   -16.269588201649988,\n",
       "   -14.181331000178387,\n",
       "   -12.10719985527982,\n",
       "   -12.17601707155014,\n",
       "   -13.774792078130508,\n",
       "   -14.286835092811689,\n",
       "   -15.156472227779048,\n",
       "   -17.167021671603322,\n",
       "   -20.811322452993725,\n",
       "   -22.214933097842316,\n",
       "   -24.245006277661997,\n",
       "   -23.718769934785133,\n",
       "   -25.523009478282493,\n",
       "   -26.15147060201049,\n",
       "   -29.692355997941164,\n",
       "   -31.205106788147845,\n",
       "   -31.04009996454792,\n",
       "   -29.30372235501345,\n",
       "   -28.203453170810768,\n",
       "   -27.04940961773721,\n",
       "   -27.009640821844066,\n",
       "   -27.926397270516134,\n",
       "   -27.759817043279035,\n",
       "   -26.016111822946748,\n",
       "   -24.701342177914274,\n",
       "   -21.89100473206495,\n",
       "   -20.90220613757311,\n",
       "   -19.316663841444292,\n",
       "   -18.564373273978546,\n",
       "   -17.23572251098913,\n",
       "   -15.833139720956826,\n",
       "   -13.161877766006898,\n",
       "   -12.537667547584126,\n",
       "   -11.073796551868394,\n",
       "   -9.925283634010855,\n",
       "   -9.905144001614778,\n",
       "   -10.618958003745202,\n",
       "   -11.195465694834281,\n",
       "   -10.753950288791613,\n",
       "   -10.651361581847516,\n",
       "   -12.355190600005411,\n",
       "   -13.483364565198643,\n",
       "   -14.953989294354614,\n",
       "   -16.30145859463159,\n",
       "   -17.11203997864473,\n",
       "   -18.369421496555468,\n",
       "   -18.579509574264026,\n",
       "   -19.058353467644608,\n",
       "   -21.25079261257282,\n",
       "   -20.93448163065394,\n",
       "   -20.699449379911055,\n",
       "   -21.07815870897705,\n",
       "   -22.189931835777372,\n",
       "   -22.25889491488964,\n",
       "   -22.22942527755115,\n",
       "   -21.57530065853463,\n",
       "   -21.07596808760324,\n",
       "   -21.99580679276317,\n",
       "   -22.190748092709928,\n",
       "   -22.409768877368354,\n",
       "   -22.904769899719188,\n",
       "   -24.679410415703206,\n",
       "   -26.485493982324257,\n",
       "   -26.263246633365593,\n",
       "   -27.29331061846947,\n",
       "   -28.581237572379674,\n",
       "   -28.907819818250776,\n",
       "   -28.650484688005328,\n",
       "   -29.686021944447468,\n",
       "   -29.51787200899387,\n",
       "   -28.42665234698783,\n",
       "   -28.42587080054271,\n",
       "   -27.65823845567679,\n",
       "   -25.014570237782454,\n",
       "   -24.647882635546267,\n",
       "   -22.089684715351915,\n",
       "   -21.37765422664795,\n",
       "   -20.88330190042354,\n",
       "   -19.829385183379102,\n",
       "   -19.38736935684099,\n",
       "   -17.199554421630317,\n",
       "   -17.036625864962158,\n",
       "   -18.465283102827012,\n",
       "   -19.54913675910573,\n",
       "   -21.82749233349741,\n",
       "   -23.302730329683833,\n",
       "   -27.162569187097937,\n",
       "   -28.960792200147097,\n",
       "   -29.941291280530848,\n",
       "   -33.621008058726595,\n",
       "   -35.65435210161865,\n",
       "   -36.945848326277336,\n",
       "   -37.19703390782734,\n",
       "   -36.706810155508464,\n",
       "   -36.24157716278776,\n",
       "   -36.9474679222186,\n",
       "   -35.130992526872184,\n",
       "   -32.02112080986132,\n",
       "   -29.681270573435558,\n",
       "   -28.784454176534886,\n",
       "   -27.46551363329313],\n",
       "  'jsc_r2': [-1.320376639935065,\n",
       "   -0.8896686228214192,\n",
       "   -0.9821751819380773,\n",
       "   -0.9526004612702066,\n",
       "   -0.8800532477266341,\n",
       "   -0.8664241483553892,\n",
       "   -0.8288124451305534,\n",
       "   -0.8250040949774871,\n",
       "   -0.8349494960487451,\n",
       "   -0.8201832751928528,\n",
       "   -0.958323537234647,\n",
       "   -1.291782489056498,\n",
       "   -1.8474916411817501,\n",
       "   -2.4641960198042603,\n",
       "   -3.112220675565954,\n",
       "   -3.5277490631811474,\n",
       "   -3.781960450647201,\n",
       "   -4.05114604156737,\n",
       "   -4.324175245945457,\n",
       "   -4.6997966184319235,\n",
       "   -4.991357430272805,\n",
       "   -5.194206940744338,\n",
       "   -5.30007472005757,\n",
       "   -5.460253392420183,\n",
       "   -5.611749116560077,\n",
       "   -5.778952196937669,\n",
       "   -5.929236834814883,\n",
       "   -6.067172884430395,\n",
       "   -6.108347855470715,\n",
       "   -6.09314635359983,\n",
       "   -6.104868751975841,\n",
       "   -6.147975707002258,\n",
       "   -6.009922982796602,\n",
       "   -5.9094064420500665,\n",
       "   -5.866803045672365,\n",
       "   -5.83290822679869,\n",
       "   -5.847587895353147,\n",
       "   -6.060983850786448,\n",
       "   -6.096439316392363,\n",
       "   -6.144140400203782,\n",
       "   -6.11611390397152,\n",
       "   -6.065517116723891,\n",
       "   -6.079477425914367,\n",
       "   -6.033997579345461,\n",
       "   -6.1038393020491295,\n",
       "   -6.10691102804459,\n",
       "   -6.011123349520505,\n",
       "   -5.940276928640885,\n",
       "   -5.97437879924922,\n",
       "   -5.999526223052879,\n",
       "   -6.1003790211482265,\n",
       "   -6.125682363325707,\n",
       "   -6.166822835337127,\n",
       "   -6.2034173016976455,\n",
       "   -6.150678627353167,\n",
       "   -6.345887220341811,\n",
       "   -6.33876571742837,\n",
       "   -6.396890036005729,\n",
       "   -6.310699705731735,\n",
       "   -6.353549861606572,\n",
       "   -6.485673578649233,\n",
       "   -6.561406046896621,\n",
       "   -6.504583408013837,\n",
       "   -6.419328639759653,\n",
       "   -6.2967919263005765,\n",
       "   -6.187736690826248,\n",
       "   -6.20470682805636,\n",
       "   -6.2452212880167375,\n",
       "   -6.113501898798867,\n",
       "   -6.48122587089268,\n",
       "   -6.663199751705565,\n",
       "   -6.916103553313152,\n",
       "   -7.183770017146115,\n",
       "   -7.50219786028946,\n",
       "   -7.587692437557511,\n",
       "   -7.82057806022539,\n",
       "   -8.042153033916115,\n",
       "   -8.169324687183465,\n",
       "   -8.321941500629787,\n",
       "   -8.677937418711476,\n",
       "   -9.235141588733534,\n",
       "   -9.339972116207262,\n",
       "   -9.421256034197711,\n",
       "   -8.77357057247151,\n",
       "   -8.684256635805859,\n",
       "   -8.423897505699,\n",
       "   -8.00215879524241,\n",
       "   -7.656171365668843,\n",
       "   -8.065187928060574,\n",
       "   -8.121012209221224,\n",
       "   -8.224138433370632,\n",
       "   -8.213021738290019,\n",
       "   -8.427245305858646,\n",
       "   -8.801925855619443,\n",
       "   -8.960593930970044,\n",
       "   -8.842985620873133,\n",
       "   -9.134336591107331,\n",
       "   -9.434021187951378,\n",
       "   -9.30381763032516,\n",
       "   -9.343564898825955,\n",
       "   -9.628666628826746,\n",
       "   -10.171874423348285,\n",
       "   -10.978476768518073,\n",
       "   -11.53045924277441,\n",
       "   -12.363238690513281,\n",
       "   -12.540815750412017,\n",
       "   -12.615422985339597,\n",
       "   -12.971066342247711,\n",
       "   -12.942370416108949,\n",
       "   -12.257443046760262,\n",
       "   -12.69656121806604,\n",
       "   -12.457651212361457,\n",
       "   -12.374253861266498,\n",
       "   -11.541704187766747,\n",
       "   -11.800347731773202,\n",
       "   -12.153118904791354,\n",
       "   -11.904764169459312,\n",
       "   -11.782476345769526,\n",
       "   -11.228613027691182,\n",
       "   -10.470972116990326,\n",
       "   -10.36115832066023,\n",
       "   -10.442710399763964,\n",
       "   -10.205165281803719,\n",
       "   -10.246094145915233,\n",
       "   -9.857911287238387,\n",
       "   -9.891423561965839,\n",
       "   -9.702010865761181,\n",
       "   -9.946639426104035,\n",
       "   -10.477114070031625,\n",
       "   -10.191611274156008,\n",
       "   -10.167222654095928,\n",
       "   -10.599604015550172,\n",
       "   -10.68213162566566,\n",
       "   -10.541327508284795,\n",
       "   -11.097951107852785,\n",
       "   -11.1450848873347,\n",
       "   -11.896723521383956,\n",
       "   -12.35736028834658,\n",
       "   -12.215984832955943,\n",
       "   -12.4726059805543,\n",
       "   -12.666253719663107,\n",
       "   -13.517481325879544,\n",
       "   -13.56723941687055,\n",
       "   -13.724037158894172,\n",
       "   -12.8713748830499,\n",
       "   -13.051290799214389,\n",
       "   -12.665878815039978,\n",
       "   -12.494427044995167,\n",
       "   -12.486237122039991,\n",
       "   -12.340609938902931,\n",
       "   -12.79720901177807,\n",
       "   -12.703748621051078,\n",
       "   -12.464357267564663,\n",
       "   -12.586846192401154,\n",
       "   -12.267422503542434,\n",
       "   -11.95352258433465,\n",
       "   -11.977686819107838,\n",
       "   -11.943786286833397,\n",
       "   -11.990293264916865,\n",
       "   -12.086626606505215,\n",
       "   -12.315887797214817,\n",
       "   -12.68923067372041,\n",
       "   -12.625442745547472,\n",
       "   -12.788098848850348,\n",
       "   -12.531297077098545,\n",
       "   -12.441647222373987,\n",
       "   -12.18955353622824,\n",
       "   -12.325498777636508,\n",
       "   -12.599325237691108,\n",
       "   -11.810907861032314,\n",
       "   -11.666143833557818,\n",
       "   -11.600064986657456,\n",
       "   -11.697900242271908,\n",
       "   -11.218331097750136,\n",
       "   -11.478474119483531,\n",
       "   -11.576346395870182,\n",
       "   -11.775213678279972,\n",
       "   -12.295295555131307,\n",
       "   -12.378040057840952,\n",
       "   -12.905796482431816,\n",
       "   -13.088013769601908,\n",
       "   -13.275124109556023,\n",
       "   -13.532711278257336,\n",
       "   -13.76561702315526,\n",
       "   -13.174200786149983,\n",
       "   -13.525129785663815,\n",
       "   -13.11586675310747,\n",
       "   -13.469172851973596,\n",
       "   -13.604551239499804,\n",
       "   -13.757173426335141,\n",
       "   -13.198831335117484,\n",
       "   -13.632917905880118,\n",
       "   -14.447500298791086,\n",
       "   -14.314275298544194,\n",
       "   -14.745418916447798,\n",
       "   -14.364519069136321,\n",
       "   -14.236829441207272,\n",
       "   -14.220025217670782,\n",
       "   -13.584715794731935,\n",
       "   -13.122923382055218,\n",
       "   -12.72358568222941,\n",
       "   -12.4713228666222,\n",
       "   -12.275448690392079,\n",
       "   -11.992171221362412,\n",
       "   -11.97381905403814,\n",
       "   -11.829819251444738,\n",
       "   -12.16336447102319,\n",
       "   -12.14202569307825,\n",
       "   -12.825012868339476,\n",
       "   -13.545171963746707,\n",
       "   -13.169616669584785,\n",
       "   -13.882548463241235,\n",
       "   -12.972761209450617,\n",
       "   -12.916909096554882,\n",
       "   -12.999588666114361,\n",
       "   -13.371751571398118,\n",
       "   -13.06875898944132,\n",
       "   -14.333103503773362,\n",
       "   -16.169456219193307,\n",
       "   -16.437433081885523,\n",
       "   -17.380466591587226,\n",
       "   -19.408221832461816,\n",
       "   -20.791779499944354,\n",
       "   -21.329406561562347,\n",
       "   -21.943532391010883,\n",
       "   -22.08522554683002,\n",
       "   -23.186552661492073,\n",
       "   -24.89757665751649,\n",
       "   -25.517083795090677,\n",
       "   -26.235932054002852,\n",
       "   -25.965478354953365,\n",
       "   -25.636162038799135,\n",
       "   -24.778733940086806,\n",
       "   -25.08799316224981,\n",
       "   -24.911656381816687,\n",
       "   -24.992974164352482,\n",
       "   -22.920006700957565,\n",
       "   -23.32171387002367,\n",
       "   -22.64228581581329,\n",
       "   -21.89961595636349,\n",
       "   -21.247240413169543,\n",
       "   -21.04507654903381,\n",
       "   -21.37890094226483,\n",
       "   -20.765913439159206,\n",
       "   -20.271332744820807,\n",
       "   -21.352372946057947,\n",
       "   -22.619992319079255,\n",
       "   -24.72508951120126,\n",
       "   -24.63009679759795,\n",
       "   -23.82756261313379,\n",
       "   -24.552785412691343,\n",
       "   -24.452408029713165,\n",
       "   -25.54745328770566,\n",
       "   -25.714860171925075,\n",
       "   -26.360255959106077,\n",
       "   -25.220020583976577,\n",
       "   -24.64605568737952,\n",
       "   -23.573915704273226,\n",
       "   -23.155631303665132,\n",
       "   -20.94659336209302,\n",
       "   -19.252044051491534,\n",
       "   -17.51099999239201,\n",
       "   -17.02149534394824,\n",
       "   -16.618322142372865,\n",
       "   -17.95332507549368,\n",
       "   -19.812005999007816,\n",
       "   -21.28372805964754,\n",
       "   -24.122797067278487,\n",
       "   -24.86701268428814,\n",
       "   -27.095954804835095,\n",
       "   -27.312552151664956,\n",
       "   -28.224824186953303,\n",
       "   -28.948850092922036,\n",
       "   -29.975692138830734,\n",
       "   -29.293918941628426,\n",
       "   -30.087688736159198,\n",
       "   -30.17622800574048,\n",
       "   -28.798727257456136,\n",
       "   -29.894842087613206,\n",
       "   -29.524890787729756,\n",
       "   -29.735090250443697,\n",
       "   -31.54419487323804,\n",
       "   -31.369913150239924,\n",
       "   -31.257014140035736,\n",
       "   -30.722120479487188,\n",
       "   -30.551541425045663,\n",
       "   -32.008314827952105,\n",
       "   -31.621605840028998,\n",
       "   -30.640489702575277,\n",
       "   -30.915477176569258,\n",
       "   -29.518314167588173,\n",
       "   -30.674941075486245,\n",
       "   -28.726516250539596,\n",
       "   -28.58385850273355,\n",
       "   -28.913006072391752,\n",
       "   -28.920628110350165,\n",
       "   -28.513022645320238,\n",
       "   -28.16154710795649,\n",
       "   -28.21874761624633,\n",
       "   -28.799592435629084,\n",
       "   -27.965010773107117,\n",
       "   -29.346929345986407,\n",
       "   -30.274448835737964,\n",
       "   -30.19341410569069,\n",
       "   -30.929492618704266,\n",
       "   -30.23866574669039,\n",
       "   -30.367327674920624,\n",
       "   -29.6710345185519,\n",
       "   -29.130652892487277,\n",
       "   -28.594996817222867,\n",
       "   -28.091632909353827,\n",
       "   -28.366397775541564,\n",
       "   -28.231471916179874,\n",
       "   -29.06584794122912,\n",
       "   -29.684524335128287,\n",
       "   -30.787667189876185,\n",
       "   -30.216407533138877,\n",
       "   -30.5024360549329,\n",
       "   -30.73787486155551,\n",
       "   -32.10016823564067,\n",
       "   -31.88869198427659,\n",
       "   -33.114465070931885,\n",
       "   -31.915671635459354,\n",
       "   -33.00660276364105,\n",
       "   -31.905288982566773,\n",
       "   -32.07474646972211,\n",
       "   -32.30706306636555,\n",
       "   -31.37212287085346,\n",
       "   -29.6679156915706,\n",
       "   -29.940801241231693,\n",
       "   -30.29952481498589,\n",
       "   -30.31353118443783,\n",
       "   -31.606303669033302,\n",
       "   -31.62938617485517,\n",
       "   -31.53666170834621,\n",
       "   -29.344690174168935,\n",
       "   -29.305730798170313,\n",
       "   -28.50019867114877,\n",
       "   -27.672611407521924,\n",
       "   -27.21278254512774,\n",
       "   -25.0901263627123,\n",
       "   -25.37966180849362,\n",
       "   -23.596152691989435,\n",
       "   -23.394785773430154,\n",
       "   -22.91892393751886,\n",
       "   -21.534446893161107,\n",
       "   -20.78798978122659,\n",
       "   -19.888241792531453,\n",
       "   -19.845315678614853,\n",
       "   -19.360081376483954,\n",
       "   -18.99312521445746,\n",
       "   -20.23233472227607,\n",
       "   -20.63430626204541,\n",
       "   -20.703869820899143,\n",
       "   -22.02557525925882,\n",
       "   -19.30041289761082,\n",
       "   -19.80347268412893,\n",
       "   -18.960888420812154,\n",
       "   -18.650424259688,\n",
       "   -18.02405997681827,\n",
       "   -18.320725322441582,\n",
       "   -18.94545446722086,\n",
       "   -19.3908230383842,\n",
       "   -18.537536596539393,\n",
       "   -17.117718650838057,\n",
       "   -18.829519269742722,\n",
       "   -17.13223633740298,\n",
       "   -18.007430061071428,\n",
       "   -17.94901498727515,\n",
       "   -16.74291602120981,\n",
       "   -19.651891556448724,\n",
       "   -20.46165030458329,\n",
       "   -20.165455219297144,\n",
       "   -19.34081639944335,\n",
       "   -18.552100409772876,\n",
       "   -18.850291809248706,\n",
       "   -17.781794716831712,\n",
       "   -19.56573395886988,\n",
       "   -19.112606038732686,\n",
       "   -20.855881701158168,\n",
       "   -23.625071637543876,\n",
       "   -23.960925400895974,\n",
       "   -24.303642558566903,\n",
       "   -23.663910726735747,\n",
       "   -24.62010002021665,\n",
       "   -24.62347227695903,\n",
       "   -24.245208280968964,\n",
       "   -22.436857670323402,\n",
       "   -21.58683130277946,\n",
       "   -20.689218850852445,\n",
       "   -19.348505114804563,\n",
       "   -18.326233984185226,\n",
       "   -18.678079855295387,\n",
       "   -18.908279586398134,\n",
       "   -18.64967330032478,\n",
       "   -20.20926478098178,\n",
       "   -20.030762908530356,\n",
       "   -19.642555490195864,\n",
       "   -20.23444733173509,\n",
       "   -20.965167394676293,\n",
       "   -21.037173433470084,\n",
       "   -21.079838410868824,\n",
       "   -21.350775881380763,\n",
       "   -19.96717141444662,\n",
       "   -19.66831471168351,\n",
       "   -18.81013464674415,\n",
       "   -18.391740878600242,\n",
       "   -17.176431717936307,\n",
       "   -17.134763290011563,\n",
       "   -16.895117600192822,\n",
       "   -16.296110844580742,\n",
       "   -17.259394447398343,\n",
       "   -16.656118340493418,\n",
       "   -15.991045233319124,\n",
       "   -14.712471389161735,\n",
       "   -15.311811171803466,\n",
       "   -14.548832950307705,\n",
       "   -15.803673262252037,\n",
       "   -13.711202462305444,\n",
       "   -12.931301281754253,\n",
       "   -12.864380308112091,\n",
       "   -11.64149954647433,\n",
       "   -11.012310782361942,\n",
       "   -11.309322312816843,\n",
       "   -11.651552242116235,\n",
       "   -13.102829913692549,\n",
       "   -14.385055601231642,\n",
       "   -15.888743647270793,\n",
       "   -15.168351015935691,\n",
       "   -16.72929163521897,\n",
       "   -17.29855779988321,\n",
       "   -15.751337750481756,\n",
       "   -16.086248514648982,\n",
       "   -16.176001746613384,\n",
       "   -18.93749484141263,\n",
       "   -18.812639045940884,\n",
       "   -17.629410502476784,\n",
       "   -16.952231096957643,\n",
       "   -18.264566088966756,\n",
       "   -19.01208740885838,\n",
       "   -19.829371149507512,\n",
       "   -20.198000536590907,\n",
       "   -21.016518597542117,\n",
       "   -20.708324140834655,\n",
       "   -19.885996574505302,\n",
       "   -19.749444316405373,\n",
       "   -18.81301428292365,\n",
       "   -18.332744505049263,\n",
       "   -17.98278593393147,\n",
       "   -17.07601662950619,\n",
       "   -16.68645555520126,\n",
       "   -15.157971092195424,\n",
       "   -15.087024072046926,\n",
       "   -12.036876996039917,\n",
       "   -9.683403203723326,\n",
       "   -9.547166222567146,\n",
       "   -9.98026212593551,\n",
       "   -8.99562704156547,\n",
       "   -8.992022403555717,\n",
       "   -9.242990510334002,\n",
       "   -10.154057022408482,\n",
       "   -9.22457014695061,\n",
       "   -9.986027091769383,\n",
       "   -10.742829784524641,\n",
       "   -10.911671073837995,\n",
       "   -12.716949968977845,\n",
       "   -13.650146007166363,\n",
       "   -13.605680525166136,\n",
       "   -13.499434958156943,\n",
       "   -15.336890979824748,\n",
       "   -15.622628273966882,\n",
       "   -15.37203287868508,\n",
       "   -16.276535369588842,\n",
       "   -15.062623465837294,\n",
       "   -13.962791260556196,\n",
       "   -13.170342220288832,\n",
       "   -12.619632769421656,\n",
       "   -11.538618268047049,\n",
       "   -10.688666646221234,\n",
       "   -9.328164098342427,\n",
       "   -8.798533252419633,\n",
       "   -8.527108881902963,\n",
       "   -8.056613850432703,\n",
       "   -7.933083975136146,\n",
       "   -7.946584019523614,\n",
       "   -7.283160131913805,\n",
       "   -7.221556156578858,\n",
       "   -6.965339284907358,\n",
       "   -6.746017937113347,\n",
       "   -6.6038472187290305,\n",
       "   -7.130070555611145,\n",
       "   -7.964201513620381,\n",
       "   -8.675137540274005,\n",
       "   -10.258287844561881,\n",
       "   -10.113871069304128,\n",
       "   -10.642683959756788,\n",
       "   -12.936907718631407,\n",
       "   -12.401899385033959,\n",
       "   -13.578583627656721,\n",
       "   -11.77506727214093],\n",
       "  'ff_r2': [-2633.4656794996117,\n",
       "   -14725.033106660927,\n",
       "   -3206.934290242944,\n",
       "   -1311.2426423940078,\n",
       "   -347.23029886937746,\n",
       "   -136.0098060316058,\n",
       "   -72.21001475560804,\n",
       "   -52.56211343800528,\n",
       "   -37.19099626588289,\n",
       "   -29.51284460622866,\n",
       "   -25.365209074138008,\n",
       "   -21.539282561271275,\n",
       "   -18.550293389676153,\n",
       "   -18.247168853491814,\n",
       "   -18.185198160463926,\n",
       "   -18.838540233241613,\n",
       "   -18.586114856394268,\n",
       "   -18.425790314935217,\n",
       "   -18.554938416406525,\n",
       "   -19.719329747431235,\n",
       "   -22.192179091160764,\n",
       "   -24.876783642056807,\n",
       "   -25.894694953691708,\n",
       "   -28.06177663754128,\n",
       "   -29.975811208801346,\n",
       "   -31.080760905802343,\n",
       "   -36.68099833169212,\n",
       "   -38.36761960078955,\n",
       "   -39.556480257579395,\n",
       "   -41.52490007523315,\n",
       "   -44.241508653276455,\n",
       "   -44.38241433929298,\n",
       "   -43.968286977789276,\n",
       "   -49.97921168905129,\n",
       "   -59.7657249601659,\n",
       "   -60.984121906332184,\n",
       "   -65.09318423086584,\n",
       "   -63.28815907447249,\n",
       "   -63.4628797507213,\n",
       "   -67.94226220056903,\n",
       "   -68.84268353931851,\n",
       "   -68.5732849577008,\n",
       "   -70.07671617886066,\n",
       "   -74.44530651704213,\n",
       "   -75.89233241594955,\n",
       "   -83.5457965964379,\n",
       "   -95.27731474643006,\n",
       "   -97.16898152429162,\n",
       "   -98.13251525362143,\n",
       "   -95.08075374535323,\n",
       "   -102.73676160484592,\n",
       "   -105.95890151984278,\n",
       "   -114.9313479885389,\n",
       "   -118.8460914982508,\n",
       "   -123.41042100512317,\n",
       "   -127.0751544257646,\n",
       "   -124.57484542743791,\n",
       "   -128.1650599398963,\n",
       "   -113.82994095409062,\n",
       "   -113.72573498839307,\n",
       "   -108.75817577344098,\n",
       "   -106.73602227425904,\n",
       "   -112.81781761465001,\n",
       "   -111.85007280607174,\n",
       "   -108.61475425765543,\n",
       "   -107.07015455262564,\n",
       "   -106.61066487927144,\n",
       "   -100.93322956624124,\n",
       "   -97.9285731698081,\n",
       "   -97.674678024767,\n",
       "   -99.08160092279745,\n",
       "   -111.65067875199041,\n",
       "   -106.20144564664264,\n",
       "   -108.31973693883587,\n",
       "   -100.93442146346239,\n",
       "   -99.67871828036907,\n",
       "   -105.16500987138785,\n",
       "   -108.31592006318932,\n",
       "   -101.33528587799734,\n",
       "   -97.30847111973989,\n",
       "   -89.71230005330365,\n",
       "   -83.99425692083118,\n",
       "   -89.32395732246059,\n",
       "   -88.73571666513364,\n",
       "   -86.93645777593623,\n",
       "   -87.39543302726577,\n",
       "   -85.26004469551276,\n",
       "   -85.70311302563958,\n",
       "   -87.78238508063568,\n",
       "   -94.75363138300052,\n",
       "   -86.37145006855187,\n",
       "   -84.59451433937888,\n",
       "   -89.62240593996307,\n",
       "   -85.44316923690519,\n",
       "   -87.3960821755322,\n",
       "   -86.30626981848576,\n",
       "   -86.96054635923849,\n",
       "   -86.31317136521436,\n",
       "   -91.11633414433913,\n",
       "   -91.16604437718031,\n",
       "   -98.22955878800941,\n",
       "   -94.17552689574659,\n",
       "   -91.22002723671473,\n",
       "   -81.6858175231573,\n",
       "   -76.69121208394999,\n",
       "   -71.64680539576933,\n",
       "   -69.3863053245165,\n",
       "   -69.52825643855849,\n",
       "   -68.04730298197362,\n",
       "   -74.3968036023948,\n",
       "   -75.78505858078941,\n",
       "   -75.31864813954815,\n",
       "   -73.06465283285509,\n",
       "   -74.01427080309672,\n",
       "   -74.56795830043926,\n",
       "   -80.6278642621779,\n",
       "   -87.52035819788185,\n",
       "   -89.40137398303517,\n",
       "   -88.68202097482528,\n",
       "   -81.81178607362841,\n",
       "   -76.71099886752955,\n",
       "   -74.334943199932,\n",
       "   -76.02999002819249,\n",
       "   -76.92513890610938,\n",
       "   -81.94566693895655,\n",
       "   -85.77031469052513,\n",
       "   -86.62304264448171,\n",
       "   -88.45634395194064,\n",
       "   -91.96562065344186,\n",
       "   -97.61459675085874,\n",
       "   -99.42796943101722,\n",
       "   -93.66685508837791,\n",
       "   -97.1241093187187,\n",
       "   -95.54976271264267,\n",
       "   -100.6580584476218,\n",
       "   -97.86502281178518,\n",
       "   -104.30743367231581,\n",
       "   -98.91267761908571,\n",
       "   -99.35597102551202,\n",
       "   -98.66452353922689,\n",
       "   -96.50040316032548,\n",
       "   -97.03612764074396,\n",
       "   -99.10027729815663,\n",
       "   -94.18572952769432,\n",
       "   -94.22079039239792,\n",
       "   -93.83593531710838,\n",
       "   -91.78659077739131,\n",
       "   -94.74826473453858,\n",
       "   -96.36714722582838,\n",
       "   -104.4401954894353,\n",
       "   -108.66318915252467,\n",
       "   -107.41967078368627,\n",
       "   -105.2953435144982,\n",
       "   -104.53860754490037,\n",
       "   -91.56298506072524,\n",
       "   -80.24058788087335,\n",
       "   -81.46011333368288,\n",
       "   -79.98884044616776,\n",
       "   -83.35852466658974,\n",
       "   -89.65410525166311,\n",
       "   -85.84885508080684,\n",
       "   -81.83595257878771,\n",
       "   -79.3341191476672,\n",
       "   -78.37229612061576,\n",
       "   -78.30468850320258,\n",
       "   -80.13407053105972,\n",
       "   -75.81735496851346,\n",
       "   -78.82456930371256,\n",
       "   -78.58102042658567,\n",
       "   -80.94124841929451,\n",
       "   -83.94558825213687,\n",
       "   -88.08830263459669,\n",
       "   -81.76025658143493,\n",
       "   -86.82543053566664,\n",
       "   -79.69335444589453,\n",
       "   -76.6765343929606,\n",
       "   -82.7758272255125,\n",
       "   -83.33674620930223,\n",
       "   -81.78942818112566,\n",
       "   -78.1864485112225,\n",
       "   -70.78633174200965,\n",
       "   -71.23345269337733,\n",
       "   -71.39073047252666,\n",
       "   -70.24791054044488,\n",
       "   -72.98529731411121,\n",
       "   -76.00740453317042,\n",
       "   -72.26424853807583,\n",
       "   -69.80305021050759,\n",
       "   -68.0516408003119,\n",
       "   -68.55922920076442,\n",
       "   -68.37504131983174,\n",
       "   -66.662089794747,\n",
       "   -68.43795477069166,\n",
       "   -64.74306786075634,\n",
       "   -62.564745130385425,\n",
       "   -61.863641004571136,\n",
       "   -62.57887779522764,\n",
       "   -62.09391328154024,\n",
       "   -65.06084108396445,\n",
       "   -61.633041975121024,\n",
       "   -63.754970882050614,\n",
       "   -64.2112546737086,\n",
       "   -66.85403562783642,\n",
       "   -62.99251058274079,\n",
       "   -61.38591904371379,\n",
       "   -64.58473885828916,\n",
       "   -67.2769307549731,\n",
       "   -66.84495159508107,\n",
       "   -62.83505457630308,\n",
       "   -62.68396192853687,\n",
       "   -64.4365232974928,\n",
       "   -62.95632530924828,\n",
       "   -63.644296388093835,\n",
       "   -60.41793410345337,\n",
       "   -64.33862330354559,\n",
       "   -61.808325557145075,\n",
       "   -61.96590451080954,\n",
       "   -61.881943628735264,\n",
       "   -63.92209753134374,\n",
       "   -62.1742450142868,\n",
       "   -62.83380599585791,\n",
       "   -65.90247892356719,\n",
       "   -66.12392890054463,\n",
       "   -69.10944532563848,\n",
       "   -70.50423262354582,\n",
       "   -71.18490237834807,\n",
       "   -67.43864649919051,\n",
       "   -65.25852538004754,\n",
       "   -65.6416759913088,\n",
       "   -65.06473027920194,\n",
       "   -64.60711655152268,\n",
       "   -63.81706278177951,\n",
       "   -65.30222722393917,\n",
       "   -64.32441155913233,\n",
       "   -68.98383394513108,\n",
       "   -71.38327660084289,\n",
       "   -68.51600241836367,\n",
       "   -69.01675594490315,\n",
       "   -68.65902447581992,\n",
       "   -69.93525263912564,\n",
       "   -71.26605320930035,\n",
       "   -72.84456051381322,\n",
       "   -73.04531584287882,\n",
       "   -73.13500834603224,\n",
       "   -71.96038699849099,\n",
       "   -72.73668427565701,\n",
       "   -72.50400857392357,\n",
       "   -70.61558204908319,\n",
       "   -70.20146410499156,\n",
       "   -69.20710540033069,\n",
       "   -66.7551946571689,\n",
       "   -66.33434147065971,\n",
       "   -65.96205374941563,\n",
       "   -65.4087926177011,\n",
       "   -68.77094983915849,\n",
       "   -68.7647397657264,\n",
       "   -66.20770766327834,\n",
       "   -61.85772691959991,\n",
       "   -63.345955438003216,\n",
       "   -68.62693908699178,\n",
       "   -64.033957466355,\n",
       "   -63.51135091172763,\n",
       "   -62.44331832200948,\n",
       "   -59.85776536566029,\n",
       "   -59.20004338289643,\n",
       "   -62.263426459557195,\n",
       "   -62.95611729692087,\n",
       "   -63.981389618802964,\n",
       "   -64.8026924743719,\n",
       "   -66.62616430347309,\n",
       "   -66.855202657975,\n",
       "   -65.6938576413269,\n",
       "   -61.410679354668716,\n",
       "   -60.551327612893765,\n",
       "   -58.17192638365406,\n",
       "   -55.87183452168444,\n",
       "   -56.49461767886417,\n",
       "   -57.85114421516904,\n",
       "   -58.62377889893317,\n",
       "   -58.36881621051247,\n",
       "   -59.14209309561676,\n",
       "   -60.81915508401248,\n",
       "   -62.66789645398778,\n",
       "   -61.44088923888037,\n",
       "   -61.045675522989356,\n",
       "   -62.68709071208308,\n",
       "   -63.91968248519929,\n",
       "   -62.801970687514434,\n",
       "   -60.73874342033314,\n",
       "   -65.46988555124189,\n",
       "   -62.15365365106651,\n",
       "   -62.4595063860199,\n",
       "   -62.95817614450741,\n",
       "   -63.321290661160475,\n",
       "   -62.379290898855785,\n",
       "   -62.35096653254996,\n",
       "   -62.29704249305354,\n",
       "   -64.9098143517721,\n",
       "   -64.72943198867965,\n",
       "   -64.83456901091246,\n",
       "   -64.40280659975991,\n",
       "   -65.80504607638919,\n",
       "   -66.61495302072876,\n",
       "   -65.2364015101305,\n",
       "   -65.09004794867752,\n",
       "   -64.51938808341498,\n",
       "   -65.25986479789096,\n",
       "   -63.80322869993921,\n",
       "   -63.455801170828394,\n",
       "   -58.729111515026204,\n",
       "   -58.308730216854634,\n",
       "   -54.672036268572235,\n",
       "   -58.93215653911887,\n",
       "   -57.76047080897674,\n",
       "   -59.19417422017136,\n",
       "   -57.72339162452995,\n",
       "   -56.98258476281832,\n",
       "   -56.72731205364676,\n",
       "   -54.56568133152567,\n",
       "   -55.735151836925695,\n",
       "   -56.76571728837804,\n",
       "   -57.96711981822959,\n",
       "   -60.1915309723659,\n",
       "   -59.20389646408526,\n",
       "   -59.91619142783003,\n",
       "   -59.92573474286547,\n",
       "   -60.39671860138124,\n",
       "   -63.29687701040925,\n",
       "   -68.95207888339935,\n",
       "   -70.05642200628463,\n",
       "   -70.52427137476936,\n",
       "   -68.64738715321114,\n",
       "   -71.79313506354005,\n",
       "   -73.48531848606248,\n",
       "   -72.69882321942062,\n",
       "   -73.43986506613678,\n",
       "   -68.17202746700664,\n",
       "   -65.20569159560885,\n",
       "   -65.43480786759318,\n",
       "   -66.5090312836462,\n",
       "   -62.83458014256753,\n",
       "   -59.21795048868318,\n",
       "   -59.49608135998456,\n",
       "   -56.94673922166863,\n",
       "   -58.2011050833983,\n",
       "   -55.44918576278908,\n",
       "   -59.112554027190555,\n",
       "   -58.4499203823539,\n",
       "   -57.954006304850154,\n",
       "   -58.11765197490891,\n",
       "   -54.96687323994607,\n",
       "   -52.32894175124773,\n",
       "   -51.33245724293831,\n",
       "   -49.48516272130816,\n",
       "   -47.673341976577944,\n",
       "   -47.53639921461829,\n",
       "   -44.85014106739877,\n",
       "   -43.999319844511085,\n",
       "   -43.6178177819623,\n",
       "   -43.71160080336009,\n",
       "   -41.43609407926189,\n",
       "   -41.08775133356684,\n",
       "   -43.376185083535475,\n",
       "   -43.96911096803858,\n",
       "   -43.114205522892014,\n",
       "   -42.528202601067655,\n",
       "   -42.8466279052152,\n",
       "   -42.488474483938184,\n",
       "   -40.43830831132237,\n",
       "   -41.88696344977706,\n",
       "   -42.99372638467,\n",
       "   -43.39135543639105,\n",
       "   -39.86316042734771,\n",
       "   -39.358951211134176,\n",
       "   -40.16716923901268,\n",
       "   -40.73510120355148,\n",
       "   -39.448730906401615,\n",
       "   -37.29936745256286,\n",
       "   -37.60310785345121,\n",
       "   -37.5705788763365,\n",
       "   -35.84602979173372,\n",
       "   -34.55161284093708,\n",
       "   -36.95972889937115,\n",
       "   -36.299047687996634,\n",
       "   -36.3985685477462,\n",
       "   -39.090749285116566,\n",
       "   -39.3316362416671,\n",
       "   -39.327948288275856,\n",
       "   -39.94796406669354,\n",
       "   -40.875645433406504,\n",
       "   -41.28752818593883,\n",
       "   -42.43985237147639,\n",
       "   -42.10341013311911,\n",
       "   -41.6455128902299,\n",
       "   -41.02738745220561,\n",
       "   -40.1816610871687,\n",
       "   -40.22740033911531,\n",
       "   -39.829065985316134,\n",
       "   -41.05055832428754,\n",
       "   -40.7651340303488,\n",
       "   -38.70233372402055,\n",
       "   -40.75338143340599,\n",
       "   -41.24629804466939,\n",
       "   -41.27083345830359,\n",
       "   -42.70845194486845,\n",
       "   -40.57959651635979,\n",
       "   -39.62122927697225,\n",
       "   -38.24243749468103,\n",
       "   -36.25493978194024,\n",
       "   -37.15256164233723,\n",
       "   -38.2158487316982,\n",
       "   -37.736498700931065,\n",
       "   -37.669918304798486,\n",
       "   -37.26369107155071,\n",
       "   -35.472276483623666,\n",
       "   -36.30008337389925,\n",
       "   -35.78731636505775,\n",
       "   -37.60504548540854,\n",
       "   -38.80725734614056,\n",
       "   -38.538241570870724,\n",
       "   -39.749526827879244,\n",
       "   -38.16897129756676,\n",
       "   -39.02386482744623,\n",
       "   -39.21753514180643,\n",
       "   -39.86827814920618,\n",
       "   -40.24190107548883,\n",
       "   -40.60648072067318,\n",
       "   -38.73494427711452,\n",
       "   -40.048538003503396,\n",
       "   -40.49052995830812,\n",
       "   -39.37418566866476,\n",
       "   -38.305532984817326,\n",
       "   -38.211369149799445,\n",
       "   -36.86418675354127,\n",
       "   -37.13292593072404,\n",
       "   -37.25589178386933,\n",
       "   -37.36116817517283,\n",
       "   -36.824581130463294,\n",
       "   -38.3004206698462,\n",
       "   -38.10844073555625,\n",
       "   -37.83607619078076,\n",
       "   -35.94696768537134,\n",
       "   -36.562136991035835,\n",
       "   -35.96015494669534,\n",
       "   -36.060008464290895,\n",
       "   -36.82976869762669,\n",
       "   -35.904777634996364,\n",
       "   -35.30685802818208,\n",
       "   -33.5891817616276,\n",
       "   -32.541612491109845,\n",
       "   -32.48404849759098,\n",
       "   -33.24612526320013,\n",
       "   -32.15856106797719,\n",
       "   -31.331168247649344,\n",
       "   -32.15342253971684,\n",
       "   -32.66569797740195,\n",
       "   -31.688424247273467,\n",
       "   -30.41178534717676,\n",
       "   -30.95291682129485,\n",
       "   -32.232395015531054,\n",
       "   -31.653481867832895,\n",
       "   -31.730765517871525,\n",
       "   -31.809889425820764,\n",
       "   -31.721035226802726,\n",
       "   -30.868245092479302,\n",
       "   -31.715363648162317,\n",
       "   -33.66029353638609,\n",
       "   -33.76877259772523,\n",
       "   -33.601052957083105,\n",
       "   -35.11883515202801,\n",
       "   -36.509290216381466,\n",
       "   -35.63024051721976,\n",
       "   -35.794160835722884,\n",
       "   -35.73069056488724,\n",
       "   -36.40616977502091,\n",
       "   -35.854677301989526,\n",
       "   -36.54919113212245,\n",
       "   -36.54568817299643,\n",
       "   -36.54734570634675,\n",
       "   -36.75468816368101,\n",
       "   -36.93471821816848,\n",
       "   -35.431260320020876,\n",
       "   -34.248593308280455,\n",
       "   -32.56021687918176,\n",
       "   -33.02717326211708,\n",
       "   -33.88918598097826,\n",
       "   -33.68340326126355,\n",
       "   -33.36773959408263,\n",
       "   -33.204155666974415,\n",
       "   -32.74703994705885,\n",
       "   -32.042259298529736,\n",
       "   -31.379996382225144,\n",
       "   -33.31279005937472,\n",
       "   -31.93924158362168,\n",
       "   -32.24707341907743,\n",
       "   -32.03882037877302,\n",
       "   -33.19793749248117,\n",
       "   -32.578786096539154,\n",
       "   -31.792016303645354,\n",
       "   -30.957959014420833],\n",
       "  'test_r2s': [-2702.2951041467595,\n",
       "   -14771.919052922054,\n",
       "   -3238.8673559924077,\n",
       "   -1332.0462546636047,\n",
       "   -361.1415403092525,\n",
       "   -145.2780507307771,\n",
       "   -78.34856291980157,\n",
       "   -56.99609363720534,\n",
       "   -41.504465714156474,\n",
       "   -35.34654146626582,\n",
       "   -33.00365737620785,\n",
       "   -30.81092592798603,\n",
       "   -31.87436639240176,\n",
       "   -34.89337469958799,\n",
       "   -37.95893293080417,\n",
       "   -41.976776792407804,\n",
       "   -45.65010276367987,\n",
       "   -48.477537440548744,\n",
       "   -48.448975753705746,\n",
       "   -45.19264822444197,\n",
       "   -42.00338963765603,\n",
       "   -41.94761842730384,\n",
       "   -40.64883139064884,\n",
       "   -41.681348452773435,\n",
       "   -42.60908030634113,\n",
       "   -42.87798844523884,\n",
       "   -48.51648275717449,\n",
       "   -50.014780793034106,\n",
       "   -51.27937943626711,\n",
       "   -53.41302814986467,\n",
       "   -56.270113870406476,\n",
       "   -57.239522249014456,\n",
       "   -56.78343193322641,\n",
       "   -62.9196690644131,\n",
       "   -72.36655351458033,\n",
       "   -73.7536018803508,\n",
       "   -78.27538010576491,\n",
       "   -77.59567570788207,\n",
       "   -78.5059993262263,\n",
       "   -83.2379254674251,\n",
       "   -84.51158257213157,\n",
       "   -84.17420943547538,\n",
       "   -85.41772838220393,\n",
       "   -90.5851395260559,\n",
       "   -91.9941438612102,\n",
       "   -99.71659814906758,\n",
       "   -110.54629111471051,\n",
       "   -112.12395772399023,\n",
       "   -112.50721069185524,\n",
       "   -109.1401341180198,\n",
       "   -116.63971211104692,\n",
       "   -119.67003747384825,\n",
       "   -128.47734601911353,\n",
       "   -132.86648303617613,\n",
       "   -137.1322776197414,\n",
       "   -141.58635802388525,\n",
       "   -139.41665711257812,\n",
       "   -143.42343028632925,\n",
       "   -128.67685562968518,\n",
       "   -128.44217306290366,\n",
       "   -123.69958041374247,\n",
       "   -121.34048180291276,\n",
       "   -127.44868150380654,\n",
       "   -126.68239655885293,\n",
       "   -122.89746199643965,\n",
       "   -121.11811163978572,\n",
       "   -120.60077332951056,\n",
       "   -114.92789491949985,\n",
       "   -111.69965920017489,\n",
       "   -112.1295027651333,\n",
       "   -114.10575779262547,\n",
       "   -127.02815723910341,\n",
       "   -122.39606207008315,\n",
       "   -124.99538153877714,\n",
       "   -118.22494025373288,\n",
       "   -117.58592777507076,\n",
       "   -123.21833703284324,\n",
       "   -126.2246411095589,\n",
       "   -119.67531021829724,\n",
       "   -116.42426900189452,\n",
       "   -110.35193571203109,\n",
       "   -105.55738096432134,\n",
       "   -111.38701190137114,\n",
       "   -111.3748573530007,\n",
       "   -109.53682126213872,\n",
       "   -110.55863476623689,\n",
       "   -107.68977427611195,\n",
       "   -108.79850666807977,\n",
       "   -111.10133308813852,\n",
       "   -118.12592832249888,\n",
       "   -109.19295496462686,\n",
       "   -107.31548219847033,\n",
       "   -111.91554498880723,\n",
       "   -106.65743806380651,\n",
       "   -108.03420034724587,\n",
       "   -105.49118218092237,\n",
       "   -105.89065487031013,\n",
       "   -104.79709635222537,\n",
       "   -108.56301192743985,\n",
       "   -107.60994376637541,\n",
       "   -114.35709680277884,\n",
       "   -111.03689314662456,\n",
       "   -108.5681619890822,\n",
       "   -99.77937758717988,\n",
       "   -95.79647745083278,\n",
       "   -91.1544027786659,\n",
       "   -88.7779654057768,\n",
       "   -89.32041693309223,\n",
       "   -87.68326905651634,\n",
       "   -93.43305266448685,\n",
       "   -95.85707971623017,\n",
       "   -96.18114842205648,\n",
       "   -94.91514700406194,\n",
       "   -95.47063179248862,\n",
       "   -97.39841473103374,\n",
       "   -104.61441878020905,\n",
       "   -111.79674044975435,\n",
       "   -113.6434231208656,\n",
       "   -112.77632940045001,\n",
       "   -105.76680423482031,\n",
       "   -101.35449780416974,\n",
       "   -99.98755583904678,\n",
       "   -102.06890404812182,\n",
       "   -103.51201462625842,\n",
       "   -109.01109472565568,\n",
       "   -113.17151864958241,\n",
       "   -114.74329311050928,\n",
       "   -116.99688692692673,\n",
       "   -122.1049477829145,\n",
       "   -126.93998114991936,\n",
       "   -127.90330829898545,\n",
       "   -123.03638674155735,\n",
       "   -127.35318838788685,\n",
       "   -124.94105947777763,\n",
       "   -130.18825178444717,\n",
       "   -126.49430620721535,\n",
       "   -131.16164834535422,\n",
       "   -125.93774963428032,\n",
       "   -125.63686463850885,\n",
       "   -123.93773428365066,\n",
       "   -121.2328409906655,\n",
       "   -122.51094369187189,\n",
       "   -124.09870571606564,\n",
       "   -118.66940209503458,\n",
       "   -117.79603121495896,\n",
       "   -116.79785127407018,\n",
       "   -114.09552983543396,\n",
       "   -116.35093957207607,\n",
       "   -117.627102772589,\n",
       "   -125.05732466814396,\n",
       "   -129.56277050706638,\n",
       "   -127.92216333569058,\n",
       "   -126.22924030499078,\n",
       "   -126.47855974713087,\n",
       "   -114.44379339937402,\n",
       "   -104.5111753566934,\n",
       "   -106.51897203691342,\n",
       "   -106.49167500112347,\n",
       "   -111.43112452183945,\n",
       "   -117.9001489769103,\n",
       "   -116.07206885144834,\n",
       "   -113.21859289917009,\n",
       "   -112.44505393752475,\n",
       "   -112.91424498265954,\n",
       "   -114.35892315380721,\n",
       "   -116.22252781356727,\n",
       "   -112.80113698086245,\n",
       "   -118.228887345054,\n",
       "   -118.98041166011572,\n",
       "   -121.96477157496676,\n",
       "   -124.99053250115657,\n",
       "   -127.8171983339031,\n",
       "   -120.47362960507496,\n",
       "   -125.29024648564369,\n",
       "   -118.15183942117235,\n",
       "   -113.44028498806415,\n",
       "   -116.39574696058308,\n",
       "   -116.06191313346801,\n",
       "   -112.82216064666206,\n",
       "   -107.17066087398011,\n",
       "   -100.12959029346983,\n",
       "   -101.10138739354844,\n",
       "   -101.83792782687021,\n",
       "   -99.83508845796933,\n",
       "   -101.72150265048978,\n",
       "   -104.51092737742096,\n",
       "   -99.99180319463235,\n",
       "   -96.67199467865058,\n",
       "   -96.23680141168273,\n",
       "   -96.01696134990667,\n",
       "   -95.3738621869291,\n",
       "   -94.07818848438404,\n",
       "   -96.50270358844536,\n",
       "   -92.29662326971541,\n",
       "   -91.07643246063857,\n",
       "   -90.68016960202186,\n",
       "   -90.38167669972967,\n",
       "   -89.34546642067278,\n",
       "   -91.16627196670072,\n",
       "   -88.26609873056256,\n",
       "   -89.33989935806053,\n",
       "   -90.0247010816575,\n",
       "   -92.18788602528214,\n",
       "   -87.77099439015416,\n",
       "   -84.8165588749086,\n",
       "   -88.60921717640386,\n",
       "   -91.28232356707214,\n",
       "   -91.01669782813686,\n",
       "   -87.75517569866518,\n",
       "   -87.80037530398934,\n",
       "   -88.97927579045354,\n",
       "   -87.81637457365966,\n",
       "   -87.61999373455106,\n",
       "   -83.41519462293353,\n",
       "   -87.19946630307695,\n",
       "   -85.73257874588263,\n",
       "   -86.53495992620212,\n",
       "   -87.46179338727293,\n",
       "   -91.01620148899562,\n",
       "   -89.15829053351325,\n",
       "   -90.83948387091633,\n",
       "   -96.25918207976731,\n",
       "   -97.99964341251027,\n",
       "   -101.8962206908582,\n",
       "   -104.12080659062049,\n",
       "   -105.58705581330995,\n",
       "   -101.95427063551996,\n",
       "   -102.47424678699755,\n",
       "   -104.9051148241254,\n",
       "   -105.40770465135185,\n",
       "   -105.06019206453084,\n",
       "   -103.88907821984823,\n",
       "   -103.11580251394673,\n",
       "   -102.16155514366162,\n",
       "   -105.9403615214803,\n",
       "   -108.1990221978956,\n",
       "   -102.58587249030239,\n",
       "   -102.7812181554105,\n",
       "   -100.99461608596943,\n",
       "   -100.85079312734891,\n",
       "   -100.95509789909742,\n",
       "   -101.07938678297641,\n",
       "   -100.50891696625985,\n",
       "   -100.3221623726285,\n",
       "   -98.58084765175326,\n",
       "   -100.27152260974135,\n",
       "   -100.98988423859649,\n",
       "   -101.17723369654658,\n",
       "   -100.6302585710058,\n",
       "   -99.15233841717088,\n",
       "   -97.86498837797757,\n",
       "   -98.22710552974897,\n",
       "   -99.74473849035083,\n",
       "   -100.43963551277399,\n",
       "   -105.34221272545481,\n",
       "   -104.9576654866614,\n",
       "   -102.23218186501522,\n",
       "   -96.46042952106201,\n",
       "   -98.02099617516319,\n",
       "   -101.08684299104982,\n",
       "   -94.61081965726419,\n",
       "   -92.96047285927497,\n",
       "   -92.23584244454953,\n",
       "   -90.19046106232821,\n",
       "   -91.41598509281954,\n",
       "   -96.84637879917886,\n",
       "   -100.4661860028524,\n",
       "   -106.50403961760522,\n",
       "   -109.65475930629766,\n",
       "   -116.0831270716136,\n",
       "   -117.78286243088164,\n",
       "   -117.56041304387013,\n",
       "   -114.03249321443388,\n",
       "   -114.85808663181112,\n",
       "   -112.74263916226491,\n",
       "   -110.66917440130436,\n",
       "   -109.08131891067637,\n",
       "   -108.23645755458388,\n",
       "   -110.80095925291872,\n",
       "   -108.76448592705975,\n",
       "   -109.05628704744386,\n",
       "   -111.35305551373177,\n",
       "   -112.43658859271389,\n",
       "   -108.97332102220835,\n",
       "   -106.92222647157587,\n",
       "   -108.89229337580106,\n",
       "   -111.43254330857113,\n",
       "   -109.38811140839553,\n",
       "   -106.01423577672782,\n",
       "   -112.08378616437054,\n",
       "   -108.72988268710787,\n",
       "   -111.2771831171888,\n",
       "   -113.13086562016625,\n",
       "   -114.76447823801314,\n",
       "   -115.28276898601857,\n",
       "   -115.1856665404141,\n",
       "   -114.20935404238912,\n",
       "   -115.4259770658713,\n",
       "   -115.09367531320694,\n",
       "   -115.87752452652633,\n",
       "   -114.98761805288986,\n",
       "   -117.44847075912801,\n",
       "   -117.21795627506836,\n",
       "   -115.80900381772761,\n",
       "   -117.7080039715737,\n",
       "   -117.36146245232503,\n",
       "   -119.89023607912065,\n",
       "   -118.68084821588721,\n",
       "   -116.4148500643337,\n",
       "   -108.94905319885193,\n",
       "   -108.09408338303564,\n",
       "   -102.80427653377905,\n",
       "   -106.71378260786815,\n",
       "   -106.85522004652455,\n",
       "   -109.36878590289811,\n",
       "   -109.96858729688903,\n",
       "   -107.55912647567175,\n",
       "   -106.80781349270987,\n",
       "   -106.43189575009116,\n",
       "   -109.09918255814355,\n",
       "   -109.86726931244137,\n",
       "   -110.84207739449349,\n",
       "   -110.63301209830215,\n",
       "   -110.96318985019413,\n",
       "   -110.25404123781138,\n",
       "   -110.55179056882591,\n",
       "   -113.80495084252487,\n",
       "   -113.51687807045774,\n",
       "   -115.51620765481132,\n",
       "   -115.80582923399967,\n",
       "   -115.39888732321609,\n",
       "   -112.08661905959778,\n",
       "   -116.8366903350508,\n",
       "   -118.93651802840714,\n",
       "   -118.19525464442725,\n",
       "   -117.1624228950608,\n",
       "   -112.99987314119585,\n",
       "   -109.15594205098253,\n",
       "   -107.6087413853868,\n",
       "   -107.40831449623838,\n",
       "   -100.34868094551598,\n",
       "   -96.61851531952212,\n",
       "   -94.49253061006495,\n",
       "   -90.79199243125359,\n",
       "   -92.1548734896123,\n",
       "   -87.07324149510909,\n",
       "   -89.42045195138213,\n",
       "   -88.22630228640013,\n",
       "   -87.63412580976475,\n",
       "   -86.50678509571374,\n",
       "   -82.58031736808395,\n",
       "   -80.96120469559544,\n",
       "   -80.70377839421135,\n",
       "   -79.37657744738772,\n",
       "   -79.6530204262424,\n",
       "   -77.98681532038822,\n",
       "   -76.09798161093303,\n",
       "   -74.45200266210122,\n",
       "   -74.60176274414864,\n",
       "   -75.71147972176189,\n",
       "   -74.79792830840212,\n",
       "   -74.90781178390344,\n",
       "   -78.04164220189858,\n",
       "   -78.97168459737884,\n",
       "   -77.9726044545535,\n",
       "   -80.734488617459,\n",
       "   -78.69866635316122,\n",
       "   -81.23841003029725,\n",
       "   -78.87449894082492,\n",
       "   -76.98096141001314,\n",
       "   -79.91454072578188,\n",
       "   -79.69776530618984,\n",
       "   -77.14161646170436,\n",
       "   -78.98938841475098,\n",
       "   -81.05746831629408,\n",
       "   -79.70238074406532,\n",
       "   -77.18647251277397,\n",
       "   -75.32277718124476,\n",
       "   -74.97495719276418,\n",
       "   -76.97165645194998,\n",
       "   -75.1917828592596,\n",
       "   -72.97353233239056,\n",
       "   -76.03896729804026,\n",
       "   -73.44265819539947,\n",
       "   -72.60305486215559,\n",
       "   -73.54480932862484,\n",
       "   -72.71807211688962,\n",
       "   -72.06733125830976,\n",
       "   -72.84197184901052,\n",
       "   -73.14113656332071,\n",
       "   -72.65974141747475,\n",
       "   -73.24354233412049,\n",
       "   -74.19390879573423,\n",
       "   -74.5594045935728,\n",
       "   -74.5388798285438,\n",
       "   -75.17042776323527,\n",
       "   -77.26189430101078,\n",
       "   -76.0933113279315,\n",
       "   -77.20765555332778,\n",
       "   -78.08748989794694,\n",
       "   -74.01188929637468,\n",
       "   -74.01655053948312,\n",
       "   -74.8479971535379,\n",
       "   -75.09491814417495,\n",
       "   -76.74726353481168,\n",
       "   -74.62063773904588,\n",
       "   -75.25989198180127,\n",
       "   -76.30825930989957,\n",
       "   -75.67564155812117,\n",
       "   -78.36181378023488,\n",
       "   -78.29390975761916,\n",
       "   -80.5816228480149,\n",
       "   -80.5432318089571,\n",
       "   -83.02877905102318,\n",
       "   -81.46370986031525,\n",
       "   -82.7245962870021,\n",
       "   -79.71717525807112,\n",
       "   -81.69099498825273,\n",
       "   -79.64707765165588,\n",
       "   -78.55275895190422,\n",
       "   -80.60984663912427,\n",
       "   -77.62827484946288,\n",
       "   -76.11027321255145,\n",
       "   -75.28782996743169,\n",
       "   -73.47667881422055,\n",
       "   -74.32858990252372,\n",
       "   -74.41067150076398,\n",
       "   -73.26937112256617,\n",
       "   -72.53514150888816,\n",
       "   -73.14261359147729,\n",
       "   -69.90506935670783,\n",
       "   -66.6780839480593,\n",
       "   -65.45886797304425,\n",
       "   -63.051701683247686,\n",
       "   -66.0550355451141,\n",
       "   -66.75804994075011,\n",
       "   -66.2282773767305,\n",
       "   -64.5743445680781,\n",
       "   -67.25633847697878,\n",
       "   -69.51069671017561,\n",
       "   -71.17779017114268,\n",
       "   -71.12526329998978,\n",
       "   -73.89819757475857,\n",
       "   -73.79546968045753,\n",
       "   -74.3436278217934,\n",
       "   -75.19774039309951,\n",
       "   -73.82177051941854,\n",
       "   -74.94833814492526,\n",
       "   -72.56891831683096,\n",
       "   -70.37539467176153,\n",
       "   -70.30693015683984,\n",
       "   -70.66148755979358,\n",
       "   -69.56246278591506,\n",
       "   -65.6440945165931,\n",
       "   -63.45232112834185,\n",
       "   -63.31773703647959,\n",
       "   -63.69136977638533,\n",
       "   -61.61989596971193,\n",
       "   -62.37807797652234,\n",
       "   -64.40056219026664,\n",
       "   -66.50845151101797,\n",
       "   -67.48099151964209,\n",
       "   -68.10392556061764,\n",
       "   -69.8203250235477,\n",
       "   -70.44450172505223,\n",
       "   -73.42053164210081,\n",
       "   -76.0324066648259,\n",
       "   -77.12611014315898,\n",
       "   -76.68524186968185,\n",
       "   -78.93219697007797,\n",
       "   -80.60728816830374,\n",
       "   -78.70257535447396,\n",
       "   -77.14069336154577,\n",
       "   -75.49097332862826,\n",
       "   -72.50639246454773,\n",
       "   -70.4627547449232,\n",
       "   -70.1012275886462,\n",
       "   -67.96675389667085,\n",
       "   -66.67077900647385,\n",
       "   -63.31371830567828,\n",
       "   -62.7992627120431,\n",
       "   -62.455362980671595,\n",
       "   -61.88863813774664,\n",
       "   -62.3665516798659,\n",
       "   -64.3307214439331,\n",
       "   -68.39237442649339,\n",
       "   -69.90472547773712,\n",
       "   -70.30897533013695,\n",
       "   -73.58237503878429,\n",
       "   -75.00802063836122,\n",
       "   -76.12177409556347,\n",
       "   -76.54569973131241,\n",
       "   -78.70661393222707,\n",
       "   -78.46207027785015,\n",
       "   -79.34437582368287,\n",
       "   -77.85800226924772,\n",
       "   -78.21562531756447,\n",
       "   -74.728997473399,\n",
       "   -74.22560012166343,\n",
       "   -70.27239525945923],\n",
       "  'train_pce_loss': [1.168936848640442,\n",
       "   1.2129099369049072,\n",
       "   1.1990488767623901,\n",
       "   1.2262448072433472,\n",
       "   1.168205976486206,\n",
       "   1.3036986589431763,\n",
       "   1.2211799621582031,\n",
       "   1.1307011842727661,\n",
       "   1.1296091079711914,\n",
       "   1.1107879877090454,\n",
       "   1.2428821325302124,\n",
       "   1.1150275468826294,\n",
       "   1.1294927597045898,\n",
       "   1.0771437883377075,\n",
       "   1.0607614517211914,\n",
       "   1.1550402641296387,\n",
       "   1.213019847869873,\n",
       "   1.1395835876464844,\n",
       "   1.1269991397857666,\n",
       "   1.130246877670288,\n",
       "   1.1099984645843506,\n",
       "   1.0840694904327393,\n",
       "   1.084449291229248,\n",
       "   1.1224291324615479,\n",
       "   1.0761991739273071,\n",
       "   1.1006571054458618,\n",
       "   1.0794082880020142,\n",
       "   1.1246013641357422,\n",
       "   1.1916838884353638,\n",
       "   1.045572280883789,\n",
       "   1.071696162223816,\n",
       "   1.0980316400527954,\n",
       "   1.0449039936065674,\n",
       "   1.164595127105713,\n",
       "   0.9719415307044983,\n",
       "   1.0335828065872192,\n",
       "   1.058860421180725,\n",
       "   1.1151310205459595,\n",
       "   1.006772756576538,\n",
       "   1.0326101779937744,\n",
       "   1.080856204032898,\n",
       "   0.9949547052383423,\n",
       "   1.0858629941940308,\n",
       "   1.1309278011322021,\n",
       "   1.041335105895996,\n",
       "   1.0166352987289429,\n",
       "   1.019461750984192,\n",
       "   1.0441442728042603,\n",
       "   0.9978572726249695,\n",
       "   1.0393892526626587,\n",
       "   1.0743790864944458,\n",
       "   1.0566041469573975,\n",
       "   0.9998956322669983,\n",
       "   1.0647709369659424,\n",
       "   0.9788575768470764,\n",
       "   0.961048424243927,\n",
       "   0.9939368367195129,\n",
       "   1.020362377166748,\n",
       "   0.9428239464759827,\n",
       "   1.0228220224380493,\n",
       "   0.957849383354187,\n",
       "   1.0228016376495361,\n",
       "   0.9554411768913269,\n",
       "   0.9642112255096436,\n",
       "   0.9848701357841492,\n",
       "   0.9667964577674866,\n",
       "   0.9973357319831848,\n",
       "   0.9827633500099182,\n",
       "   0.9466778039932251,\n",
       "   0.9423466920852661,\n",
       "   1.0540380477905273,\n",
       "   0.9892457723617554,\n",
       "   0.9426493048667908,\n",
       "   1.0756175518035889,\n",
       "   0.9042436480522156,\n",
       "   0.9663124084472656,\n",
       "   0.9835751056671143,\n",
       "   0.9140708446502686,\n",
       "   0.94621342420578,\n",
       "   0.9661121964454651,\n",
       "   0.9333248734474182,\n",
       "   0.9894310235977173,\n",
       "   0.9906259775161743,\n",
       "   1.0035659074783325,\n",
       "   0.9913486242294312,\n",
       "   0.9194673299789429,\n",
       "   0.9956704378128052,\n",
       "   0.9201503396034241,\n",
       "   0.8900353312492371,\n",
       "   0.9894949793815613,\n",
       "   0.9546105265617371,\n",
       "   0.9395856857299805,\n",
       "   0.9677777886390686,\n",
       "   0.9609417915344238,\n",
       "   0.9349573850631714,\n",
       "   0.8844876885414124,\n",
       "   0.9206201434135437,\n",
       "   0.911419689655304,\n",
       "   0.935280442237854,\n",
       "   0.8773361444473267,\n",
       "   0.9081394672393799,\n",
       "   0.9109261631965637,\n",
       "   0.9051684141159058,\n",
       "   0.8683035373687744,\n",
       "   0.8909854888916016,\n",
       "   0.85627281665802,\n",
       "   0.8258809447288513,\n",
       "   0.8816229104995728,\n",
       "   0.97654128074646,\n",
       "   0.8226401805877686,\n",
       "   0.8715237975120544,\n",
       "   0.8621314764022827,\n",
       "   0.8736231923103333,\n",
       "   0.8711212873458862,\n",
       "   0.9179620742797852,\n",
       "   0.8903261423110962,\n",
       "   0.8079741597175598,\n",
       "   0.931708037853241,\n",
       "   0.8498097658157349,\n",
       "   0.8215193748474121,\n",
       "   0.8304896950721741,\n",
       "   0.8294467329978943,\n",
       "   0.8450127840042114,\n",
       "   0.8573803305625916,\n",
       "   0.8353734612464905,\n",
       "   0.7982804179191589,\n",
       "   0.8539888858795166,\n",
       "   0.9028759002685547,\n",
       "   0.7902418971061707,\n",
       "   0.885684072971344,\n",
       "   0.8650697469711304,\n",
       "   0.8864967226982117,\n",
       "   0.8936819434165955,\n",
       "   0.9023193120956421,\n",
       "   0.8521507382392883,\n",
       "   0.8771889805793762,\n",
       "   0.820067822933197,\n",
       "   0.8404839038848877,\n",
       "   0.8661302924156189,\n",
       "   0.8286570906639099,\n",
       "   0.79981929063797,\n",
       "   0.8814437389373779,\n",
       "   0.8043869137763977,\n",
       "   0.8277209401130676,\n",
       "   0.7554283142089844,\n",
       "   0.8469642996788025,\n",
       "   0.7636181116104126,\n",
       "   0.8517218232154846,\n",
       "   0.7771955728530884,\n",
       "   0.8622965812683105,\n",
       "   0.8311883211135864,\n",
       "   0.790964663028717,\n",
       "   0.8464404940605164,\n",
       "   0.7767675518989563,\n",
       "   0.8638108372688293,\n",
       "   0.8289141654968262,\n",
       "   0.8082186579704285,\n",
       "   0.9050097465515137,\n",
       "   0.7806505560874939,\n",
       "   0.8030182123184204,\n",
       "   0.8161128163337708,\n",
       "   0.853553831577301,\n",
       "   0.7899808883666992,\n",
       "   0.795086681842804,\n",
       "   0.8133789896965027,\n",
       "   0.8605065941810608,\n",
       "   0.8132643103599548,\n",
       "   0.8532691597938538,\n",
       "   0.7937861084938049,\n",
       "   0.7878392338752747,\n",
       "   0.7405850887298584,\n",
       "   0.8133283853530884,\n",
       "   0.780137300491333,\n",
       "   0.7678099870681763,\n",
       "   0.8432372808456421,\n",
       "   0.8108782172203064,\n",
       "   0.7423835396766663,\n",
       "   0.7662481665611267,\n",
       "   0.8139956593513489,\n",
       "   0.7874245643615723,\n",
       "   0.794559895992279,\n",
       "   0.8085521459579468,\n",
       "   0.7619172930717468,\n",
       "   0.7632947564125061,\n",
       "   0.7720655202865601,\n",
       "   0.741584300994873,\n",
       "   0.7395283579826355,\n",
       "   0.7924841046333313,\n",
       "   0.7629011273384094,\n",
       "   0.7637290358543396,\n",
       "   0.7887763977050781,\n",
       "   0.836428165435791,\n",
       "   0.7546613216400146,\n",
       "   0.7312122583389282,\n",
       "   0.7612472772598267,\n",
       "   0.7816905379295349,\n",
       "   0.7736987471580505,\n",
       "   0.7407352328300476,\n",
       "   0.7647892236709595,\n",
       "   0.7413821816444397,\n",
       "   0.7739322781562805,\n",
       "   0.7616006135940552,\n",
       "   0.755249559879303,\n",
       "   0.7963284254074097,\n",
       "   0.7834540605545044,\n",
       "   0.7849459052085876,\n",
       "   0.7874875068664551,\n",
       "   0.7640966773033142,\n",
       "   0.7802695631980896,\n",
       "   0.7845717668533325,\n",
       "   0.7197127938270569,\n",
       "   0.7359983921051025,\n",
       "   0.7351193428039551,\n",
       "   0.7426722049713135,\n",
       "   0.7866299748420715,\n",
       "   0.7547508478164673,\n",
       "   0.7183290123939514,\n",
       "   0.7469032406806946,\n",
       "   0.7601317763328552,\n",
       "   0.72370445728302,\n",
       "   0.720673680305481,\n",
       "   0.7654285430908203,\n",
       "   0.8038346171379089,\n",
       "   0.7324830293655396,\n",
       "   0.7373496294021606,\n",
       "   0.7255831360816956,\n",
       "   0.750042200088501,\n",
       "   0.7061573266983032,\n",
       "   0.7301516532897949,\n",
       "   0.7316301465034485,\n",
       "   0.7981830835342407,\n",
       "   0.7317896485328674,\n",
       "   0.7089283466339111,\n",
       "   0.7246308326721191,\n",
       "   0.7390491962432861,\n",
       "   0.7387919425964355,\n",
       "   0.7214844226837158,\n",
       "   0.7691911458969116,\n",
       "   0.721531331539154,\n",
       "   0.727120578289032,\n",
       "   0.7468533515930176,\n",
       "   0.7568789720535278,\n",
       "   0.7481402158737183,\n",
       "   0.7171213030815125,\n",
       "   0.7413776516914368,\n",
       "   0.6957635283470154,\n",
       "   0.7239353656768799,\n",
       "   0.8306022882461548,\n",
       "   0.7758617401123047,\n",
       "   0.6677596569061279,\n",
       "   0.6779245734214783,\n",
       "   0.7187280654907227,\n",
       "   0.7387142777442932,\n",
       "   0.6990832686424255,\n",
       "   0.7366382479667664,\n",
       "   0.7249836921691895,\n",
       "   0.7289869785308838,\n",
       "   0.6999978423118591,\n",
       "   0.7248828411102295,\n",
       "   0.7651861906051636,\n",
       "   0.6928614974021912,\n",
       "   0.6763492226600647,\n",
       "   0.7391972541809082,\n",
       "   0.7292618751525879,\n",
       "   0.6951918005943298,\n",
       "   0.6561156511306763,\n",
       "   0.7061634063720703,\n",
       "   0.7220367789268494,\n",
       "   0.7065849304199219,\n",
       "   0.7113136053085327,\n",
       "   0.6831315755844116,\n",
       "   0.695127010345459,\n",
       "   0.7084171772003174,\n",
       "   0.6900650262832642,\n",
       "   0.7352012395858765,\n",
       "   0.7030156850814819,\n",
       "   0.7003838419914246,\n",
       "   0.6934482455253601,\n",
       "   0.723789632320404,\n",
       "   0.7212663292884827,\n",
       "   0.7062519192695618,\n",
       "   0.6833934783935547,\n",
       "   0.7137258648872375,\n",
       "   0.7216011881828308,\n",
       "   0.6627152562141418,\n",
       "   0.6636045575141907,\n",
       "   0.7184362411499023,\n",
       "   0.7322521209716797,\n",
       "   0.6513694524765015,\n",
       "   0.6913195848464966,\n",
       "   0.7090703248977661,\n",
       "   0.6862189769744873,\n",
       "   0.7327260971069336,\n",
       "   0.7166357040405273,\n",
       "   0.693891167640686,\n",
       "   0.6802839040756226,\n",
       "   0.681128740310669,\n",
       "   0.6763545870780945,\n",
       "   0.7287154793739319,\n",
       "   0.6999345421791077,\n",
       "   0.6897455453872681,\n",
       "   0.6833697557449341,\n",
       "   0.6878894567489624,\n",
       "   0.7085579633712769,\n",
       "   0.6674132347106934,\n",
       "   0.6625957489013672,\n",
       "   0.7069647908210754,\n",
       "   0.695060133934021,\n",
       "   0.7144134640693665,\n",
       "   0.6787617206573486,\n",
       "   0.6903839111328125,\n",
       "   0.6507670879364014,\n",
       "   0.6747340559959412,\n",
       "   0.6672892570495605,\n",
       "   0.6832049489021301,\n",
       "   0.6502271890640259,\n",
       "   0.6823880076408386,\n",
       "   0.658001184463501,\n",
       "   0.6665125489234924,\n",
       "   0.637432873249054,\n",
       "   0.6652963757514954,\n",
       "   0.6344415545463562,\n",
       "   0.6891552805900574,\n",
       "   0.6709223985671997,\n",
       "   0.6639212965965271,\n",
       "   0.6533164381980896,\n",
       "   0.6770921349525452,\n",
       "   0.6672019362449646,\n",
       "   0.6642085909843445,\n",
       "   0.638679563999176,\n",
       "   0.6654900908470154,\n",
       "   0.6860989332199097,\n",
       "   0.6603283882141113,\n",
       "   0.6894799470901489,\n",
       "   0.6508340835571289,\n",
       "   0.6691118478775024,\n",
       "   0.6405099034309387,\n",
       "   0.6815701723098755,\n",
       "   0.6944645643234253,\n",
       "   0.6448778510093689,\n",
       "   0.7006874680519104,\n",
       "   0.6834824681282043,\n",
       "   0.6616033911705017,\n",
       "   0.651789665222168,\n",
       "   0.638949453830719,\n",
       "   0.624962568283081,\n",
       "   0.6540924310684204,\n",
       "   0.6591857671737671,\n",
       "   0.644838809967041,\n",
       "   0.6732317805290222,\n",
       "   0.6399489641189575,\n",
       "   0.6417975425720215,\n",
       "   0.6395642757415771,\n",
       "   0.6308483481407166,\n",
       "   0.6105254292488098,\n",
       "   0.6668016314506531,\n",
       "   0.6351290345191956,\n",
       "   0.67182457447052,\n",
       "   0.6474602818489075,\n",
       "   0.6201475262641907,\n",
       "   0.6612284779548645,\n",
       "   0.637911319732666,\n",
       "   0.6458994150161743,\n",
       "   0.6492271423339844,\n",
       "   0.6115065813064575,\n",
       "   0.6658450365066528,\n",
       "   0.6549767255783081,\n",
       "   0.6406829953193665,\n",
       "   0.6951674818992615,\n",
       "   0.6447644233703613,\n",
       "   0.6649085879325867,\n",
       "   0.6565535664558411,\n",
       "   0.6558566689491272,\n",
       "   0.626218318939209,\n",
       "   0.6131512522697449,\n",
       "   0.6493192911148071,\n",
       "   0.6896823644638062,\n",
       "   0.6950517892837524,\n",
       "   0.620136559009552,\n",
       "   0.668464183807373,\n",
       "   0.6212270855903625,\n",
       "   0.6276745796203613,\n",
       "   0.6086710691452026,\n",
       "   0.6354148983955383,\n",
       "   0.6326656341552734,\n",
       "   0.6466922163963318,\n",
       "   0.598374605178833,\n",
       "   0.6668847799301147,\n",
       "   0.6262540817260742,\n",
       "   0.6250651478767395,\n",
       "   0.6781030297279358,\n",
       "   0.638929545879364,\n",
       "   0.6343174576759338,\n",
       "   0.6058835387229919,\n",
       "   0.6628669500350952,\n",
       "   0.6620826125144958,\n",
       "   0.658937931060791,\n",
       "   0.6146105527877808,\n",
       "   0.612954318523407,\n",
       "   0.6262492537498474,\n",
       "   0.6409990191459656,\n",
       "   0.622562825679779,\n",
       "   0.6233885884284973,\n",
       "   0.6240814924240112,\n",
       "   0.5954707860946655,\n",
       "   0.6221778392791748,\n",
       "   0.612397313117981,\n",
       "   0.6284817457199097,\n",
       "   0.607062041759491,\n",
       "   0.6238088011741638,\n",
       "   0.6444390416145325,\n",
       "   0.6248896718025208,\n",
       "   0.6254810094833374,\n",
       "   0.6345654726028442,\n",
       "   0.6309176087379456,\n",
       "   0.6544051170349121,\n",
       "   0.6264909505844116,\n",
       "   0.6518942713737488,\n",
       "   0.6267488598823547,\n",
       "   0.6296420693397522,\n",
       "   0.6197810173034668,\n",
       "   0.6469472646713257,\n",
       "   0.6208273768424988,\n",
       "   0.6645996570587158,\n",
       "   0.6393706798553467,\n",
       "   0.6149213314056396,\n",
       "   0.6259046792984009,\n",
       "   0.6300379633903503,\n",
       "   0.649783730506897,\n",
       "   0.6161519885063171,\n",
       "   0.6318570375442505,\n",
       "   0.6367069482803345,\n",
       "   0.6223811507225037,\n",
       "   0.610604465007782,\n",
       "   0.6274061799049377,\n",
       "   0.5978504419326782,\n",
       "   0.6204630732536316,\n",
       "   0.6318091750144958,\n",
       "   0.5962499380111694,\n",
       "   0.622136116027832,\n",
       "   0.6438496708869934,\n",
       "   0.6358330845832825,\n",
       "   0.6510936617851257,\n",
       "   0.6125815510749817,\n",
       "   0.5982135534286499,\n",
       "   0.6114051938056946,\n",
       "   0.6191065907478333,\n",
       "   0.606960654258728,\n",
       "   0.5920683145523071,\n",
       "   0.6176088452339172,\n",
       "   0.6380374431610107,\n",
       "   0.6128801703453064,\n",
       "   0.5773934125900269,\n",
       "   0.5835410952568054,\n",
       "   0.6259323954582214,\n",
       "   0.6083500385284424,\n",
       "   0.6173248291015625,\n",
       "   0.6171770691871643,\n",
       "   0.6031194925308228,\n",
       "   0.6126096844673157,\n",
       "   0.5755534172058105,\n",
       "   0.5836353302001953,\n",
       "   0.5823783874511719,\n",
       "   0.6198265552520752,\n",
       "   0.6084814667701721,\n",
       "   0.6329626441001892,\n",
       "   0.6186364889144897,\n",
       "   0.5996260643005371,\n",
       "   0.5991470813751221,\n",
       "   0.59963458776474,\n",
       "   0.594265341758728,\n",
       "   0.5977978706359863,\n",
       "   0.6114855408668518,\n",
       "   0.5701649188995361,\n",
       "   0.5663043260574341,\n",
       "   0.6041309833526611,\n",
       "   0.5637216567993164,\n",
       "   0.5695796012878418,\n",
       "   0.6021676659584045,\n",
       "   0.6041580438613892,\n",
       "   0.5621060729026794,\n",
       "   0.6073516011238098,\n",
       "   0.5728567838668823,\n",
       "   0.6047510504722595,\n",
       "   0.5505672693252563,\n",
       "   0.5962796807289124,\n",
       "   0.5995914936065674,\n",
       "   0.5875142812728882,\n",
       "   0.5900792479515076,\n",
       "   0.6286436319351196,\n",
       "   0.5802341103553772,\n",
       "   0.6018850803375244,\n",
       "   0.6114375591278076,\n",
       "   0.6103017926216125,\n",
       "   0.5715897083282471,\n",
       "   0.6063934564590454,\n",
       "   0.5963401794433594,\n",
       "   0.5812996625900269,\n",
       "   0.5830344557762146,\n",
       "   0.5907019972801208],\n",
       "  'train_voc_loss': [0.9043201208114624,\n",
       "   1.215969204902649,\n",
       "   0.9755975008010864,\n",
       "   0.9343646764755249,\n",
       "   0.9003124237060547,\n",
       "   0.8385628461837769,\n",
       "   0.8773842453956604,\n",
       "   0.986173152923584,\n",
       "   0.9007179141044617,\n",
       "   0.7830036878585815,\n",
       "   0.8632716536521912,\n",
       "   0.7317275404930115,\n",
       "   0.8686960339546204,\n",
       "   0.766214907169342,\n",
       "   0.8408886790275574,\n",
       "   0.796619176864624,\n",
       "   0.8106244206428528,\n",
       "   0.8519912362098694,\n",
       "   0.794655442237854,\n",
       "   0.8061168789863586,\n",
       "   0.9013686776161194,\n",
       "   0.7153119444847107,\n",
       "   0.7860034704208374,\n",
       "   0.7653960585594177,\n",
       "   0.7038272023200989,\n",
       "   0.7222885489463806,\n",
       "   0.8401886820793152,\n",
       "   0.7610639333724976,\n",
       "   0.7452875971794128,\n",
       "   0.7592397332191467,\n",
       "   0.7004240155220032,\n",
       "   0.616645336151123,\n",
       "   0.679275393486023,\n",
       "   0.6463701725006104,\n",
       "   0.7226846814155579,\n",
       "   0.6362530589103699,\n",
       "   0.676022469997406,\n",
       "   0.6773756742477417,\n",
       "   0.6480210423469543,\n",
       "   0.604316771030426,\n",
       "   0.7979298233985901,\n",
       "   0.5848286747932434,\n",
       "   0.6305683255195618,\n",
       "   0.5993587374687195,\n",
       "   0.7128996849060059,\n",
       "   0.693781852722168,\n",
       "   0.6466037631034851,\n",
       "   0.589900553226471,\n",
       "   0.643250048160553,\n",
       "   0.5879764556884766,\n",
       "   0.6896097660064697,\n",
       "   0.5516164302825928,\n",
       "   0.7020820379257202,\n",
       "   0.6581279635429382,\n",
       "   0.6172506809234619,\n",
       "   0.5760244131088257,\n",
       "   0.5271738171577454,\n",
       "   0.5801417827606201,\n",
       "   0.5451101660728455,\n",
       "   0.5279541611671448,\n",
       "   0.596122682094574,\n",
       "   0.5076258778572083,\n",
       "   0.4826042652130127,\n",
       "   0.5595338344573975,\n",
       "   0.5312682390213013,\n",
       "   0.5275431275367737,\n",
       "   0.6006179451942444,\n",
       "   0.5124495029449463,\n",
       "   0.5173662304878235,\n",
       "   0.5209252834320068,\n",
       "   0.5642886161804199,\n",
       "   0.5690098404884338,\n",
       "   0.6552351117134094,\n",
       "   0.5317369699478149,\n",
       "   0.48981159925460815,\n",
       "   0.530092716217041,\n",
       "   0.5831341743469238,\n",
       "   0.6563342809677124,\n",
       "   0.5057286024093628,\n",
       "   0.481862872838974,\n",
       "   0.568281888961792,\n",
       "   0.46433043479919434,\n",
       "   0.5866279602050781,\n",
       "   0.4897752106189728,\n",
       "   0.5374614596366882,\n",
       "   0.49033913016319275,\n",
       "   0.5467858910560608,\n",
       "   0.4413733184337616,\n",
       "   0.5151745080947876,\n",
       "   0.5477212071418762,\n",
       "   0.4588527977466583,\n",
       "   0.528150200843811,\n",
       "   0.5360720753669739,\n",
       "   0.5071286559104919,\n",
       "   0.5212305188179016,\n",
       "   0.4869860112667084,\n",
       "   0.4774863123893738,\n",
       "   0.46561580896377563,\n",
       "   0.47317975759506226,\n",
       "   0.5457326173782349,\n",
       "   0.5205641388893127,\n",
       "   0.47124728560447693,\n",
       "   0.4595177173614502,\n",
       "   0.4791238307952881,\n",
       "   0.4499882757663727,\n",
       "   0.43412280082702637,\n",
       "   0.5468607544898987,\n",
       "   0.4995492994785309,\n",
       "   0.5292243957519531,\n",
       "   0.44116517901420593,\n",
       "   0.5071222186088562,\n",
       "   0.4329603612422943,\n",
       "   0.4451623558998108,\n",
       "   0.5679377913475037,\n",
       "   0.4960574805736542,\n",
       "   0.42458948493003845,\n",
       "   0.5003377199172974,\n",
       "   0.5457754135131836,\n",
       "   0.4236241579055786,\n",
       "   0.5066836476325989,\n",
       "   0.4738976061344147,\n",
       "   0.5099095106124878,\n",
       "   0.44914376735687256,\n",
       "   0.4860963523387909,\n",
       "   0.4424777626991272,\n",
       "   0.43722063302993774,\n",
       "   0.5159408450126648,\n",
       "   0.4511999785900116,\n",
       "   0.4874109923839569,\n",
       "   0.4526560604572296,\n",
       "   0.4409277141094208,\n",
       "   0.4910489618778229,\n",
       "   0.4662756025791168,\n",
       "   0.4323596954345703,\n",
       "   0.39153921604156494,\n",
       "   0.5099127888679504,\n",
       "   0.49476733803749084,\n",
       "   0.4244060218334198,\n",
       "   0.42553773522377014,\n",
       "   0.4617757797241211,\n",
       "   0.4603008031845093,\n",
       "   0.3681984841823578,\n",
       "   0.4942185580730438,\n",
       "   0.4142262935638428,\n",
       "   0.37312552332878113,\n",
       "   0.4196510314941406,\n",
       "   0.4104938805103302,\n",
       "   0.45878270268440247,\n",
       "   0.38238832354545593,\n",
       "   0.391449898481369,\n",
       "   0.4156913459300995,\n",
       "   0.37841159105300903,\n",
       "   0.4061356484889984,\n",
       "   0.39394673705101013,\n",
       "   0.4562235176563263,\n",
       "   0.3654620349407196,\n",
       "   0.38626447319984436,\n",
       "   0.4053962528705597,\n",
       "   0.3959636390209198,\n",
       "   0.44704845547676086,\n",
       "   0.40135514736175537,\n",
       "   0.3582170903682709,\n",
       "   0.3271734416484833,\n",
       "   0.42952001094818115,\n",
       "   0.47475558519363403,\n",
       "   0.4131483733654022,\n",
       "   0.43890872597694397,\n",
       "   0.3792419135570526,\n",
       "   0.36998096108436584,\n",
       "   0.3936860263347626,\n",
       "   0.3812233805656433,\n",
       "   0.4248017370700836,\n",
       "   0.3966689109802246,\n",
       "   0.3457011580467224,\n",
       "   0.3836381137371063,\n",
       "   0.39030125737190247,\n",
       "   0.39042115211486816,\n",
       "   0.4062974452972412,\n",
       "   0.41435757279396057,\n",
       "   0.3926286995410919,\n",
       "   0.39092859625816345,\n",
       "   0.3656150698661804,\n",
       "   0.37735316157341003,\n",
       "   0.3857884407043457,\n",
       "   0.36258506774902344,\n",
       "   0.3606381118297577,\n",
       "   0.32355377078056335,\n",
       "   0.4040152132511139,\n",
       "   0.3638007938861847,\n",
       "   0.3889394998550415,\n",
       "   0.3387959897518158,\n",
       "   0.3696925640106201,\n",
       "   0.3618578612804413,\n",
       "   0.3349660038948059,\n",
       "   0.3586720824241638,\n",
       "   0.37986135482788086,\n",
       "   0.37138739228248596,\n",
       "   0.2965361773967743,\n",
       "   0.35080140829086304,\n",
       "   0.3558920919895172,\n",
       "   0.3685053288936615,\n",
       "   0.36889833211898804,\n",
       "   0.37281468510627747,\n",
       "   0.36417990922927856,\n",
       "   0.32962191104888916,\n",
       "   0.36241036653518677,\n",
       "   0.34963759779930115,\n",
       "   0.31525641679763794,\n",
       "   0.35344064235687256,\n",
       "   0.3027505874633789,\n",
       "   0.34231576323509216,\n",
       "   0.30712535977363586,\n",
       "   0.3804333209991455,\n",
       "   0.3833429515361786,\n",
       "   0.32927289605140686,\n",
       "   0.3540981411933899,\n",
       "   0.33328378200531006,\n",
       "   0.3344566226005554,\n",
       "   0.3027898967266083,\n",
       "   0.3892180919647217,\n",
       "   0.2998155653476715,\n",
       "   0.32719162106513977,\n",
       "   0.33900269865989685,\n",
       "   0.3264642357826233,\n",
       "   0.31821221113204956,\n",
       "   0.3371058404445648,\n",
       "   0.29060280323028564,\n",
       "   0.3506733477115631,\n",
       "   0.3005976378917694,\n",
       "   0.3150802254676819,\n",
       "   0.3342452943325043,\n",
       "   0.33982962369918823,\n",
       "   0.2826041281223297,\n",
       "   0.3106267750263214,\n",
       "   0.2907268702983856,\n",
       "   0.31175583600997925,\n",
       "   0.29846078157424927,\n",
       "   0.31506606936454773,\n",
       "   0.3253178000450134,\n",
       "   0.30534812808036804,\n",
       "   0.32094502449035645,\n",
       "   0.3175104558467865,\n",
       "   0.3439236879348755,\n",
       "   0.26847171783447266,\n",
       "   0.30001601576805115,\n",
       "   0.3053869903087616,\n",
       "   0.32068854570388794,\n",
       "   0.33518943190574646,\n",
       "   0.3276417851448059,\n",
       "   0.26654377579689026,\n",
       "   0.2580503225326538,\n",
       "   0.2806674838066101,\n",
       "   0.2526799738407135,\n",
       "   0.2902253568172455,\n",
       "   0.2929072082042694,\n",
       "   0.26778340339660645,\n",
       "   0.2774239778518677,\n",
       "   0.30022525787353516,\n",
       "   0.3388216197490692,\n",
       "   0.28691256046295166,\n",
       "   0.2783738374710083,\n",
       "   0.3141610622406006,\n",
       "   0.29778242111206055,\n",
       "   0.2724444270133972,\n",
       "   0.3105451464653015,\n",
       "   0.29123741388320923,\n",
       "   0.3077823519706726,\n",
       "   0.2593159079551697,\n",
       "   0.30322861671447754,\n",
       "   0.25623956322669983,\n",
       "   0.3134972155094147,\n",
       "   0.30547401309013367,\n",
       "   0.3028019070625305,\n",
       "   0.31682559847831726,\n",
       "   0.24672141671180725,\n",
       "   0.29411646723747253,\n",
       "   0.29117757081985474,\n",
       "   0.2961497902870178,\n",
       "   0.2893768548965454,\n",
       "   0.25705593824386597,\n",
       "   0.251059353351593,\n",
       "   0.28992098569869995,\n",
       "   0.23446819186210632,\n",
       "   0.27197062969207764,\n",
       "   0.283268541097641,\n",
       "   0.2777993381023407,\n",
       "   0.24594277143478394,\n",
       "   0.24556316435337067,\n",
       "   0.2673497498035431,\n",
       "   0.2527265250682831,\n",
       "   0.29515761137008667,\n",
       "   0.23562195897102356,\n",
       "   0.28988203406333923,\n",
       "   0.2871323823928833,\n",
       "   0.2742668688297272,\n",
       "   0.2359476387500763,\n",
       "   0.27477502822875977,\n",
       "   0.22098496556282043,\n",
       "   0.2928851544857025,\n",
       "   0.265335351228714,\n",
       "   0.2583271563053131,\n",
       "   0.25624844431877136,\n",
       "   0.2707501947879791,\n",
       "   0.2531355619430542,\n",
       "   0.3109351098537445,\n",
       "   0.2515103220939636,\n",
       "   0.26456722617149353,\n",
       "   0.22602298855781555,\n",
       "   0.25895801186561584,\n",
       "   0.279531329870224,\n",
       "   0.27447080612182617,\n",
       "   0.27092427015304565,\n",
       "   0.2551811635494232,\n",
       "   0.24725186824798584,\n",
       "   0.23014123737812042,\n",
       "   0.2672255039215088,\n",
       "   0.23301048576831818,\n",
       "   0.2564536929130554,\n",
       "   0.21727634966373444,\n",
       "   0.2575380206108093,\n",
       "   0.28577274084091187,\n",
       "   0.2796550989151001,\n",
       "   0.22459447383880615,\n",
       "   0.2690885066986084,\n",
       "   0.23466137051582336,\n",
       "   0.2774633765220642,\n",
       "   0.27262088656425476,\n",
       "   0.20558136701583862,\n",
       "   0.24679963290691376,\n",
       "   0.2428847849369049,\n",
       "   0.24183616042137146,\n",
       "   0.27861103415489197,\n",
       "   0.23608621954917908,\n",
       "   0.21638473868370056,\n",
       "   0.2401738315820694,\n",
       "   0.25716444849967957,\n",
       "   0.246451735496521,\n",
       "   0.2155347317457199,\n",
       "   0.24328383803367615,\n",
       "   0.21185573935508728,\n",
       "   0.2364555448293686,\n",
       "   0.24401119351387024,\n",
       "   0.22872336208820343,\n",
       "   0.22503353655338287,\n",
       "   0.2304939329624176,\n",
       "   0.26046842336654663,\n",
       "   0.22599166631698608,\n",
       "   0.24926790595054626,\n",
       "   0.2139270305633545,\n",
       "   0.23805268108844757,\n",
       "   0.24821510910987854,\n",
       "   0.22695185244083405,\n",
       "   0.20014941692352295,\n",
       "   0.23754547536373138,\n",
       "   0.22525060176849365,\n",
       "   0.21892638504505157,\n",
       "   0.21242141723632812,\n",
       "   0.261959969997406,\n",
       "   0.24105292558670044,\n",
       "   0.25140321254730225,\n",
       "   0.22350715100765228,\n",
       "   0.23488052189350128,\n",
       "   0.21292097866535187,\n",
       "   0.20630860328674316,\n",
       "   0.22971510887145996,\n",
       "   0.23508022725582123,\n",
       "   0.21811170876026154,\n",
       "   0.21860380470752716,\n",
       "   0.23581326007843018,\n",
       "   0.2161836326122284,\n",
       "   0.20807883143424988,\n",
       "   0.18893678486347198,\n",
       "   0.20752273499965668,\n",
       "   0.23544231057167053,\n",
       "   0.19697880744934082,\n",
       "   0.21327504515647888,\n",
       "   0.19042833149433136,\n",
       "   0.19594503939151764,\n",
       "   0.22567886114120483,\n",
       "   0.23023255169391632,\n",
       "   0.2051234394311905,\n",
       "   0.22435517609119415,\n",
       "   0.21771731972694397,\n",
       "   0.20409610867500305,\n",
       "   0.21077995002269745,\n",
       "   0.21849800646305084,\n",
       "   0.22318722307682037,\n",
       "   0.19492018222808838,\n",
       "   0.21365511417388916,\n",
       "   0.19161498546600342,\n",
       "   0.22642283141613007,\n",
       "   0.1747123748064041,\n",
       "   0.20439539849758148,\n",
       "   0.22262760996818542,\n",
       "   0.20301921665668488,\n",
       "   0.2189023345708847,\n",
       "   0.174447700381279,\n",
       "   0.19662129878997803,\n",
       "   0.20509864389896393,\n",
       "   0.19389288127422333,\n",
       "   0.223749577999115,\n",
       "   0.1930587738752365,\n",
       "   0.20507122576236725,\n",
       "   0.1857486516237259,\n",
       "   0.20424556732177734,\n",
       "   0.20951780676841736,\n",
       "   0.22177866101264954,\n",
       "   0.2183704674243927,\n",
       "   0.22643738985061646,\n",
       "   0.19727708399295807,\n",
       "   0.2028142511844635,\n",
       "   0.18385420739650726,\n",
       "   0.17394894361495972,\n",
       "   0.20762957632541656,\n",
       "   0.19548490643501282,\n",
       "   0.20016276836395264,\n",
       "   0.20281819999217987,\n",
       "   0.19512096047401428,\n",
       "   0.19605900347232819,\n",
       "   0.21450647711753845,\n",
       "   0.20406785607337952,\n",
       "   0.20826201140880585,\n",
       "   0.1856616884469986,\n",
       "   0.21738937497138977,\n",
       "   0.19837288558483124,\n",
       "   0.17960788309574127,\n",
       "   0.19960594177246094,\n",
       "   0.20209981501102448,\n",
       "   0.2009783536195755,\n",
       "   0.2069741189479828,\n",
       "   0.17910678684711456,\n",
       "   0.18767350912094116,\n",
       "   0.2058158665895462,\n",
       "   0.1775890588760376,\n",
       "   0.1928613781929016,\n",
       "   0.21976251900196075,\n",
       "   0.19073991477489471,\n",
       "   0.1883453130722046,\n",
       "   0.16549445688724518,\n",
       "   0.18092657625675201,\n",
       "   0.17585904896259308,\n",
       "   0.20053428411483765,\n",
       "   0.18355411291122437,\n",
       "   0.19338421523571014,\n",
       "   0.18438860774040222,\n",
       "   0.18474215269088745,\n",
       "   0.1753402054309845,\n",
       "   0.1789788454771042,\n",
       "   0.19658653438091278,\n",
       "   0.17354610562324524,\n",
       "   0.18621088564395905,\n",
       "   0.16659986972808838,\n",
       "   0.18457356095314026,\n",
       "   0.17678499221801758,\n",
       "   0.17179323732852936,\n",
       "   0.197229266166687,\n",
       "   0.19939595460891724,\n",
       "   0.198438361287117,\n",
       "   0.18098565936088562,\n",
       "   0.1816982924938202,\n",
       "   0.19593878090381622,\n",
       "   0.17959463596343994,\n",
       "   0.1821122169494629,\n",
       "   0.18684324622154236,\n",
       "   0.16804857552051544,\n",
       "   0.19591741263866425,\n",
       "   0.17387238144874573,\n",
       "   0.18614651262760162,\n",
       "   0.17515811324119568,\n",
       "   0.1920800507068634,\n",
       "   0.1834864616394043,\n",
       "   0.16909393668174744,\n",
       "   0.1698702871799469,\n",
       "   0.18066072463989258,\n",
       "   0.1705051213502884,\n",
       "   0.1724289059638977,\n",
       "   0.18152929842472076,\n",
       "   0.17905189096927643,\n",
       "   0.18052148818969727,\n",
       "   0.17652468383312225,\n",
       "   0.1934506893157959,\n",
       "   0.1898171305656433,\n",
       "   0.1455613076686859,\n",
       "   0.19862845540046692,\n",
       "   0.21415802836418152,\n",
       "   0.17308075726032257,\n",
       "   0.18131856620311737,\n",
       "   0.18644936382770538,\n",
       "   0.1687472015619278,\n",
       "   0.16960439085960388,\n",
       "   0.15771310031414032,\n",
       "   0.17550857365131378,\n",
       "   0.16781772673130035,\n",
       "   0.15063506364822388,\n",
       "   0.19764338433742523,\n",
       "   0.16977372765541077,\n",
       "   0.17267274856567383,\n",
       "   0.19679349660873413,\n",
       "   0.17461039125919342,\n",
       "   0.1792982965707779],\n",
       "  'train_jsc_loss': [0.585832417011261,\n",
       "   0.5888107419013977,\n",
       "   0.5486746430397034,\n",
       "   0.5757713317871094,\n",
       "   0.5306047201156616,\n",
       "   0.5031160712242126,\n",
       "   0.5309052467346191,\n",
       "   0.5449895858764648,\n",
       "   0.46966853737831116,\n",
       "   0.6122630834579468,\n",
       "   0.5251622200012207,\n",
       "   0.468963623046875,\n",
       "   0.5813408493995667,\n",
       "   0.47970137000083923,\n",
       "   0.4713711440563202,\n",
       "   0.5087366104125977,\n",
       "   0.464043527841568,\n",
       "   0.5790756344795227,\n",
       "   0.5202822685241699,\n",
       "   0.47114306688308716,\n",
       "   0.4139106571674347,\n",
       "   0.5126024484634399,\n",
       "   0.4893556237220764,\n",
       "   0.4857557415962219,\n",
       "   0.4795856177806854,\n",
       "   0.49521875381469727,\n",
       "   0.41057172417640686,\n",
       "   0.415046751499176,\n",
       "   0.4115854501724243,\n",
       "   0.4539722502231598,\n",
       "   0.4225797653198242,\n",
       "   0.49311381578445435,\n",
       "   0.506583571434021,\n",
       "   0.488934189081192,\n",
       "   0.4586818516254425,\n",
       "   0.47242140769958496,\n",
       "   0.4760284423828125,\n",
       "   0.4789818227291107,\n",
       "   0.3981476426124573,\n",
       "   0.4251990020275116,\n",
       "   0.4459065794944763,\n",
       "   0.4422586262226105,\n",
       "   0.5266619324684143,\n",
       "   0.4504835903644562,\n",
       "   0.4013158679008484,\n",
       "   0.4725702404975891,\n",
       "   0.4682372808456421,\n",
       "   0.4586527347564697,\n",
       "   0.4154323637485504,\n",
       "   0.45391157269477844,\n",
       "   0.45415133237838745,\n",
       "   0.47135859727859497,\n",
       "   0.42379137873649597,\n",
       "   0.49367016553878784,\n",
       "   0.3910135328769684,\n",
       "   0.4415133595466614,\n",
       "   0.40273743867874146,\n",
       "   0.43174394965171814,\n",
       "   0.3918726146221161,\n",
       "   0.451895147562027,\n",
       "   0.38399413228034973,\n",
       "   0.32878920435905457,\n",
       "   0.358176052570343,\n",
       "   0.41054263710975647,\n",
       "   0.36971163749694824,\n",
       "   0.4275481700897217,\n",
       "   0.36570584774017334,\n",
       "   0.43233931064605713,\n",
       "   0.37213146686553955,\n",
       "   0.46229788661003113,\n",
       "   0.39378851652145386,\n",
       "   0.3859316110610962,\n",
       "   0.3567880690097809,\n",
       "   0.33571362495422363,\n",
       "   0.3710290491580963,\n",
       "   0.3940167725086212,\n",
       "   0.3932466208934784,\n",
       "   0.4014792740345001,\n",
       "   0.396878719329834,\n",
       "   0.38371211290359497,\n",
       "   0.3771362006664276,\n",
       "   0.38071972131729126,\n",
       "   0.3905797600746155,\n",
       "   0.4528113007545471,\n",
       "   0.3734683692455292,\n",
       "   0.37236377596855164,\n",
       "   0.40822699666023254,\n",
       "   0.3587520718574524,\n",
       "   0.39390063285827637,\n",
       "   0.3520773947238922,\n",
       "   0.3525000214576721,\n",
       "   0.36222323775291443,\n",
       "   0.37414470314979553,\n",
       "   0.37422072887420654,\n",
       "   0.37766745686531067,\n",
       "   0.39264625310897827,\n",
       "   0.3735947012901306,\n",
       "   0.3555079400539398,\n",
       "   0.38904711604118347,\n",
       "   0.3760329484939575,\n",
       "   0.3265644311904907,\n",
       "   0.3709445595741272,\n",
       "   0.351873517036438,\n",
       "   0.3822263181209564,\n",
       "   0.3882592022418976,\n",
       "   0.40197187662124634,\n",
       "   0.37552550435066223,\n",
       "   0.3418768644332886,\n",
       "   0.39254653453826904,\n",
       "   0.4113033711910248,\n",
       "   0.3914352357387543,\n",
       "   0.37433892488479614,\n",
       "   0.3521692752838135,\n",
       "   0.3693818151950836,\n",
       "   0.3642328977584839,\n",
       "   0.3327448070049286,\n",
       "   0.39026179909706116,\n",
       "   0.3608628511428833,\n",
       "   0.35325825214385986,\n",
       "   0.39963558316230774,\n",
       "   0.3981876075267792,\n",
       "   0.38578400015830994,\n",
       "   0.3927437663078308,\n",
       "   0.34798476099967957,\n",
       "   0.36828598380088806,\n",
       "   0.3174007833003998,\n",
       "   0.343760222196579,\n",
       "   0.3364173471927643,\n",
       "   0.3588370382785797,\n",
       "   0.3950728178024292,\n",
       "   0.3870375454425812,\n",
       "   0.36094385385513306,\n",
       "   0.3417179584503174,\n",
       "   0.3491182327270508,\n",
       "   0.3829938769340515,\n",
       "   0.37651529908180237,\n",
       "   0.3501714766025543,\n",
       "   0.3362759053707123,\n",
       "   0.3876715898513794,\n",
       "   0.3862549066543579,\n",
       "   0.3526415526866913,\n",
       "   0.33764535188674927,\n",
       "   0.3674633204936981,\n",
       "   0.3233787417411804,\n",
       "   0.3652910888195038,\n",
       "   0.3753807842731476,\n",
       "   0.3572467267513275,\n",
       "   0.31663620471954346,\n",
       "   0.36497557163238525,\n",
       "   0.33156436681747437,\n",
       "   0.3744819760322571,\n",
       "   0.3575013279914856,\n",
       "   0.39208364486694336,\n",
       "   0.36994585394859314,\n",
       "   0.3630638122558594,\n",
       "   0.3225224018096924,\n",
       "   0.35470426082611084,\n",
       "   0.31873488426208496,\n",
       "   0.35859784483909607,\n",
       "   0.3740205466747284,\n",
       "   0.35730040073394775,\n",
       "   0.34927991032600403,\n",
       "   0.3154827058315277,\n",
       "   0.3498035669326782,\n",
       "   0.37090930342674255,\n",
       "   0.3161299228668213,\n",
       "   0.36401835083961487,\n",
       "   0.3424561321735382,\n",
       "   0.3613295555114746,\n",
       "   0.3287063241004944,\n",
       "   0.3861163258552551,\n",
       "   0.32307469844818115,\n",
       "   0.3839032053947449,\n",
       "   0.3377450704574585,\n",
       "   0.33169761300086975,\n",
       "   0.3425852656364441,\n",
       "   0.3128374516963959,\n",
       "   0.3487407863140106,\n",
       "   0.3550899624824524,\n",
       "   0.3130156993865967,\n",
       "   0.34473660588264465,\n",
       "   0.3303106725215912,\n",
       "   0.3332562744617462,\n",
       "   0.37639549374580383,\n",
       "   0.3196690082550049,\n",
       "   0.3388560712337494,\n",
       "   0.34764182567596436,\n",
       "   0.3601113259792328,\n",
       "   0.36567333340644836,\n",
       "   0.35670799016952515,\n",
       "   0.3490947186946869,\n",
       "   0.34270769357681274,\n",
       "   0.37897852063179016,\n",
       "   0.3335835635662079,\n",
       "   0.34132182598114014,\n",
       "   0.33997273445129395,\n",
       "   0.345929890871048,\n",
       "   0.3284507095813751,\n",
       "   0.3423451781272888,\n",
       "   0.3479117751121521,\n",
       "   0.3520582914352417,\n",
       "   0.37579935789108276,\n",
       "   0.3395329415798187,\n",
       "   0.34879574179649353,\n",
       "   0.32090288400650024,\n",
       "   0.336730420589447,\n",
       "   0.33325353264808655,\n",
       "   0.33211517333984375,\n",
       "   0.32345131039619446,\n",
       "   0.35914695262908936,\n",
       "   0.3387586176395416,\n",
       "   0.33376383781433105,\n",
       "   0.3574824631214142,\n",
       "   0.31957629323005676,\n",
       "   0.33754828572273254,\n",
       "   0.33129650354385376,\n",
       "   0.32608726620674133,\n",
       "   0.34892192482948303,\n",
       "   0.36009886860847473,\n",
       "   0.3405904173851013,\n",
       "   0.3060816824436188,\n",
       "   0.33862754702568054,\n",
       "   0.3362327218055725,\n",
       "   0.3515497148036957,\n",
       "   0.3585367202758789,\n",
       "   0.32914310693740845,\n",
       "   0.33847591280937195,\n",
       "   0.3276807963848114,\n",
       "   0.3176167607307434,\n",
       "   0.3228084146976471,\n",
       "   0.29874172806739807,\n",
       "   0.36198627948760986,\n",
       "   0.3605395555496216,\n",
       "   0.3101860284805298,\n",
       "   0.3009306490421295,\n",
       "   0.3015827238559723,\n",
       "   0.33015191555023193,\n",
       "   0.3132348358631134,\n",
       "   0.32397618889808655,\n",
       "   0.32011911273002625,\n",
       "   0.30225497484207153,\n",
       "   0.3158642053604126,\n",
       "   0.30647578835487366,\n",
       "   0.30134737491607666,\n",
       "   0.32419583201408386,\n",
       "   0.3130893409252167,\n",
       "   0.33553117513656616,\n",
       "   0.31231313943862915,\n",
       "   0.3243256211280823,\n",
       "   0.3320353329181671,\n",
       "   0.32514819502830505,\n",
       "   0.2932032346725464,\n",
       "   0.3387719988822937,\n",
       "   0.28657570481300354,\n",
       "   0.325288861989975,\n",
       "   0.30967411398887634,\n",
       "   0.28988540172576904,\n",
       "   0.29625698924064636,\n",
       "   0.32179224491119385,\n",
       "   0.35443535447120667,\n",
       "   0.2977779805660248,\n",
       "   0.30391475558280945,\n",
       "   0.3171035945415497,\n",
       "   0.3040594756603241,\n",
       "   0.30552732944488525,\n",
       "   0.3283213675022125,\n",
       "   0.307108610868454,\n",
       "   0.3110969364643097,\n",
       "   0.2971051335334778,\n",
       "   0.303188681602478,\n",
       "   0.31811702251434326,\n",
       "   0.3746129274368286,\n",
       "   0.31995469331741333,\n",
       "   0.2929387092590332,\n",
       "   0.3472234308719635,\n",
       "   0.32769840955734253,\n",
       "   0.30318236351013184,\n",
       "   0.3388899862766266,\n",
       "   0.30120497941970825,\n",
       "   0.31633615493774414,\n",
       "   0.3369095027446747,\n",
       "   0.3088003098964691,\n",
       "   0.29706230759620667,\n",
       "   0.30663421750068665,\n",
       "   0.32259926199913025,\n",
       "   0.31954425573349,\n",
       "   0.304524689912796,\n",
       "   0.29311129450798035,\n",
       "   0.31135591864585876,\n",
       "   0.3034956455230713,\n",
       "   0.2940727472305298,\n",
       "   0.29443109035491943,\n",
       "   0.32199278473854065,\n",
       "   0.3089112341403961,\n",
       "   0.2897663116455078,\n",
       "   0.3236658275127411,\n",
       "   0.30188363790512085,\n",
       "   0.3124449849128723,\n",
       "   0.29204604029655457,\n",
       "   0.2927294969558716,\n",
       "   0.2843415439128876,\n",
       "   0.2989557385444641,\n",
       "   0.30204471945762634,\n",
       "   0.29810890555381775,\n",
       "   0.2918265163898468,\n",
       "   0.3056071698665619,\n",
       "   0.2954958975315094,\n",
       "   0.31189343333244324,\n",
       "   0.27769264578819275,\n",
       "   0.29973408579826355,\n",
       "   0.3080318868160248,\n",
       "   0.31624236702919006,\n",
       "   0.2956506609916687,\n",
       "   0.2971110939979553,\n",
       "   0.2712280750274658,\n",
       "   0.2862524390220642,\n",
       "   0.2935561537742615,\n",
       "   0.28245455026626587,\n",
       "   0.2871675491333008,\n",
       "   0.29337242245674133,\n",
       "   0.2974931001663208,\n",
       "   0.29460301995277405,\n",
       "   0.30780380964279175,\n",
       "   0.3157418966293335,\n",
       "   0.29561352729797363,\n",
       "   0.27894580364227295,\n",
       "   0.3097746968269348,\n",
       "   0.30888015031814575,\n",
       "   0.28350767493247986,\n",
       "   0.30612242221832275,\n",
       "   0.28671154379844666,\n",
       "   0.31392955780029297,\n",
       "   0.2842307388782501,\n",
       "   0.30158302187919617,\n",
       "   0.2937355041503906,\n",
       "   0.2803243100643158,\n",
       "   0.30006253719329834,\n",
       "   0.30312490463256836,\n",
       "   0.3084108531475067,\n",
       "   0.2892487645149231,\n",
       "   0.27737337350845337,\n",
       "   0.3032837510108948,\n",
       "   0.2900097072124481,\n",
       "   0.2943231165409088,\n",
       "   0.3092035949230194,\n",
       "   0.303541898727417,\n",
       "   0.28808140754699707,\n",
       "   0.29506704211235046,\n",
       "   0.2969512939453125,\n",
       "   0.30399787425994873,\n",
       "   0.28286606073379517,\n",
       "   0.3041593134403229,\n",
       "   0.2950151264667511,\n",
       "   0.29909101128578186,\n",
       "   0.29377588629722595,\n",
       "   0.29952892661094666,\n",
       "   0.286639541387558,\n",
       "   0.29024940729141235,\n",
       "   0.3016487658023834,\n",
       "   0.3037305474281311,\n",
       "   0.2989143133163452,\n",
       "   0.2939314544200897,\n",
       "   0.3062876760959625,\n",
       "   0.2708131670951843,\n",
       "   0.30480602383613586,\n",
       "   0.30740517377853394,\n",
       "   0.2615755498409271,\n",
       "   0.29288679361343384,\n",
       "   0.2941831052303314,\n",
       "   0.2893148362636566,\n",
       "   0.31067559123039246,\n",
       "   0.29505133628845215,\n",
       "   0.2854776382446289,\n",
       "   0.29731354117393494,\n",
       "   0.2607802748680115,\n",
       "   0.28048086166381836,\n",
       "   0.2984260022640228,\n",
       "   0.27740713953971863,\n",
       "   0.2829836308956146,\n",
       "   0.2825266420841217,\n",
       "   0.28709572553634644,\n",
       "   0.26924359798431396,\n",
       "   0.26794248819351196,\n",
       "   0.275367796421051,\n",
       "   0.2756391167640686,\n",
       "   0.2812096178531647,\n",
       "   0.2886652648448944,\n",
       "   0.2683674693107605,\n",
       "   0.26591408252716064,\n",
       "   0.2762283682823181,\n",
       "   0.2882903218269348,\n",
       "   0.28400954604148865,\n",
       "   0.28372427821159363,\n",
       "   0.2823333740234375,\n",
       "   0.2981618344783783,\n",
       "   0.2836211919784546,\n",
       "   0.2678132653236389,\n",
       "   0.28500714898109436,\n",
       "   0.2736479341983795,\n",
       "   0.27311021089553833,\n",
       "   0.28404250741004944,\n",
       "   0.266252338886261,\n",
       "   0.29103562235832214,\n",
       "   0.2883431613445282,\n",
       "   0.24892908334732056,\n",
       "   0.28493204712867737,\n",
       "   0.27556538581848145,\n",
       "   0.2711295187473297,\n",
       "   0.26917755603790283,\n",
       "   0.2788478136062622,\n",
       "   0.2895126938819885,\n",
       "   0.2908070683479309,\n",
       "   0.2850440442562103,\n",
       "   0.2714046239852905,\n",
       "   0.28640443086624146,\n",
       "   0.2925245463848114,\n",
       "   0.2675740420818329,\n",
       "   0.28041189908981323,\n",
       "   0.2822644114494324,\n",
       "   0.28259211778640747,\n",
       "   0.26057738065719604,\n",
       "   0.28741517663002014,\n",
       "   0.26933881640434265,\n",
       "   0.2788439393043518,\n",
       "   0.29260051250457764,\n",
       "   0.2803458571434021,\n",
       "   0.26876839995384216,\n",
       "   0.2624092698097229,\n",
       "   0.2708348035812378,\n",
       "   0.2584119141101837,\n",
       "   0.28088879585266113,\n",
       "   0.2764577269554138,\n",
       "   0.2820857763290405,\n",
       "   0.28714317083358765,\n",
       "   0.26412028074264526,\n",
       "   0.2899356484413147,\n",
       "   0.2797641456127167,\n",
       "   0.2718518078327179,\n",
       "   0.29945454001426697,\n",
       "   0.27944639325141907,\n",
       "   0.30407077074050903,\n",
       "   0.2704728841781616,\n",
       "   0.28129440546035767,\n",
       "   0.28074726462364197,\n",
       "   0.3059394955635071,\n",
       "   0.2604250907897949,\n",
       "   0.26065653562545776,\n",
       "   0.2734137177467346,\n",
       "   0.26706641912460327,\n",
       "   0.2896924316883087,\n",
       "   0.26291945576667786,\n",
       "   0.274989515542984,\n",
       "   0.28295937180519104,\n",
       "   0.2698746621608734,\n",
       "   0.2694126069545746,\n",
       "   0.27071478962898254,\n",
       "   0.28340551257133484,\n",
       "   0.2767540514469147,\n",
       "   0.29876771569252014,\n",
       "   0.27065029740333557,\n",
       "   0.28696826100349426,\n",
       "   0.2537911534309387,\n",
       "   0.2751862406730652,\n",
       "   0.2584017217159271,\n",
       "   0.2752692699432373,\n",
       "   0.288748174905777,\n",
       "   0.2762102782726288,\n",
       "   0.28074726462364197,\n",
       "   0.2691645622253418,\n",
       "   0.24922795593738556,\n",
       "   0.27396780252456665,\n",
       "   0.26202303171157837,\n",
       "   0.2725854516029358,\n",
       "   0.2826814353466034,\n",
       "   0.2784171998500824,\n",
       "   0.25811588764190674,\n",
       "   0.2589265704154968,\n",
       "   0.2781021296977997,\n",
       "   0.2690791189670563,\n",
       "   0.2752954363822937,\n",
       "   0.27145853638648987,\n",
       "   0.2680058479309082,\n",
       "   0.2814391851425171,\n",
       "   0.26095256209373474,\n",
       "   0.2744749188423157,\n",
       "   0.26933014392852783,\n",
       "   0.28169623017311096,\n",
       "   0.24902217090129852,\n",
       "   0.2932504117488861,\n",
       "   0.2760831415653229,\n",
       "   0.2792746126651764,\n",
       "   0.28117120265960693,\n",
       "   0.26790910959243774,\n",
       "   0.2942671477794647,\n",
       "   0.2604579031467438,\n",
       "   0.2515828013420105,\n",
       "   0.27254021167755127,\n",
       "   0.2596125602722168,\n",
       "   0.28687721490859985,\n",
       "   0.2614475190639496],\n",
       "  'train_ff_loss': [0.5009480714797974,\n",
       "   0.5021740794181824,\n",
       "   0.48884090781211853,\n",
       "   0.5206086039543152,\n",
       "   0.4913040101528168,\n",
       "   0.49344825744628906,\n",
       "   0.5213541984558105,\n",
       "   0.5287377238273621,\n",
       "   0.5240587592124939,\n",
       "   0.5061955451965332,\n",
       "   0.48053863644599915,\n",
       "   0.5118865370750427,\n",
       "   0.5099048018455505,\n",
       "   0.5024185180664062,\n",
       "   0.5126197338104248,\n",
       "   0.4779563546180725,\n",
       "   0.5118661522865295,\n",
       "   0.49607327580451965,\n",
       "   0.49098217487335205,\n",
       "   0.49877870082855225,\n",
       "   0.5114498734474182,\n",
       "   0.5061072707176208,\n",
       "   0.48853424191474915,\n",
       "   0.505672037601471,\n",
       "   0.5044031739234924,\n",
       "   0.50035560131073,\n",
       "   0.4875710606575012,\n",
       "   0.5058837532997131,\n",
       "   0.5047544836997986,\n",
       "   0.5190306305885315,\n",
       "   0.49532049894332886,\n",
       "   0.5016722679138184,\n",
       "   0.49384191632270813,\n",
       "   0.49924179911613464,\n",
       "   0.49666115641593933,\n",
       "   0.4945337474346161,\n",
       "   0.5206751227378845,\n",
       "   0.5060299634933472,\n",
       "   0.49545910954475403,\n",
       "   0.5121086835861206,\n",
       "   0.5079644918441772,\n",
       "   0.49449095129966736,\n",
       "   0.5067542195320129,\n",
       "   0.5029675364494324,\n",
       "   0.4982018768787384,\n",
       "   0.5137581825256348,\n",
       "   0.4957726001739502,\n",
       "   0.5007138848304749,\n",
       "   0.49250301718711853,\n",
       "   0.48984232544898987,\n",
       "   0.5051204562187195,\n",
       "   0.4928836226463318,\n",
       "   0.5158967971801758,\n",
       "   0.4978753924369812,\n",
       "   0.5036892294883728,\n",
       "   0.5007779598236084,\n",
       "   0.5046757459640503,\n",
       "   0.5233115553855896,\n",
       "   0.485065221786499,\n",
       "   0.4723617136478424,\n",
       "   0.5036914348602295,\n",
       "   0.4902394413948059,\n",
       "   0.4975663721561432,\n",
       "   0.504011332988739,\n",
       "   0.5071132183074951,\n",
       "   0.49950626492500305,\n",
       "   0.5038647651672363,\n",
       "   0.4920371174812317,\n",
       "   0.4773612320423126,\n",
       "   0.49389520287513733,\n",
       "   0.49080604314804077,\n",
       "   0.491041362285614,\n",
       "   0.5021561980247498,\n",
       "   0.4927859902381897,\n",
       "   0.4937784671783447,\n",
       "   0.4971179962158203,\n",
       "   0.5054703950881958,\n",
       "   0.4915519952774048,\n",
       "   0.4908606708049774,\n",
       "   0.5171149969100952,\n",
       "   0.5000506639480591,\n",
       "   0.48434388637542725,\n",
       "   0.5066758990287781,\n",
       "   0.5174844861030579,\n",
       "   0.5052495002746582,\n",
       "   0.5042930841445923,\n",
       "   0.5004521012306213,\n",
       "   0.5047730803489685,\n",
       "   0.4825372099876404,\n",
       "   0.5009142160415649,\n",
       "   0.48741471767425537,\n",
       "   0.49680444598197937,\n",
       "   0.48525917530059814,\n",
       "   0.5003515481948853,\n",
       "   0.528191864490509,\n",
       "   0.4932369887828827,\n",
       "   0.5009673833847046,\n",
       "   0.48214250802993774,\n",
       "   0.49659135937690735,\n",
       "   0.4855272173881531,\n",
       "   0.494121253490448,\n",
       "   0.5187653303146362,\n",
       "   0.49250608682632446,\n",
       "   0.5011315941810608,\n",
       "   0.48266056180000305,\n",
       "   0.4962610602378845,\n",
       "   0.48850417137145996,\n",
       "   0.5180689692497253,\n",
       "   0.52694171667099,\n",
       "   0.4826294779777527,\n",
       "   0.46415162086486816,\n",
       "   0.508539617061615,\n",
       "   0.4901345670223236,\n",
       "   0.4992978274822235,\n",
       "   0.5099508166313171,\n",
       "   0.494634211063385,\n",
       "   0.47333380579948425,\n",
       "   0.5019336938858032,\n",
       "   0.48879915475845337,\n",
       "   0.49778640270233154,\n",
       "   0.48887327313423157,\n",
       "   0.5033808946609497,\n",
       "   0.498549222946167,\n",
       "   0.49896615743637085,\n",
       "   0.49126213788986206,\n",
       "   0.5012498497962952,\n",
       "   0.5042307376861572,\n",
       "   0.49102139472961426,\n",
       "   0.4844115972518921,\n",
       "   0.48547136783599854,\n",
       "   0.4817401170730591,\n",
       "   0.5048828125,\n",
       "   0.48777154088020325,\n",
       "   0.4784078001976013,\n",
       "   0.4819369912147522,\n",
       "   0.4829120934009552,\n",
       "   0.4796587824821472,\n",
       "   0.491505891084671,\n",
       "   0.5032379627227783,\n",
       "   0.4844787120819092,\n",
       "   0.5077030062675476,\n",
       "   0.4945299029350281,\n",
       "   0.49911367893218994,\n",
       "   0.48963436484336853,\n",
       "   0.4941900670528412,\n",
       "   0.5089311599731445,\n",
       "   0.5012960433959961,\n",
       "   0.48931267857551575,\n",
       "   0.5175061821937561,\n",
       "   0.4849180281162262,\n",
       "   0.49922415614128113,\n",
       "   0.5011810660362244,\n",
       "   0.4710649251937866,\n",
       "   0.4900146722793579,\n",
       "   0.4861506521701813,\n",
       "   0.4810789227485657,\n",
       "   0.49881497025489807,\n",
       "   0.5142492055892944,\n",
       "   0.4910629689693451,\n",
       "   0.5068464875221252,\n",
       "   0.4925304353237152,\n",
       "   0.4784955680370331,\n",
       "   0.5103525519371033,\n",
       "   0.48266932368278503,\n",
       "   0.4791947901248932,\n",
       "   0.4884135127067566,\n",
       "   0.49171996116638184,\n",
       "   0.49019935727119446,\n",
       "   0.5064112544059753,\n",
       "   0.47381648421287537,\n",
       "   0.48285284638404846,\n",
       "   0.4892280101776123,\n",
       "   0.516258716583252,\n",
       "   0.48250967264175415,\n",
       "   0.4982103407382965,\n",
       "   0.48610374331474304,\n",
       "   0.4916396737098694,\n",
       "   0.48885175585746765,\n",
       "   0.4997480511665344,\n",
       "   0.4832842946052551,\n",
       "   0.4793349504470825,\n",
       "   0.4824916422367096,\n",
       "   0.4706266522407532,\n",
       "   0.49436354637145996,\n",
       "   0.5077211260795593,\n",
       "   0.49078992009162903,\n",
       "   0.5044038891792297,\n",
       "   0.4734414219856262,\n",
       "   0.459005743265152,\n",
       "   0.46731120347976685,\n",
       "   0.46925634145736694,\n",
       "   0.49327120184898376,\n",
       "   0.48438259959220886,\n",
       "   0.4843500256538391,\n",
       "   0.48965707421302795,\n",
       "   0.505203366279602,\n",
       "   0.4782634675502777,\n",
       "   0.5213169455528259,\n",
       "   0.4830858111381531,\n",
       "   0.4875544011592865,\n",
       "   0.4728802442550659,\n",
       "   0.4858478307723999,\n",
       "   0.4761209487915039,\n",
       "   0.4898502826690674,\n",
       "   0.48605868220329285,\n",
       "   0.4835898280143738,\n",
       "   0.5007956624031067,\n",
       "   0.459308922290802,\n",
       "   0.4841589033603668,\n",
       "   0.4811129570007324,\n",
       "   0.4841447174549103,\n",
       "   0.4829036593437195,\n",
       "   0.47863712906837463,\n",
       "   0.49358224868774414,\n",
       "   0.4685288667678833,\n",
       "   0.5039998888969421,\n",
       "   0.49057191610336304,\n",
       "   0.4670473337173462,\n",
       "   0.5110923647880554,\n",
       "   0.4822774827480316,\n",
       "   0.49504342675209045,\n",
       "   0.5033900141716003,\n",
       "   0.4884871542453766,\n",
       "   0.499975323677063,\n",
       "   0.49419552087783813,\n",
       "   0.4758346676826477,\n",
       "   0.4894804060459137,\n",
       "   0.4891243577003479,\n",
       "   0.4823096990585327,\n",
       "   0.47487926483154297,\n",
       "   0.48684999346733093,\n",
       "   0.4882332980632782,\n",
       "   0.47900959849357605,\n",
       "   0.47318342328071594,\n",
       "   0.4799637198448181,\n",
       "   0.4683562219142914,\n",
       "   0.49025750160217285,\n",
       "   0.47138306498527527,\n",
       "   0.4884081184864044,\n",
       "   0.4884483814239502,\n",
       "   0.47712889313697815,\n",
       "   0.4803374707698822,\n",
       "   0.49290040135383606,\n",
       "   0.4792346954345703,\n",
       "   0.4775937497615814,\n",
       "   0.4639537036418915,\n",
       "   0.4952883720397949,\n",
       "   0.4767771363258362,\n",
       "   0.4484673738479614,\n",
       "   0.4771606922149658,\n",
       "   0.4901583790779114,\n",
       "   0.48084890842437744,\n",
       "   0.4833020269870758,\n",
       "   0.47224995493888855,\n",
       "   0.49685657024383545,\n",
       "   0.52629154920578,\n",
       "   0.47445422410964966,\n",
       "   0.4822700023651123,\n",
       "   0.4776727259159088,\n",
       "   0.47286272048950195,\n",
       "   0.47013288736343384,\n",
       "   0.47243744134902954,\n",
       "   0.5041617155075073,\n",
       "   0.4820229411125183,\n",
       "   0.4758155345916748,\n",
       "   0.48291903734207153,\n",
       "   0.48902037739753723,\n",
       "   0.4648238718509674,\n",
       "   0.4668467342853546,\n",
       "   0.47185468673706055,\n",
       "   0.4836820960044861,\n",
       "   0.47420212626457214,\n",
       "   0.4746635854244232,\n",
       "   0.4841528534889221,\n",
       "   0.478907972574234,\n",
       "   0.47763824462890625,\n",
       "   0.4648886024951935,\n",
       "   0.451354444026947,\n",
       "   0.4821564853191376,\n",
       "   0.47466713190078735,\n",
       "   0.47425562143325806,\n",
       "   0.4954795241355896,\n",
       "   0.4645872116088867,\n",
       "   0.47176870703697205,\n",
       "   0.46875813603401184,\n",
       "   0.4721536338329315,\n",
       "   0.48734742403030396,\n",
       "   0.47746410965919495,\n",
       "   0.4675148129463196,\n",
       "   0.49545103311538696,\n",
       "   0.48470574617385864,\n",
       "   0.49501940608024597,\n",
       "   0.4612220227718353,\n",
       "   0.4744555652141571,\n",
       "   0.47626665234565735,\n",
       "   0.4729861915111542,\n",
       "   0.48313701152801514,\n",
       "   0.4995976388454437,\n",
       "   0.4756510257720947,\n",
       "   0.47028785943984985,\n",
       "   0.48751717805862427,\n",
       "   0.5008175373077393,\n",
       "   0.48626914620399475,\n",
       "   0.45460715889930725,\n",
       "   0.44547560811042786,\n",
       "   0.463428258895874,\n",
       "   0.48853081464767456,\n",
       "   0.45750829577445984,\n",
       "   0.46652716398239136,\n",
       "   0.4746667742729187,\n",
       "   0.4862712621688843,\n",
       "   0.47017979621887207,\n",
       "   0.45723551511764526,\n",
       "   0.47349706292152405,\n",
       "   0.4870820939540863,\n",
       "   0.46341609954833984,\n",
       "   0.47959065437316895,\n",
       "   0.4738764464855194,\n",
       "   0.46050992608070374,\n",
       "   0.4607371687889099,\n",
       "   0.46383970975875854,\n",
       "   0.46931686997413635,\n",
       "   0.4709351658821106,\n",
       "   0.46489083766937256,\n",
       "   0.4750586450099945,\n",
       "   0.47973453998565674,\n",
       "   0.47375401854515076,\n",
       "   0.4755656123161316,\n",
       "   0.4826728403568268,\n",
       "   0.44675934314727783,\n",
       "   0.45837581157684326,\n",
       "   0.4800625145435333,\n",
       "   0.46286261081695557,\n",
       "   0.4664500951766968,\n",
       "   0.4868163764476776,\n",
       "   0.4838518798351288,\n",
       "   0.4665648341178894,\n",
       "   0.48466888070106506,\n",
       "   0.4541033208370209,\n",
       "   0.4622361958026886,\n",
       "   0.4826677441596985,\n",
       "   0.490646630525589,\n",
       "   0.4559057354927063,\n",
       "   0.4904636740684509,\n",
       "   0.46050864458084106,\n",
       "   0.46467965841293335,\n",
       "   0.4577251076698303,\n",
       "   0.4539359211921692,\n",
       "   0.47209376096725464,\n",
       "   0.4810408055782318,\n",
       "   0.44959205389022827,\n",
       "   0.45236098766326904,\n",
       "   0.4527534246444702,\n",
       "   0.4609616994857788,\n",
       "   0.4666694104671478,\n",
       "   0.4544922113418579,\n",
       "   0.4664875268936157,\n",
       "   0.4728199541568756,\n",
       "   0.4745621681213379,\n",
       "   0.4718095660209656,\n",
       "   0.46363210678100586,\n",
       "   0.4382965862751007,\n",
       "   0.4680538773536682,\n",
       "   0.47357383370399475,\n",
       "   0.47226402163505554,\n",
       "   0.44110238552093506,\n",
       "   0.43678179383277893,\n",
       "   0.46176326274871826,\n",
       "   0.47558537125587463,\n",
       "   0.4454517364501953,\n",
       "   0.4556124210357666,\n",
       "   0.46462514996528625,\n",
       "   0.461833655834198,\n",
       "   0.445372611284256,\n",
       "   0.4267911911010742,\n",
       "   0.46869534254074097,\n",
       "   0.4515204429626465,\n",
       "   0.465656578540802,\n",
       "   0.4552869498729706,\n",
       "   0.438712477684021,\n",
       "   0.4538571238517761,\n",
       "   0.4591113328933716,\n",
       "   0.4680190980434418,\n",
       "   0.45664456486701965,\n",
       "   0.4514576494693756,\n",
       "   0.45946362614631653,\n",
       "   0.448071151971817,\n",
       "   0.46205374598503113,\n",
       "   0.43197667598724365,\n",
       "   0.4266343414783478,\n",
       "   0.46914559602737427,\n",
       "   0.45489826798439026,\n",
       "   0.46712109446525574,\n",
       "   0.4535250961780548,\n",
       "   0.4448644816875458,\n",
       "   0.4482547640800476,\n",
       "   0.42413702607154846,\n",
       "   0.4352986514568329,\n",
       "   0.4368067979812622,\n",
       "   0.4501062035560608,\n",
       "   0.45240795612335205,\n",
       "   0.433704674243927,\n",
       "   0.4423123598098755,\n",
       "   0.43195441365242004,\n",
       "   0.43802982568740845,\n",
       "   0.4668103754520416,\n",
       "   0.44817614555358887,\n",
       "   0.4551713466644287,\n",
       "   0.44212397933006287,\n",
       "   0.44122251868247986,\n",
       "   0.4285980463027954,\n",
       "   0.4238538444042206,\n",
       "   0.4584665894508362,\n",
       "   0.43161502480506897,\n",
       "   0.4377617835998535,\n",
       "   0.45708465576171875,\n",
       "   0.447238564491272,\n",
       "   0.4288760721683502,\n",
       "   0.44779473543167114,\n",
       "   0.4459580183029175,\n",
       "   0.4492107927799225,\n",
       "   0.4290788173675537,\n",
       "   0.44031304121017456,\n",
       "   0.4291366934776306,\n",
       "   0.4554022550582886,\n",
       "   0.4449121356010437,\n",
       "   0.44332167506217957,\n",
       "   0.4291473627090454,\n",
       "   0.4233165383338928,\n",
       "   0.4667954742908478,\n",
       "   0.4257631301879883,\n",
       "   0.44683903455734253,\n",
       "   0.42840832471847534,\n",
       "   0.4338516592979431,\n",
       "   0.4266713857650757,\n",
       "   0.44512736797332764,\n",
       "   0.43785542249679565,\n",
       "   0.4309842586517334,\n",
       "   0.4411010146141052,\n",
       "   0.4213636517524719,\n",
       "   0.4476858973503113,\n",
       "   0.44319796562194824,\n",
       "   0.425765722990036,\n",
       "   0.43119892477989197,\n",
       "   0.45009374618530273,\n",
       "   0.42745697498321533,\n",
       "   0.415693998336792,\n",
       "   0.4215121865272522,\n",
       "   0.41701218485832214,\n",
       "   0.42261093854904175,\n",
       "   0.44172921776771545,\n",
       "   0.4274008870124817,\n",
       "   0.42615535855293274,\n",
       "   0.41255325078964233,\n",
       "   0.42813411355018616,\n",
       "   0.4242802858352661,\n",
       "   0.4354828894138336,\n",
       "   0.42610403895378113,\n",
       "   0.4459066092967987,\n",
       "   0.41421523690223694,\n",
       "   0.41609346866607666,\n",
       "   0.42115700244903564,\n",
       "   0.41782093048095703,\n",
       "   0.4369613528251648,\n",
       "   0.4300632178783417,\n",
       "   0.4113081097602844,\n",
       "   0.4152061641216278,\n",
       "   0.42635342478752136,\n",
       "   0.43244248628616333,\n",
       "   0.4314771294593811,\n",
       "   0.40498489141464233,\n",
       "   0.4297966957092285,\n",
       "   0.4082910120487213,\n",
       "   0.4396262466907501,\n",
       "   0.43646931648254395,\n",
       "   0.4186457693576813,\n",
       "   0.39632782340049744,\n",
       "   0.4134035110473633,\n",
       "   0.4051879644393921,\n",
       "   0.4210811257362366,\n",
       "   0.4254966080188751,\n",
       "   0.4032183885574341,\n",
       "   0.4174148142337799,\n",
       "   0.40345820784568787,\n",
       "   0.4082874059677124,\n",
       "   0.41486260294914246,\n",
       "   0.4011353552341461,\n",
       "   0.4263345003128052,\n",
       "   0.4122332036495209,\n",
       "   0.4127776622772217,\n",
       "   0.41610187292099,\n",
       "   0.40585774183273315,\n",
       "   0.40312087535858154,\n",
       "   0.42755061388015747,\n",
       "   0.41883665323257446,\n",
       "   0.40580204129219055,\n",
       "   0.4159352481365204,\n",
       "   0.4147486686706543,\n",
       "   0.41453516483306885,\n",
       "   0.40182143449783325]},\n",
       " 4: {'lr': 1e-05,\n",
       "  'best_loss_epoch': 162,\n",
       "  'best_acc_epoch': 115,\n",
       "  'best_r2_epoch': 5,\n",
       "  'pce_loss': [0.983291745185852,\n",
       "   0.409058541059494,\n",
       "   0.23875628411769867,\n",
       "   0.17130111157894135,\n",
       "   0.1342734843492508,\n",
       "   0.11282246559858322,\n",
       "   0.10321705043315887,\n",
       "   0.10414762049913406,\n",
       "   0.10604264587163925,\n",
       "   0.11041412502527237,\n",
       "   0.11403340101242065,\n",
       "   0.12084218859672546,\n",
       "   0.12264026701450348,\n",
       "   0.11611159890890121,\n",
       "   0.11730445176362991,\n",
       "   0.11591155081987381,\n",
       "   0.11116461455821991,\n",
       "   0.10566370189189911,\n",
       "   0.10416645556688309,\n",
       "   0.10410431772470474,\n",
       "   0.10566386580467224,\n",
       "   0.11242929100990295,\n",
       "   0.10871197283267975,\n",
       "   0.11263588815927505,\n",
       "   0.11825159192085266,\n",
       "   0.124321848154068,\n",
       "   0.1355171501636505,\n",
       "   0.14189009368419647,\n",
       "   0.1539655327796936,\n",
       "   0.16262301802635193,\n",
       "   0.1726486086845398,\n",
       "   0.18778935074806213,\n",
       "   0.19763436913490295,\n",
       "   0.2152184098958969,\n",
       "   0.22238731384277344,\n",
       "   0.2438708245754242,\n",
       "   0.24256785213947296,\n",
       "   0.2469344139099121,\n",
       "   0.2583343982696533,\n",
       "   0.2685655951499939,\n",
       "   0.2740890085697174,\n",
       "   0.283890962600708,\n",
       "   0.29698067903518677,\n",
       "   0.30776673555374146,\n",
       "   0.3107612431049347,\n",
       "   0.3238750994205475,\n",
       "   0.32216277718544006,\n",
       "   0.3176290988922119,\n",
       "   0.32230281829833984,\n",
       "   0.31909188628196716,\n",
       "   0.321355402469635,\n",
       "   0.31934332847595215,\n",
       "   0.31535154581069946,\n",
       "   0.31173279881477356,\n",
       "   0.3175455331802368,\n",
       "   0.31882891058921814,\n",
       "   0.31338056921958923,\n",
       "   0.3105522692203522,\n",
       "   0.3077142536640167,\n",
       "   0.30811646580696106,\n",
       "   0.2987341582775116,\n",
       "   0.2931731641292572,\n",
       "   0.29088571667671204,\n",
       "   0.29302501678466797,\n",
       "   0.2912040650844574,\n",
       "   0.2879490554332733,\n",
       "   0.28012949228286743,\n",
       "   0.2785278856754303,\n",
       "   0.2867206633090973,\n",
       "   0.28754279017448425,\n",
       "   0.2811632752418518,\n",
       "   0.2810525894165039,\n",
       "   0.2691987454891205,\n",
       "   0.2708929479122162,\n",
       "   0.2668072283267975,\n",
       "   0.263775497674942,\n",
       "   0.2543632686138153,\n",
       "   0.2516179382801056,\n",
       "   0.24938920140266418,\n",
       "   0.23974865674972534,\n",
       "   0.23847360908985138,\n",
       "   0.24141159653663635,\n",
       "   0.24323436617851257,\n",
       "   0.24003995954990387,\n",
       "   0.24185121059417725,\n",
       "   0.2508482336997986,\n",
       "   0.2570608854293823,\n",
       "   0.2628561556339264,\n",
       "   0.2622145414352417,\n",
       "   0.2678627073764801,\n",
       "   0.2781374454498291,\n",
       "   0.27549150586128235,\n",
       "   0.2697595953941345,\n",
       "   0.2583606541156769,\n",
       "   0.2543354332447052,\n",
       "   0.25995907187461853,\n",
       "   0.254072368144989,\n",
       "   0.2600710093975067,\n",
       "   0.26451626420021057,\n",
       "   0.25793394446372986,\n",
       "   0.2514747083187103,\n",
       "   0.2488013505935669,\n",
       "   0.24984152615070343,\n",
       "   0.25155109167099,\n",
       "   0.2561160624027252,\n",
       "   0.24654537439346313,\n",
       "   0.2396976500749588,\n",
       "   0.23815006017684937,\n",
       "   0.23497001826763153,\n",
       "   0.23282411694526672,\n",
       "   0.23165464401245117,\n",
       "   0.22816641628742218,\n",
       "   0.2221992462873459,\n",
       "   0.215616375207901,\n",
       "   0.20948633551597595,\n",
       "   0.20531249046325684,\n",
       "   0.202127143740654,\n",
       "   0.19906875491142273,\n",
       "   0.20027975738048553,\n",
       "   0.20056599378585815,\n",
       "   0.20605792105197906,\n",
       "   0.21326009929180145,\n",
       "   0.21479129791259766,\n",
       "   0.21873722970485687,\n",
       "   0.2205723524093628,\n",
       "   0.22227945923805237,\n",
       "   0.22412511706352234,\n",
       "   0.23739449679851532,\n",
       "   0.24641112983226776,\n",
       "   0.2505205273628235,\n",
       "   0.2557328939437866,\n",
       "   0.2519913911819458,\n",
       "   0.2588469088077545,\n",
       "   0.25937721133232117,\n",
       "   0.2519753575325012,\n",
       "   0.246872216463089,\n",
       "   0.24548864364624023,\n",
       "   0.24402517080307007,\n",
       "   0.24483798444271088,\n",
       "   0.23732033371925354,\n",
       "   0.2361934334039688,\n",
       "   0.2358851432800293,\n",
       "   0.23271667957305908,\n",
       "   0.2305968850851059,\n",
       "   0.22662928700447083,\n",
       "   0.23143498599529266,\n",
       "   0.2310691475868225,\n",
       "   0.23010259866714478,\n",
       "   0.23215487599372864,\n",
       "   0.22866380214691162,\n",
       "   0.2265026867389679,\n",
       "   0.22935159504413605,\n",
       "   0.23414593935012817,\n",
       "   0.23203687369823456,\n",
       "   0.23596149682998657,\n",
       "   0.23132890462875366,\n",
       "   0.2301403135061264,\n",
       "   0.22869980335235596,\n",
       "   0.22600694000720978,\n",
       "   0.22645597159862518,\n",
       "   0.22507113218307495,\n",
       "   0.23117822408676147,\n",
       "   0.22533124685287476,\n",
       "   0.23020830750465393,\n",
       "   0.23521266877651215,\n",
       "   0.23734387755393982,\n",
       "   0.23971866071224213,\n",
       "   0.2433084398508072,\n",
       "   0.2435479313135147,\n",
       "   0.2472253292798996,\n",
       "   0.25319087505340576,\n",
       "   0.253048837184906,\n",
       "   0.2551804780960083,\n",
       "   0.2652290165424347,\n",
       "   0.26266494393348694,\n",
       "   0.2665557861328125,\n",
       "   0.27100232243537903,\n",
       "   0.273133248090744,\n",
       "   0.27915501594543457,\n",
       "   0.276056170463562,\n",
       "   0.27629509568214417,\n",
       "   0.28064998984336853,\n",
       "   0.2800748646259308,\n",
       "   0.28263264894485474,\n",
       "   0.28750714659690857,\n",
       "   0.28274938464164734,\n",
       "   0.2880512773990631,\n",
       "   0.2828206419944763,\n",
       "   0.28653576970100403,\n",
       "   0.28756701946258545,\n",
       "   0.2867407500743866,\n",
       "   0.2843646705150604,\n",
       "   0.278906911611557,\n",
       "   0.2730501890182495,\n",
       "   0.27106937766075134,\n",
       "   0.27391114830970764,\n",
       "   0.2663750946521759,\n",
       "   0.2631821930408478,\n",
       "   0.257548063993454,\n",
       "   0.2535495460033417,\n",
       "   0.2511835992336273,\n",
       "   0.2522697448730469,\n",
       "   0.24651096761226654,\n",
       "   0.241733580827713,\n",
       "   0.24104541540145874,\n",
       "   0.23554573953151703,\n",
       "   0.23846375942230225,\n",
       "   0.24011042714118958,\n",
       "   0.24379490315914154,\n",
       "   0.24614953994750977,\n",
       "   0.24630005657672882,\n",
       "   0.2524366080760956,\n",
       "   0.25526490807533264,\n",
       "   0.2525365948677063,\n",
       "   0.24876612424850464,\n",
       "   0.24504023790359497,\n",
       "   0.24285361170768738,\n",
       "   0.24413111805915833,\n",
       "   0.24499091506004333,\n",
       "   0.24911707639694214,\n",
       "   0.25554516911506653,\n",
       "   0.2567359507083893,\n",
       "   0.25854671001434326,\n",
       "   0.26111674308776855,\n",
       "   0.2618427872657776,\n",
       "   0.262552946805954,\n",
       "   0.2612431049346924,\n",
       "   0.25952303409576416,\n",
       "   0.25975650548934937,\n",
       "   0.26136937737464905,\n",
       "   0.2633698284626007,\n",
       "   0.2622738182544708,\n",
       "   0.26155877113342285,\n",
       "   0.2616848647594452,\n",
       "   0.25437620282173157,\n",
       "   0.2552693784236908,\n",
       "   0.25691401958465576,\n",
       "   0.2581802010536194,\n",
       "   0.2609584927558899,\n",
       "   0.26742538809776306,\n",
       "   0.2691556215286255,\n",
       "   0.2740250825881958,\n",
       "   0.2790904641151428,\n",
       "   0.284066379070282,\n",
       "   0.2851735055446625,\n",
       "   0.291036993265152,\n",
       "   0.2900322377681732,\n",
       "   0.2890937626361847,\n",
       "   0.2848007380962372,\n",
       "   0.28470638394355774,\n",
       "   0.2813531756401062,\n",
       "   0.2774006128311157,\n",
       "   0.27248135209083557,\n",
       "   0.26921120285987854,\n",
       "   0.2704715430736542,\n",
       "   0.26684319972991943,\n",
       "   0.2636629045009613,\n",
       "   0.26110556721687317,\n",
       "   0.25883403420448303,\n",
       "   0.25635582208633423,\n",
       "   0.25403928756713867,\n",
       "   0.25045326352119446,\n",
       "   0.25146302580833435,\n",
       "   0.2506265640258789,\n",
       "   0.2491379827260971,\n",
       "   0.24834813177585602,\n",
       "   0.2506490647792816,\n",
       "   0.24705983698368073,\n",
       "   0.24645757675170898,\n",
       "   0.24395713210105896,\n",
       "   0.24489502608776093,\n",
       "   0.2411847561597824,\n",
       "   0.23977422714233398,\n",
       "   0.23766416311264038,\n",
       "   0.23699451982975006,\n",
       "   0.2414190173149109,\n",
       "   0.24462415277957916,\n",
       "   0.25102266669273376,\n",
       "   0.25244030356407166,\n",
       "   0.25499266386032104,\n",
       "   0.26099297404289246,\n",
       "   0.26252683997154236,\n",
       "   0.26123619079589844,\n",
       "   0.2597377896308899,\n",
       "   0.2581014335155487,\n",
       "   0.25441300868988037,\n",
       "   0.25254571437835693,\n",
       "   0.25306811928749084,\n",
       "   0.25026142597198486,\n",
       "   0.24845656752586365,\n",
       "   0.24918276071548462,\n",
       "   0.24947232007980347,\n",
       "   0.252227246761322,\n",
       "   0.25599148869514465,\n",
       "   0.25653234124183655,\n",
       "   0.26170769333839417,\n",
       "   0.2710348665714264,\n",
       "   0.2722172737121582,\n",
       "   0.2741054892539978,\n",
       "   0.28171902894973755,\n",
       "   0.2847693860530853,\n",
       "   0.2851044833660126,\n",
       "   0.2880883514881134,\n",
       "   0.28883764147758484,\n",
       "   0.2914096415042877,\n",
       "   0.2932133674621582,\n",
       "   0.29793962836265564,\n",
       "   0.2955825626850128,\n",
       "   0.294033408164978,\n",
       "   0.2944079339504242,\n",
       "   0.2920593321323395,\n",
       "   0.2881925702095032,\n",
       "   0.28403279185295105,\n",
       "   0.2829831838607788,\n",
       "   0.28267502784729004,\n",
       "   0.28140178322792053,\n",
       "   0.2823045253753662,\n",
       "   0.2805500626564026,\n",
       "   0.2783471345901489,\n",
       "   0.2714458405971527,\n",
       "   0.27063286304473877,\n",
       "   0.2707294225692749,\n",
       "   0.2678173780441284,\n",
       "   0.26386183500289917,\n",
       "   0.2596701681613922,\n",
       "   0.2586893141269684,\n",
       "   0.2562057077884674,\n",
       "   0.2557383179664612,\n",
       "   0.25101181864738464,\n",
       "   0.24900034070014954,\n",
       "   0.24712805449962616,\n",
       "   0.24529533088207245,\n",
       "   0.24443168938159943,\n",
       "   0.24023129045963287,\n",
       "   0.24183207750320435,\n",
       "   0.24525195360183716,\n",
       "   0.25023898482322693,\n",
       "   0.24951979517936707,\n",
       "   0.24965216219425201,\n",
       "   0.24978624284267426,\n",
       "   0.25148439407348633,\n",
       "   0.25319933891296387,\n",
       "   0.2556747794151306,\n",
       "   0.26095253229141235,\n",
       "   0.2634488344192505,\n",
       "   0.2638211250305176,\n",
       "   0.26099807024002075,\n",
       "   0.2619860768318176,\n",
       "   0.2628902196884155,\n",
       "   0.26476263999938965,\n",
       "   0.2673082947731018,\n",
       "   0.2721068561077118,\n",
       "   0.2747126817703247,\n",
       "   0.27879321575164795,\n",
       "   0.28435295820236206,\n",
       "   0.28974658250808716,\n",
       "   0.29259729385375977,\n",
       "   0.29114043712615967,\n",
       "   0.2917528450489044,\n",
       "   0.29212677478790283,\n",
       "   0.2927190065383911,\n",
       "   0.29364508390426636,\n",
       "   0.2926441729068756,\n",
       "   0.2925039529800415,\n",
       "   0.2895563244819641,\n",
       "   0.2857271730899811,\n",
       "   0.2785297930240631,\n",
       "   0.27455800771713257,\n",
       "   0.26987791061401367,\n",
       "   0.26526081562042236,\n",
       "   0.26708319783210754,\n",
       "   0.2636033892631531,\n",
       "   0.26464951038360596,\n",
       "   0.2626526653766632,\n",
       "   0.2615748643875122,\n",
       "   0.25932997465133667,\n",
       "   0.25694984197616577,\n",
       "   0.25295159220695496,\n",
       "   0.2474903017282486,\n",
       "   0.2458353489637375,\n",
       "   0.246487557888031,\n",
       "   0.24688675999641418,\n",
       "   0.24697719514369965,\n",
       "   0.24508343636989594,\n",
       "   0.2483842372894287,\n",
       "   0.2475585639476776,\n",
       "   0.2460188865661621,\n",
       "   0.24716298282146454,\n",
       "   0.24760065972805023,\n",
       "   0.2500818967819214,\n",
       "   0.25175973773002625,\n",
       "   0.2517035901546478,\n",
       "   0.24834147095680237,\n",
       "   0.24941787123680115,\n",
       "   0.2505398988723755,\n",
       "   0.2525714337825775,\n",
       "   0.25306573510169983,\n",
       "   0.25577312707901,\n",
       "   0.2573544681072235,\n",
       "   0.25763458013534546,\n",
       "   0.2589307427406311,\n",
       "   0.26383331418037415,\n",
       "   0.26729869842529297,\n",
       "   0.2732186019420624,\n",
       "   0.27689608931541443,\n",
       "   0.2781517207622528,\n",
       "   0.2769179344177246,\n",
       "   0.27651458978652954,\n",
       "   0.27731484174728394,\n",
       "   0.27506640553474426,\n",
       "   0.2722322940826416,\n",
       "   0.27435484528541565,\n",
       "   0.2742166519165039,\n",
       "   0.27479618787765503,\n",
       "   0.2751963138580322,\n",
       "   0.2760044038295746,\n",
       "   0.27803242206573486,\n",
       "   0.2778283655643463,\n",
       "   0.27800852060317993,\n",
       "   0.2773852050304413,\n",
       "   0.27708426117897034,\n",
       "   0.27956268191337585,\n",
       "   0.2758124768733978,\n",
       "   0.27400219440460205,\n",
       "   0.27212047576904297,\n",
       "   0.269062340259552,\n",
       "   0.2695519030094147,\n",
       "   0.27225440740585327,\n",
       "   0.27705058455467224,\n",
       "   0.2781992256641388,\n",
       "   0.2800409197807312,\n",
       "   0.2808888554573059,\n",
       "   0.28181004524230957,\n",
       "   0.28551018238067627,\n",
       "   0.28495702147483826,\n",
       "   0.28130853176116943,\n",
       "   0.2836059033870697,\n",
       "   0.27901676297187805,\n",
       "   0.27279752492904663,\n",
       "   0.27343639731407166,\n",
       "   0.2664637267589569,\n",
       "   0.26278063654899597,\n",
       "   0.2613905966281891,\n",
       "   0.25642332434654236,\n",
       "   0.2552032470703125,\n",
       "   0.2513299584388733,\n",
       "   0.2500971257686615,\n",
       "   0.2471833974123001,\n",
       "   0.24423816800117493,\n",
       "   0.24576891958713531,\n",
       "   0.2472236603498459,\n",
       "   0.247817263007164,\n",
       "   0.24949437379837036,\n",
       "   0.2527269721031189,\n",
       "   0.25601813197135925,\n",
       "   0.25583556294441223,\n",
       "   0.25936681032180786,\n",
       "   0.2638627290725708,\n",
       "   0.26708242297172546,\n",
       "   0.27113232016563416,\n",
       "   0.27167457342147827,\n",
       "   0.2716679871082306,\n",
       "   0.27148640155792236,\n",
       "   0.27288442850112915,\n",
       "   0.27525532245635986,\n",
       "   0.2752712070941925,\n",
       "   0.27613064646720886,\n",
       "   0.27523642778396606,\n",
       "   0.276276171207428,\n",
       "   0.2754422724246979,\n",
       "   0.27371013164520264,\n",
       "   0.2730424106121063,\n",
       "   0.27264976501464844,\n",
       "   0.27552732825279236,\n",
       "   0.2752620279788971,\n",
       "   0.2770795226097107,\n",
       "   0.2771783471107483,\n",
       "   0.27850690484046936,\n",
       "   0.2773890793323517,\n",
       "   0.27203094959259033,\n",
       "   0.2716362476348877,\n",
       "   0.2712284028530121,\n",
       "   0.2711609899997711,\n",
       "   0.2692002058029175,\n",
       "   0.27045565843582153,\n",
       "   0.2727792263031006,\n",
       "   0.27264249324798584,\n",
       "   0.2743951082229614,\n",
       "   0.27461352944374084,\n",
       "   0.2754165828227997,\n",
       "   0.2704504728317261,\n",
       "   0.2699275612831116,\n",
       "   0.26649296283721924,\n",
       "   0.265506386756897,\n",
       "   0.26431629061698914,\n",
       "   0.26228055357933044,\n",
       "   0.25969409942626953,\n",
       "   0.2541356682777405,\n",
       "   0.25595590472221375,\n",
       "   0.2566521465778351],\n",
       "  'voc_loss': [0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6231197714805603,\n",
       "   0.6044306755065918,\n",
       "   0.5597619414329529,\n",
       "   0.47451427578926086,\n",
       "   0.40215200185775757,\n",
       "   0.3433700501918793,\n",
       "   0.2967052459716797,\n",
       "   0.26225483417510986,\n",
       "   0.24844509363174438,\n",
       "   0.21965834498405457,\n",
       "   0.19585512578487396,\n",
       "   0.18177740275859833,\n",
       "   0.1668892800807953,\n",
       "   0.15986743569374084,\n",
       "   0.14488625526428223,\n",
       "   0.13404437899589539,\n",
       "   0.12331424653530121,\n",
       "   0.11038050055503845,\n",
       "   0.09601357579231262,\n",
       "   0.08559436351060867,\n",
       "   0.0732327252626419,\n",
       "   0.05725528299808502,\n",
       "   0.0527183897793293,\n",
       "   0.05777665972709656,\n",
       "   0.05451999232172966,\n",
       "   0.053984738886356354,\n",
       "   0.05516475439071655,\n",
       "   0.06279820948839188,\n",
       "   0.07183650881052017,\n",
       "   0.07601600885391235,\n",
       "   0.0819665938615799,\n",
       "   0.08234355598688126,\n",
       "   0.08305440098047256,\n",
       "   0.08505566418170929,\n",
       "   0.08961924910545349,\n",
       "   0.08893327414989471,\n",
       "   0.09783096611499786,\n",
       "   0.09186035394668579,\n",
       "   0.08637796342372894,\n",
       "   0.08385885506868362,\n",
       "   0.08291681855916977,\n",
       "   0.0783160850405693,\n",
       "   0.0697934627532959,\n",
       "   0.05801689624786377,\n",
       "   0.05249093845486641,\n",
       "   0.04592743515968323,\n",
       "   0.043412137776613235,\n",
       "   0.04174831882119179,\n",
       "   0.04196171835064888,\n",
       "   0.03872999921441078,\n",
       "   0.03501516580581665,\n",
       "   0.033225271850824356,\n",
       "   0.028117170557379723,\n",
       "   0.026933610439300537,\n",
       "   0.025880170986056328,\n",
       "   0.02843775413930416,\n",
       "   0.031146083027124405,\n",
       "   0.03556070476770401,\n",
       "   0.037457093596458435,\n",
       "   0.038959041237831116,\n",
       "   0.04047751426696777,\n",
       "   0.04145755246281624,\n",
       "   0.04065112769603729,\n",
       "   0.04132991284132004,\n",
       "   0.04446182772517204,\n",
       "   0.046168893575668335,\n",
       "   0.047138020396232605,\n",
       "   0.05064525082707405,\n",
       "   0.05592872202396393,\n",
       "   0.0599750354886055,\n",
       "   0.06135401502251625,\n",
       "   0.06540940701961517,\n",
       "   0.06656023859977722,\n",
       "   0.06782129406929016,\n",
       "   0.07205475866794586,\n",
       "   0.07437343150377274,\n",
       "   0.07634059339761734,\n",
       "   0.07556542009115219,\n",
       "   0.07886756956577301,\n",
       "   0.07852113991975784,\n",
       "   0.07857359200716019,\n",
       "   0.07856941968202591,\n",
       "   0.08018574118614197,\n",
       "   0.0826183632016182,\n",
       "   0.08483334630727768,\n",
       "   0.08655572682619095,\n",
       "   0.08733584731817245,\n",
       "   0.08743662387132645,\n",
       "   0.087589330971241,\n",
       "   0.08957528322935104,\n",
       "   0.08361349999904633,\n",
       "   0.07714371383190155,\n",
       "   0.07564008235931396,\n",
       "   0.07073802500963211,\n",
       "   0.07028383016586304,\n",
       "   0.0671197846531868,\n",
       "   0.05996980518102646,\n",
       "   0.054083310067653656,\n",
       "   0.049776382744312286,\n",
       "   0.04386055842041969,\n",
       "   0.04147141054272652,\n",
       "   0.04091672971844673,\n",
       "   0.039189863950014114,\n",
       "   0.035443224012851715,\n",
       "   0.03494690731167793,\n",
       "   0.03462512046098709,\n",
       "   0.034960970282554626,\n",
       "   0.036582138389348984,\n",
       "   0.038287919014692307,\n",
       "   0.03932592645287514,\n",
       "   0.04081110283732414,\n",
       "   0.04183317720890045,\n",
       "   0.041842348873615265,\n",
       "   0.041849080473184586,\n",
       "   0.04407930746674538,\n",
       "   0.045532505959272385,\n",
       "   0.0448886938393116,\n",
       "   0.04519008845090866,\n",
       "   0.04615296050906181,\n",
       "   0.047597337514162064,\n",
       "   0.046571116894483566,\n",
       "   0.04463272914290428,\n",
       "   0.04377102106809616,\n",
       "   0.03995496779680252,\n",
       "   0.039703190326690674,\n",
       "   0.03915199264883995,\n",
       "   0.037481267005205154,\n",
       "   0.035417042672634125,\n",
       "   0.03379296511411667,\n",
       "   0.03502880409359932,\n",
       "   0.03861035779118538,\n",
       "   0.0392480306327343,\n",
       "   0.04337269812822342,\n",
       "   0.047293588519096375,\n",
       "   0.04901834949851036,\n",
       "   0.04838774353265762,\n",
       "   0.05134058743715286,\n",
       "   0.05553990975022316,\n",
       "   0.055294886231422424,\n",
       "   0.05999781936407089,\n",
       "   0.0616731122136116,\n",
       "   0.06179286539554596,\n",
       "   0.06069287285208702,\n",
       "   0.0566721111536026,\n",
       "   0.05747438594698906,\n",
       "   0.05744059383869171,\n",
       "   0.05943402275443077,\n",
       "   0.060863565653562546,\n",
       "   0.06200514733791351,\n",
       "   0.06412793695926666,\n",
       "   0.06336633861064911,\n",
       "   0.06334871798753738,\n",
       "   0.06185246258974075,\n",
       "   0.05949504300951958,\n",
       "   0.06228767707943916,\n",
       "   0.06063567101955414,\n",
       "   0.059228722006082535,\n",
       "   0.05904056131839752,\n",
       "   0.05863684043288231,\n",
       "   0.059027090668678284,\n",
       "   0.062040846794843674,\n",
       "   0.06149180233478546,\n",
       "   0.06285829097032547,\n",
       "   0.0665956661105156,\n",
       "   0.06889904290437698,\n",
       "   0.06929911673069,\n",
       "   0.06974834203720093,\n",
       "   0.06651546061038971,\n",
       "   0.06705662608146667,\n",
       "   0.0663161426782608,\n",
       "   0.06353364884853363,\n",
       "   0.0657779797911644,\n",
       "   0.06199342757463455,\n",
       "   0.061408717185258865,\n",
       "   0.06124948337674141,\n",
       "   0.05616850405931473,\n",
       "   0.055722981691360474,\n",
       "   0.059699926525354385,\n",
       "   0.058806486427783966,\n",
       "   0.06315398216247559,\n",
       "   0.0632757842540741,\n",
       "   0.06302078068256378,\n",
       "   0.06583063304424286,\n",
       "   0.06734588742256165,\n",
       "   0.06980109959840775,\n",
       "   0.07188120484352112,\n",
       "   0.07178669422864914,\n",
       "   0.07493696361780167,\n",
       "   0.07793552428483963,\n",
       "   0.07944279909133911,\n",
       "   0.08135755360126495,\n",
       "   0.08377763628959656,\n",
       "   0.08432778716087341,\n",
       "   0.0843864381313324,\n",
       "   0.08649484068155289,\n",
       "   0.08996722847223282,\n",
       "   0.09163986146450043,\n",
       "   0.09119696915149689,\n",
       "   0.09284495562314987,\n",
       "   0.09148531407117844,\n",
       "   0.09515484422445297,\n",
       "   0.0946338102221489,\n",
       "   0.09697794914245605,\n",
       "   0.09608582407236099,\n",
       "   0.09297990053892136,\n",
       "   0.09259059280157089,\n",
       "   0.09049367904663086,\n",
       "   0.08742796629667282,\n",
       "   0.08280568569898605,\n",
       "   0.0799444168806076,\n",
       "   0.07906754314899445,\n",
       "   0.08092337846755981,\n",
       "   0.08324038982391357,\n",
       "   0.08131704479455948,\n",
       "   0.07892843335866928,\n",
       "   0.07716254144906998,\n",
       "   0.07450991868972778,\n",
       "   0.06933843344449997,\n",
       "   0.06802482157945633,\n",
       "   0.06676372140645981,\n",
       "   0.0647101029753685,\n",
       "   0.06839361041784286,\n",
       "   0.06770770996809006,\n",
       "   0.06646397709846497,\n",
       "   0.06654982268810272,\n",
       "   0.0649932473897934,\n",
       "   0.06607884913682938,\n",
       "   0.06868217140436172,\n",
       "   0.07131682336330414,\n",
       "   0.07007510960102081,\n",
       "   0.06915312260389328,\n",
       "   0.07189837098121643,\n",
       "   0.07326583564281464,\n",
       "   0.07541609555482864,\n",
       "   0.0758967474102974,\n",
       "   0.07608634978532791,\n",
       "   0.07762476801872253,\n",
       "   0.07890131324529648,\n",
       "   0.08043012022972107,\n",
       "   0.08202541619539261,\n",
       "   0.08308412879705429,\n",
       "   0.08719759434461594,\n",
       "   0.09011124819517136,\n",
       "   0.09225700795650482,\n",
       "   0.09358933568000793,\n",
       "   0.09419962763786316,\n",
       "   0.09984728693962097,\n",
       "   0.10052260756492615,\n",
       "   0.09974588453769684,\n",
       "   0.10169237107038498,\n",
       "   0.10361027717590332,\n",
       "   0.10179529339075089,\n",
       "   0.10025106370449066,\n",
       "   0.0985785499215126,\n",
       "   0.09730014204978943,\n",
       "   0.09776801615953445,\n",
       "   0.10029633343219757,\n",
       "   0.09898871183395386,\n",
       "   0.1013650968670845,\n",
       "   0.1018148809671402,\n",
       "   0.09809383749961853,\n",
       "   0.0995536595582962,\n",
       "   0.10163792967796326,\n",
       "   0.10277148336172104,\n",
       "   0.1047278493642807,\n",
       "   0.10554596781730652,\n",
       "   0.10403982549905777,\n",
       "   0.10669843852519989,\n",
       "   0.11119535565376282,\n",
       "   0.10918300598859787,\n",
       "   0.10742945224046707,\n",
       "   0.1054571196436882,\n",
       "   0.10207946598529816,\n",
       "   0.09608834981918335,\n",
       "   0.09458541870117188,\n",
       "   0.09328200668096542,\n",
       "   0.09287267178297043,\n",
       "   0.08878473937511444,\n",
       "   0.08689770847558975,\n",
       "   0.08707913756370544,\n",
       "   0.08404836803674698,\n",
       "   0.08371567726135254,\n",
       "   0.08521369844675064,\n",
       "   0.0803593173623085,\n",
       "   0.07860767096281052,\n",
       "   0.07904866337776184,\n",
       "   0.08387307822704315,\n",
       "   0.08669447898864746,\n",
       "   0.09041732549667358,\n",
       "   0.09223576635122299,\n",
       "   0.09186673164367676,\n",
       "   0.09690149873495102,\n",
       "   0.09691329300403595,\n",
       "   0.1006619781255722,\n",
       "   0.1006680354475975,\n",
       "   0.0999227911233902,\n",
       "   0.09623846411705017,\n",
       "   0.09821593761444092,\n",
       "   0.09499513357877731,\n",
       "   0.09289892017841339,\n",
       "   0.09356532990932465,\n",
       "   0.08872352540493011,\n",
       "   0.08829034864902496,\n",
       "   0.08838137239217758,\n",
       "   0.08850287646055222,\n",
       "   0.09217986464500427,\n",
       "   0.0915083959698677,\n",
       "   0.09046901762485504,\n",
       "   0.09196128696203232,\n",
       "   0.08971048146486282,\n",
       "   0.09188376367092133,\n",
       "   0.09307868778705597,\n",
       "   0.095394067466259,\n",
       "   0.09792199730873108,\n",
       "   0.09599831700325012,\n",
       "   0.09731332212686539,\n",
       "   0.09912346303462982,\n",
       "   0.10023249685764313,\n",
       "   0.09969612210988998,\n",
       "   0.09856674820184708,\n",
       "   0.10017164796590805,\n",
       "   0.09756840020418167,\n",
       "   0.09505103528499603,\n",
       "   0.09361731261014938,\n",
       "   0.09064336121082306,\n",
       "   0.08741412311792374,\n",
       "   0.0887950137257576,\n",
       "   0.08676150441169739,\n",
       "   0.08896397054195404,\n",
       "   0.08941955119371414,\n",
       "   0.08856746554374695,\n",
       "   0.08916532248258591,\n",
       "   0.08700332790613174,\n",
       "   0.08366253972053528,\n",
       "   0.08160684257745743,\n",
       "   0.08474384248256683,\n",
       "   0.08163979649543762,\n",
       "   0.08279816806316376,\n",
       "   0.08429179340600967,\n",
       "   0.08302982896566391,\n",
       "   0.08110406249761581,\n",
       "   0.07942286133766174,\n",
       "   0.0796397253870964,\n",
       "   0.07657445967197418,\n",
       "   0.07808449119329453,\n",
       "   0.0794614925980568,\n",
       "   0.0839381292462349,\n",
       "   0.08291672170162201,\n",
       "   0.08375921845436096,\n",
       "   0.08316300809383392,\n",
       "   0.08464472740888596,\n",
       "   0.08896192163228989,\n",
       "   0.08830147981643677,\n",
       "   0.08800501376390457,\n",
       "   0.09011302888393402,\n",
       "   0.09244270622730255,\n",
       "   0.09324868768453598,\n",
       "   0.09813269227743149,\n",
       "   0.09677231311798096,\n",
       "   0.09953559935092926,\n",
       "   0.10513616353273392,\n",
       "   0.10593917965888977,\n",
       "   0.10795732587575912,\n",
       "   0.10811596363782883,\n",
       "   0.1107010766863823,\n",
       "   0.11122722178697586,\n",
       "   0.1120968759059906,\n",
       "   0.11498118191957474,\n",
       "   0.1131228506565094,\n",
       "   0.11355490237474442,\n",
       "   0.10773654282093048,\n",
       "   0.10726537555456161,\n",
       "   0.10839839279651642,\n",
       "   0.10704591870307922,\n",
       "   0.10308748483657837,\n",
       "   0.1012820154428482,\n",
       "   0.10006750375032425,\n",
       "   0.10008177161216736,\n",
       "   0.10275331884622574,\n",
       "   0.1008337140083313,\n",
       "   0.09547734260559082,\n",
       "   0.09573087841272354,\n",
       "   0.09493815153837204,\n",
       "   0.09089863300323486,\n",
       "   0.08937041461467743,\n",
       "   0.08503883332014084,\n",
       "   0.08423464000225067,\n",
       "   0.0856345146894455,\n",
       "   0.08479708433151245,\n",
       "   0.08594585955142975,\n",
       "   0.08392997086048126,\n",
       "   0.0867825299501419,\n",
       "   0.08395804464817047,\n",
       "   0.08043821901082993,\n",
       "   0.07940403372049332,\n",
       "   0.07721556723117828,\n",
       "   0.07690490037202835,\n",
       "   0.07530388981103897,\n",
       "   0.07164481282234192,\n",
       "   0.07198306173086166,\n",
       "   0.0733313262462616,\n",
       "   0.07231266051530838,\n",
       "   0.07410112768411636,\n",
       "   0.0780630111694336,\n",
       "   0.08061648905277252,\n",
       "   0.08142789453268051,\n",
       "   0.08316860347986221,\n",
       "   0.08531253039836884,\n",
       "   0.08768227696418762,\n",
       "   0.09111431241035461,\n",
       "   0.09364970028400421,\n",
       "   0.09737015515565872,\n",
       "   0.09968946129083633,\n",
       "   0.10067209601402283,\n",
       "   0.1021028533577919,\n",
       "   0.10418595373630524,\n",
       "   0.10526266694068909,\n",
       "   0.1070510670542717,\n",
       "   0.10854216665029526,\n",
       "   0.10681835561990738,\n",
       "   0.10762445628643036,\n",
       "   0.10773059725761414,\n",
       "   0.11040840297937393,\n",
       "   0.1104101836681366,\n",
       "   0.10922784358263016,\n",
       "   0.10709281265735626,\n",
       "   0.10121670365333557,\n",
       "   0.09945754706859589,\n",
       "   0.09891559183597565,\n",
       "   0.09409667551517487,\n",
       "   0.09087496250867844,\n",
       "   0.09044413268566132,\n",
       "   0.0880197063088417,\n",
       "   0.08611750602722168,\n",
       "   0.08557633310556412,\n",
       "   0.08323337882757187,\n",
       "   0.07965253293514252,\n",
       "   0.07735001295804977,\n",
       "   0.07402622699737549,\n",
       "   0.06954475492238998,\n",
       "   0.06610387563705444,\n",
       "   0.06490227580070496,\n",
       "   0.06338895857334137,\n",
       "   0.06155472621321678,\n",
       "   0.061533570289611816,\n",
       "   0.05997975915670395,\n",
       "   0.06101517006754875,\n",
       "   0.06404067575931549,\n",
       "   0.06418582051992416,\n",
       "   0.0646335780620575,\n",
       "   0.06620510667562485,\n",
       "   0.06521201133728027,\n",
       "   0.0639679878950119,\n",
       "   0.06499142944812775,\n",
       "   0.06392161548137665,\n",
       "   0.06528530269861221,\n",
       "   0.06444503366947174,\n",
       "   0.06445890665054321,\n",
       "   0.06571253389120102,\n",
       "   0.06688850373029709,\n",
       "   0.06841623783111572,\n",
       "   0.0706845372915268,\n",
       "   0.07498554140329361,\n",
       "   0.07620350271463394,\n",
       "   0.07779596745967865,\n",
       "   0.08208736032247543,\n",
       "   0.08371738344430923,\n",
       "   0.08460985124111176,\n",
       "   0.08652685582637787,\n",
       "   0.08787665516138077,\n",
       "   0.08882943540811539,\n",
       "   0.09050793200731277,\n",
       "   0.09274156391620636,\n",
       "   0.0928858295083046,\n",
       "   0.09406062960624695,\n",
       "   0.09364904463291168,\n",
       "   0.09330222755670547,\n",
       "   0.08848841488361359,\n",
       "   0.09052915126085281,\n",
       "   0.08921626955270767,\n",
       "   0.08775436133146286,\n",
       "   0.08633603155612946,\n",
       "   0.08909498900175095,\n",
       "   0.08986957371234894,\n",
       "   0.08943881094455719,\n",
       "   0.08945102244615555,\n",
       "   0.08867483586072922,\n",
       "   0.08980526030063629,\n",
       "   0.08850014209747314,\n",
       "   0.08828983455896378],\n",
       "  'jsc_loss': [0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7865577340126038,\n",
       "   0.7860673069953918,\n",
       "   0.7810538411140442,\n",
       "   0.7768797278404236,\n",
       "   0.7730905413627625,\n",
       "   0.7707724571228027,\n",
       "   0.7674673199653625,\n",
       "   0.7636145949363708,\n",
       "   0.7634532451629639,\n",
       "   0.7632567286491394,\n",
       "   0.7628078460693359,\n",
       "   0.7614034414291382,\n",
       "   0.7608817219734192,\n",
       "   0.7597241401672363,\n",
       "   0.7585011124610901,\n",
       "   0.7590191960334778,\n",
       "   0.7563153505325317,\n",
       "   0.7511425018310547,\n",
       "   0.7481869459152222,\n",
       "   0.7448837161064148,\n",
       "   0.7440080046653748,\n",
       "   0.7440750002861023,\n",
       "   0.7422356009483337,\n",
       "   0.7421336770057678,\n",
       "   0.7387953400611877,\n",
       "   0.7384093999862671,\n",
       "   0.737764835357666,\n",
       "   0.7338202595710754,\n",
       "   0.7259907126426697,\n",
       "   0.7185884714126587,\n",
       "   0.7190777063369751,\n",
       "   0.7105477452278137,\n",
       "   0.7110307216644287,\n",
       "   0.7049654722213745,\n",
       "   0.7059196829795837,\n",
       "   0.7036948204040527,\n",
       "   0.7008635997772217,\n",
       "   0.6941801905632019,\n",
       "   0.7055193781852722,\n",
       "   0.7099980711936951,\n",
       "   0.7190393209457397,\n",
       "   0.724532425403595,\n",
       "   0.7228814363479614,\n",
       "   0.7131620645523071,\n",
       "   0.7077038288116455,\n",
       "   0.7031629085540771,\n",
       "   0.7004277110099792,\n",
       "   0.7081255912780762,\n",
       "   0.7177003026008606,\n",
       "   0.7168661952018738,\n",
       "   0.7112602591514587,\n",
       "   0.7076804041862488,\n",
       "   0.7099282741546631,\n",
       "   0.7061032056808472,\n",
       "   0.7100425958633423,\n",
       "   0.7145582437515259,\n",
       "   0.7114931344985962,\n",
       "   0.7080098986625671,\n",
       "   0.7081184983253479,\n",
       "   0.7071999311447144,\n",
       "   0.705034613609314,\n",
       "   0.7046182751655579,\n",
       "   0.6957533955574036,\n",
       "   0.6888647675514221,\n",
       "   0.6850531697273254,\n",
       "   0.6862717866897583,\n",
       "   0.6881970167160034,\n",
       "   0.6832920908927917,\n",
       "   0.6834882497787476,\n",
       "   0.67708820104599,\n",
       "   0.6760753393173218,\n",
       "   0.6727942824363708,\n",
       "   0.6677813529968262,\n",
       "   0.6636797189712524,\n",
       "   0.6582216024398804,\n",
       "   0.6616475582122803,\n",
       "   0.6606999039649963,\n",
       "   0.6619511842727661,\n",
       "   0.664424479007721,\n",
       "   0.6610085368156433,\n",
       "   0.6609485745429993,\n",
       "   0.6571434140205383,\n",
       "   0.6550859212875366,\n",
       "   0.6540943384170532,\n",
       "   0.6497710943222046,\n",
       "   0.6498163938522339,\n",
       "   0.648023784160614,\n",
       "   0.6468811631202698,\n",
       "   0.6495600342750549,\n",
       "   0.6550173163414001,\n",
       "   0.6533805727958679,\n",
       "   0.6539811491966248,\n",
       "   0.6622871160507202,\n",
       "   0.6664812564849854,\n",
       "   0.6692332625389099,\n",
       "   0.6690657734870911,\n",
       "   0.6696400046348572,\n",
       "   0.6662886738777161,\n",
       "   0.669655442237854,\n",
       "   0.6677406430244446,\n",
       "   0.6703701019287109,\n",
       "   0.6787098050117493,\n",
       "   0.6819215416908264,\n",
       "   0.6838732957839966,\n",
       "   0.6857718229293823,\n",
       "   0.6817439794540405,\n",
       "   0.6820386052131653,\n",
       "   0.6768558025360107,\n",
       "   0.6737966537475586,\n",
       "   0.6669046878814697,\n",
       "   0.6655811667442322,\n",
       "   0.6618605256080627,\n",
       "   0.6616747975349426,\n",
       "   0.6570967435836792,\n",
       "   0.6555712223052979,\n",
       "   0.659454345703125,\n",
       "   0.6594259738922119,\n",
       "   0.659294843673706,\n",
       "   0.6564435958862305,\n",
       "   0.6543228030204773,\n",
       "   0.6513661742210388,\n",
       "   0.6540905237197876,\n",
       "   0.6495693922042847,\n",
       "   0.6488204598426819,\n",
       "   0.6494755148887634,\n",
       "   0.6493756771087646,\n",
       "   0.648758590221405,\n",
       "   0.6477875113487244,\n",
       "   0.6493017673492432,\n",
       "   0.6494587063789368,\n",
       "   0.6481719613075256,\n",
       "   0.6469900012016296,\n",
       "   0.6412913203239441,\n",
       "   0.6402782201766968,\n",
       "   0.6413353681564331,\n",
       "   0.6323524117469788,\n",
       "   0.6327136754989624,\n",
       "   0.6277995705604553,\n",
       "   0.6222174763679504,\n",
       "   0.6182577013969421,\n",
       "   0.6179866194725037,\n",
       "   0.6145043969154358,\n",
       "   0.6075996160507202,\n",
       "   0.5988767743110657,\n",
       "   0.5975403189659119,\n",
       "   0.5990090370178223,\n",
       "   0.5980944037437439,\n",
       "   0.5975223183631897,\n",
       "   0.5959805250167847,\n",
       "   0.5977992415428162,\n",
       "   0.5969625115394592,\n",
       "   0.5934832692146301,\n",
       "   0.5896199345588684,\n",
       "   0.5856753587722778,\n",
       "   0.5819622278213501,\n",
       "   0.5801909565925598,\n",
       "   0.5813909769058228,\n",
       "   0.5829307436943054,\n",
       "   0.58553546667099,\n",
       "   0.5838902592658997,\n",
       "   0.5831247568130493,\n",
       "   0.5790020823478699,\n",
       "   0.5814894437789917,\n",
       "   0.5824621319770813,\n",
       "   0.5806650519371033,\n",
       "   0.5798677206039429,\n",
       "   0.5789912343025208,\n",
       "   0.5804484486579895,\n",
       "   0.5798104405403137,\n",
       "   0.5791966915130615,\n",
       "   0.581861138343811,\n",
       "   0.5834199786186218,\n",
       "   0.5857515931129456,\n",
       "   0.5828114748001099,\n",
       "   0.5805679559707642,\n",
       "   0.5823146104812622,\n",
       "   0.5830420851707458,\n",
       "   0.5865972638130188,\n",
       "   0.5861737728118896,\n",
       "   0.5801236629486084,\n",
       "   0.578572690486908,\n",
       "   0.574669361114502,\n",
       "   0.5712847113609314,\n",
       "   0.572008490562439,\n",
       "   0.5691869258880615,\n",
       "   0.5672967433929443,\n",
       "   0.5665912628173828,\n",
       "   0.5653644800186157,\n",
       "   0.5638116002082825,\n",
       "   0.5641587972640991,\n",
       "   0.5648304224014282,\n",
       "   0.5627461671829224,\n",
       "   0.5636908411979675,\n",
       "   0.5637281537055969,\n",
       "   0.5646445155143738,\n",
       "   0.5606551170349121,\n",
       "   0.5645716786384583,\n",
       "   0.5596866607666016,\n",
       "   0.5620637536048889,\n",
       "   0.5597010850906372,\n",
       "   0.5579074621200562,\n",
       "   0.5590865015983582,\n",
       "   0.5574448704719543,\n",
       "   0.5578902363777161,\n",
       "   0.5603060126304626,\n",
       "   0.5594968199729919,\n",
       "   0.5603640079498291,\n",
       "   0.5603407025337219,\n",
       "   0.5612537264823914,\n",
       "   0.5618923902511597,\n",
       "   0.5607842803001404,\n",
       "   0.5596281886100769,\n",
       "   0.5594742894172668,\n",
       "   0.559388279914856,\n",
       "   0.5600502490997314,\n",
       "   0.5589765310287476,\n",
       "   0.5624681115150452,\n",
       "   0.5596422553062439,\n",
       "   0.5587081909179688,\n",
       "   0.5555182099342346,\n",
       "   0.5543532967567444,\n",
       "   0.5546280145645142,\n",
       "   0.5579040050506592,\n",
       "   0.5590769052505493,\n",
       "   0.5575443506240845,\n",
       "   0.5575181245803833,\n",
       "   0.5585120916366577,\n",
       "   0.5596256852149963,\n",
       "   0.5647286176681519,\n",
       "   0.5636304020881653,\n",
       "   0.5609845519065857,\n",
       "   0.5618749260902405,\n",
       "   0.560511589050293,\n",
       "   0.5592831969261169,\n",
       "   0.5610893368721008,\n",
       "   0.5626087188720703,\n",
       "   0.562132716178894,\n",
       "   0.5621134042739868,\n",
       "   0.5590752363204956,\n",
       "   0.5637083053588867,\n",
       "   0.5615043044090271,\n",
       "   0.5622473359107971,\n",
       "   0.5577415823936462,\n",
       "   0.5558066368103027,\n",
       "   0.5547521114349365,\n",
       "   0.5536931157112122,\n",
       "   0.5529552698135376,\n",
       "   0.55142742395401,\n",
       "   0.5489635467529297,\n",
       "   0.5531109571456909,\n",
       "   0.5524217486381531,\n",
       "   0.553142786026001,\n",
       "   0.5549002885818481,\n",
       "   0.5547970533370972,\n",
       "   0.5536152720451355,\n",
       "   0.5568324327468872,\n",
       "   0.5582253336906433,\n",
       "   0.5565977096557617,\n",
       "   0.5571702718734741,\n",
       "   0.5579102039337158,\n",
       "   0.5580234527587891,\n",
       "   0.5569272041320801,\n",
       "   0.5594810247421265,\n",
       "   0.560067892074585,\n",
       "   0.5587126612663269,\n",
       "   0.5582451224327087,\n",
       "   0.5567565560340881,\n",
       "   0.5561044216156006,\n",
       "   0.5552179217338562,\n",
       "   0.5555629730224609,\n",
       "   0.5543491840362549,\n",
       "   0.5543702840805054,\n",
       "   0.556120753288269,\n",
       "   0.5592806339263916,\n",
       "   0.5568064451217651,\n",
       "   0.5545480251312256,\n",
       "   0.55536288022995,\n",
       "   0.555594801902771,\n",
       "   0.554029643535614,\n",
       "   0.5536760687828064,\n",
       "   0.5534884333610535,\n",
       "   0.5563482642173767,\n",
       "   0.5578292608261108,\n",
       "   0.5585606098175049,\n",
       "   0.5591074228286743,\n",
       "   0.5573652982711792,\n",
       "   0.5581459403038025,\n",
       "   0.5555118918418884,\n",
       "   0.5582363605499268,\n",
       "   0.5590604543685913,\n",
       "   0.5577606558799744,\n",
       "   0.5598078370094299,\n",
       "   0.5578102469444275,\n",
       "   0.558880090713501,\n",
       "   0.5578641891479492,\n",
       "   0.5577089786529541,\n",
       "   0.5578889846801758,\n",
       "   0.5543936491012573,\n",
       "   0.5525075197219849,\n",
       "   0.5491330027580261,\n",
       "   0.5487729907035828,\n",
       "   0.5448973178863525,\n",
       "   0.5434239506721497,\n",
       "   0.5454913377761841,\n",
       "   0.5456822514533997,\n",
       "   0.5423840880393982,\n",
       "   0.540723979473114,\n",
       "   0.5424327850341797,\n",
       "   0.5436064600944519,\n",
       "   0.5429795980453491,\n",
       "   0.546242356300354,\n",
       "   0.5468472242355347,\n",
       "   0.5448492169380188,\n",
       "   0.5476422309875488,\n",
       "   0.5484127402305603,\n",
       "   0.5489444136619568,\n",
       "   0.5496943593025208,\n",
       "   0.5513032078742981,\n",
       "   0.5486024022102356,\n",
       "   0.5444232225418091,\n",
       "   0.5446646809577942,\n",
       "   0.5432916283607483,\n",
       "   0.5389602780342102,\n",
       "   0.5354452729225159,\n",
       "   0.5325500965118408,\n",
       "   0.5324819087982178,\n",
       "   0.5342928171157837,\n",
       "   0.5327140092849731,\n",
       "   0.5341327786445618,\n",
       "   0.5326120853424072,\n",
       "   0.5329822897911072,\n",
       "   0.5348140597343445,\n",
       "   0.5385383367538452,\n",
       "   0.5400329232215881,\n",
       "   0.5392041802406311,\n",
       "   0.540528416633606,\n",
       "   0.5406056642532349,\n",
       "   0.5388980507850647,\n",
       "   0.5383551120758057,\n",
       "   0.5364334583282471,\n",
       "   0.5366684198379517,\n",
       "   0.5369171500205994,\n",
       "   0.5349724888801575,\n",
       "   0.5320603847503662,\n",
       "   0.5330656170845032,\n",
       "   0.5348640084266663,\n",
       "   0.5377190113067627,\n",
       "   0.536929190158844,\n",
       "   0.5347099304199219,\n",
       "   0.5345124006271362,\n",
       "   0.5372997522354126,\n",
       "   0.5394893288612366,\n",
       "   0.5379073619842529,\n",
       "   0.5404099822044373,\n",
       "   0.5424367785453796,\n",
       "   0.5404950380325317,\n",
       "   0.5391613245010376,\n",
       "   0.5391160845756531,\n",
       "   0.5405957698822021,\n",
       "   0.5415558815002441,\n",
       "   0.5427600145339966,\n",
       "   0.5416341423988342,\n",
       "   0.5418011546134949,\n",
       "   0.5426768064498901,\n",
       "   0.5463924407958984,\n",
       "   0.5487526655197144,\n",
       "   0.5496199727058411,\n",
       "   0.5507369637489319,\n",
       "   0.5509704947471619,\n",
       "   0.5524067878723145,\n",
       "   0.5541091561317444,\n",
       "   0.556706428527832,\n",
       "   0.5553946495056152,\n",
       "   0.5541389584541321,\n",
       "   0.555393636226654,\n",
       "   0.5561714768409729,\n",
       "   0.5557761788368225,\n",
       "   0.5574043393135071,\n",
       "   0.5566074252128601,\n",
       "   0.5576624870300293,\n",
       "   0.56085604429245,\n",
       "   0.5626234412193298,\n",
       "   0.561743974685669,\n",
       "   0.5600515604019165,\n",
       "   0.5612298846244812,\n",
       "   0.5653331279754639,\n",
       "   0.5678446292877197,\n",
       "   0.5699545741081238,\n",
       "   0.569722592830658,\n",
       "   0.5688800811767578,\n",
       "   0.5696446895599365,\n",
       "   0.5706444978713989,\n",
       "   0.5746480822563171,\n",
       "   0.5739833116531372,\n",
       "   0.5759214758872986,\n",
       "   0.5752667784690857,\n",
       "   0.5754588842391968,\n",
       "   0.5753962397575378,\n",
       "   0.5752290487289429,\n",
       "   0.573281466960907,\n",
       "   0.5697894096374512,\n",
       "   0.5700036883354187,\n",
       "   0.5708611607551575,\n",
       "   0.569299578666687,\n",
       "   0.5676449537277222,\n",
       "   0.5686904191970825,\n",
       "   0.5687295794487,\n",
       "   0.5689292550086975,\n",
       "   0.572334349155426,\n",
       "   0.5739983916282654,\n",
       "   0.5750612020492554,\n",
       "   0.5738373398780823,\n",
       "   0.5726262331008911,\n",
       "   0.5713699460029602,\n",
       "   0.569913923740387,\n",
       "   0.5698537826538086,\n",
       "   0.5704619288444519,\n",
       "   0.5711514949798584,\n",
       "   0.5706809163093567,\n",
       "   0.5697845816612244,\n",
       "   0.5686439871788025,\n",
       "   0.5688035488128662,\n",
       "   0.5705634951591492,\n",
       "   0.5687786340713501,\n",
       "   0.5673664212226868,\n",
       "   0.5687953233718872,\n",
       "   0.5690796375274658,\n",
       "   0.5674959421157837,\n",
       "   0.5654779672622681,\n",
       "   0.56414794921875,\n",
       "   0.5632376670837402,\n",
       "   0.56290602684021,\n",
       "   0.563035249710083,\n",
       "   0.5630393624305725,\n",
       "   0.5634252429008484,\n",
       "   0.5615235567092896,\n",
       "   0.5641528964042664,\n",
       "   0.56424480676651,\n",
       "   0.5665028691291809,\n",
       "   0.5673885941505432,\n",
       "   0.5668731927871704,\n",
       "   0.5673742294311523,\n",
       "   0.568588137626648,\n",
       "   0.5696958899497986,\n",
       "   0.5699662566184998,\n",
       "   0.5711476802825928,\n",
       "   0.5724828243255615,\n",
       "   0.5736308693885803,\n",
       "   0.5724606513977051,\n",
       "   0.5723667144775391,\n",
       "   0.5714172124862671,\n",
       "   0.572393536567688,\n",
       "   0.5730135440826416,\n",
       "   0.5726568102836609,\n",
       "   0.5711705684661865,\n",
       "   0.5724397301673889,\n",
       "   0.57177734375,\n",
       "   0.5709818005561829,\n",
       "   0.5699339509010315,\n",
       "   0.5673481225967407,\n",
       "   0.566209077835083,\n",
       "   0.5666167140007019,\n",
       "   0.5681126713752747,\n",
       "   0.5673729181289673,\n",
       "   0.5689446330070496,\n",
       "   0.5703111886978149,\n",
       "   0.573121964931488,\n",
       "   0.5741856694221497,\n",
       "   0.5751140713691711,\n",
       "   0.5751901865005493,\n",
       "   0.5760672092437744,\n",
       "   0.5770846009254456,\n",
       "   0.5790002346038818,\n",
       "   0.5788444876670837,\n",
       "   0.579020619392395,\n",
       "   0.5811365842819214,\n",
       "   0.5791416168212891,\n",
       "   0.580486536026001,\n",
       "   0.5803412199020386,\n",
       "   0.5851383209228516,\n",
       "   0.5856923460960388],\n",
       "  'ff_loss': [0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.91255122423172,\n",
       "   0.8833719491958618,\n",
       "   0.8501785397529602,\n",
       "   0.7968923449516296,\n",
       "   0.7528389096260071,\n",
       "   0.7270216345787048,\n",
       "   0.675902247428894,\n",
       "   0.6009680032730103,\n",
       "   0.5200700163841248,\n",
       "   0.4529898762702942,\n",
       "   0.3991282284259796,\n",
       "   0.3655683398246765,\n",
       "   0.32947155833244324,\n",
       "   0.2966940701007843,\n",
       "   0.27509573101997375,\n",
       "   0.260056734085083,\n",
       "   0.24791869521141052,\n",
       "   0.235953688621521,\n",
       "   0.22585006058216095,\n",
       "   0.21786344051361084,\n",
       "   0.21366803348064423,\n",
       "   0.21264171600341797,\n",
       "   0.20617924630641937,\n",
       "   0.20581971108913422,\n",
       "   0.20038175582885742,\n",
       "   0.19268900156021118,\n",
       "   0.19074681401252747,\n",
       "   0.18842634558677673,\n",
       "   0.18662254512310028,\n",
       "   0.18682505190372467,\n",
       "   0.18602459132671356,\n",
       "   0.18684253096580505,\n",
       "   0.1855209618806839,\n",
       "   0.18168793618679047,\n",
       "   0.1791178435087204,\n",
       "   0.1789044737815857,\n",
       "   0.1760990023612976,\n",
       "   0.17605997622013092,\n",
       "   0.17326027154922485,\n",
       "   0.16503852605819702,\n",
       "   0.1576458364725113,\n",
       "   0.15563824772834778,\n",
       "   0.15708352625370026,\n",
       "   0.16264145076274872,\n",
       "   0.16289453208446503,\n",
       "   0.16619369387626648,\n",
       "   0.1694207787513733,\n",
       "   0.1767626702785492,\n",
       "   0.18088313937187195,\n",
       "   0.18381880223751068,\n",
       "   0.19007891416549683,\n",
       "   0.19581155478954315,\n",
       "   0.19737590849399567,\n",
       "   0.20056122541427612,\n",
       "   0.2096961885690689,\n",
       "   0.21512188017368317,\n",
       "   0.22122366726398468,\n",
       "   0.2271883189678192,\n",
       "   0.2331714630126953,\n",
       "   0.23280327022075653,\n",
       "   0.23117277026176453,\n",
       "   0.22854118049144745,\n",
       "   0.23062653839588165,\n",
       "   0.2281734049320221,\n",
       "   0.2280392050743103,\n",
       "   0.22728337347507477,\n",
       "   0.23146747052669525,\n",
       "   0.23326390981674194,\n",
       "   0.24076725542545319,\n",
       "   0.23608607053756714,\n",
       "   0.2402694970369339,\n",
       "   0.240861713886261,\n",
       "   0.2456383854150772,\n",
       "   0.24582742154598236,\n",
       "   0.24239376187324524,\n",
       "   0.23675453662872314,\n",
       "   0.23269535601139069,\n",
       "   0.2262805551290512,\n",
       "   0.21822133660316467,\n",
       "   0.22152182459831238,\n",
       "   0.21598441898822784,\n",
       "   0.2163294106721878,\n",
       "   0.21640992164611816,\n",
       "   0.21393559873104095,\n",
       "   0.21492834389209747,\n",
       "   0.21754714846611023,\n",
       "   0.2129231095314026,\n",
       "   0.20855635404586792,\n",
       "   0.20343002676963806,\n",
       "   0.2072385996580124,\n",
       "   0.20245687663555145,\n",
       "   0.19936756789684296,\n",
       "   0.19953639805316925,\n",
       "   0.19357019662857056,\n",
       "   0.1956682801246643,\n",
       "   0.19838294386863708,\n",
       "   0.19696296751499176,\n",
       "   0.19548481702804565,\n",
       "   0.19880899786949158,\n",
       "   0.20224180817604065,\n",
       "   0.20734091103076935,\n",
       "   0.2059575915336609,\n",
       "   0.20804503560066223,\n",
       "   0.21024669706821442,\n",
       "   0.2120530903339386,\n",
       "   0.21710044145584106,\n",
       "   0.22286474704742432,\n",
       "   0.2328019142150879,\n",
       "   0.23747989535331726,\n",
       "   0.2395029515028,\n",
       "   0.23355980217456818,\n",
       "   0.23322591185569763,\n",
       "   0.2351582646369934,\n",
       "   0.23067913949489594,\n",
       "   0.22790148854255676,\n",
       "   0.22322432696819305,\n",
       "   0.22335407137870789,\n",
       "   0.22806967794895172,\n",
       "   0.22822795808315277,\n",
       "   0.23167350888252258,\n",
       "   0.2371288686990738,\n",
       "   0.2484568953514099,\n",
       "   0.24842500686645508,\n",
       "   0.24816949665546417,\n",
       "   0.24752433598041534,\n",
       "   0.2501887083053589,\n",
       "   0.2544727325439453,\n",
       "   0.25631967186927795,\n",
       "   0.2577313482761383,\n",
       "   0.25822320580482483,\n",
       "   0.2530460059642792,\n",
       "   0.2536875307559967,\n",
       "   0.25194793939590454,\n",
       "   0.254797101020813,\n",
       "   0.24955566227436066,\n",
       "   0.25577354431152344,\n",
       "   0.24968406558036804,\n",
       "   0.24789446592330933,\n",
       "   0.24795396625995636,\n",
       "   0.24970363080501556,\n",
       "   0.2512824833393097,\n",
       "   0.24989302456378937,\n",
       "   0.2506025731563568,\n",
       "   0.24986128509044647,\n",
       "   0.24531430006027222,\n",
       "   0.24519097805023193,\n",
       "   0.24450644850730896,\n",
       "   0.24334394931793213,\n",
       "   0.24386724829673767,\n",
       "   0.24076198041439056,\n",
       "   0.23756100237369537,\n",
       "   0.23278097808361053,\n",
       "   0.23692886531352997,\n",
       "   0.2391318678855896,\n",
       "   0.23805421590805054,\n",
       "   0.24106694757938385,\n",
       "   0.24174152314662933,\n",
       "   0.24607190489768982,\n",
       "   0.24995790421962738,\n",
       "   0.24921327829360962,\n",
       "   0.25317880511283875,\n",
       "   0.2562318444252014,\n",
       "   0.2635483741760254,\n",
       "   0.2630774974822998,\n",
       "   0.2665382921695709,\n",
       "   0.2643989622592926,\n",
       "   0.2602226436138153,\n",
       "   0.2543874680995941,\n",
       "   0.25730448961257935,\n",
       "   0.2542210817337036,\n",
       "   0.2550705075263977,\n",
       "   0.24844300746917725,\n",
       "   0.24903903901576996,\n",
       "   0.2473410964012146,\n",
       "   0.24395336210727692,\n",
       "   0.24219894409179688,\n",
       "   0.23761482536792755,\n",
       "   0.23406191170215607,\n",
       "   0.2357717752456665,\n",
       "   0.2374415397644043,\n",
       "   0.24030549824237823,\n",
       "   0.24443335831165314,\n",
       "   0.2406495213508606,\n",
       "   0.2400723099708557,\n",
       "   0.2372175008058548,\n",
       "   0.23403885960578918,\n",
       "   0.2344893366098404,\n",
       "   0.23716270923614502,\n",
       "   0.2421732395887375,\n",
       "   0.2472173124551773,\n",
       "   0.2526327967643738,\n",
       "   0.24817074835300446,\n",
       "   0.24280384182929993,\n",
       "   0.24482136964797974,\n",
       "   0.24713709950447083,\n",
       "   0.24260596930980682,\n",
       "   0.24638774991035461,\n",
       "   0.24599561095237732,\n",
       "   0.249813050031662,\n",
       "   0.2532307207584381,\n",
       "   0.2504614591598511,\n",
       "   0.25544700026512146,\n",
       "   0.25774404406547546,\n",
       "   0.2655736207962036,\n",
       "   0.2664852738380432,\n",
       "   0.2671893537044525,\n",
       "   0.2695583403110504,\n",
       "   0.2643849551677704,\n",
       "   0.2657710909843445,\n",
       "   0.25814229249954224,\n",
       "   0.2562407851219177,\n",
       "   0.25536611676216125,\n",
       "   0.252326637506485,\n",
       "   0.25382253527641296,\n",
       "   0.2529294192790985,\n",
       "   0.25823989510536194,\n",
       "   0.2608473002910614,\n",
       "   0.2600610852241516,\n",
       "   0.25898101925849915,\n",
       "   0.26190391182899475,\n",
       "   0.2602733075618744,\n",
       "   0.2650512158870697,\n",
       "   0.2683335840702057,\n",
       "   0.2693727910518646,\n",
       "   0.2651389539241791,\n",
       "   0.2632448673248291,\n",
       "   0.2612926661968231,\n",
       "   0.2587496340274811,\n",
       "   0.25870567560195923,\n",
       "   0.25569191575050354,\n",
       "   0.25969061255455017,\n",
       "   0.26474282145500183,\n",
       "   0.2665862739086151,\n",
       "   0.2660718560218811,\n",
       "   0.2673413157463074,\n",
       "   0.26652991771698,\n",
       "   0.26628321409225464,\n",
       "   0.2692487835884094,\n",
       "   0.26831579208374023,\n",
       "   0.2641970217227936,\n",
       "   0.2632572650909424,\n",
       "   0.27266988158226013,\n",
       "   0.26872169971466064,\n",
       "   0.2693995237350464,\n",
       "   0.268200159072876,\n",
       "   0.2671971917152405,\n",
       "   0.2699374556541443,\n",
       "   0.2700389325618744,\n",
       "   0.2683872878551483,\n",
       "   0.26584964990615845,\n",
       "   0.26893413066864014,\n",
       "   0.26879850029945374,\n",
       "   0.2668609023094177,\n",
       "   0.26703375577926636,\n",
       "   0.26530784368515015,\n",
       "   0.2693442404270172,\n",
       "   0.27158281207084656,\n",
       "   0.2718910872936249,\n",
       "   0.2692262828350067,\n",
       "   0.2699585556983948,\n",
       "   0.26816311478614807,\n",
       "   0.2655211389064789,\n",
       "   0.2637063264846802,\n",
       "   0.2613258957862854,\n",
       "   0.263205349445343,\n",
       "   0.26250797510147095,\n",
       "   0.26419341564178467,\n",
       "   0.27111849188804626,\n",
       "   0.2738935947418213,\n",
       "   0.2754669785499573,\n",
       "   0.27786940336227417,\n",
       "   0.283822238445282,\n",
       "   0.279249906539917,\n",
       "   0.2798742949962616,\n",
       "   0.2770629823207855,\n",
       "   0.2793166935443878,\n",
       "   0.2767649292945862,\n",
       "   0.27619677782058716,\n",
       "   0.2755061089992523,\n",
       "   0.27832651138305664,\n",
       "   0.2755451500415802,\n",
       "   0.26939693093299866,\n",
       "   0.2671433687210083,\n",
       "   0.26025423407554626,\n",
       "   0.26064640283584595,\n",
       "   0.26173135638237,\n",
       "   0.2576686143875122,\n",
       "   0.2601890563964844,\n",
       "   0.26269665360450745,\n",
       "   0.2636989951133728,\n",
       "   0.2660376727581024,\n",
       "   0.2666962146759033,\n",
       "   0.2660924792289734,\n",
       "   0.2661253809928894,\n",
       "   0.26926809549331665,\n",
       "   0.2685920000076294,\n",
       "   0.2698947787284851,\n",
       "   0.2669879198074341,\n",
       "   0.2673115134239197,\n",
       "   0.26666080951690674,\n",
       "   0.26568034291267395,\n",
       "   0.26516231894493103,\n",
       "   0.2596879303455353,\n",
       "   0.25969451665878296,\n",
       "   0.25995200872421265,\n",
       "   0.25582006573677063,\n",
       "   0.2530621290206909,\n",
       "   0.25165149569511414,\n",
       "   0.2559759020805359,\n",
       "   0.2606648802757263,\n",
       "   0.2614284157752991,\n",
       "   0.2636394500732422,\n",
       "   0.2616058886051178,\n",
       "   0.2615814507007599,\n",
       "   0.2619515657424927,\n",
       "   0.26194941997528076,\n",
       "   0.26126959919929504,\n",
       "   0.2600919008255005,\n",
       "   0.2550017237663269,\n",
       "   0.2549862563610077,\n",
       "   0.2538185119628906,\n",
       "   0.25530773401260376,\n",
       "   0.2581280767917633,\n",
       "   0.25493067502975464,\n",
       "   0.2580016851425171,\n",
       "   0.2536497414112091,\n",
       "   0.2538948655128479,\n",
       "   0.2526884078979492,\n",
       "   0.2536180317401886,\n",
       "   0.2577327489852905,\n",
       "   0.2560559809207916,\n",
       "   0.25973284244537354,\n",
       "   0.2611260712146759,\n",
       "   0.26573416590690613,\n",
       "   0.26270347833633423,\n",
       "   0.2630493640899658,\n",
       "   0.26263612508773804,\n",
       "   0.2648920714855194,\n",
       "   0.26298534870147705,\n",
       "   0.267630934715271,\n",
       "   0.26968303322792053,\n",
       "   0.27008056640625,\n",
       "   0.2702796459197998,\n",
       "   0.26795074343681335,\n",
       "   0.26289820671081543,\n",
       "   0.26156240701675415,\n",
       "   0.26625049114227295,\n",
       "   0.26257428526878357,\n",
       "   0.25941845774650574,\n",
       "   0.2588312327861786,\n",
       "   0.25250640511512756,\n",
       "   0.24996306002140045,\n",
       "   0.24886536598205566,\n",
       "   0.24911420047283173,\n",
       "   0.2499200701713562,\n",
       "   0.24896305799484253,\n",
       "   0.24141232669353485,\n",
       "   0.2378612905740738,\n",
       "   0.23918373882770538,\n",
       "   0.2382240891456604,\n",
       "   0.2382851392030716,\n",
       "   0.2392088770866394,\n",
       "   0.23528823256492615,\n",
       "   0.23606449365615845,\n",
       "   0.23731717467308044,\n",
       "   0.23904381692409515,\n",
       "   0.24170951545238495,\n",
       "   0.24450267851352692,\n",
       "   0.2482960820198059,\n",
       "   0.25165465474128723,\n",
       "   0.25101739168167114,\n",
       "   0.2584627866744995,\n",
       "   0.2641628682613373,\n",
       "   0.26806640625,\n",
       "   0.26690900325775146,\n",
       "   0.26514172554016113,\n",
       "   0.2602452039718628,\n",
       "   0.25508931279182434,\n",
       "   0.2551981508731842,\n",
       "   0.2567140758037567,\n",
       "   0.2586594223976135,\n",
       "   0.25972259044647217,\n",
       "   0.2645999491214752,\n",
       "   0.2670716643333435,\n",
       "   0.27132004499435425,\n",
       "   0.27866828441619873,\n",
       "   0.279987096786499,\n",
       "   0.2763139307498932,\n",
       "   0.2773655652999878,\n",
       "   0.2769060432910919,\n",
       "   0.27300387620925903,\n",
       "   0.2785569727420807,\n",
       "   0.2783032953739166,\n",
       "   0.2839042842388153,\n",
       "   0.28168633580207825,\n",
       "   0.2898569107055664,\n",
       "   0.29383647441864014,\n",
       "   0.29861563444137573,\n",
       "   0.2976147532463074,\n",
       "   0.29957127571105957,\n",
       "   0.2963596284389496,\n",
       "   0.29543620347976685,\n",
       "   0.29839497804641724,\n",
       "   0.29324403405189514,\n",
       "   0.29624617099761963,\n",
       "   0.29497092962265015,\n",
       "   0.2916286289691925,\n",
       "   0.28572848439216614,\n",
       "   0.28276196122169495,\n",
       "   0.2837633490562439,\n",
       "   0.27738282084465027,\n",
       "   0.27552321553230286,\n",
       "   0.2759896516799927,\n",
       "   0.2729610502719879,\n",
       "   0.2655632793903351,\n",
       "   0.26563501358032227,\n",
       "   0.2666766047477722,\n",
       "   0.2734343409538269,\n",
       "   0.27375614643096924,\n",
       "   0.26646021008491516,\n",
       "   0.26587334275245667,\n",
       "   0.2624499499797821,\n",
       "   0.25805404782295227,\n",
       "   0.26129987835884094,\n",
       "   0.2612471580505371,\n",
       "   0.261336088180542,\n",
       "   0.26355016231536865,\n",
       "   0.26038557291030884,\n",
       "   0.26098886132240295,\n",
       "   0.25836312770843506,\n",
       "   0.2589752674102783,\n",
       "   0.25851893424987793,\n",
       "   0.2656545341014862,\n",
       "   0.2695249021053314,\n",
       "   0.27482807636260986,\n",
       "   0.2784980237483978,\n",
       "   0.2847263216972351,\n",
       "   0.28411033749580383,\n",
       "   0.28163155913352966,\n",
       "   0.283037006855011,\n",
       "   0.2834557890892029,\n",
       "   0.2814944088459015,\n",
       "   0.279225617647171,\n",
       "   0.2773953676223755,\n",
       "   0.2744804322719574,\n",
       "   0.27295830845832825,\n",
       "   0.27255478501319885,\n",
       "   0.2729376256465912,\n",
       "   0.2680957317352295,\n",
       "   0.26907163858413696,\n",
       "   0.26655927300453186,\n",
       "   0.269072562456131,\n",
       "   0.2687752842903137,\n",
       "   0.2702467143535614,\n",
       "   0.26729267835617065,\n",
       "   0.2686521112918854,\n",
       "   0.2706610858440399,\n",
       "   0.2718680799007416,\n",
       "   0.27272874116897583,\n",
       "   0.26982852816581726,\n",
       "   0.27008256316185,\n",
       "   0.2695000469684601,\n",
       "   0.26561665534973145,\n",
       "   0.26548102498054504,\n",
       "   0.26383692026138306,\n",
       "   0.2630646228790283,\n",
       "   0.2604120969772339,\n",
       "   0.26523396372795105,\n",
       "   0.2686261832714081,\n",
       "   0.26734912395477295,\n",
       "   0.2629445493221283,\n",
       "   0.26372596621513367,\n",
       "   0.26604706048965454,\n",
       "   0.2645009756088257,\n",
       "   0.2611611485481262,\n",
       "   0.26165103912353516,\n",
       "   0.262248158454895,\n",
       "   0.2638523578643799,\n",
       "   0.263162225484848,\n",
       "   0.2638300955295563,\n",
       "   0.26319873332977295,\n",
       "   0.26253700256347656,\n",
       "   0.2604750692844391,\n",
       "   0.2576044201850891,\n",
       "   0.2521442472934723,\n",
       "   0.24832558631896973,\n",
       "   0.2452041655778885,\n",
       "   0.24407726526260376,\n",
       "   0.24343574047088623,\n",
       "   0.2424941062927246],\n",
       "  'test_losses': [3.305520474910736,\n",
       "   2.731287270784378,\n",
       "   2.5609850138425827,\n",
       "   2.4935298413038254,\n",
       "   2.456502214074135,\n",
       "   2.4350511953234673,\n",
       "   2.425445780158043,\n",
       "   2.426376350224018,\n",
       "   2.4282713755965233,\n",
       "   2.4326428547501564,\n",
       "   2.4362621307373047,\n",
       "   2.395202547311783,\n",
       "   2.3191384822130203,\n",
       "   2.1740759536623955,\n",
       "   2.0588530972599983,\n",
       "   1.9728609696030617,\n",
       "   1.8703298419713974,\n",
       "   1.755444273352623,\n",
       "   1.659239299595356,\n",
       "   1.5633102729916573,\n",
       "   1.4867145270109177,\n",
       "   1.440828874707222,\n",
       "   1.3819525390863419,\n",
       "   1.3422879353165627,\n",
       "   1.3090060353279114,\n",
       "   1.285890281200409,\n",
       "   1.270364686846733,\n",
       "   1.2516775280237198,\n",
       "   1.2390858978033066,\n",
       "   1.2288886681199074,\n",
       "   1.2209528088569641,\n",
       "   1.2185680717229843,\n",
       "   1.216256145387888,\n",
       "   1.2373158931732178,\n",
       "   1.2363082580268383,\n",
       "   1.2468599155545235,\n",
       "   1.2396219223737717,\n",
       "   1.2463459149003029,\n",
       "   1.2616771683096886,\n",
       "   1.2754146605730057,\n",
       "   1.2861551940441132,\n",
       "   1.295312650501728,\n",
       "   1.307689718902111,\n",
       "   1.313305675983429,\n",
       "   1.3179077357053757,\n",
       "   1.329477682709694,\n",
       "   1.329913005232811,\n",
       "   1.3115401417016983,\n",
       "   1.3005295246839523,\n",
       "   1.287066973745823,\n",
       "   1.2724658027291298,\n",
       "   1.264328382909298,\n",
       "   1.2471940070390701,\n",
       "   1.2383108288049698,\n",
       "   1.236625824123621,\n",
       "   1.2318136394023895,\n",
       "   1.2203936763107777,\n",
       "   1.2345826365053654,\n",
       "   1.2405571825802326,\n",
       "   1.2497045882046223,\n",
       "   1.24836066365242,\n",
       "   1.2450914271175861,\n",
       "   1.2295408602803946,\n",
       "   1.2282236814498901,\n",
       "   1.2299433331936598,\n",
       "   1.23193640075624,\n",
       "   1.2406248338520527,\n",
       "   1.2589772120118141,\n",
       "   1.2742154151201248,\n",
       "   1.2705653607845306,\n",
       "   1.2604939639568329,\n",
       "   1.2609795965254307,\n",
       "   1.2465796172618866,\n",
       "   1.2504388615489006,\n",
       "   1.2538665048778057,\n",
       "   1.2487208992242813,\n",
       "   1.2409786581993103,\n",
       "   1.2436455972492695,\n",
       "   1.2532851099967957,\n",
       "   1.240844376385212,\n",
       "   1.2447153963148594,\n",
       "   1.243436112999916,\n",
       "   1.2442977577447891,\n",
       "   1.2387418448925018,\n",
       "   1.2425715178251266,\n",
       "   1.2501732185482979,\n",
       "   1.249388925731182,\n",
       "   1.2481903806328773,\n",
       "   1.2363916486501694,\n",
       "   1.243981011211872,\n",
       "   1.245489738881588,\n",
       "   1.2381716892123222,\n",
       "   1.230034977197647,\n",
       "   1.2131362184882164,\n",
       "   1.2157446816563606,\n",
       "   1.224761851131916,\n",
       "   1.2162825092673302,\n",
       "   1.220488466322422,\n",
       "   1.216544158756733,\n",
       "   1.2156964018940926,\n",
       "   1.1946884989738464,\n",
       "   1.180398553609848,\n",
       "   1.1791123449802399,\n",
       "   1.1656304076313972,\n",
       "   1.1718845665454865,\n",
       "   1.160071887075901,\n",
       "   1.1435115858912468,\n",
       "   1.1372782215476036,\n",
       "   1.1385727152228355,\n",
       "   1.132307056337595,\n",
       "   1.1344481147825718,\n",
       "   1.13732785359025,\n",
       "   1.1359154023230076,\n",
       "   1.130539558827877,\n",
       "   1.1255521066486835,\n",
       "   1.1266780570149422,\n",
       "   1.126241534948349,\n",
       "   1.1381082497537136,\n",
       "   1.1437882147729397,\n",
       "   1.1497649736702442,\n",
       "   1.1591386310756207,\n",
       "   1.170240730047226,\n",
       "   1.175665207207203,\n",
       "   1.1770372726023197,\n",
       "   1.1742971278727055,\n",
       "   1.173074897378683,\n",
       "   1.1692236848175526,\n",
       "   1.1844509169459343,\n",
       "   1.187696736305952,\n",
       "   1.1953725405037403,\n",
       "   1.2012934051454067,\n",
       "   1.2067558132112026,\n",
       "   1.208139680325985,\n",
       "   1.2030728980898857,\n",
       "   1.1986572295427322,\n",
       "   1.1956388913094997,\n",
       "   1.1967374868690968,\n",
       "   1.1922054812312126,\n",
       "   1.1906851008534431,\n",
       "   1.1819385178387165,\n",
       "   1.181940320879221,\n",
       "   1.178390096873045,\n",
       "   1.176857776939869,\n",
       "   1.1821630895137787,\n",
       "   1.1745789758861065,\n",
       "   1.1843548640608788,\n",
       "   1.1798813119530678,\n",
       "   1.1828387416899204,\n",
       "   1.1848624348640442,\n",
       "   1.1865372136235237,\n",
       "   1.1864482834935188,\n",
       "   1.1823288053274155,\n",
       "   1.1857196055352688,\n",
       "   1.1799056380987167,\n",
       "   1.1711025945842266,\n",
       "   1.1666741520166397,\n",
       "   1.1618803553283215,\n",
       "   1.155124794691801,\n",
       "   1.150137037038803,\n",
       "   1.149332508444786,\n",
       "   1.1405028700828552,\n",
       "   1.1349075362086296,\n",
       "   1.1229893490672112,\n",
       "   1.126375537365675,\n",
       "   1.1345635987818241,\n",
       "   1.1371408998966217,\n",
       "   1.1382112242281437,\n",
       "   1.1444014310836792,\n",
       "   1.1499419175088406,\n",
       "   1.1524282097816467,\n",
       "   1.1618937961757183,\n",
       "   1.1603924185037613,\n",
       "   1.167262502014637,\n",
       "   1.1768644079566002,\n",
       "   1.1782932355999947,\n",
       "   1.1816448420286179,\n",
       "   1.1839040517807007,\n",
       "   1.1795716434717178,\n",
       "   1.1874063909053802,\n",
       "   1.1797181516885757,\n",
       "   1.1739013344049454,\n",
       "   1.1763604208827019,\n",
       "   1.1735694631934166,\n",
       "   1.1720475144684315,\n",
       "   1.1725777126848698,\n",
       "   1.1601080670952797,\n",
       "   1.1618375331163406,\n",
       "   1.1563929207623005,\n",
       "   1.160310722887516,\n",
       "   1.1700236797332764,\n",
       "   1.1737420111894608,\n",
       "   1.177570402622223,\n",
       "   1.1681985408067703,\n",
       "   1.161036342382431,\n",
       "   1.160402588546276,\n",
       "   1.1628732979297638,\n",
       "   1.1592483893036842,\n",
       "   1.1614556387066841,\n",
       "   1.1577804908156395,\n",
       "   1.158782348036766,\n",
       "   1.159843310713768,\n",
       "   1.1555028408765793,\n",
       "   1.1456510871648788,\n",
       "   1.1401283144950867,\n",
       "   1.1419740989804268,\n",
       "   1.1347102001309395,\n",
       "   1.141855850815773,\n",
       "   1.1411146074533463,\n",
       "   1.1506117060780525,\n",
       "   1.1556959971785545,\n",
       "   1.1546625271439552,\n",
       "   1.1662082597613335,\n",
       "   1.173715054988861,\n",
       "   1.1788405552506447,\n",
       "   1.1688864156603813,\n",
       "   1.1693918630480766,\n",
       "   1.1625922918319702,\n",
       "   1.1580077931284904,\n",
       "   1.153268776834011,\n",
       "   1.1451112478971481,\n",
       "   1.1499399989843369,\n",
       "   1.1504703164100647,\n",
       "   1.1520039737224579,\n",
       "   1.1565623357892036,\n",
       "   1.1531974598765373,\n",
       "   1.158319391310215,\n",
       "   1.1569410264492035,\n",
       "   1.150176279246807,\n",
       "   1.1486547365784645,\n",
       "   1.150821290910244,\n",
       "   1.1479814276099205,\n",
       "   1.1551929339766502,\n",
       "   1.1569883450865746,\n",
       "   1.1575718820095062,\n",
       "   1.145041510462761,\n",
       "   1.1459756046533585,\n",
       "   1.1439277902245522,\n",
       "   1.144320197403431,\n",
       "   1.1464992016553879,\n",
       "   1.1475457102060318,\n",
       "   1.152627371251583,\n",
       "   1.1685702800750732,\n",
       "   1.1780194789171219,\n",
       "   1.1830986812710762,\n",
       "   1.1859296932816505,\n",
       "   1.1921653524041176,\n",
       "   1.1935659050941467,\n",
       "   1.2019724771380424,\n",
       "   1.1971770524978638,\n",
       "   1.1919133737683296,\n",
       "   1.1895694956183434,\n",
       "   1.1977796778082848,\n",
       "   1.1905974969267845,\n",
       "   1.1919570714235306,\n",
       "   1.1948697566986084,\n",
       "   1.1903727352619171,\n",
       "   1.1955610513687134,\n",
       "   1.1907423436641693,\n",
       "   1.190675511956215,\n",
       "   1.1854021474719048,\n",
       "   1.1888310313224792,\n",
       "   1.1787886396050453,\n",
       "   1.1743816286325455,\n",
       "   1.1709909811615944,\n",
       "   1.1654390841722488,\n",
       "   1.1684156581759453,\n",
       "   1.1739556342363358,\n",
       "   1.1669031828641891,\n",
       "   1.1701599135994911,\n",
       "   1.168152317404747,\n",
       "   1.1642947643995285,\n",
       "   1.1611598432064056,\n",
       "   1.1599155366420746,\n",
       "   1.1553768143057823,\n",
       "   1.161760151386261,\n",
       "   1.1676982939243317,\n",
       "   1.1694551035761833,\n",
       "   1.186009868979454,\n",
       "   1.1954394578933716,\n",
       "   1.1976661011576653,\n",
       "   1.2032190337777138,\n",
       "   1.211287222802639,\n",
       "   1.2026334553956985,\n",
       "   1.1944130957126617,\n",
       "   1.1879949569702148,\n",
       "   1.1837682649493217,\n",
       "   1.1782877370715141,\n",
       "   1.1732675582170486,\n",
       "   1.1682282164692879,\n",
       "   1.1682114005088806,\n",
       "   1.1631465628743172,\n",
       "   1.1587056815624237,\n",
       "   1.1638649478554726,\n",
       "   1.1534114852547646,\n",
       "   1.1503344401717186,\n",
       "   1.157850593328476,\n",
       "   1.1681713610887527,\n",
       "   1.173130452632904,\n",
       "   1.1808955371379852,\n",
       "   1.1911422237753868,\n",
       "   1.1990220546722412,\n",
       "   1.2065314576029778,\n",
       "   1.2096547335386276,\n",
       "   1.2147324234247208,\n",
       "   1.218711070716381,\n",
       "   1.2198740988969803,\n",
       "   1.2195847630500793,\n",
       "   1.2190227806568146,\n",
       "   1.2154005095362663,\n",
       "   1.2117283195257187,\n",
       "   1.211112841963768,\n",
       "   1.1998886615037918,\n",
       "   1.1908911615610123,\n",
       "   1.1889232620596886,\n",
       "   1.188838891685009,\n",
       "   1.1872906982898712,\n",
       "   1.1812686994671822,\n",
       "   1.1751780956983566,\n",
       "   1.1754173263907433,\n",
       "   1.1705941930413246,\n",
       "   1.1688423603773117,\n",
       "   1.1708715111017227,\n",
       "   1.1703086718916893,\n",
       "   1.1690475344657898,\n",
       "   1.1600041389465332,\n",
       "   1.1586760357022285,\n",
       "   1.159031555056572,\n",
       "   1.1596691757440567,\n",
       "   1.1486892625689507,\n",
       "   1.1487957015633583,\n",
       "   1.1479654386639595,\n",
       "   1.1430206820368767,\n",
       "   1.1452530324459076,\n",
       "   1.1371920183300972,\n",
       "   1.1394215375185013,\n",
       "   1.1360101774334908,\n",
       "   1.1442320719361305,\n",
       "   1.1375721096992493,\n",
       "   1.1366573870182037,\n",
       "   1.1416032239794731,\n",
       "   1.1393994688987732,\n",
       "   1.1410577818751335,\n",
       "   1.1392494514584541,\n",
       "   1.1428993344306946,\n",
       "   1.14024106413126,\n",
       "   1.145907148718834,\n",
       "   1.1379880011081696,\n",
       "   1.1438090950250626,\n",
       "   1.1427794471383095,\n",
       "   1.1484056934714317,\n",
       "   1.1529094502329826,\n",
       "   1.1601486206054688,\n",
       "   1.164664976298809,\n",
       "   1.1625225991010666,\n",
       "   1.165864072740078,\n",
       "   1.171376146376133,\n",
       "   1.1816839650273323,\n",
       "   1.174986556172371,\n",
       "   1.1713639795780182,\n",
       "   1.170789435505867,\n",
       "   1.166787289083004,\n",
       "   1.1675425544381142,\n",
       "   1.1618714034557343,\n",
       "   1.162688784301281,\n",
       "   1.1644534319639206,\n",
       "   1.1648519486188889,\n",
       "   1.150119997560978,\n",
       "   1.1452619209885597,\n",
       "   1.1403463631868362,\n",
       "   1.1403202563524246,\n",
       "   1.1499938294291496,\n",
       "   1.1466588079929352,\n",
       "   1.1483050510287285,\n",
       "   1.1492699012160301,\n",
       "   1.1500881537795067,\n",
       "   1.1487623378634453,\n",
       "   1.1498723179101944,\n",
       "   1.1530312225222588,\n",
       "   1.150465115904808,\n",
       "   1.1538049206137657,\n",
       "   1.1468756347894669,\n",
       "   1.1544160768389702,\n",
       "   1.1622152626514435,\n",
       "   1.1665882021188736,\n",
       "   1.167133390903473,\n",
       "   1.163602277636528,\n",
       "   1.157068558037281,\n",
       "   1.153304561972618,\n",
       "   1.1579589173197746,\n",
       "   1.1617388427257538,\n",
       "   1.1626029312610626,\n",
       "   1.1625517085194588,\n",
       "   1.1620185300707817,\n",
       "   1.1627818048000336,\n",
       "   1.16740183532238,\n",
       "   1.1720547303557396,\n",
       "   1.1746918112039566,\n",
       "   1.1743289977312088,\n",
       "   1.177179604768753,\n",
       "   1.181342527270317,\n",
       "   1.1784880310297012,\n",
       "   1.1909167915582657,\n",
       "   1.1896115988492966,\n",
       "   1.1987909898161888,\n",
       "   1.2033195868134499,\n",
       "   1.2130688279867172,\n",
       "   1.2176138833165169,\n",
       "   1.2201567068696022,\n",
       "   1.215454488992691,\n",
       "   1.216265432536602,\n",
       "   1.2125677466392517,\n",
       "   1.216751791536808,\n",
       "   1.2206960693001747,\n",
       "   1.2220247089862823,\n",
       "   1.22732575237751,\n",
       "   1.227862112224102,\n",
       "   1.2282258942723274,\n",
       "   1.2240984290838242,\n",
       "   1.2217342257499695,\n",
       "   1.222052276134491,\n",
       "   1.2181204706430435,\n",
       "   1.223317213356495,\n",
       "   1.2207911685109138,\n",
       "   1.215280294418335,\n",
       "   1.2084770277142525,\n",
       "   1.2076128870248795,\n",
       "   1.2104204297065735,\n",
       "   1.225074164569378,\n",
       "   1.2333472892642021,\n",
       "   1.2265389934182167,\n",
       "   1.2273760586977005,\n",
       "   1.2236956357955933,\n",
       "   1.221642442047596,\n",
       "   1.2271341681480408,\n",
       "   1.2252858057618141,\n",
       "   1.2201993614435196,\n",
       "   1.2195242643356323,\n",
       "   1.2095407992601395,\n",
       "   1.2024865597486496,\n",
       "   1.194540187716484,\n",
       "   1.1851175054907799,\n",
       "   1.1823071986436844,\n",
       "   1.183843471109867,\n",
       "   1.1794321537017822,\n",
       "   1.1844029799103737,\n",
       "   1.1821409985423088,\n",
       "   1.1819719225168228,\n",
       "   1.1741217151284218,\n",
       "   1.16404390335083,\n",
       "   1.1615883484482765,\n",
       "   1.1596893519163132,\n",
       "   1.1572491973638535,\n",
       "   1.1551483124494553,\n",
       "   1.1551023088395596,\n",
       "   1.153555691242218,\n",
       "   1.1529265269637108,\n",
       "   1.1571815721690655,\n",
       "   1.1673438996076584,\n",
       "   1.1667525693774223,\n",
       "   1.171710729598999,\n",
       "   1.1718131825327873,\n",
       "   1.1745406985282898,\n",
       "   1.1739255636930466,\n",
       "   1.178088828921318,\n",
       "   1.1776172965765,\n",
       "   1.1816914454102516,\n",
       "   1.1848676353693008,\n",
       "   1.184024065732956,\n",
       "   1.187084160745144,\n",
       "   1.1835765168070793,\n",
       "   1.1846024692058563,\n",
       "   1.1862405389547348,\n",
       "   1.1859087720513344,\n",
       "   1.1883824244141579,\n",
       "   1.1893346458673477,\n",
       "   1.1940088495612144,\n",
       "   1.1922896280884743,\n",
       "   1.1982846707105637,\n",
       "   1.1998902410268784,\n",
       "   1.193465806543827,\n",
       "   1.1900269463658333,\n",
       "   1.1935749724507332,\n",
       "   1.1973225325345993,\n",
       "   1.1955316439270973,\n",
       "   1.1959886252880096,\n",
       "   1.2012012749910355,\n",
       "   1.202378548681736,\n",
       "   1.201849952340126,\n",
       "   1.203495092689991,\n",
       "   1.204530157148838,\n",
       "   1.1984881684184074,\n",
       "   1.1978008300065994,\n",
       "   1.194907508790493,\n",
       "   1.19200100004673,\n",
       "   1.18703593313694,\n",
       "   1.1791987791657448,\n",
       "   1.1740596368908882,\n",
       "   1.168359413743019,\n",
       "   1.1730301082134247,\n",
       "   1.1731284335255623],\n",
       "  'pce_acc': [96.16820526123047,\n",
       "   63.353843688964844,\n",
       "   49.40534973144531,\n",
       "   41.306297302246094,\n",
       "   35.92051696777344,\n",
       "   33.70142364501953,\n",
       "   31.877899169921875,\n",
       "   30.668617248535156,\n",
       "   30.520889282226562,\n",
       "   30.775117874145508,\n",
       "   31.2220516204834,\n",
       "   32.73834228515625,\n",
       "   33.822601318359375,\n",
       "   33.574867248535156,\n",
       "   34.29489517211914,\n",
       "   34.24051284790039,\n",
       "   33.4411735534668,\n",
       "   32.56203079223633,\n",
       "   32.15657424926758,\n",
       "   32.04287338256836,\n",
       "   32.143211364746094,\n",
       "   33.01851272583008,\n",
       "   32.52276611328125,\n",
       "   33.38872528076172,\n",
       "   34.84027862548828,\n",
       "   36.44868087768555,\n",
       "   38.710262298583984,\n",
       "   39.95820617675781,\n",
       "   42.059661865234375,\n",
       "   43.54572677612305,\n",
       "   45.04463195800781,\n",
       "   47.20803451538086,\n",
       "   48.566341400146484,\n",
       "   50.82541275024414,\n",
       "   51.788063049316406,\n",
       "   54.46547317504883,\n",
       "   54.38509750366211,\n",
       "   54.976444244384766,\n",
       "   56.312835693359375,\n",
       "   57.460975646972656,\n",
       "   58.08794021606445,\n",
       "   59.140628814697266,\n",
       "   60.514076232910156,\n",
       "   61.63191223144531,\n",
       "   61.917049407958984,\n",
       "   63.23569869995117,\n",
       "   63.038230895996094,\n",
       "   62.54568099975586,\n",
       "   63.013668060302734,\n",
       "   62.66645812988281,\n",
       "   62.8742561340332,\n",
       "   62.63752746582031,\n",
       "   62.1978874206543,\n",
       "   61.82688522338867,\n",
       "   62.399269104003906,\n",
       "   62.50141906738281,\n",
       "   61.93133544921875,\n",
       "   61.65492630004883,\n",
       "   61.332427978515625,\n",
       "   61.3553466796875,\n",
       "   60.30369567871094,\n",
       "   59.68226623535156,\n",
       "   59.39402770996094,\n",
       "   59.621055603027344,\n",
       "   59.45307159423828,\n",
       "   59.09769058227539,\n",
       "   58.18186950683594,\n",
       "   57.96012878417969,\n",
       "   58.85817337036133,\n",
       "   58.95404052734375,\n",
       "   58.251548767089844,\n",
       "   58.26774215698242,\n",
       "   56.98089599609375,\n",
       "   57.169891357421875,\n",
       "   56.7079963684082,\n",
       "   56.33992004394531,\n",
       "   55.228172302246094,\n",
       "   54.9305419921875,\n",
       "   54.663352966308594,\n",
       "   53.52793884277344,\n",
       "   53.37411117553711,\n",
       "   53.664886474609375,\n",
       "   53.846717834472656,\n",
       "   53.468971252441406,\n",
       "   53.595184326171875,\n",
       "   54.64549255371094,\n",
       "   55.34575653076172,\n",
       "   56.056602478027344,\n",
       "   55.96818923950195,\n",
       "   56.5983772277832,\n",
       "   57.712345123291016,\n",
       "   57.41492462158203,\n",
       "   56.79093551635742,\n",
       "   55.50086975097656,\n",
       "   55.05944061279297,\n",
       "   55.75586700439453,\n",
       "   55.04169464111328,\n",
       "   55.69525909423828,\n",
       "   56.167964935302734,\n",
       "   55.377262115478516,\n",
       "   54.63570785522461,\n",
       "   54.31731414794922,\n",
       "   54.41978454589844,\n",
       "   54.6130256652832,\n",
       "   55.123023986816406,\n",
       "   54.1015625,\n",
       "   53.31526565551758,\n",
       "   53.150760650634766,\n",
       "   52.7977409362793,\n",
       "   52.579654693603516,\n",
       "   52.50648880004883,\n",
       "   52.09895324707031,\n",
       "   51.35176086425781,\n",
       "   50.54613494873047,\n",
       "   49.85311508178711,\n",
       "   49.344268798828125,\n",
       "   48.90159606933594,\n",
       "   48.52928161621094,\n",
       "   48.74269104003906,\n",
       "   48.81714630126953,\n",
       "   49.58158874511719,\n",
       "   50.53466033935547,\n",
       "   50.72142028808594,\n",
       "   51.197410583496094,\n",
       "   51.377315521240234,\n",
       "   51.55708312988281,\n",
       "   51.80271911621094,\n",
       "   53.415828704833984,\n",
       "   54.40756607055664,\n",
       "   54.889591217041016,\n",
       "   55.48405075073242,\n",
       "   54.981971740722656,\n",
       "   55.7972297668457,\n",
       "   55.81697082519531,\n",
       "   54.923583984375,\n",
       "   54.37174987792969,\n",
       "   54.178165435791016,\n",
       "   54.004520416259766,\n",
       "   54.13454818725586,\n",
       "   53.24553680419922,\n",
       "   53.17610549926758,\n",
       "   53.12604904174805,\n",
       "   52.72354507446289,\n",
       "   52.478729248046875,\n",
       "   52.02656555175781,\n",
       "   52.54431915283203,\n",
       "   52.45702362060547,\n",
       "   52.44427490234375,\n",
       "   52.709285736083984,\n",
       "   52.28573226928711,\n",
       "   52.055442810058594,\n",
       "   52.47626876831055,\n",
       "   53.011627197265625,\n",
       "   52.75905227661133,\n",
       "   53.22821044921875,\n",
       "   52.66169357299805,\n",
       "   52.53871536254883,\n",
       "   52.32391357421875,\n",
       "   52.054603576660156,\n",
       "   52.105194091796875,\n",
       "   51.99229049682617,\n",
       "   52.773834228515625,\n",
       "   52.112857818603516,\n",
       "   52.66501998901367,\n",
       "   53.30954360961914,\n",
       "   53.545188903808594,\n",
       "   53.8538703918457,\n",
       "   54.26654052734375,\n",
       "   54.23805618286133,\n",
       "   54.678131103515625,\n",
       "   55.32567596435547,\n",
       "   55.2889518737793,\n",
       "   55.533973693847656,\n",
       "   56.656551361083984,\n",
       "   56.36155700683594,\n",
       "   56.77776336669922,\n",
       "   57.28143310546875,\n",
       "   57.479679107666016,\n",
       "   58.092044830322266,\n",
       "   57.74989700317383,\n",
       "   57.70906066894531,\n",
       "   58.130409240722656,\n",
       "   57.92509841918945,\n",
       "   58.192466735839844,\n",
       "   58.776973724365234,\n",
       "   58.26047134399414,\n",
       "   58.827537536621094,\n",
       "   58.20469284057617,\n",
       "   58.63562774658203,\n",
       "   58.779273986816406,\n",
       "   58.69424819946289,\n",
       "   58.49825668334961,\n",
       "   57.934303283691406,\n",
       "   57.38722610473633,\n",
       "   57.19078826904297,\n",
       "   57.48277282714844,\n",
       "   56.68510818481445,\n",
       "   56.370269775390625,\n",
       "   55.738555908203125,\n",
       "   55.28914260864258,\n",
       "   54.972076416015625,\n",
       "   55.16402053833008,\n",
       "   54.516483306884766,\n",
       "   53.981422424316406,\n",
       "   53.92226028442383,\n",
       "   53.29765701293945,\n",
       "   53.59069061279297,\n",
       "   53.83644104003906,\n",
       "   54.291046142578125,\n",
       "   54.62812805175781,\n",
       "   54.592124938964844,\n",
       "   55.30732345581055,\n",
       "   55.56596374511719,\n",
       "   55.20756149291992,\n",
       "   54.7103157043457,\n",
       "   54.2597770690918,\n",
       "   54.021488189697266,\n",
       "   54.136104583740234,\n",
       "   54.25441360473633,\n",
       "   54.75401306152344,\n",
       "   55.48688888549805,\n",
       "   55.59012985229492,\n",
       "   55.77662658691406,\n",
       "   56.04425811767578,\n",
       "   56.10480880737305,\n",
       "   56.26226806640625,\n",
       "   56.08982467651367,\n",
       "   55.91302490234375,\n",
       "   55.89629364013672,\n",
       "   56.197547912597656,\n",
       "   56.368614196777344,\n",
       "   56.3213996887207,\n",
       "   56.18704605102539,\n",
       "   56.22038269042969,\n",
       "   55.30753707885742,\n",
       "   55.33167266845703,\n",
       "   55.578678131103516,\n",
       "   55.66702651977539,\n",
       "   55.99680709838867,\n",
       "   56.75146484375,\n",
       "   56.99756622314453,\n",
       "   57.52482986450195,\n",
       "   58.17509841918945,\n",
       "   58.77666473388672,\n",
       "   58.92816925048828,\n",
       "   59.61254119873047,\n",
       "   59.502349853515625,\n",
       "   59.342002868652344,\n",
       "   58.83514404296875,\n",
       "   58.76153564453125,\n",
       "   58.36285400390625,\n",
       "   57.885887145996094,\n",
       "   57.198429107666016,\n",
       "   56.832271575927734,\n",
       "   56.94919967651367,\n",
       "   56.561649322509766,\n",
       "   56.28034973144531,\n",
       "   55.921573638916016,\n",
       "   55.65518569946289,\n",
       "   55.40266036987305,\n",
       "   55.060340881347656,\n",
       "   54.57094192504883,\n",
       "   54.74139404296875,\n",
       "   54.74179458618164,\n",
       "   54.588722229003906,\n",
       "   54.492515563964844,\n",
       "   54.66033935546875,\n",
       "   54.25789260864258,\n",
       "   54.249935150146484,\n",
       "   53.95686721801758,\n",
       "   54.03706359863281,\n",
       "   53.636505126953125,\n",
       "   53.50539016723633,\n",
       "   53.292579650878906,\n",
       "   53.22492599487305,\n",
       "   53.82817459106445,\n",
       "   54.34115982055664,\n",
       "   55.17262268066406,\n",
       "   55.42934036254883,\n",
       "   55.73299026489258,\n",
       "   56.43720626831055,\n",
       "   56.62118911743164,\n",
       "   56.462162017822266,\n",
       "   56.30812072753906,\n",
       "   56.15498352050781,\n",
       "   55.773563385009766,\n",
       "   55.56114959716797,\n",
       "   55.620201110839844,\n",
       "   55.297916412353516,\n",
       "   55.076045989990234,\n",
       "   55.190826416015625,\n",
       "   55.215728759765625,\n",
       "   55.53187561035156,\n",
       "   56.020748138427734,\n",
       "   56.05605697631836,\n",
       "   56.70682907104492,\n",
       "   57.8638801574707,\n",
       "   57.969696044921875,\n",
       "   58.16807174682617,\n",
       "   59.043800354003906,\n",
       "   59.3625602722168,\n",
       "   59.40610885620117,\n",
       "   59.73838424682617,\n",
       "   59.86285400390625,\n",
       "   60.10348892211914,\n",
       "   60.301979064941406,\n",
       "   60.82516860961914,\n",
       "   60.65372085571289,\n",
       "   60.48329544067383,\n",
       "   60.498382568359375,\n",
       "   60.27965545654297,\n",
       "   59.779876708984375,\n",
       "   59.27241516113281,\n",
       "   59.24819564819336,\n",
       "   59.20463180541992,\n",
       "   59.05292510986328,\n",
       "   59.202239990234375,\n",
       "   59.02473831176758,\n",
       "   58.767974853515625,\n",
       "   57.91366195678711,\n",
       "   57.76691436767578,\n",
       "   57.82353210449219,\n",
       "   57.55179977416992,\n",
       "   57.12250518798828,\n",
       "   56.627166748046875,\n",
       "   56.569854736328125,\n",
       "   56.29918670654297,\n",
       "   56.22136688232422,\n",
       "   55.6088981628418,\n",
       "   55.369842529296875,\n",
       "   55.17293930053711,\n",
       "   54.91427230834961,\n",
       "   54.804508209228516,\n",
       "   54.24800109863281,\n",
       "   54.463623046875,\n",
       "   54.862815856933594,\n",
       "   55.47770309448242,\n",
       "   55.368568420410156,\n",
       "   55.33748245239258,\n",
       "   55.419986724853516,\n",
       "   55.64162826538086,\n",
       "   55.86301803588867,\n",
       "   56.184120178222656,\n",
       "   56.843536376953125,\n",
       "   57.120323181152344,\n",
       "   57.180763244628906,\n",
       "   56.827266693115234,\n",
       "   56.952720642089844,\n",
       "   57.03665542602539,\n",
       "   57.266075134277344,\n",
       "   57.530887603759766,\n",
       "   58.096527099609375,\n",
       "   58.38747024536133,\n",
       "   58.81926345825195,\n",
       "   59.38965606689453,\n",
       "   60.00847244262695,\n",
       "   60.307945251464844,\n",
       "   60.18120574951172,\n",
       "   60.26100540161133,\n",
       "   60.27833938598633,\n",
       "   60.335391998291016,\n",
       "   60.35796356201172,\n",
       "   60.275386810302734,\n",
       "   60.19679260253906,\n",
       "   59.8594856262207,\n",
       "   59.38673782348633,\n",
       "   58.583961486816406,\n",
       "   58.07933807373047,\n",
       "   57.451725006103516,\n",
       "   56.965965270996094,\n",
       "   57.190792083740234,\n",
       "   56.760440826416016,\n",
       "   56.89933776855469,\n",
       "   56.71712112426758,\n",
       "   56.59825134277344,\n",
       "   56.32917785644531,\n",
       "   55.9849853515625,\n",
       "   55.52204895019531,\n",
       "   54.76966094970703,\n",
       "   54.590850830078125,\n",
       "   54.76186752319336,\n",
       "   54.84726333618164,\n",
       "   54.88878631591797,\n",
       "   54.677547454833984,\n",
       "   55.16736602783203,\n",
       "   55.057125091552734,\n",
       "   54.88618469238281,\n",
       "   54.996131896972656,\n",
       "   55.11368942260742,\n",
       "   55.36919403076172,\n",
       "   55.52840805053711,\n",
       "   55.514503479003906,\n",
       "   55.07241439819336,\n",
       "   55.259639739990234,\n",
       "   55.44914627075195,\n",
       "   55.709171295166016,\n",
       "   55.795623779296875,\n",
       "   56.135196685791016,\n",
       "   56.44574737548828,\n",
       "   56.40729522705078,\n",
       "   56.580543518066406,\n",
       "   57.186485290527344,\n",
       "   57.5613899230957,\n",
       "   58.202789306640625,\n",
       "   58.60521697998047,\n",
       "   58.74765396118164,\n",
       "   58.53076171875,\n",
       "   58.507232666015625,\n",
       "   58.567787170410156,\n",
       "   58.28122329711914,\n",
       "   57.92978286743164,\n",
       "   58.17487335205078,\n",
       "   58.162498474121094,\n",
       "   58.22631072998047,\n",
       "   58.2852783203125,\n",
       "   58.28725814819336,\n",
       "   58.49488830566406,\n",
       "   58.38784408569336,\n",
       "   58.42219543457031,\n",
       "   58.368980407714844,\n",
       "   58.308021545410156,\n",
       "   58.584224700927734,\n",
       "   58.13945007324219,\n",
       "   57.875091552734375,\n",
       "   57.61640167236328,\n",
       "   57.228492736816406,\n",
       "   57.335750579833984,\n",
       "   57.69752883911133,\n",
       "   58.22861099243164,\n",
       "   58.41606140136719,\n",
       "   58.6566276550293,\n",
       "   58.78299331665039,\n",
       "   58.89421081542969,\n",
       "   59.38994216918945,\n",
       "   59.324012756347656,\n",
       "   58.895748138427734,\n",
       "   59.16678237915039,\n",
       "   58.64458084106445,\n",
       "   57.99860382080078,\n",
       "   58.12387466430664,\n",
       "   57.32042694091797,\n",
       "   56.94860076904297,\n",
       "   56.81937789916992,\n",
       "   56.1877326965332,\n",
       "   56.089019775390625,\n",
       "   55.63511657714844,\n",
       "   55.53466033935547,\n",
       "   55.17041778564453,\n",
       "   54.77483367919922,\n",
       "   54.92082214355469,\n",
       "   55.165035247802734,\n",
       "   55.289058685302734,\n",
       "   55.483192443847656,\n",
       "   55.897552490234375,\n",
       "   56.32225799560547,\n",
       "   56.27943420410156,\n",
       "   56.684391021728516,\n",
       "   57.176185607910156,\n",
       "   57.54808044433594,\n",
       "   58.044700622558594,\n",
       "   58.1068115234375,\n",
       "   58.09183883666992,\n",
       "   58.05732727050781,\n",
       "   58.247711181640625,\n",
       "   58.538021087646484,\n",
       "   58.5195198059082,\n",
       "   58.62423324584961,\n",
       "   58.51317596435547,\n",
       "   58.646888732910156,\n",
       "   58.55265808105469,\n",
       "   58.29994201660156,\n",
       "   58.215003967285156,\n",
       "   58.1873779296875,\n",
       "   58.491973876953125,\n",
       "   58.44402313232422,\n",
       "   58.661014556884766,\n",
       "   58.65669250488281,\n",
       "   58.77817916870117,\n",
       "   58.68566131591797,\n",
       "   58.01229476928711,\n",
       "   57.900333404541016,\n",
       "   57.811378479003906,\n",
       "   57.791202545166016,\n",
       "   57.55939483642578,\n",
       "   57.73500061035156,\n",
       "   58.04351043701172,\n",
       "   58.028846740722656,\n",
       "   58.20222473144531,\n",
       "   58.227149963378906,\n",
       "   58.23712921142578,\n",
       "   57.577144622802734,\n",
       "   57.439273834228516,\n",
       "   57.01968765258789,\n",
       "   56.85626220703125,\n",
       "   56.67857360839844,\n",
       "   56.47166442871094,\n",
       "   56.13101577758789,\n",
       "   55.492313385009766,\n",
       "   55.684513092041016,\n",
       "   55.81504821777344],\n",
       "  'voc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   98.43453216552734,\n",
       "   94.48393249511719,\n",
       "   86.92156219482422,\n",
       "   79.96056365966797,\n",
       "   73.85997772216797,\n",
       "   68.62723541259766,\n",
       "   64.49164581298828,\n",
       "   62.7711067199707,\n",
       "   58.996986389160156,\n",
       "   55.68741989135742,\n",
       "   53.64596939086914,\n",
       "   51.387977600097656,\n",
       "   50.29549026489258,\n",
       "   47.86323547363281,\n",
       "   46.030513763427734,\n",
       "   44.13842010498047,\n",
       "   41.7239990234375,\n",
       "   38.878421783447266,\n",
       "   36.66971206665039,\n",
       "   33.86470031738281,\n",
       "   29.84267234802246,\n",
       "   28.63212013244629,\n",
       "   30.056814193725586,\n",
       "   29.2202205657959,\n",
       "   29.142526626586914,\n",
       "   29.51403045654297,\n",
       "   31.562179565429688,\n",
       "   33.806495666503906,\n",
       "   34.80427932739258,\n",
       "   36.162288665771484,\n",
       "   36.2498893737793,\n",
       "   36.41117477416992,\n",
       "   36.849029541015625,\n",
       "   37.82048034667969,\n",
       "   37.66904830932617,\n",
       "   39.493072509765625,\n",
       "   38.265438079833984,\n",
       "   37.09611129760742,\n",
       "   36.53352355957031,\n",
       "   36.30604934692383,\n",
       "   35.265777587890625,\n",
       "   33.27253723144531,\n",
       "   30.320194244384766,\n",
       "   28.819557189941406,\n",
       "   26.94131088256836,\n",
       "   26.176189422607422,\n",
       "   25.610807418823242,\n",
       "   25.638399124145508,\n",
       "   24.60416030883789,\n",
       "   23.37457847595215,\n",
       "   22.724397659301758,\n",
       "   20.739765167236328,\n",
       "   20.08562469482422,\n",
       "   19.579429626464844,\n",
       "   20.52505111694336,\n",
       "   21.578495025634766,\n",
       "   23.15729331970215,\n",
       "   23.80238151550293,\n",
       "   24.28665542602539,\n",
       "   24.79336929321289,\n",
       "   25.09431266784668,\n",
       "   24.844453811645508,\n",
       "   25.060007095336914,\n",
       "   26.078426361083984,\n",
       "   26.614151000976562,\n",
       "   26.930072784423828,\n",
       "   27.90990447998047,\n",
       "   29.366609573364258,\n",
       "   30.43906021118164,\n",
       "   30.805936813354492,\n",
       "   31.809925079345703,\n",
       "   32.110877990722656,\n",
       "   32.440067291259766,\n",
       "   33.49493408203125,\n",
       "   34.071468353271484,\n",
       "   34.52791976928711,\n",
       "   34.37385940551758,\n",
       "   35.14149856567383,\n",
       "   35.0731315612793,\n",
       "   35.099395751953125,\n",
       "   35.092918395996094,\n",
       "   35.46902084350586,\n",
       "   35.99974060058594,\n",
       "   36.46820068359375,\n",
       "   36.83417892456055,\n",
       "   36.98735046386719,\n",
       "   36.98942947387695,\n",
       "   37.01781463623047,\n",
       "   37.4444465637207,\n",
       "   36.1522331237793,\n",
       "   34.74647521972656,\n",
       "   34.423187255859375,\n",
       "   33.26737976074219,\n",
       "   33.188743591308594,\n",
       "   32.46577835083008,\n",
       "   30.700891494750977,\n",
       "   29.14891242980957,\n",
       "   27.962675094604492,\n",
       "   26.221168518066406,\n",
       "   25.517892837524414,\n",
       "   25.366178512573242,\n",
       "   24.83225440979004,\n",
       "   23.60915184020996,\n",
       "   23.457263946533203,\n",
       "   23.342443466186523,\n",
       "   23.46426010131836,\n",
       "   24.01238441467285,\n",
       "   24.577938079833984,\n",
       "   24.912372589111328,\n",
       "   25.394893646240234,\n",
       "   25.710308074951172,\n",
       "   25.707561492919922,\n",
       "   25.715538024902344,\n",
       "   26.395097732543945,\n",
       "   26.81940460205078,\n",
       "   26.628440856933594,\n",
       "   26.692508697509766,\n",
       "   26.99521255493164,\n",
       "   27.40138816833496,\n",
       "   27.11137580871582,\n",
       "   26.525745391845703,\n",
       "   26.274818420410156,\n",
       "   25.090923309326172,\n",
       "   25.044301986694336,\n",
       "   24.8844051361084,\n",
       "   24.336456298828125,\n",
       "   23.65872573852539,\n",
       "   23.10771942138672,\n",
       "   23.509239196777344,\n",
       "   24.678922653198242,\n",
       "   24.86921501159668,\n",
       "   26.16657829284668,\n",
       "   27.32521629333496,\n",
       "   27.811771392822266,\n",
       "   27.60489845275879,\n",
       "   28.421836853027344,\n",
       "   29.557449340820312,\n",
       "   29.49675941467285,\n",
       "   30.721023559570312,\n",
       "   31.118684768676758,\n",
       "   31.134870529174805,\n",
       "   30.87855339050293,\n",
       "   29.813095092773438,\n",
       "   30.03903579711914,\n",
       "   30.043813705444336,\n",
       "   30.562171936035156,\n",
       "   30.943330764770508,\n",
       "   31.244409561157227,\n",
       "   31.791568756103516,\n",
       "   31.6010684967041,\n",
       "   31.604463577270508,\n",
       "   31.22318458557129,\n",
       "   30.623546600341797,\n",
       "   31.351226806640625,\n",
       "   30.93383026123047,\n",
       "   30.56916618347168,\n",
       "   30.52961540222168,\n",
       "   30.423721313476562,\n",
       "   30.53350830078125,\n",
       "   31.312763214111328,\n",
       "   31.179841995239258,\n",
       "   31.522777557373047,\n",
       "   32.44779586791992,\n",
       "   33.0198860168457,\n",
       "   33.123680114746094,\n",
       "   33.25973892211914,\n",
       "   32.498897552490234,\n",
       "   32.649600982666016,\n",
       "   32.46815490722656,\n",
       "   31.78863525390625,\n",
       "   32.37470626831055,\n",
       "   31.433122634887695,\n",
       "   31.290191650390625,\n",
       "   31.25334930419922,\n",
       "   29.926210403442383,\n",
       "   29.806114196777344,\n",
       "   30.851612091064453,\n",
       "   30.61600112915039,\n",
       "   31.73232650756836,\n",
       "   31.766185760498047,\n",
       "   31.701696395874023,\n",
       "   32.39447784423828,\n",
       "   32.75954818725586,\n",
       "   33.35111999511719,\n",
       "   33.83885955810547,\n",
       "   33.81631851196289,\n",
       "   34.54579162597656,\n",
       "   35.233489990234375,\n",
       "   35.57065200805664,\n",
       "   35.99610900878906,\n",
       "   36.52485275268555,\n",
       "   36.64771270751953,\n",
       "   36.66225814819336,\n",
       "   37.12466812133789,\n",
       "   37.86365509033203,\n",
       "   38.21943283081055,\n",
       "   38.118919372558594,\n",
       "   38.464290618896484,\n",
       "   38.179988861083984,\n",
       "   38.93819808959961,\n",
       "   38.82353210449219,\n",
       "   39.29447555541992,\n",
       "   39.10265350341797,\n",
       "   38.44953155517578,\n",
       "   38.361236572265625,\n",
       "   37.911163330078125,\n",
       "   37.257991790771484,\n",
       "   36.24551010131836,\n",
       "   35.59750747680664,\n",
       "   35.378055572509766,\n",
       "   35.79414749145508,\n",
       "   36.30217361450195,\n",
       "   35.88270568847656,\n",
       "   35.339229583740234,\n",
       "   34.93057632446289,\n",
       "   34.31089401245117,\n",
       "   33.07691955566406,\n",
       "   32.749332427978516,\n",
       "   32.44413757324219,\n",
       "   31.927114486694336,\n",
       "   32.83866882324219,\n",
       "   32.67206573486328,\n",
       "   32.36515808105469,\n",
       "   32.37519836425781,\n",
       "   31.974050521850586,\n",
       "   32.2380256652832,\n",
       "   32.88246536254883,\n",
       "   33.51166534423828,\n",
       "   33.233642578125,\n",
       "   33.02104568481445,\n",
       "   33.688411712646484,\n",
       "   33.98683166503906,\n",
       "   34.4745979309082,\n",
       "   34.56275939941406,\n",
       "   34.62395095825195,\n",
       "   34.98963928222656,\n",
       "   35.286407470703125,\n",
       "   35.633487701416016,\n",
       "   36.01226043701172,\n",
       "   36.23381042480469,\n",
       "   37.108219146728516,\n",
       "   37.70751953125,\n",
       "   38.141597747802734,\n",
       "   38.402565002441406,\n",
       "   38.52020263671875,\n",
       "   39.655330657958984,\n",
       "   39.76592254638672,\n",
       "   39.59067916870117,\n",
       "   39.974029541015625,\n",
       "   40.353843688964844,\n",
       "   40.003639221191406,\n",
       "   39.6899299621582,\n",
       "   39.36473846435547,\n",
       "   39.12006378173828,\n",
       "   39.22019577026367,\n",
       "   39.72772979736328,\n",
       "   39.44934844970703,\n",
       "   39.949424743652344,\n",
       "   40.042579650878906,\n",
       "   39.32787322998047,\n",
       "   39.64498519897461,\n",
       "   40.084651947021484,\n",
       "   40.33707809448242,\n",
       "   40.71617126464844,\n",
       "   40.866817474365234,\n",
       "   40.572200775146484,\n",
       "   41.09349822998047,\n",
       "   41.94042205810547,\n",
       "   41.5550422668457,\n",
       "   41.21002960205078,\n",
       "   40.83160400390625,\n",
       "   40.166465759277344,\n",
       "   38.9566764831543,\n",
       "   38.63568878173828,\n",
       "   38.352298736572266,\n",
       "   38.27140426635742,\n",
       "   37.39616775512695,\n",
       "   36.9669303894043,\n",
       "   37.00128936767578,\n",
       "   36.323707580566406,\n",
       "   36.23396301269531,\n",
       "   36.55966567993164,\n",
       "   35.48012161254883,\n",
       "   35.066650390625,\n",
       "   35.15178680419922,\n",
       "   36.21663284301758,\n",
       "   36.82590866088867,\n",
       "   37.610416412353516,\n",
       "   37.973968505859375,\n",
       "   37.891273498535156,\n",
       "   38.93896484375,\n",
       "   38.93948745727539,\n",
       "   39.7208251953125,\n",
       "   39.740848541259766,\n",
       "   39.60960388183594,\n",
       "   38.88978576660156,\n",
       "   39.30500030517578,\n",
       "   38.65348815917969,\n",
       "   38.23040008544922,\n",
       "   38.37333297729492,\n",
       "   37.35219192504883,\n",
       "   37.26352310180664,\n",
       "   37.29602813720703,\n",
       "   37.35877227783203,\n",
       "   38.14973449707031,\n",
       "   38.001644134521484,\n",
       "   37.80107498168945,\n",
       "   38.13045883178711,\n",
       "   37.674705505371094,\n",
       "   38.14252471923828,\n",
       "   38.39653015136719,\n",
       "   38.88628005981445,\n",
       "   39.410057067871094,\n",
       "   39.0191535949707,\n",
       "   39.29642868041992,\n",
       "   39.65753173828125,\n",
       "   39.882774353027344,\n",
       "   39.76667785644531,\n",
       "   39.54941177368164,\n",
       "   39.881778717041016,\n",
       "   39.37069320678711,\n",
       "   38.8599853515625,\n",
       "   38.563594818115234,\n",
       "   37.954124450683594,\n",
       "   37.2753791809082,\n",
       "   37.57638931274414,\n",
       "   37.14410400390625,\n",
       "   37.62932586669922,\n",
       "   37.731689453125,\n",
       "   37.548423767089844,\n",
       "   37.6775016784668,\n",
       "   37.23098373413086,\n",
       "   36.505096435546875,\n",
       "   36.05202865600586,\n",
       "   36.74479293823242,\n",
       "   36.07559585571289,\n",
       "   36.33335494995117,\n",
       "   36.665565490722656,\n",
       "   36.395545959472656,\n",
       "   35.97092056274414,\n",
       "   35.60114288330078,\n",
       "   35.65523910522461,\n",
       "   34.966033935546875,\n",
       "   35.30622100830078,\n",
       "   35.613224029541016,\n",
       "   36.59806823730469,\n",
       "   36.37116241455078,\n",
       "   36.5567626953125,\n",
       "   36.43152618408203,\n",
       "   36.75737762451172,\n",
       "   37.68532943725586,\n",
       "   37.54643249511719,\n",
       "   37.49175262451172,\n",
       "   37.94148635864258,\n",
       "   38.4321174621582,\n",
       "   38.60008239746094,\n",
       "   39.595458984375,\n",
       "   39.31684494018555,\n",
       "   39.873008728027344,\n",
       "   40.98212432861328,\n",
       "   41.136085510253906,\n",
       "   41.52934265136719,\n",
       "   41.56425476074219,\n",
       "   42.05356979370117,\n",
       "   42.156795501708984,\n",
       "   42.32105255126953,\n",
       "   42.858028411865234,\n",
       "   42.507118225097656,\n",
       "   42.589351654052734,\n",
       "   41.47344970703125,\n",
       "   41.38641357421875,\n",
       "   41.61017990112305,\n",
       "   41.35343551635742,\n",
       "   40.58407974243164,\n",
       "   40.24038314819336,\n",
       "   40.003639221191406,\n",
       "   40.006649017333984,\n",
       "   40.53692626953125,\n",
       "   40.16035461425781,\n",
       "   39.07648849487305,\n",
       "   39.1370849609375,\n",
       "   38.97813415527344,\n",
       "   38.1419563293457,\n",
       "   37.823612213134766,\n",
       "   36.898170471191406,\n",
       "   36.726192474365234,\n",
       "   37.030296325683594,\n",
       "   36.850303649902344,\n",
       "   37.09860610961914,\n",
       "   36.65803146362305,\n",
       "   37.27566146850586,\n",
       "   36.66148376464844,\n",
       "   35.87867736816406,\n",
       "   35.6425666809082,\n",
       "   35.14612579345703,\n",
       "   35.06690979003906,\n",
       "   34.698272705078125,\n",
       "   33.837772369384766,\n",
       "   33.91446304321289,\n",
       "   34.23153305053711,\n",
       "   33.9935188293457,\n",
       "   34.40807342529297,\n",
       "   35.32048416137695,\n",
       "   35.88512420654297,\n",
       "   36.06258010864258,\n",
       "   36.439056396484375,\n",
       "   36.909854888916016,\n",
       "   37.4223747253418,\n",
       "   38.146060943603516,\n",
       "   38.67058563232422,\n",
       "   39.42886734008789,\n",
       "   39.89445114135742,\n",
       "   40.08476257324219,\n",
       "   40.3697509765625,\n",
       "   40.776248931884766,\n",
       "   40.98529815673828,\n",
       "   41.33283615112305,\n",
       "   41.62137222290039,\n",
       "   41.29059982299805,\n",
       "   41.44790267944336,\n",
       "   41.46176528930664,\n",
       "   41.98154830932617,\n",
       "   41.9758186340332,\n",
       "   41.74652862548828,\n",
       "   41.331329345703125,\n",
       "   40.181766510009766,\n",
       "   39.8362922668457,\n",
       "   39.74002456665039,\n",
       "   38.75828552246094,\n",
       "   38.08954620361328,\n",
       "   38.00157928466797,\n",
       "   37.48480224609375,\n",
       "   37.07844924926758,\n",
       "   36.96762466430664,\n",
       "   36.453453063964844,\n",
       "   35.66306686401367,\n",
       "   35.14286422729492,\n",
       "   34.37443542480469,\n",
       "   33.311981201171875,\n",
       "   32.47105026245117,\n",
       "   32.17745590209961,\n",
       "   31.799617767333984,\n",
       "   31.335847854614258,\n",
       "   31.333934783935547,\n",
       "   30.932723999023438,\n",
       "   31.206438064575195,\n",
       "   31.9761962890625,\n",
       "   32.00666046142578,\n",
       "   32.11906051635742,\n",
       "   32.511634826660156,\n",
       "   32.26332473754883,\n",
       "   31.956708908081055,\n",
       "   32.21819305419922,\n",
       "   31.95581817626953,\n",
       "   32.29988098144531,\n",
       "   32.090415954589844,\n",
       "   32.090110778808594,\n",
       "   32.40397644042969,\n",
       "   32.69977951049805,\n",
       "   33.071678161621094,\n",
       "   33.620208740234375,\n",
       "   34.63043975830078,\n",
       "   34.91046905517578,\n",
       "   35.2669792175293,\n",
       "   36.22743606567383,\n",
       "   36.57398986816406,\n",
       "   36.76164627075195,\n",
       "   37.17931365966797,\n",
       "   37.4654426574707,\n",
       "   37.66521072387695,\n",
       "   38.023712158203125,\n",
       "   38.49086380004883,\n",
       "   38.51604080200195,\n",
       "   38.75235366821289,\n",
       "   38.66999435424805,\n",
       "   38.60192108154297,\n",
       "   37.59041976928711,\n",
       "   38.027523040771484,\n",
       "   37.74809265136719,\n",
       "   37.445255279541016,\n",
       "   37.138851165771484,\n",
       "   37.729209899902344,\n",
       "   37.89506149291992,\n",
       "   37.804012298583984,\n",
       "   37.813385009765625,\n",
       "   37.647193908691406,\n",
       "   37.8872184753418,\n",
       "   37.609352111816406,\n",
       "   37.560062408447266],\n",
       "  'jsc_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   99.96806335449219,\n",
       "   99.6351089477539,\n",
       "   99.34842681884766,\n",
       "   99.07998657226562,\n",
       "   98.91159057617188,\n",
       "   98.6655502319336,\n",
       "   98.36914825439453,\n",
       "   98.35647583007812,\n",
       "   98.34104919433594,\n",
       "   98.3056640625,\n",
       "   98.1939468383789,\n",
       "   98.15200805664062,\n",
       "   98.05819702148438,\n",
       "   97.9577865600586,\n",
       "   98.00048828125,\n",
       "   97.80372619628906,\n",
       "   97.46047973632812,\n",
       "   97.25979614257812,\n",
       "   97.03540802001953,\n",
       "   96.98596954345703,\n",
       "   96.98926544189453,\n",
       "   96.84778594970703,\n",
       "   96.8215103149414,\n",
       "   96.57996368408203,\n",
       "   96.55085754394531,\n",
       "   96.51180267333984,\n",
       "   96.22267150878906,\n",
       "   95.69275665283203,\n",
       "   95.19901275634766,\n",
       "   95.25970458984375,\n",
       "   94.70099639892578,\n",
       "   94.74626159667969,\n",
       "   94.31864166259766,\n",
       "   94.38408660888672,\n",
       "   94.23736572265625,\n",
       "   94.03528594970703,\n",
       "   93.56401062011719,\n",
       "   94.354736328125,\n",
       "   94.64990234375,\n",
       "   95.27848815917969,\n",
       "   95.65331268310547,\n",
       "   95.54776000976562,\n",
       "   94.8758773803711,\n",
       "   94.49178314208984,\n",
       "   94.17301177978516,\n",
       "   93.9643325805664,\n",
       "   94.51983642578125,\n",
       "   95.18003845214844,\n",
       "   95.10457611083984,\n",
       "   94.71331787109375,\n",
       "   94.46572875976562,\n",
       "   94.63630676269531,\n",
       "   94.34751892089844,\n",
       "   94.63760375976562,\n",
       "   94.96334838867188,\n",
       "   94.75421905517578,\n",
       "   94.51359558105469,\n",
       "   94.52076721191406,\n",
       "   94.46676635742188,\n",
       "   94.3172607421875,\n",
       "   94.28872680664062,\n",
       "   93.64668273925781,\n",
       "   93.13053131103516,\n",
       "   92.85176849365234,\n",
       "   92.93659973144531,\n",
       "   93.0771255493164,\n",
       "   92.70751953125,\n",
       "   92.7207260131836,\n",
       "   92.25164031982422,\n",
       "   92.17161560058594,\n",
       "   91.9256362915039,\n",
       "   91.54740905761719,\n",
       "   91.2297592163086,\n",
       "   90.81893920898438,\n",
       "   91.08612823486328,\n",
       "   91.01432037353516,\n",
       "   91.11077117919922,\n",
       "   91.30547332763672,\n",
       "   91.04527282714844,\n",
       "   91.04285430908203,\n",
       "   90.75100708007812,\n",
       "   90.59717559814453,\n",
       "   90.52509307861328,\n",
       "   90.18013763427734,\n",
       "   90.1941909790039,\n",
       "   90.04795837402344,\n",
       "   89.96630096435547,\n",
       "   90.18235778808594,\n",
       "   90.62651062011719,\n",
       "   90.47256469726562,\n",
       "   90.5281982421875,\n",
       "   91.18743896484375,\n",
       "   91.51177215576172,\n",
       "   91.70133972167969,\n",
       "   91.69013214111328,\n",
       "   91.72489929199219,\n",
       "   91.45989227294922,\n",
       "   91.70947265625,\n",
       "   91.55876922607422,\n",
       "   91.76606750488281,\n",
       "   92.41024780273438,\n",
       "   92.64482116699219,\n",
       "   92.79244232177734,\n",
       "   92.91773986816406,\n",
       "   92.60491943359375,\n",
       "   92.61051177978516,\n",
       "   92.21952056884766,\n",
       "   91.9672622680664,\n",
       "   91.4283447265625,\n",
       "   91.31394958496094,\n",
       "   91.02604675292969,\n",
       "   90.99629974365234,\n",
       "   90.62168884277344,\n",
       "   90.49401092529297,\n",
       "   90.7999038696289,\n",
       "   90.79501342773438,\n",
       "   90.78506469726562,\n",
       "   90.55150604248047,\n",
       "   90.37820434570312,\n",
       "   90.1183853149414,\n",
       "   90.33688354492188,\n",
       "   89.96758270263672,\n",
       "   89.90238952636719,\n",
       "   89.94265747070312,\n",
       "   89.91692352294922,\n",
       "   89.8814468383789,\n",
       "   89.78616333007812,\n",
       "   89.90558624267578,\n",
       "   89.9164047241211,\n",
       "   89.81098175048828,\n",
       "   89.70338439941406,\n",
       "   89.23426818847656,\n",
       "   89.15376281738281,\n",
       "   89.23886108398438,\n",
       "   88.5162353515625,\n",
       "   88.53191375732422,\n",
       "   88.1283187866211,\n",
       "   87.65715026855469,\n",
       "   87.29088592529297,\n",
       "   87.27425384521484,\n",
       "   86.97785186767578,\n",
       "   86.35069274902344,\n",
       "   85.60608673095703,\n",
       "   85.45585632324219,\n",
       "   85.61819458007812,\n",
       "   85.55868530273438,\n",
       "   85.46867370605469,\n",
       "   85.27762603759766,\n",
       "   85.4106216430664,\n",
       "   85.3359603881836,\n",
       "   84.99705505371094,\n",
       "   84.6299057006836,\n",
       "   84.28216552734375,\n",
       "   83.93329620361328,\n",
       "   83.71798706054688,\n",
       "   83.84037017822266,\n",
       "   83.97935485839844,\n",
       "   84.22669982910156,\n",
       "   84.07189178466797,\n",
       "   83.9864730834961,\n",
       "   83.58477020263672,\n",
       "   83.82310485839844,\n",
       "   83.91554260253906,\n",
       "   83.74085235595703,\n",
       "   83.66857147216797,\n",
       "   83.58607482910156,\n",
       "   83.71881866455078,\n",
       "   83.66081237792969,\n",
       "   83.60627746582031,\n",
       "   83.85980224609375,\n",
       "   84.01025390625,\n",
       "   84.2348403930664,\n",
       "   83.95191955566406,\n",
       "   83.74543762207031,\n",
       "   83.9140625,\n",
       "   83.9896240234375,\n",
       "   84.32091522216797,\n",
       "   84.28850555419922,\n",
       "   83.70732116699219,\n",
       "   83.56263732910156,\n",
       "   83.18744659423828,\n",
       "   82.85935974121094,\n",
       "   82.94602966308594,\n",
       "   82.66292572021484,\n",
       "   82.48466491699219,\n",
       "   82.42340087890625,\n",
       "   82.30673217773438,\n",
       "   82.15267944335938,\n",
       "   82.18474578857422,\n",
       "   82.25837707519531,\n",
       "   82.03608703613281,\n",
       "   82.13646697998047,\n",
       "   82.13044738769531,\n",
       "   82.22701263427734,\n",
       "   81.8202896118164,\n",
       "   82.21388244628906,\n",
       "   81.72215270996094,\n",
       "   81.96121978759766,\n",
       "   81.71510314941406,\n",
       "   81.52904510498047,\n",
       "   81.6534423828125,\n",
       "   81.48200225830078,\n",
       "   81.51410675048828,\n",
       "   81.75713348388672,\n",
       "   81.69017028808594,\n",
       "   81.78707122802734,\n",
       "   81.77383422851562,\n",
       "   81.868408203125,\n",
       "   81.93789672851562,\n",
       "   81.8309326171875,\n",
       "   81.72881317138672,\n",
       "   81.71387481689453,\n",
       "   81.7032241821289,\n",
       "   81.76878356933594,\n",
       "   81.67733764648438,\n",
       "   82.03510284423828,\n",
       "   81.73475646972656,\n",
       "   81.65097045898438,\n",
       "   81.32823944091797,\n",
       "   81.20092010498047,\n",
       "   81.23204803466797,\n",
       "   81.54254913330078,\n",
       "   81.65235900878906,\n",
       "   81.513427734375,\n",
       "   81.52188873291016,\n",
       "   81.6355209350586,\n",
       "   81.75038146972656,\n",
       "   82.28861236572266,\n",
       "   82.14447021484375,\n",
       "   81.85464477539062,\n",
       "   81.93728637695312,\n",
       "   81.80819702148438,\n",
       "   81.67696380615234,\n",
       "   81.86876678466797,\n",
       "   82.0372314453125,\n",
       "   81.9820556640625,\n",
       "   81.96150970458984,\n",
       "   81.64402770996094,\n",
       "   82.13160705566406,\n",
       "   81.90924835205078,\n",
       "   82.00153350830078,\n",
       "   81.5317611694336,\n",
       "   81.33740997314453,\n",
       "   81.20626831054688,\n",
       "   81.08159637451172,\n",
       "   80.98674011230469,\n",
       "   80.81172943115234,\n",
       "   80.54186248779297,\n",
       "   80.98965454101562,\n",
       "   80.92374420166016,\n",
       "   81.01213073730469,\n",
       "   81.20687103271484,\n",
       "   81.19815826416016,\n",
       "   81.06974029541016,\n",
       "   81.39934539794922,\n",
       "   81.50837707519531,\n",
       "   81.29861450195312,\n",
       "   81.35199737548828,\n",
       "   81.42428588867188,\n",
       "   81.4217300415039,\n",
       "   81.27299499511719,\n",
       "   81.55364990234375,\n",
       "   81.60513305664062,\n",
       "   81.46788787841797,\n",
       "   81.45355224609375,\n",
       "   81.31531524658203,\n",
       "   81.26380157470703,\n",
       "   81.1374740600586,\n",
       "   81.18019104003906,\n",
       "   81.0515365600586,\n",
       "   81.08053588867188,\n",
       "   81.28129577636719,\n",
       "   81.61865234375,\n",
       "   81.38761901855469,\n",
       "   81.16242980957031,\n",
       "   81.27481079101562,\n",
       "   81.29603576660156,\n",
       "   81.14506530761719,\n",
       "   81.09764862060547,\n",
       "   81.11265563964844,\n",
       "   81.40498352050781,\n",
       "   81.5570297241211,\n",
       "   81.62617492675781,\n",
       "   81.70358276367188,\n",
       "   81.53626251220703,\n",
       "   81.60645294189453,\n",
       "   81.3216323852539,\n",
       "   81.60104370117188,\n",
       "   81.6604232788086,\n",
       "   81.5352554321289,\n",
       "   81.74553680419922,\n",
       "   81.55830383300781,\n",
       "   81.65288543701172,\n",
       "   81.5266342163086,\n",
       "   81.47880554199219,\n",
       "   81.49614715576172,\n",
       "   81.14726257324219,\n",
       "   80.9534683227539,\n",
       "   80.59864044189453,\n",
       "   80.54368591308594,\n",
       "   80.12582397460938,\n",
       "   80.00655364990234,\n",
       "   80.27129364013672,\n",
       "   80.27278900146484,\n",
       "   79.9587631225586,\n",
       "   79.80351257324219,\n",
       "   79.98853302001953,\n",
       "   80.14773559570312,\n",
       "   80.07963562011719,\n",
       "   80.43982696533203,\n",
       "   80.48497772216797,\n",
       "   80.2677001953125,\n",
       "   80.56141662597656,\n",
       "   80.64958190917969,\n",
       "   80.68075561523438,\n",
       "   80.70164489746094,\n",
       "   80.85332489013672,\n",
       "   80.5141830444336,\n",
       "   80.07125854492188,\n",
       "   80.0728759765625,\n",
       "   79.89766693115234,\n",
       "   79.42298889160156,\n",
       "   79.05560302734375,\n",
       "   78.69434356689453,\n",
       "   78.64911651611328,\n",
       "   78.84739685058594,\n",
       "   78.68289184570312,\n",
       "   78.8753890991211,\n",
       "   78.6746826171875,\n",
       "   78.72224426269531,\n",
       "   78.91343688964844,\n",
       "   79.33655548095703,\n",
       "   79.51380920410156,\n",
       "   79.40873718261719,\n",
       "   79.5676040649414,\n",
       "   79.60075378417969,\n",
       "   79.43727111816406,\n",
       "   79.36416625976562,\n",
       "   79.20459747314453,\n",
       "   79.23971557617188,\n",
       "   79.27184295654297,\n",
       "   79.05084228515625,\n",
       "   78.78286743164062,\n",
       "   78.92654418945312,\n",
       "   79.06623840332031,\n",
       "   79.37548828125,\n",
       "   79.28251647949219,\n",
       "   78.99405670166016,\n",
       "   78.92138671875,\n",
       "   79.22533416748047,\n",
       "   79.46361541748047,\n",
       "   79.28363037109375,\n",
       "   79.54146575927734,\n",
       "   79.74749755859375,\n",
       "   79.48151397705078,\n",
       "   79.30461883544922,\n",
       "   79.25189208984375,\n",
       "   79.35758209228516,\n",
       "   79.45388793945312,\n",
       "   79.57193756103516,\n",
       "   79.42570495605469,\n",
       "   79.44966888427734,\n",
       "   79.56254577636719,\n",
       "   79.96273040771484,\n",
       "   80.24761199951172,\n",
       "   80.35250091552734,\n",
       "   80.48413848876953,\n",
       "   80.5227279663086,\n",
       "   80.69218444824219,\n",
       "   80.89521026611328,\n",
       "   81.19080352783203,\n",
       "   81.04154205322266,\n",
       "   80.89299011230469,\n",
       "   81.03783416748047,\n",
       "   81.12885284423828,\n",
       "   81.07752990722656,\n",
       "   81.2576675415039,\n",
       "   81.16132354736328,\n",
       "   81.28202819824219,\n",
       "   81.64309692382812,\n",
       "   81.84632110595703,\n",
       "   81.7480239868164,\n",
       "   81.56855773925781,\n",
       "   81.70225524902344,\n",
       "   82.16622161865234,\n",
       "   82.43962860107422,\n",
       "   82.66715240478516,\n",
       "   82.64124298095703,\n",
       "   82.55213165283203,\n",
       "   82.63362884521484,\n",
       "   82.7425537109375,\n",
       "   83.17053985595703,\n",
       "   83.10730743408203,\n",
       "   83.3067626953125,\n",
       "   83.23654174804688,\n",
       "   83.25807189941406,\n",
       "   83.24989318847656,\n",
       "   83.23253631591797,\n",
       "   83.02687072753906,\n",
       "   82.6556396484375,\n",
       "   82.67413330078125,\n",
       "   82.76721954345703,\n",
       "   82.5980453491211,\n",
       "   82.42572784423828,\n",
       "   82.54007720947266,\n",
       "   82.54279327392578,\n",
       "   82.59767150878906,\n",
       "   82.92052459716797,\n",
       "   83.09468841552734,\n",
       "   83.2053451538086,\n",
       "   83.0794677734375,\n",
       "   82.95407104492188,\n",
       "   82.81971740722656,\n",
       "   82.66116333007812,\n",
       "   82.65434265136719,\n",
       "   82.71540069580078,\n",
       "   82.79399871826172,\n",
       "   82.7432632446289,\n",
       "   82.64835357666016,\n",
       "   82.52743530273438,\n",
       "   82.54662322998047,\n",
       "   82.73194122314453,\n",
       "   82.54539489746094,\n",
       "   82.39635467529297,\n",
       "   82.5521011352539,\n",
       "   82.58264923095703,\n",
       "   82.4139404296875,\n",
       "   82.1983642578125,\n",
       "   82.05152130126953,\n",
       "   81.95172119140625,\n",
       "   81.918701171875,\n",
       "   81.93357849121094,\n",
       "   81.93464660644531,\n",
       "   81.98150634765625,\n",
       "   81.772216796875,\n",
       "   82.06300354003906,\n",
       "   82.07560729980469,\n",
       "   82.31999969482422,\n",
       "   82.42227935791016,\n",
       "   82.37150573730469,\n",
       "   82.4302749633789,\n",
       "   82.56272888183594,\n",
       "   82.68311309814453,\n",
       "   82.71259307861328,\n",
       "   82.8368911743164,\n",
       "   82.97674560546875,\n",
       "   83.09646606445312,\n",
       "   82.9745864868164,\n",
       "   82.96441650390625,\n",
       "   82.86613464355469,\n",
       "   82.96631622314453,\n",
       "   83.03234100341797,\n",
       "   82.99464416503906,\n",
       "   82.83943939208984,\n",
       "   82.9720687866211,\n",
       "   82.90291595458984,\n",
       "   82.82115173339844,\n",
       "   82.71002197265625,\n",
       "   82.43128967285156,\n",
       "   82.30906677246094,\n",
       "   82.35665130615234,\n",
       "   82.51821899414062,\n",
       "   82.4407730102539,\n",
       "   82.60941314697266,\n",
       "   82.75826263427734,\n",
       "   83.05616760253906,\n",
       "   83.1705093383789,\n",
       "   83.26881408691406,\n",
       "   83.27534484863281,\n",
       "   83.36532592773438,\n",
       "   83.47112274169922,\n",
       "   83.66741943359375,\n",
       "   83.65123748779297,\n",
       "   83.66692352294922,\n",
       "   83.88089752197266,\n",
       "   83.6730728149414,\n",
       "   83.81138610839844,\n",
       "   83.79589080810547,\n",
       "   84.28260040283203,\n",
       "   84.33861541748047],\n",
       "  'ff_acc': [100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   100.0,\n",
       "   98.06041717529297,\n",
       "   95.70448303222656,\n",
       "   92.00090026855469,\n",
       "   88.49378967285156,\n",
       "   86.15835571289062,\n",
       "   82.31246185302734,\n",
       "   76.63011169433594,\n",
       "   70.87420654296875,\n",
       "   65.77777862548828,\n",
       "   61.27177429199219,\n",
       "   58.363075256347656,\n",
       "   55.282371520996094,\n",
       "   52.307308197021484,\n",
       "   50.20934295654297,\n",
       "   48.72279357910156,\n",
       "   47.4415283203125,\n",
       "   46.29926681518555,\n",
       "   45.39224624633789,\n",
       "   44.50886917114258,\n",
       "   44.07293701171875,\n",
       "   43.94704818725586,\n",
       "   43.38369369506836,\n",
       "   43.725826263427734,\n",
       "   43.2966423034668,\n",
       "   42.418853759765625,\n",
       "   42.32838821411133,\n",
       "   42.17473220825195,\n",
       "   42.04133605957031,\n",
       "   42.09760284423828,\n",
       "   42.0594596862793,\n",
       "   42.18715286254883,\n",
       "   42.15726089477539,\n",
       "   41.70744323730469,\n",
       "   41.59019470214844,\n",
       "   41.6055908203125,\n",
       "   41.11144256591797,\n",
       "   41.075653076171875,\n",
       "   40.771949768066406,\n",
       "   39.6897087097168,\n",
       "   38.69597625732422,\n",
       "   38.386077880859375,\n",
       "   38.630001068115234,\n",
       "   39.498443603515625,\n",
       "   39.72384262084961,\n",
       "   40.32230758666992,\n",
       "   40.83369445800781,\n",
       "   41.83852005004883,\n",
       "   42.37855529785156,\n",
       "   42.807884216308594,\n",
       "   43.67273712158203,\n",
       "   44.39383316040039,\n",
       "   44.645748138427734,\n",
       "   45.02808380126953,\n",
       "   46.14043045043945,\n",
       "   46.77889633178711,\n",
       "   47.493247985839844,\n",
       "   48.22407531738281,\n",
       "   48.939640045166016,\n",
       "   48.888118743896484,\n",
       "   48.755332946777344,\n",
       "   48.413169860839844,\n",
       "   48.71712875366211,\n",
       "   48.537452697753906,\n",
       "   48.53062057495117,\n",
       "   48.462371826171875,\n",
       "   48.93306350708008,\n",
       "   49.14978790283203,\n",
       "   49.95900344848633,\n",
       "   49.53891372680664,\n",
       "   49.98920822143555,\n",
       "   50.121437072753906,\n",
       "   50.673954010009766,\n",
       "   50.747276306152344,\n",
       "   50.32845687866211,\n",
       "   49.6539306640625,\n",
       "   49.23488235473633,\n",
       "   48.474586486816406,\n",
       "   47.56564712524414,\n",
       "   47.923160552978516,\n",
       "   47.2992057800293,\n",
       "   47.3982048034668,\n",
       "   47.43640899658203,\n",
       "   47.15580368041992,\n",
       "   47.26708984375,\n",
       "   47.64203643798828,\n",
       "   47.14666748046875,\n",
       "   46.6285514831543,\n",
       "   45.993919372558594,\n",
       "   46.42766571044922,\n",
       "   45.8729362487793,\n",
       "   45.46479415893555,\n",
       "   45.51546859741211,\n",
       "   44.7773551940918,\n",
       "   45.09978485107422,\n",
       "   45.453121185302734,\n",
       "   45.348731994628906,\n",
       "   45.15868377685547,\n",
       "   45.6405029296875,\n",
       "   46.07965087890625,\n",
       "   46.6732177734375,\n",
       "   46.568450927734375,\n",
       "   46.85877990722656,\n",
       "   47.13322067260742,\n",
       "   47.37335968017578,\n",
       "   47.94511795043945,\n",
       "   48.60627746582031,\n",
       "   49.73950958251953,\n",
       "   50.21189498901367,\n",
       "   50.40200424194336,\n",
       "   49.71434783935547,\n",
       "   49.63795852661133,\n",
       "   49.81636047363281,\n",
       "   49.26533508300781,\n",
       "   48.947750091552734,\n",
       "   48.4073486328125,\n",
       "   48.423641204833984,\n",
       "   48.966556549072266,\n",
       "   48.95226287841797,\n",
       "   49.34067916870117,\n",
       "   49.89494323730469,\n",
       "   51.1927604675293,\n",
       "   51.21794891357422,\n",
       "   51.223899841308594,\n",
       "   51.18821716308594,\n",
       "   51.47693634033203,\n",
       "   51.90428924560547,\n",
       "   52.083988189697266,\n",
       "   52.23270034790039,\n",
       "   52.28444290161133,\n",
       "   51.717525482177734,\n",
       "   51.79526901245117,\n",
       "   51.568145751953125,\n",
       "   51.84459686279297,\n",
       "   51.291622161865234,\n",
       "   51.949588775634766,\n",
       "   51.29273986816406,\n",
       "   51.07953643798828,\n",
       "   51.06827163696289,\n",
       "   51.199275970458984,\n",
       "   51.37961959838867,\n",
       "   51.15068054199219,\n",
       "   51.20423889160156,\n",
       "   51.157474517822266,\n",
       "   50.666786193847656,\n",
       "   50.601261138916016,\n",
       "   50.50343322753906,\n",
       "   50.39756393432617,\n",
       "   50.38604736328125,\n",
       "   50.07776641845703,\n",
       "   49.76059341430664,\n",
       "   49.19957733154297,\n",
       "   49.58831787109375,\n",
       "   49.789161682128906,\n",
       "   49.66796875,\n",
       "   49.99113845825195,\n",
       "   50.03346252441406,\n",
       "   50.52494430541992,\n",
       "   50.991905212402344,\n",
       "   50.93988037109375,\n",
       "   51.371036529541016,\n",
       "   51.7099494934082,\n",
       "   52.43515396118164,\n",
       "   52.345001220703125,\n",
       "   52.7394905090332,\n",
       "   52.468692779541016,\n",
       "   52.06362533569336,\n",
       "   51.470924377441406,\n",
       "   51.818443298339844,\n",
       "   51.50385665893555,\n",
       "   51.633811950683594,\n",
       "   50.905372619628906,\n",
       "   51.03529739379883,\n",
       "   50.92202377319336,\n",
       "   50.57284927368164,\n",
       "   50.38834762573242,\n",
       "   49.908164978027344,\n",
       "   49.54315185546875,\n",
       "   49.71173858642578,\n",
       "   49.88451385498047,\n",
       "   50.176631927490234,\n",
       "   50.58015060424805,\n",
       "   50.14717483520508,\n",
       "   50.035160064697266,\n",
       "   49.72245788574219,\n",
       "   49.395511627197266,\n",
       "   49.42831802368164,\n",
       "   49.643619537353516,\n",
       "   50.25505447387695,\n",
       "   50.814884185791016,\n",
       "   51.35380554199219,\n",
       "   50.914852142333984,\n",
       "   50.30549240112305,\n",
       "   50.49188232421875,\n",
       "   50.75534439086914,\n",
       "   50.34335708618164,\n",
       "   50.772708892822266,\n",
       "   50.7960205078125,\n",
       "   51.184356689453125,\n",
       "   51.54573440551758,\n",
       "   51.30371856689453,\n",
       "   51.82628631591797,\n",
       "   52.09126281738281,\n",
       "   52.870643615722656,\n",
       "   52.94377136230469,\n",
       "   52.99871063232422,\n",
       "   53.23923110961914,\n",
       "   52.71059799194336,\n",
       "   52.84956741333008,\n",
       "   52.01829528808594,\n",
       "   51.82144546508789,\n",
       "   51.66134262084961,\n",
       "   51.27544403076172,\n",
       "   51.37245178222656,\n",
       "   51.25450897216797,\n",
       "   51.74837875366211,\n",
       "   51.978458404541016,\n",
       "   51.863807678222656,\n",
       "   51.70642852783203,\n",
       "   52.0491943359375,\n",
       "   51.878570556640625,\n",
       "   52.41843032836914,\n",
       "   52.73979949951172,\n",
       "   52.82760238647461,\n",
       "   52.46074295043945,\n",
       "   52.239280700683594,\n",
       "   52.107852935791016,\n",
       "   51.87239074707031,\n",
       "   51.845829010009766,\n",
       "   51.549339294433594,\n",
       "   51.95647048950195,\n",
       "   52.464454650878906,\n",
       "   52.659271240234375,\n",
       "   52.573673248291016,\n",
       "   52.68574905395508,\n",
       "   52.61153030395508,\n",
       "   52.59552764892578,\n",
       "   52.8818473815918,\n",
       "   52.814674377441406,\n",
       "   52.39716339111328,\n",
       "   52.28857421875,\n",
       "   53.29555892944336,\n",
       "   52.95702362060547,\n",
       "   52.99175262451172,\n",
       "   52.83770751953125,\n",
       "   52.7470817565918,\n",
       "   52.95655059814453,\n",
       "   52.9360466003418,\n",
       "   52.804630279541016,\n",
       "   52.61023712158203,\n",
       "   52.955810546875,\n",
       "   52.96123504638672,\n",
       "   52.766597747802734,\n",
       "   52.79107666015625,\n",
       "   52.607242584228516,\n",
       "   53.06432342529297,\n",
       "   53.2620964050293,\n",
       "   53.34450149536133,\n",
       "   53.08213806152344,\n",
       "   53.08676528930664,\n",
       "   52.88724136352539,\n",
       "   52.57748794555664,\n",
       "   52.399784088134766,\n",
       "   52.137935638427734,\n",
       "   52.337486267089844,\n",
       "   52.233306884765625,\n",
       "   52.351497650146484,\n",
       "   53.033626556396484,\n",
       "   53.316593170166016,\n",
       "   53.4745979309082,\n",
       "   53.73312759399414,\n",
       "   54.26095199584961,\n",
       "   53.812896728515625,\n",
       "   53.902523040771484,\n",
       "   53.62995910644531,\n",
       "   53.8402214050293,\n",
       "   53.57819747924805,\n",
       "   53.5158576965332,\n",
       "   53.37590408325195,\n",
       "   53.648311614990234,\n",
       "   53.354454040527344,\n",
       "   52.653987884521484,\n",
       "   52.403587341308594,\n",
       "   51.67415237426758,\n",
       "   51.708961486816406,\n",
       "   51.748130798339844,\n",
       "   51.393775939941406,\n",
       "   51.66476821899414,\n",
       "   51.906497955322266,\n",
       "   52.055572509765625,\n",
       "   52.2939453125,\n",
       "   52.28947448730469,\n",
       "   52.11605453491211,\n",
       "   52.12398910522461,\n",
       "   52.4385986328125,\n",
       "   52.335201263427734,\n",
       "   52.46906280517578,\n",
       "   52.135498046875,\n",
       "   52.13667678833008,\n",
       "   52.07612991333008,\n",
       "   51.98423767089844,\n",
       "   51.96780014038086,\n",
       "   51.40774917602539,\n",
       "   51.44746780395508,\n",
       "   51.49344253540039,\n",
       "   51.06984329223633,\n",
       "   50.79419708251953,\n",
       "   50.66950225830078,\n",
       "   51.14968490600586,\n",
       "   51.70269012451172,\n",
       "   51.76184844970703,\n",
       "   51.9879264831543,\n",
       "   51.82044982910156,\n",
       "   51.829463958740234,\n",
       "   51.83911895751953,\n",
       "   51.88249969482422,\n",
       "   51.76548767089844,\n",
       "   51.582698822021484,\n",
       "   51.152793884277344,\n",
       "   51.17117691040039,\n",
       "   51.13416290283203,\n",
       "   51.25638198852539,\n",
       "   51.52028274536133,\n",
       "   51.22526931762695,\n",
       "   51.51214599609375,\n",
       "   51.00788497924805,\n",
       "   51.03303146362305,\n",
       "   50.925167083740234,\n",
       "   51.103275299072266,\n",
       "   51.56195068359375,\n",
       "   51.378936767578125,\n",
       "   51.748130798339844,\n",
       "   51.89484786987305,\n",
       "   52.32826614379883,\n",
       "   52.06383514404297,\n",
       "   52.115684509277344,\n",
       "   52.05564498901367,\n",
       "   52.29940414428711,\n",
       "   52.13096237182617,\n",
       "   52.5655403137207,\n",
       "   52.74791717529297,\n",
       "   52.78575897216797,\n",
       "   52.81377410888672,\n",
       "   52.541011810302734,\n",
       "   52.0296630859375,\n",
       "   51.891361236572266,\n",
       "   52.33297348022461,\n",
       "   51.984256744384766,\n",
       "   51.628692626953125,\n",
       "   51.52780532836914,\n",
       "   50.83001708984375,\n",
       "   50.49237060546875,\n",
       "   50.33631896972656,\n",
       "   50.307960510253906,\n",
       "   50.330631256103516,\n",
       "   50.128875732421875,\n",
       "   49.33904266357422,\n",
       "   48.94925308227539,\n",
       "   49.087982177734375,\n",
       "   48.98530197143555,\n",
       "   49.015594482421875,\n",
       "   49.08491134643555,\n",
       "   48.681243896484375,\n",
       "   48.69883346557617,\n",
       "   48.8264274597168,\n",
       "   49.07492446899414,\n",
       "   49.373374938964844,\n",
       "   49.810020446777344,\n",
       "   50.21070861816406,\n",
       "   50.633121490478516,\n",
       "   50.60680389404297,\n",
       "   51.42778778076172,\n",
       "   52.00799560546875,\n",
       "   52.388755798339844,\n",
       "   52.234519958496094,\n",
       "   52.0982551574707,\n",
       "   51.59343338012695,\n",
       "   51.07761001586914,\n",
       "   51.03179168701172,\n",
       "   51.208030700683594,\n",
       "   51.3956298828125,\n",
       "   51.5115966796875,\n",
       "   52.03915786743164,\n",
       "   52.32243347167969,\n",
       "   52.794898986816406,\n",
       "   53.575557708740234,\n",
       "   53.70021057128906,\n",
       "   53.332374572753906,\n",
       "   53.42023468017578,\n",
       "   53.343421936035156,\n",
       "   52.89134979248047,\n",
       "   53.423065185546875,\n",
       "   53.44855880737305,\n",
       "   53.960968017578125,\n",
       "   53.836334228515625,\n",
       "   54.619598388671875,\n",
       "   54.98371887207031,\n",
       "   55.43062210083008,\n",
       "   55.31269836425781,\n",
       "   55.511962890625,\n",
       "   55.19231033325195,\n",
       "   55.064666748046875,\n",
       "   55.29757308959961,\n",
       "   54.787052154541016,\n",
       "   55.04267883300781,\n",
       "   54.91648483276367,\n",
       "   54.57925796508789,\n",
       "   53.987396240234375,\n",
       "   53.69536209106445,\n",
       "   53.76905822753906,\n",
       "   53.09864807128906,\n",
       "   52.93254852294922,\n",
       "   52.99198532104492,\n",
       "   52.69017791748047,\n",
       "   51.91728973388672,\n",
       "   51.93550491333008,\n",
       "   52.07896423339844,\n",
       "   52.78591537475586,\n",
       "   52.807003021240234,\n",
       "   52.071224212646484,\n",
       "   51.99204635620117,\n",
       "   51.63688659667969,\n",
       "   51.16114044189453,\n",
       "   51.46735382080078,\n",
       "   51.46412658691406,\n",
       "   51.48938751220703,\n",
       "   51.76469039916992,\n",
       "   51.499420166015625,\n",
       "   51.629722595214844,\n",
       "   51.365482330322266,\n",
       "   51.41197967529297,\n",
       "   51.369049072265625,\n",
       "   52.19178009033203,\n",
       "   52.63513946533203,\n",
       "   53.15564727783203,\n",
       "   53.503204345703125,\n",
       "   54.11983871459961,\n",
       "   54.05260467529297,\n",
       "   53.82560348510742,\n",
       "   53.99454116821289,\n",
       "   54.09885787963867,\n",
       "   53.8980598449707,\n",
       "   53.647361755371094,\n",
       "   53.45416259765625,\n",
       "   53.2121467590332,\n",
       "   53.020328521728516,\n",
       "   52.96424102783203,\n",
       "   52.975345611572266,\n",
       "   52.498985290527344,\n",
       "   52.593624114990234,\n",
       "   52.332252502441406,\n",
       "   52.520965576171875,\n",
       "   52.45830535888672,\n",
       "   52.64464569091797,\n",
       "   52.370487213134766,\n",
       "   52.51094055175781,\n",
       "   52.73657989501953,\n",
       "   52.81037521362305,\n",
       "   52.90563201904297,\n",
       "   52.637325286865234,\n",
       "   52.61239242553711,\n",
       "   52.54214859008789,\n",
       "   52.178043365478516,\n",
       "   52.16594314575195,\n",
       "   51.977264404296875,\n",
       "   51.936363220214844,\n",
       "   51.69367218017578,\n",
       "   52.208984375,\n",
       "   52.53527069091797,\n",
       "   52.34512710571289,\n",
       "   51.904361724853516,\n",
       "   51.988075256347656,\n",
       "   52.16352844238281,\n",
       "   52.01740264892578,\n",
       "   51.70395278930664,\n",
       "   51.69709396362305,\n",
       "   51.74140930175781,\n",
       "   51.905906677246094,\n",
       "   51.79927444458008,\n",
       "   51.8587532043457,\n",
       "   51.7972526550293,\n",
       "   51.741554260253906,\n",
       "   51.47010040283203,\n",
       "   51.084129333496094,\n",
       "   50.51457977294922,\n",
       "   50.062782287597656,\n",
       "   49.69458770751953,\n",
       "   49.5035400390625,\n",
       "   49.38221740722656,\n",
       "   49.258872985839844],\n",
       "  'test_accs': [396.16820526123047,\n",
       "   363.35384368896484,\n",
       "   349.4053497314453,\n",
       "   341.3062973022461,\n",
       "   335.92051696777344,\n",
       "   333.70142364501953,\n",
       "   331.8778991699219,\n",
       "   330.66861724853516,\n",
       "   330.52088928222656,\n",
       "   330.7751178741455,\n",
       "   331.2220516204834,\n",
       "   329.23329162597656,\n",
       "   324.0110168457031,\n",
       "   312.49732971191406,\n",
       "   302.7492485046387,\n",
       "   294.258846282959,\n",
       "   284.3808708190918,\n",
       "   273.68378829956055,\n",
       "   265.80188751220703,\n",
       "   256.8176383972168,\n",
       "   249.0704689025879,\n",
       "   244.66266632080078,\n",
       "   238.54154205322266,\n",
       "   235.0715103149414,\n",
       "   231.82444763183594,\n",
       "   229.86753845214844,\n",
       "   228.65935897827148,\n",
       "   226.33794784545898,\n",
       "   224.67137908935547,\n",
       "   223.02997207641602,\n",
       "   221.17621612548828,\n",
       "   219.1497631072998,\n",
       "   218.6403522491455,\n",
       "   222.56583976745605,\n",
       "   222.3054141998291,\n",
       "   223.83057975769043,\n",
       "   223.68799591064453,\n",
       "   225.97315216064453,\n",
       "   229.19607543945312,\n",
       "   231.34882736206055,\n",
       "   233.29895401000977,\n",
       "   234.42545700073242,\n",
       "   235.90402221679688,\n",
       "   236.76834869384766,\n",
       "   237.87858200073242,\n",
       "   239.0221405029297,\n",
       "   239.86541748046875,\n",
       "   237.57952880859375,\n",
       "   236.08074188232422,\n",
       "   234.14939498901367,\n",
       "   232.57727813720703,\n",
       "   231.03564453125,\n",
       "   228.4190673828125,\n",
       "   226.02960968017578,\n",
       "   225.18003463745117,\n",
       "   223.80032348632812,\n",
       "   222.50522994995117,\n",
       "   223.4589900970459,\n",
       "   223.9992847442627,\n",
       "   224.04587936401367,\n",
       "   223.0043239593506,\n",
       "   222.34825706481934,\n",
       "   219.6554183959961,\n",
       "   219.22654724121094,\n",
       "   219.34594345092773,\n",
       "   220.36597061157227,\n",
       "   221.7734489440918,\n",
       "   224.5215358734131,\n",
       "   226.70477104187012,\n",
       "   226.84213256835938,\n",
       "   226.2659797668457,\n",
       "   226.41153144836426,\n",
       "   224.8899974822998,\n",
       "   225.40495491027832,\n",
       "   226.28039169311523,\n",
       "   226.17066192626953,\n",
       "   225.6049041748047,\n",
       "   226.51100158691406,\n",
       "   228.45573234558105,\n",
       "   227.82317352294922,\n",
       "   228.45798301696777,\n",
       "   229.2429313659668,\n",
       "   229.76208114624023,\n",
       "   229.50808334350586,\n",
       "   230.35517501831055,\n",
       "   231.44801712036133,\n",
       "   231.81607818603516,\n",
       "   231.62577438354492,\n",
       "   230.92697525024414,\n",
       "   231.76628494262695,\n",
       "   232.03658294677734,\n",
       "   231.4534568786621,\n",
       "   230.9261245727539,\n",
       "   229.4753532409668,\n",
       "   229.880859375,\n",
       "   231.24640274047852,\n",
       "   230.28648376464844,\n",
       "   230.61871337890625,\n",
       "   230.22497177124023,\n",
       "   230.29222869873047,\n",
       "   227.41188430786133,\n",
       "   225.12575912475586,\n",
       "   224.8835334777832,\n",
       "   222.83789825439453,\n",
       "   223.60574340820312,\n",
       "   222.06842041015625,\n",
       "   219.33119010925293,\n",
       "   217.64071464538574,\n",
       "   217.02742958068848,\n",
       "   215.3530387878418,\n",
       "   215.22579765319824,\n",
       "   215.22102165222168,\n",
       "   214.55456733703613,\n",
       "   212.98984718322754,\n",
       "   212.37387084960938,\n",
       "   212.3567295074463,\n",
       "   212.43202590942383,\n",
       "   213.99064826965332,\n",
       "   215.09129333496094,\n",
       "   215.89759063720703,\n",
       "   217.10107803344727,\n",
       "   218.52774810791016,\n",
       "   219.03778457641602,\n",
       "   219.0960235595703,\n",
       "   219.32508277893066,\n",
       "   219.39434814453125,\n",
       "   219.07432174682617,\n",
       "   221.04215621948242,\n",
       "   221.78338623046875,\n",
       "   222.9456081390381,\n",
       "   223.51641654968262,\n",
       "   223.69677734375,\n",
       "   223.91168594360352,\n",
       "   222.62580490112305,\n",
       "   221.95600700378418,\n",
       "   221.5281047821045,\n",
       "   221.20397567749023,\n",
       "   220.2987403869629,\n",
       "   219.8531723022461,\n",
       "   219.1576042175293,\n",
       "   219.90943717956543,\n",
       "   219.75811576843262,\n",
       "   220.36065864562988,\n",
       "   221.59119987487793,\n",
       "   221.04688262939453,\n",
       "   221.9802532196045,\n",
       "   221.957763671875,\n",
       "   222.98684692382812,\n",
       "   223.19072151184082,\n",
       "   224.0170135498047,\n",
       "   224.2571315765381,\n",
       "   223.9960880279541,\n",
       "   224.24818229675293,\n",
       "   222.9684829711914,\n",
       "   222.45026779174805,\n",
       "   221.83868217468262,\n",
       "   221.73263931274414,\n",
       "   221.32195854187012,\n",
       "   220.9759464263916,\n",
       "   221.24878311157227,\n",
       "   220.3318042755127,\n",
       "   219.92856788635254,\n",
       "   218.5304470062256,\n",
       "   218.53358459472656,\n",
       "   219.9469337463379,\n",
       "   220.0288429260254,\n",
       "   219.92517280578613,\n",
       "   220.598726272583,\n",
       "   221.06430435180664,\n",
       "   221.48748016357422,\n",
       "   223.00653076171875,\n",
       "   222.80864906311035,\n",
       "   223.7740707397461,\n",
       "   225.3826446533203,\n",
       "   225.83892059326172,\n",
       "   226.21050643920898,\n",
       "   226.5841522216797,\n",
       "   225.67620086669922,\n",
       "   226.6319808959961,\n",
       "   225.70838165283203,\n",
       "   224.71627807617188,\n",
       "   225.23359298706055,\n",
       "   224.30906105041504,\n",
       "   224.14553451538086,\n",
       "   224.27174377441406,\n",
       "   222.1611042022705,\n",
       "   222.26063537597656,\n",
       "   222.26026916503906,\n",
       "   222.56964492797852,\n",
       "   224.25591659545898,\n",
       "   224.64731979370117,\n",
       "   225.0149440765381,\n",
       "   224.42787551879883,\n",
       "   223.92737197875977,\n",
       "   224.17842864990234,\n",
       "   224.70676803588867,\n",
       "   224.25065994262695,\n",
       "   224.84818649291992,\n",
       "   224.93442153930664,\n",
       "   225.2373161315918,\n",
       "   225.50943756103516,\n",
       "   225.46308517456055,\n",
       "   224.41571807861328,\n",
       "   223.79848861694336,\n",
       "   224.28693771362305,\n",
       "   223.92807006835938,\n",
       "   224.88956451416016,\n",
       "   224.90406036376953,\n",
       "   226.12443923950195,\n",
       "   226.6122283935547,\n",
       "   226.8701286315918,\n",
       "   228.09360885620117,\n",
       "   229.08214950561523,\n",
       "   229.4078712463379,\n",
       "   227.92390823364258,\n",
       "   227.8336067199707,\n",
       "   226.89403533935547,\n",
       "   226.06591415405273,\n",
       "   225.06459426879883,\n",
       "   223.89886093139648,\n",
       "   224.3398323059082,\n",
       "   224.5276222229004,\n",
       "   224.86835098266602,\n",
       "   225.05654907226562,\n",
       "   224.3887176513672,\n",
       "   224.7282943725586,\n",
       "   224.15301132202148,\n",
       "   222.72216033935547,\n",
       "   222.2899513244629,\n",
       "   222.52181243896484,\n",
       "   221.90311241149902,\n",
       "   223.29237365722656,\n",
       "   223.3021354675293,\n",
       "   223.18192672729492,\n",
       "   221.82081604003906,\n",
       "   221.5801067352295,\n",
       "   221.6593132019043,\n",
       "   222.0728530883789,\n",
       "   222.6825408935547,\n",
       "   222.73536682128906,\n",
       "   223.2071304321289,\n",
       "   225.22024536132812,\n",
       "   226.47356033325195,\n",
       "   227.33836364746094,\n",
       "   227.69856643676758,\n",
       "   228.4835433959961,\n",
       "   228.83789825439453,\n",
       "   229.79887008666992,\n",
       "   229.42777633666992,\n",
       "   229.02560424804688,\n",
       "   228.82252502441406,\n",
       "   230.09786224365234,\n",
       "   229.53993606567383,\n",
       "   229.83438873291016,\n",
       "   230.22670364379883,\n",
       "   229.8109893798828,\n",
       "   230.85374069213867,\n",
       "   230.26757049560547,\n",
       "   230.18210220336914,\n",
       "   229.89617538452148,\n",
       "   230.37152862548828,\n",
       "   229.06757736206055,\n",
       "   228.53533172607422,\n",
       "   228.10387802124023,\n",
       "   227.39762496948242,\n",
       "   227.76377487182617,\n",
       "   228.46189498901367,\n",
       "   227.5936050415039,\n",
       "   228.2711524963379,\n",
       "   228.00995635986328,\n",
       "   227.26430892944336,\n",
       "   227.06584930419922,\n",
       "   227.18798446655273,\n",
       "   226.83733367919922,\n",
       "   227.67792892456055,\n",
       "   228.43667602539062,\n",
       "   228.56347274780273,\n",
       "   230.6517448425293,\n",
       "   232.1106414794922,\n",
       "   232.1843605041504,\n",
       "   232.65335845947266,\n",
       "   233.26739501953125,\n",
       "   232.04665756225586,\n",
       "   230.6352081298828,\n",
       "   229.87418365478516,\n",
       "   229.28139877319336,\n",
       "   228.67455291748047,\n",
       "   227.6697006225586,\n",
       "   226.82094192504883,\n",
       "   226.77718353271484,\n",
       "   225.94952392578125,\n",
       "   225.3849754333496,\n",
       "   226.1137809753418,\n",
       "   224.56264114379883,\n",
       "   223.99409866333008,\n",
       "   224.8815574645996,\n",
       "   226.77032470703125,\n",
       "   227.60543823242188,\n",
       "   228.78263473510742,\n",
       "   230.18599700927734,\n",
       "   230.95276260375977,\n",
       "   232.19157791137695,\n",
       "   232.42010116577148,\n",
       "   233.41125106811523,\n",
       "   233.81919860839844,\n",
       "   233.8532371520996,\n",
       "   233.5056495666504,\n",
       "   233.69526290893555,\n",
       "   232.9338836669922,\n",
       "   232.34016799926758,\n",
       "   232.38276290893555,\n",
       "   230.65817260742188,\n",
       "   229.59657287597656,\n",
       "   229.51832580566406,\n",
       "   229.53565216064453,\n",
       "   229.76865005493164,\n",
       "   229.14534378051758,\n",
       "   228.44878387451172,\n",
       "   228.64675903320312,\n",
       "   227.83474349975586,\n",
       "   227.79711151123047,\n",
       "   228.21454238891602,\n",
       "   228.52982330322266,\n",
       "   228.63481521606445,\n",
       "   227.4442024230957,\n",
       "   227.55229568481445,\n",
       "   227.7107391357422,\n",
       "   227.83457565307617,\n",
       "   226.60800552368164,\n",
       "   226.53025817871094,\n",
       "   226.67385864257812,\n",
       "   225.8090476989746,\n",
       "   225.7461929321289,\n",
       "   224.6864471435547,\n",
       "   224.61064910888672,\n",
       "   223.84772491455078,\n",
       "   224.94044876098633,\n",
       "   223.95202255249023,\n",
       "   224.14134216308594,\n",
       "   224.78650283813477,\n",
       "   224.46665573120117,\n",
       "   224.71163940429688,\n",
       "   224.3655548095703,\n",
       "   224.37124252319336,\n",
       "   223.88530349731445,\n",
       "   224.8886375427246,\n",
       "   223.64139938354492,\n",
       "   224.46086883544922,\n",
       "   224.50786590576172,\n",
       "   224.94940567016602,\n",
       "   225.1631622314453,\n",
       "   225.81998443603516,\n",
       "   226.37029266357422,\n",
       "   225.73504638671875,\n",
       "   226.29314422607422,\n",
       "   227.11381149291992,\n",
       "   228.6762580871582,\n",
       "   227.9007911682129,\n",
       "   227.65105819702148,\n",
       "   227.47738647460938,\n",
       "   227.19462966918945,\n",
       "   227.58650588989258,\n",
       "   226.9410057067871,\n",
       "   226.9230499267578,\n",
       "   227.1978416442871,\n",
       "   227.3232192993164,\n",
       "   225.80560302734375,\n",
       "   225.61810684204102,\n",
       "   224.77793884277344,\n",
       "   225.04961013793945,\n",
       "   226.65212631225586,\n",
       "   226.26506805419922,\n",
       "   226.6513900756836,\n",
       "   226.7277069091797,\n",
       "   226.9597625732422,\n",
       "   226.86551666259766,\n",
       "   226.93130493164062,\n",
       "   227.54767990112305,\n",
       "   226.94137573242188,\n",
       "   227.38526153564453,\n",
       "   226.26782608032227,\n",
       "   227.11113357543945,\n",
       "   228.06950759887695,\n",
       "   228.3824691772461,\n",
       "   228.23357772827148,\n",
       "   227.74826431274414,\n",
       "   226.9673957824707,\n",
       "   226.60311889648438,\n",
       "   227.37459182739258,\n",
       "   227.6327896118164,\n",
       "   227.1913299560547,\n",
       "   227.20472717285156,\n",
       "   226.98269653320312,\n",
       "   226.7618637084961,\n",
       "   227.1965103149414,\n",
       "   227.26042938232422,\n",
       "   227.47969436645508,\n",
       "   227.6591911315918,\n",
       "   227.9983139038086,\n",
       "   228.4924201965332,\n",
       "   227.97624588012695,\n",
       "   229.63323593139648,\n",
       "   229.239990234375,\n",
       "   229.74468994140625,\n",
       "   230.25033950805664,\n",
       "   230.95300674438477,\n",
       "   231.24854278564453,\n",
       "   231.27737045288086,\n",
       "   230.27038955688477,\n",
       "   230.34127807617188,\n",
       "   230.0961799621582,\n",
       "   230.4035987854004,\n",
       "   230.9754524230957,\n",
       "   231.64060974121094,\n",
       "   232.44962310791016,\n",
       "   232.52439498901367,\n",
       "   232.7630958557129,\n",
       "   232.51763153076172,\n",
       "   232.56680297851562,\n",
       "   232.93973922729492,\n",
       "   232.7513885498047,\n",
       "   233.71286010742188,\n",
       "   233.62393188476562,\n",
       "   233.0757598876953,\n",
       "   232.44351959228516,\n",
       "   232.48303985595703,\n",
       "   232.99768447875977,\n",
       "   234.7368049621582,\n",
       "   235.7516746520996,\n",
       "   234.9832305908203,\n",
       "   235.17604446411133,\n",
       "   234.8357162475586,\n",
       "   234.85661697387695,\n",
       "   235.49427795410156,\n",
       "   235.1890106201172,\n",
       "   234.43186569213867,\n",
       "   233.9072380065918,\n",
       "   232.7235565185547,\n",
       "   232.01670455932617,\n",
       "   230.77507781982422,\n",
       "   229.3685760498047,\n",
       "   229.0511703491211,\n",
       "   229.04135513305664,\n",
       "   228.29767608642578,\n",
       "   228.7643928527832,\n",
       "   228.17442321777344,\n",
       "   227.73150634765625,\n",
       "   226.56425094604492,\n",
       "   225.02639389038086,\n",
       "   224.1790657043457,\n",
       "   223.65364456176758,\n",
       "   223.29815292358398,\n",
       "   222.86481857299805,\n",
       "   222.66906929016113,\n",
       "   222.64055633544922,\n",
       "   222.29549026489258,\n",
       "   222.93067741394043,\n",
       "   224.44772720336914,\n",
       "   224.47600555419922,\n",
       "   225.12889099121094,\n",
       "   225.38097381591797,\n",
       "   225.43885803222656,\n",
       "   225.15545463562012,\n",
       "   225.8231430053711,\n",
       "   225.7012176513672,\n",
       "   226.30708694458008,\n",
       "   226.5476951599121,\n",
       "   226.38824844360352,\n",
       "   226.92091369628906,\n",
       "   226.75589752197266,\n",
       "   226.9503288269043,\n",
       "   227.4097023010254,\n",
       "   227.99050521850586,\n",
       "   228.4078254699707,\n",
       "   228.66033554077148,\n",
       "   229.72772979736328,\n",
       "   229.7455062866211,\n",
       "   230.45883178710938,\n",
       "   230.83153533935547,\n",
       "   230.13193130493164,\n",
       "   229.82655715942383,\n",
       "   230.3413848876953,\n",
       "   230.88636779785156,\n",
       "   230.70225143432617,\n",
       "   230.94956970214844,\n",
       "   231.46676635742188,\n",
       "   231.54268646240234,\n",
       "   230.96736526489258,\n",
       "   231.32929229736328,\n",
       "   231.20930099487305,\n",
       "   230.29077529907227,\n",
       "   229.98709869384766,\n",
       "   229.87023544311523,\n",
       "   229.50237655639648,\n",
       "   228.8780632019043,\n",
       "   228.02090454101562,\n",
       "   227.28418350219727,\n",
       "   226.67896270751953,\n",
       "   226.95868301391602,\n",
       "   226.97259902954102],\n",
       "  'pce_r2': [-2.1237821299410347,\n",
       "   -0.6501478104296095,\n",
       "   -0.23914513015862982,\n",
       "   -0.06462948877492436,\n",
       "   0.01737317122665938,\n",
       "   0.04379679500966627,\n",
       "   0.000663670493137869,\n",
       "   -0.1130418938379556,\n",
       "   -0.2677639192705601,\n",
       "   -0.4679328371946916,\n",
       "   -0.6328003133394884,\n",
       "   -0.8067500572148254,\n",
       "   -0.9325353133580587,\n",
       "   -1.059019180955199,\n",
       "   -1.295069605508027,\n",
       "   -1.4444259620509614,\n",
       "   -1.430286155703894,\n",
       "   -1.4571613879001766,\n",
       "   -1.5189050417659566,\n",
       "   -1.640704065636037,\n",
       "   -1.821940435084247,\n",
       "   -2.313556159220073,\n",
       "   -2.6068907938040717,\n",
       "   -3.3312286864920457,\n",
       "   -4.036342846030166,\n",
       "   -5.0832190768677865,\n",
       "   -6.458286865485218,\n",
       "   -7.483015578369448,\n",
       "   -9.176243258785112,\n",
       "   -10.90713522847111,\n",
       "   -12.077899945551152,\n",
       "   -13.676160534622689,\n",
       "   -14.941919899805017,\n",
       "   -16.39699990426195,\n",
       "   -17.601057527516925,\n",
       "   -19.981133161500804,\n",
       "   -20.661930661369922,\n",
       "   -22.184058189354946,\n",
       "   -23.183841072928953,\n",
       "   -23.821895774830196,\n",
       "   -24.36067703257032,\n",
       "   -24.565012366413537,\n",
       "   -24.78198760203185,\n",
       "   -25.107041667364,\n",
       "   -24.91085520691958,\n",
       "   -25.269058079600097,\n",
       "   -24.97157432002985,\n",
       "   -24.19484793077389,\n",
       "   -24.342795117276022,\n",
       "   -23.886450702100085,\n",
       "   -23.62328533814288,\n",
       "   -22.943602938579723,\n",
       "   -22.391123574854298,\n",
       "   -22.137552722613638,\n",
       "   -22.1073702214438,\n",
       "   -21.824100148288014,\n",
       "   -21.36676156175164,\n",
       "   -21.28870099664696,\n",
       "   -20.81047577184757,\n",
       "   -20.54627957505818,\n",
       "   -19.52839488738161,\n",
       "   -19.095774473492806,\n",
       "   -18.65235582657562,\n",
       "   -18.668838155750272,\n",
       "   -18.8451263793678,\n",
       "   -18.638927133107106,\n",
       "   -17.654970290753308,\n",
       "   -17.229050314603587,\n",
       "   -17.679460453834135,\n",
       "   -17.843119615126977,\n",
       "   -17.485198744493914,\n",
       "   -17.78744407275323,\n",
       "   -17.52964425739513,\n",
       "   -17.762781590098843,\n",
       "   -17.574384878088612,\n",
       "   -17.245968257279902,\n",
       "   -16.48782108325948,\n",
       "   -16.53870780823642,\n",
       "   -16.401877790283518,\n",
       "   -15.874705289836214,\n",
       "   -15.755986238490802,\n",
       "   -15.465667027825674,\n",
       "   -15.353079015400304,\n",
       "   -15.256607108691465,\n",
       "   -14.754389940045101,\n",
       "   -15.170771994192169,\n",
       "   -15.34310000045135,\n",
       "   -15.905541616912728,\n",
       "   -15.720614411350276,\n",
       "   -15.867993555796449,\n",
       "   -16.110970047546473,\n",
       "   -15.937243708832995,\n",
       "   -15.788132593965166,\n",
       "   -15.308414654136715,\n",
       "   -15.266103952499279,\n",
       "   -15.884170286454097,\n",
       "   -15.306483631244447,\n",
       "   -15.369965633507281,\n",
       "   -15.334379203433908,\n",
       "   -14.813952168144919,\n",
       "   -14.55438579396697,\n",
       "   -14.383096412203315,\n",
       "   -14.350841649543128,\n",
       "   -14.371840815888486,\n",
       "   -14.47037860330519,\n",
       "   -14.722301554303257,\n",
       "   -14.569293150098284,\n",
       "   -14.679699859659419,\n",
       "   -14.754419839301203,\n",
       "   -14.875934843347975,\n",
       "   -15.30355773373304,\n",
       "   -15.290210150619604,\n",
       "   -14.893913163776261,\n",
       "   -14.746840282781571,\n",
       "   -15.151752193672653,\n",
       "   -15.185204052352276,\n",
       "   -14.8000441382548,\n",
       "   -14.884517800286837,\n",
       "   -15.398899627875188,\n",
       "   -15.761404155876669,\n",
       "   -16.512617626892734,\n",
       "   -17.268408893902908,\n",
       "   -17.30846917025384,\n",
       "   -17.378957651535437,\n",
       "   -17.035299419388085,\n",
       "   -16.83136845563441,\n",
       "   -17.130658089257146,\n",
       "   -17.841945907711935,\n",
       "   -17.602096727641204,\n",
       "   -17.843948230217887,\n",
       "   -18.05197287197782,\n",
       "   -17.182606034333155,\n",
       "   -17.784808303045676,\n",
       "   -17.425058072020676,\n",
       "   -16.69352174500788,\n",
       "   -16.877363307473953,\n",
       "   -16.542135066432323,\n",
       "   -16.4900859885165,\n",
       "   -16.848885561334633,\n",
       "   -16.496284330329782,\n",
       "   -17.033389809709497,\n",
       "   -16.918668551282163,\n",
       "   -16.599387853638422,\n",
       "   -16.607362719768442,\n",
       "   -16.71296963565323,\n",
       "   -16.379993746982617,\n",
       "   -16.001286391548923,\n",
       "   -16.9218003090691,\n",
       "   -17.23187160526012,\n",
       "   -17.00214005649166,\n",
       "   -17.225753084579445,\n",
       "   -18.188330434476924,\n",
       "   -18.042070730284294,\n",
       "   -17.951157802397507,\n",
       "   -18.116731969919645,\n",
       "   -17.74949575897585,\n",
       "   -17.934035197860943,\n",
       "   -17.471024620416877,\n",
       "   -17.899330066336763,\n",
       "   -17.880680517204386,\n",
       "   -18.461632242945022,\n",
       "   -19.322081633228862,\n",
       "   -19.59865729014209,\n",
       "   -19.287499493664775,\n",
       "   -20.146269969704345,\n",
       "   -20.11031808388772,\n",
       "   -20.566788462623528,\n",
       "   -20.612999487718763,\n",
       "   -19.91093151588829,\n",
       "   -20.240531246441552,\n",
       "   -20.006734978461367,\n",
       "   -19.742620849824807,\n",
       "   -19.887970489402367,\n",
       "   -20.20136131901447,\n",
       "   -20.01388351504287,\n",
       "   -19.936670638602784,\n",
       "   -20.263031193817856,\n",
       "   -19.887153971254456,\n",
       "   -19.584343942992813,\n",
       "   -19.398467813086143,\n",
       "   -18.65664987165173,\n",
       "   -18.281992781383728,\n",
       "   -16.938790699719853,\n",
       "   -16.934765422251843,\n",
       "   -17.6627041906691,\n",
       "   -17.46201209194938,\n",
       "   -17.64972741711286,\n",
       "   -16.93000211154255,\n",
       "   -17.340869054792577,\n",
       "   -17.720674231089134,\n",
       "   -17.715491247783316,\n",
       "   -18.209315720223877,\n",
       "   -18.264204074943418,\n",
       "   -19.022618981670863,\n",
       "   -19.169504241492398,\n",
       "   -19.03609072433475,\n",
       "   -19.11770908892889,\n",
       "   -19.470295090704237,\n",
       "   -19.30805331351269,\n",
       "   -19.22952668224072,\n",
       "   -18.658524997699644,\n",
       "   -19.507684337999045,\n",
       "   -19.428753745673944,\n",
       "   -19.45580249797087,\n",
       "   -19.67062655885627,\n",
       "   -19.705602336389163,\n",
       "   -19.205129664746583,\n",
       "   -19.948996157868848,\n",
       "   -20.42224343987906,\n",
       "   -21.43117386775902,\n",
       "   -20.74547370404076,\n",
       "   -21.156358805220904,\n",
       "   -20.452160775986393,\n",
       "   -19.76559971145357,\n",
       "   -18.872535837372528,\n",
       "   -18.522619012698947,\n",
       "   -18.62738185514136,\n",
       "   -18.353847474985375,\n",
       "   -18.609359663729748,\n",
       "   -19.068338754118997,\n",
       "   -19.30599638738076,\n",
       "   -19.01129916125714,\n",
       "   -18.91589467673514,\n",
       "   -18.76736176459889,\n",
       "   -18.568747156214158,\n",
       "   -19.48150055771979,\n",
       "   -19.105193395768822,\n",
       "   -19.220870475722567,\n",
       "   -18.751219334251555,\n",
       "   -20.220466363435886,\n",
       "   -19.667425935132034,\n",
       "   -20.545340571827772,\n",
       "   -19.84899500447512,\n",
       "   -20.085045439032182,\n",
       "   -18.77653086433575,\n",
       "   -18.011364886320223,\n",
       "   -18.706813984531244,\n",
       "   -18.206005932728242,\n",
       "   -18.46104533133278,\n",
       "   -19.05668164712821,\n",
       "   -19.747153399805114,\n",
       "   -19.847529090068644,\n",
       "   -21.285390702896965,\n",
       "   -22.366592202913335,\n",
       "   -22.880120457559727,\n",
       "   -24.04297899330379,\n",
       "   -23.903421802370918,\n",
       "   -22.896260534837964,\n",
       "   -22.006072058430334,\n",
       "   -21.158673425236756,\n",
       "   -20.55449339741494,\n",
       "   -19.830384917041254,\n",
       "   -18.028582184554327,\n",
       "   -17.861390212010722,\n",
       "   -17.690731596140814,\n",
       "   -17.729747113458203,\n",
       "   -18.344719632507392,\n",
       "   -17.573357310340203,\n",
       "   -17.357932099459422,\n",
       "   -17.565208575021106,\n",
       "   -16.762598989443735,\n",
       "   -15.964440954021498,\n",
       "   -16.467621710939575,\n",
       "   -17.359334774790707,\n",
       "   -17.458880523653892,\n",
       "   -17.36311526268778,\n",
       "   -16.559728108531367,\n",
       "   -16.52380320240073,\n",
       "   -17.126861933102756,\n",
       "   -17.040417164919294,\n",
       "   -16.843110570790476,\n",
       "   -17.016571909201957,\n",
       "   -17.30475814767498,\n",
       "   -17.53143293673545,\n",
       "   -17.633881886782525,\n",
       "   -18.678760438581087,\n",
       "   -20.516745674930867,\n",
       "   -22.079023366945734,\n",
       "   -23.675492920982173,\n",
       "   -24.07428280693583,\n",
       "   -24.894816968335466,\n",
       "   -25.171523687961006,\n",
       "   -24.92836846340164,\n",
       "   -25.16112970158674,\n",
       "   -25.643175897274293,\n",
       "   -26.199118569586606,\n",
       "   -26.121238510811832,\n",
       "   -26.199404606181915,\n",
       "   -26.124689000571628,\n",
       "   -25.821754425548455,\n",
       "   -26.465971142722157,\n",
       "   -26.29920405889771,\n",
       "   -26.452222398692005,\n",
       "   -27.999711612699414,\n",
       "   -27.5035258560871,\n",
       "   -29.319338233612616,\n",
       "   -32.980969713295806,\n",
       "   -32.36943130388063,\n",
       "   -32.24294453074661,\n",
       "   -33.96499838168169,\n",
       "   -33.84846895455125,\n",
       "   -34.02648164679981,\n",
       "   -34.61578720051149,\n",
       "   -35.97906893757618,\n",
       "   -35.11434837877935,\n",
       "   -35.536401513266064,\n",
       "   -36.73672275118153,\n",
       "   -39.3592106964009,\n",
       "   -38.91100792667787,\n",
       "   -38.12793075284537,\n",
       "   -39.04314905099582,\n",
       "   -35.58922829102398,\n",
       "   -33.38985768088664,\n",
       "   -36.25909960181527,\n",
       "   -35.86358223866144,\n",
       "   -35.41368444506956,\n",
       "   -37.28247426012873,\n",
       "   -37.545427509511384,\n",
       "   -36.69918665679438,\n",
       "   -33.02725409750171,\n",
       "   -31.346896318633988,\n",
       "   -32.64935863202667,\n",
       "   -34.107435972665606,\n",
       "   -34.336345871854085,\n",
       "   -33.10824269865503,\n",
       "   -34.68936696982396,\n",
       "   -34.81920442670411,\n",
       "   -33.935242455748565,\n",
       "   -31.48061959137739,\n",
       "   -31.096340989332866,\n",
       "   -31.447902905691656,\n",
       "   -30.174126055165434,\n",
       "   -29.91718891318684,\n",
       "   -28.003928766485114,\n",
       "   -28.776559801610848,\n",
       "   -29.015162208111004,\n",
       "   -30.28697482325405,\n",
       "   -29.45567320793134,\n",
       "   -28.27929404612713,\n",
       "   -29.92131319151526,\n",
       "   -30.593351097629263,\n",
       "   -31.48683945194317,\n",
       "   -32.79077647148618,\n",
       "   -35.05885718092654,\n",
       "   -35.04563662076127,\n",
       "   -35.79081397496216,\n",
       "   -34.44109427672394,\n",
       "   -34.97930574558175,\n",
       "   -34.48717579185916,\n",
       "   -35.23159391041269,\n",
       "   -34.7244744144816,\n",
       "   -36.19123466250954,\n",
       "   -36.46856808774364,\n",
       "   -36.203154153338616,\n",
       "   -35.482841780952285,\n",
       "   -37.11419036091294,\n",
       "   -37.12406484334648,\n",
       "   -38.013680621633256,\n",
       "   -38.635413563899355,\n",
       "   -37.9714619175123,\n",
       "   -37.90956263451972,\n",
       "   -35.47599423210505,\n",
       "   -36.2199596774297,\n",
       "   -34.23324741282629,\n",
       "   -33.49349429944498,\n",
       "   -31.632014695832176,\n",
       "   -30.624307940849683,\n",
       "   -28.681539112923016,\n",
       "   -26.077089306964186,\n",
       "   -26.3581283031781,\n",
       "   -26.858968083084257,\n",
       "   -25.792397474538625,\n",
       "   -26.31007142631091,\n",
       "   -27.125220261542577,\n",
       "   -27.16928595616635,\n",
       "   -26.795537043572338,\n",
       "   -25.13210834007736,\n",
       "   -24.802851918114044,\n",
       "   -22.456776051246354,\n",
       "   -22.589979479766757,\n",
       "   -24.22421208993631,\n",
       "   -25.00933958791896,\n",
       "   -25.644675301533614,\n",
       "   -25.765503992918024,\n",
       "   -28.218654812126204,\n",
       "   -27.90992454006227,\n",
       "   -27.966722386804957,\n",
       "   -27.466308446226105,\n",
       "   -29.06918728963392,\n",
       "   -28.447699840292206,\n",
       "   -27.6865562977383,\n",
       "   -27.457463562471172,\n",
       "   -26.05921853619581,\n",
       "   -27.467752263914996,\n",
       "   -28.944504833244995,\n",
       "   -29.810012845462534,\n",
       "   -30.543402343116032,\n",
       "   -31.61700463522522,\n",
       "   -35.84093600217753,\n",
       "   -33.510989607834595,\n",
       "   -34.36239453700198,\n",
       "   -36.45712130549511,\n",
       "   -36.355307753587354,\n",
       "   -36.29669062011272,\n",
       "   -36.39977029962251,\n",
       "   -36.75500712741535,\n",
       "   -33.85009341710242,\n",
       "   -34.41975347312307,\n",
       "   -33.64536577556336,\n",
       "   -32.14840231882657,\n",
       "   -30.814042888479275,\n",
       "   -31.21114777360819,\n",
       "   -31.186476740562867,\n",
       "   -31.382125116599994,\n",
       "   -31.813452246467065,\n",
       "   -29.722035329859718,\n",
       "   -29.531152191910625,\n",
       "   -27.646931223317157,\n",
       "   -27.972961734238968,\n",
       "   -28.124184432489745,\n",
       "   -27.482753126854604,\n",
       "   -27.73637718287517,\n",
       "   -26.77698291696349,\n",
       "   -25.415436880849025,\n",
       "   -24.44633320369754,\n",
       "   -23.50856469874889,\n",
       "   -24.392186625832267,\n",
       "   -25.664935326058636,\n",
       "   -26.03701816551708,\n",
       "   -27.313989481816655,\n",
       "   -28.288528072816025,\n",
       "   -29.111655459863012,\n",
       "   -29.443601581604216,\n",
       "   -32.076362517068866,\n",
       "   -31.879995362656828,\n",
       "   -30.773593345152605,\n",
       "   -31.526407776060452,\n",
       "   -30.591729455074084,\n",
       "   -31.17708833478636,\n",
       "   -32.91332684736564,\n",
       "   -31.700363669268818,\n",
       "   -32.699708726539164,\n",
       "   -33.32647074979731,\n",
       "   -30.893240703972353,\n",
       "   -31.925844490855923,\n",
       "   -31.361943850370928,\n",
       "   -32.33770068422121,\n",
       "   -31.382424050118175,\n",
       "   -29.81955160287124,\n",
       "   -29.118146756926432,\n",
       "   -31.248826231243925,\n",
       "   -32.875088994476954,\n",
       "   -33.16369456258249,\n",
       "   -34.824734555069185,\n",
       "   -36.79403999861125,\n",
       "   -36.07461827674465,\n",
       "   -36.37983347987447,\n",
       "   -36.18258477748218,\n",
       "   -36.729623691399276,\n",
       "   -39.00404206173773,\n",
       "   -39.24742395658515,\n",
       "   -38.67961291591544,\n",
       "   -37.91790792275681,\n",
       "   -39.349621170455045,\n",
       "   -40.52712879670942,\n",
       "   -39.67952831759721,\n",
       "   -40.18068579084303,\n",
       "   -39.56236544249931,\n",
       "   -40.2659042670281,\n",
       "   -40.07886478969008,\n",
       "   -37.56539954710845,\n",
       "   -37.02861007843527,\n",
       "   -37.45539952576834,\n",
       "   -37.28127722881859,\n",
       "   -36.552500353253066,\n",
       "   -37.37471048068424,\n",
       "   -36.88894966512809,\n",
       "   -36.29904517567929,\n",
       "   -37.23128131837866,\n",
       "   -33.9155800759342,\n",
       "   -31.88652798588624,\n",
       "   -30.679347050071755,\n",
       "   -30.376998711963196,\n",
       "   -29.904525767551707,\n",
       "   -30.967384089962245,\n",
       "   -32.487688694315324,\n",
       "   -32.45298433254963,\n",
       "   -32.07519739673244,\n",
       "   -32.05495120658448,\n",
       "   -29.98276831319534,\n",
       "   -27.31871333839504,\n",
       "   -25.761884668939345,\n",
       "   -24.926334771655313,\n",
       "   -24.034505423222672,\n",
       "   -23.265332624724643,\n",
       "   -23.491061462046147,\n",
       "   -22.48342381215525,\n",
       "   -22.02282831143469,\n",
       "   -21.901179078406,\n",
       "   -22.715101378076195],\n",
       "  'voc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -1192.3787307808548,\n",
       "   -202.14003567308262,\n",
       "   -161.1186126992384,\n",
       "   -135.72155083609107,\n",
       "   -127.50880897071917,\n",
       "   -117.84479016321333,\n",
       "   -109.51098559777532,\n",
       "   -110.49860372045055,\n",
       "   -103.124352948256,\n",
       "   -97.25103414668177,\n",
       "   -97.02252544753634,\n",
       "   -93.08943047522354,\n",
       "   -93.6261788641691,\n",
       "   -89.21327610423013,\n",
       "   -88.00560883611509,\n",
       "   -84.96331795109461,\n",
       "   -76.39027120050567,\n",
       "   -68.75377344305551,\n",
       "   -61.31657986066376,\n",
       "   -52.423784648084315,\n",
       "   -40.26687952048279,\n",
       "   -40.42506768111666,\n",
       "   -49.598874858026576,\n",
       "   -53.09155244817715,\n",
       "   -67.7526643847421,\n",
       "   -85.97815760932173,\n",
       "   -129.26782263992212,\n",
       "   -183.6319509438655,\n",
       "   -232.35770937324358,\n",
       "   -271.4385702960516,\n",
       "   -272.5589320614467,\n",
       "   -276.78646389760223,\n",
       "   -267.137561174326,\n",
       "   -232.98488809862306,\n",
       "   -205.56548994675362,\n",
       "   -171.07050395714145,\n",
       "   -158.6616437506633,\n",
       "   -140.23800413537867,\n",
       "   -117.89598486499868,\n",
       "   -99.3452798843105,\n",
       "   -87.1423182780935,\n",
       "   -77.42106330990131,\n",
       "   -68.8141658360129,\n",
       "   -61.669850640334005,\n",
       "   -55.034293937363714,\n",
       "   -50.512893190051294,\n",
       "   -39.49577493989184,\n",
       "   -34.68576758236928,\n",
       "   -31.533553937006864,\n",
       "   -29.82768844190369,\n",
       "   -28.411421056481345,\n",
       "   -20.804311064667107,\n",
       "   -15.078000928053473,\n",
       "   -13.03836261060931,\n",
       "   -13.128799013027294,\n",
       "   -14.941984394414142,\n",
       "   -17.05635376987688,\n",
       "   -17.827527348467232,\n",
       "   -17.922302025016453,\n",
       "   -18.846007434499015,\n",
       "   -18.83687815863166,\n",
       "   -18.705762023548004,\n",
       "   -19.01548414441656,\n",
       "   -21.864824005321744,\n",
       "   -23.377211024039642,\n",
       "   -24.947559948022665,\n",
       "   -24.45555885989496,\n",
       "   -25.820452528825097,\n",
       "   -27.000412639881542,\n",
       "   -27.803017948642886,\n",
       "   -27.854931998665442,\n",
       "   -28.978903164549862,\n",
       "   -30.420581019707537,\n",
       "   -33.924978658129795,\n",
       "   -37.025983154093616,\n",
       "   -37.86365863121077,\n",
       "   -39.81800606487339,\n",
       "   -42.282960722080915,\n",
       "   -42.94089616579001,\n",
       "   -44.338175522333934,\n",
       "   -43.70200740801443,\n",
       "   -45.307660860820164,\n",
       "   -44.952687463964914,\n",
       "   -43.61897404422447,\n",
       "   -43.13353741918469,\n",
       "   -41.71456101726803,\n",
       "   -39.83347773617727,\n",
       "   -39.550976669797365,\n",
       "   -40.37595267421441,\n",
       "   -38.65882353189368,\n",
       "   -40.892052202538295,\n",
       "   -43.02022914869334,\n",
       "   -40.927406835683215,\n",
       "   -44.30505400769246,\n",
       "   -49.3384661191451,\n",
       "   -51.991326956372696,\n",
       "   -51.2350968983106,\n",
       "   -51.06419129629954,\n",
       "   -46.550407696922626,\n",
       "   -50.718189877443045,\n",
       "   -55.02824575614092,\n",
       "   -57.01304344457099,\n",
       "   -55.71268032049176,\n",
       "   -60.221688457480454,\n",
       "   -58.84256517223894,\n",
       "   -62.090640602723695,\n",
       "   -65.77529172962448,\n",
       "   -69.64015843502192,\n",
       "   -69.6629594677691,\n",
       "   -75.13143877547039,\n",
       "   -74.39949729881366,\n",
       "   -71.29069901360262,\n",
       "   -72.39628503002156,\n",
       "   -72.53884452227372,\n",
       "   -69.7327820401006,\n",
       "   -69.64116690004978,\n",
       "   -61.32537079201829,\n",
       "   -67.23450621765298,\n",
       "   -62.99104371082782,\n",
       "   -65.10106702291179,\n",
       "   -61.060490823299055,\n",
       "   -63.1876263717444,\n",
       "   -59.81674340237505,\n",
       "   -70.96492740717889,\n",
       "   -77.55956905385315,\n",
       "   -73.80096467509648,\n",
       "   -75.22325054829359,\n",
       "   -73.74032916576515,\n",
       "   -65.08346693405726,\n",
       "   -62.74961630572492,\n",
       "   -58.42691914855281,\n",
       "   -62.82510087367814,\n",
       "   -62.903761794328,\n",
       "   -60.574331531400446,\n",
       "   -53.74804385817085,\n",
       "   -51.018121388461715,\n",
       "   -50.06485892116801,\n",
       "   -50.43946827733734,\n",
       "   -49.52086532686644,\n",
       "   -45.13312116517754,\n",
       "   -43.346521496987954,\n",
       "   -46.018333190100584,\n",
       "   -42.7984621482757,\n",
       "   -44.855388496032425,\n",
       "   -46.73928764780356,\n",
       "   -47.06347036330501,\n",
       "   -49.466280146202664,\n",
       "   -51.40758990015032,\n",
       "   -54.35058529082711,\n",
       "   -54.025573090853875,\n",
       "   -55.34558136214679,\n",
       "   -54.260671230264236,\n",
       "   -54.63888274686952,\n",
       "   -58.22144389901975,\n",
       "   -58.49038520252677,\n",
       "   -57.77332624676289,\n",
       "   -59.17982835056803,\n",
       "   -59.043514337419545,\n",
       "   -60.979639857804365,\n",
       "   -63.09302361265638,\n",
       "   -64.41302602219999,\n",
       "   -63.743571520290985,\n",
       "   -64.10688733373924,\n",
       "   -68.38004395298162,\n",
       "   -71.07418799465339,\n",
       "   -81.66144436358091,\n",
       "   -91.56113069992114,\n",
       "   -105.32198992439095,\n",
       "   -109.09366657986206,\n",
       "   -120.24817032388522,\n",
       "   -160.89280531072174,\n",
       "   -177.1878263750243,\n",
       "   -184.2444365012286,\n",
       "   -188.1415384159143,\n",
       "   -178.77605400461644,\n",
       "   -176.01760497520283,\n",
       "   -175.43873786852237,\n",
       "   -168.61051837478882,\n",
       "   -174.6428091425715,\n",
       "   -183.39824836006693,\n",
       "   -177.68325429330682,\n",
       "   -169.60844679573634,\n",
       "   -161.1500997262945,\n",
       "   -158.80703028440766,\n",
       "   -146.293354552261,\n",
       "   -143.0408200866629,\n",
       "   -135.58472162820453,\n",
       "   -138.3556831664578,\n",
       "   -136.9877224993287,\n",
       "   -136.01823528380933,\n",
       "   -133.11161941229983,\n",
       "   -136.76467419307102,\n",
       "   -138.99804310147547,\n",
       "   -147.702389322554,\n",
       "   -147.7253829833403,\n",
       "   -153.27100474201748,\n",
       "   -143.21314813684535,\n",
       "   -145.34060539451698,\n",
       "   -142.82375212202476,\n",
       "   -141.2467796752426,\n",
       "   -132.5676747864234,\n",
       "   -125.36053699933846,\n",
       "   -115.91624271142197,\n",
       "   -104.29100523425167,\n",
       "   -99.39289353055192,\n",
       "   -92.04102785211938,\n",
       "   -89.31001395840687,\n",
       "   -82.98012064351579,\n",
       "   -75.67902872760872,\n",
       "   -68.37844667966453,\n",
       "   -69.31892720651274,\n",
       "   -69.5090963783531,\n",
       "   -70.16530631523091,\n",
       "   -66.94189748020466,\n",
       "   -64.21648914877068,\n",
       "   -60.86925660510651,\n",
       "   -56.101688181595215,\n",
       "   -53.454507366422185,\n",
       "   -53.44260195140231,\n",
       "   -50.86712718532647,\n",
       "   -53.71452449352374,\n",
       "   -53.3021545349726,\n",
       "   -52.29935468784754,\n",
       "   -50.56254599667382,\n",
       "   -47.43195752911467,\n",
       "   -47.32526289871875,\n",
       "   -49.717085750359075,\n",
       "   -50.71444866645612,\n",
       "   -53.39846020465461,\n",
       "   -54.532849648357306,\n",
       "   -57.97476561236178,\n",
       "   -53.651749279201155,\n",
       "   -52.03982943270905,\n",
       "   -48.77570172661109,\n",
       "   -51.43563188415731,\n",
       "   -54.324635315018796,\n",
       "   -56.605810790316475,\n",
       "   -58.018470138283035,\n",
       "   -63.92245094395112,\n",
       "   -61.84630760072685,\n",
       "   -59.08903048578197,\n",
       "   -55.63518538026888,\n",
       "   -53.34419263506186,\n",
       "   -51.44464307042651,\n",
       "   -50.42126717533454,\n",
       "   -50.28727968523786,\n",
       "   -47.686822089299284,\n",
       "   -45.500567908066266,\n",
       "   -45.579881308465396,\n",
       "   -46.19335314443786,\n",
       "   -46.85300967030759,\n",
       "   -45.877945475322065,\n",
       "   -46.793675672249655,\n",
       "   -48.23976277113485,\n",
       "   -49.13909757269074,\n",
       "   -49.687323768014046,\n",
       "   -47.48498648329431,\n",
       "   -51.2667249415853,\n",
       "   -52.10616480670161,\n",
       "   -55.759510145562004,\n",
       "   -60.02487865263025,\n",
       "   -65.41044240845928,\n",
       "   -72.92651658924252,\n",
       "   -71.70272423742021,\n",
       "   -69.25416967163366,\n",
       "   -68.23285072151424,\n",
       "   -69.32273220042438,\n",
       "   -66.97584666253303,\n",
       "   -66.30042521954668,\n",
       "   -63.98467262778853,\n",
       "   -64.92607783870515,\n",
       "   -64.17958012408981,\n",
       "   -61.5406964191339,\n",
       "   -57.87023678043098,\n",
       "   -55.01147418075118,\n",
       "   -55.38754667269466,\n",
       "   -51.61587042598633,\n",
       "   -47.58039985307909,\n",
       "   -46.77532210875468,\n",
       "   -43.30053844476708,\n",
       "   -41.18462553023649,\n",
       "   -41.32642648788075,\n",
       "   -38.82805290563246,\n",
       "   -36.56088413647673,\n",
       "   -35.40612192137324,\n",
       "   -35.93289766249918,\n",
       "   -36.246523326887825,\n",
       "   -36.43527158080732,\n",
       "   -35.48958659207812,\n",
       "   -35.017927118732835,\n",
       "   -36.68673166957952,\n",
       "   -36.68547267647741,\n",
       "   -39.5076699728525,\n",
       "   -41.20583176048943,\n",
       "   -42.720922839148315,\n",
       "   -44.505203499579544,\n",
       "   -46.463494679954884,\n",
       "   -46.289277118084804,\n",
       "   -46.98658512739249,\n",
       "   -47.88664216591362,\n",
       "   -46.0768495755912,\n",
       "   -46.438850302658174,\n",
       "   -48.117698310639824,\n",
       "   -53.395482263202204,\n",
       "   -57.56075544683096,\n",
       "   -56.09303447002231,\n",
       "   -59.382290653490294,\n",
       "   -63.447619919131355,\n",
       "   -67.21428836789323,\n",
       "   -71.3491725706629,\n",
       "   -73.31553427877192,\n",
       "   -78.18500370145149,\n",
       "   -82.24771158540754,\n",
       "   -81.93610216122141,\n",
       "   -85.83269157209959,\n",
       "   -85.047107133957,\n",
       "   -87.09717506236072,\n",
       "   -83.77036411977839,\n",
       "   -86.81866758436665,\n",
       "   -91.80459359097168,\n",
       "   -96.4068779918093,\n",
       "   -97.2792764227149,\n",
       "   -96.27070638538774,\n",
       "   -100.43555747795273,\n",
       "   -102.33265019496618,\n",
       "   -107.20573470376291,\n",
       "   -107.2217138786367,\n",
       "   -119.26680373062608,\n",
       "   -124.57797038296624,\n",
       "   -122.34532799675262,\n",
       "   -124.86912536711243,\n",
       "   -135.622684475544,\n",
       "   -131.28294876423928,\n",
       "   -129.61284056523527,\n",
       "   -136.01014730534123,\n",
       "   -148.5636382100874,\n",
       "   -152.74390149534645,\n",
       "   -162.89534558659614,\n",
       "   -173.02837070179592,\n",
       "   -175.5122455378477,\n",
       "   -184.65888321161742,\n",
       "   -196.05127684094077,\n",
       "   -203.47643513815763,\n",
       "   -195.72792673973652,\n",
       "   -187.89793390409577,\n",
       "   -178.92165435065158,\n",
       "   -174.01584495038767,\n",
       "   -177.4592448459083,\n",
       "   -187.20760843497945,\n",
       "   -191.93004689613798,\n",
       "   -195.93939368416517,\n",
       "   -198.0580243453827,\n",
       "   -217.13296973130608,\n",
       "   -226.3552211259994,\n",
       "   -237.76175342997118,\n",
       "   -240.65576086728825,\n",
       "   -234.0824993768747,\n",
       "   -226.34566770035303,\n",
       "   -226.7961584663609,\n",
       "   -236.80812681357295,\n",
       "   -230.05224866038282,\n",
       "   -240.67221225287378,\n",
       "   -255.38421624437206,\n",
       "   -243.01641841325286,\n",
       "   -248.76318051398232,\n",
       "   -247.25946441922025,\n",
       "   -235.648192787845,\n",
       "   -229.52543513976366,\n",
       "   -233.21036555146213,\n",
       "   -213.25098725415313,\n",
       "   -224.80078936981693,\n",
       "   -238.68598349311037,\n",
       "   -249.75003977197645,\n",
       "   -263.8731064558749,\n",
       "   -322.28813365617196,\n",
       "   -357.08495132374566,\n",
       "   -361.10539166297974,\n",
       "   -359.9181332176883,\n",
       "   -395.48707338782094,\n",
       "   -386.555543969737,\n",
       "   -467.9907768452626,\n",
       "   -521.7446219907142,\n",
       "   -566.8348622984716,\n",
       "   -651.0189294052202,\n",
       "   -711.7925499031697,\n",
       "   -798.8245321930823,\n",
       "   -803.2784391994633,\n",
       "   -867.5604489752793,\n",
       "   -799.3027181649148,\n",
       "   -663.3352901883321,\n",
       "   -595.0125735413746,\n",
       "   -514.1093882780191,\n",
       "   -421.8219438834312,\n",
       "   -371.4818102180474,\n",
       "   -359.8235381975579,\n",
       "   -304.3367688003986,\n",
       "   -291.55287008780107,\n",
       "   -260.7238009890106,\n",
       "   -249.51910368095847,\n",
       "   -253.13568885886485,\n",
       "   -257.70694466551697,\n",
       "   -257.269410390824,\n",
       "   -273.41817303271506,\n",
       "   -244.9814436863493,\n",
       "   -241.77030480441283,\n",
       "   -222.9497500442871,\n",
       "   -232.48797064423943,\n",
       "   -243.19825928343712,\n",
       "   -236.12661612974227,\n",
       "   -232.1361955684304,\n",
       "   -225.59413268390145,\n",
       "   -221.83734671138942,\n",
       "   -208.3412011104218,\n",
       "   -211.04355914849498,\n",
       "   -203.5747710400808,\n",
       "   -199.51633799249495,\n",
       "   -199.8470552131342,\n",
       "   -200.15635811152026,\n",
       "   -199.8027014757131,\n",
       "   -202.4549398750347,\n",
       "   -190.73028923227162,\n",
       "   -204.14328046644795,\n",
       "   -194.90732051933514,\n",
       "   -189.18151108807194,\n",
       "   -181.61002681888056,\n",
       "   -183.04992404016494,\n",
       "   -191.53165103509681,\n",
       "   -217.16193149104714,\n",
       "   -214.95691646711532,\n",
       "   -220.21248339887526,\n",
       "   -229.75521371851622,\n",
       "   -221.03611142678773,\n",
       "   -225.38131588925123,\n",
       "   -242.83191532533516,\n",
       "   -231.40848791972965,\n",
       "   -239.67341433531672,\n",
       "   -237.11968549628372,\n",
       "   -223.13742831147727,\n",
       "   -212.10648490727178,\n",
       "   -197.0193752182402,\n",
       "   -203.09266899934158,\n",
       "   -204.0824478861712,\n",
       "   -201.7448676775814,\n",
       "   -206.1109430178658,\n",
       "   -197.5534581626559,\n",
       "   -212.75115207239074,\n",
       "   -226.5174676242667,\n",
       "   -209.6471609070475,\n",
       "   -209.5999096214059,\n",
       "   -222.52840456934933,\n",
       "   -212.9761183485825,\n",
       "   -219.62660169391788,\n",
       "   -241.2496000239351,\n",
       "   -254.37156296928805,\n",
       "   -272.86992881190434,\n",
       "   -269.17500149883386,\n",
       "   -249.5793347419318,\n",
       "   -260.46380101580525,\n",
       "   -289.20123496454534,\n",
       "   -290.8030629520618,\n",
       "   -311.2320636427322,\n",
       "   -322.25368636022347,\n",
       "   -312.3219933531952,\n",
       "   -277.46191989882084,\n",
       "   -274.073340678874,\n",
       "   -231.60203055827031,\n",
       "   -212.09731627950032,\n",
       "   -218.33540676534264,\n",
       "   -212.09716442494494,\n",
       "   -203.6052206582513,\n",
       "   -214.7584058494908,\n",
       "   -216.7888704132621,\n",
       "   -206.74362625274514,\n",
       "   -192.6583127502478,\n",
       "   -196.06438391039552,\n",
       "   -204.0549134764096,\n",
       "   -199.3647156022364,\n",
       "   -213.24165853820128,\n",
       "   -210.04354580207104,\n",
       "   -229.39861110201406,\n",
       "   -223.4705559325692,\n",
       "   -230.77916553225148,\n",
       "   -238.08571525364735,\n",
       "   -236.81252622234138,\n",
       "   -258.79791677191827,\n",
       "   -253.68499713923046,\n",
       "   -254.72473519214114,\n",
       "   -248.3441526534903,\n",
       "   -235.6784656324165],\n",
       "  'jsc_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -1063004.5138774128,\n",
       "   -8091.948457898441,\n",
       "   -2523.6844029178874,\n",
       "   -1259.1670632372668,\n",
       "   -896.6731688928148,\n",
       "   -593.6101730081581,\n",
       "   -395.1184661338267,\n",
       "   -388.95964455083856,\n",
       "   -381.63489705265386,\n",
       "   -365.6097697708903,\n",
       "   -321.05809394444725,\n",
       "   -306.40098915805487,\n",
       "   -276.9888031322645,\n",
       "   -249.9211009926016,\n",
       "   -260.93020067862994,\n",
       "   -238.27347658386066,\n",
       "   -235.48991821186905,\n",
       "   -231.20640600435044,\n",
       "   -232.5519384948044,\n",
       "   -247.85156513424496,\n",
       "   -252.8672802640236,\n",
       "   -235.47004305810464,\n",
       "   -223.8373826556986,\n",
       "   -206.47786840300483,\n",
       "   -204.59754430073548,\n",
       "   -208.8064888278758,\n",
       "   -191.4638531977243,\n",
       "   -183.99684920355645,\n",
       "   -178.97788079055147,\n",
       "   -192.05164529171276,\n",
       "   -193.2431640501957,\n",
       "   -201.9149748821985,\n",
       "   -186.94749021652555,\n",
       "   -187.09794390911574,\n",
       "   -188.7945709093546,\n",
       "   -181.26638363366348,\n",
       "   -169.10038875343633,\n",
       "   -188.77294829304398,\n",
       "   -190.25078869045998,\n",
       "   -210.9657526399056,\n",
       "   -218.43362530975284,\n",
       "   -219.14494903607516,\n",
       "   -197.24684521418803,\n",
       "   -182.69780195792555,\n",
       "   -173.24505291374183,\n",
       "   -160.62191751805233,\n",
       "   -183.62133766556246,\n",
       "   -199.09661295711533,\n",
       "   -189.1936000411195,\n",
       "   -177.06912296667417,\n",
       "   -168.10078912640304,\n",
       "   -180.12931878489235,\n",
       "   -163.78080138725485,\n",
       "   -178.17111685048062,\n",
       "   -196.03927220222187,\n",
       "   -189.75954775220984,\n",
       "   -181.38454778460078,\n",
       "   -181.8930755059171,\n",
       "   -184.20518999949502,\n",
       "   -179.35640966455955,\n",
       "   -179.55236628371307,\n",
       "   -151.87469973479483,\n",
       "   -131.86023189112896,\n",
       "   -124.09990486827107,\n",
       "   -125.60902384608693,\n",
       "   -129.6868774865335,\n",
       "   -117.67109526764956,\n",
       "   -117.94950292565792,\n",
       "   -107.59651826163834,\n",
       "   -105.23216762981191,\n",
       "   -99.86871899679137,\n",
       "   -92.30639560023708,\n",
       "   -86.02922483491352,\n",
       "   -79.83773203827425,\n",
       "   -84.53533214585607,\n",
       "   -83.38550175703328,\n",
       "   -85.07307080919544,\n",
       "   -89.02229976944685,\n",
       "   -84.46520730195135,\n",
       "   -84.55906079417969,\n",
       "   -79.69685196869125,\n",
       "   -77.6349264995204,\n",
       "   -76.84570053276141,\n",
       "   -71.40350122704852,\n",
       "   -72.05213339103943,\n",
       "   -69.81060577236097,\n",
       "   -69.08442729437529,\n",
       "   -72.36527881188672,\n",
       "   -80.37059706374178,\n",
       "   -76.37232204735449,\n",
       "   -78.68036418483483,\n",
       "   -90.5964415497133,\n",
       "   -97.58745512233459,\n",
       "   -99.67796704066936,\n",
       "   -99.6879485062634,\n",
       "   -99.34305980373091,\n",
       "   -92.67554943792857,\n",
       "   -97.00275336243712,\n",
       "   -93.28113808767158,\n",
       "   -97.98244831975823,\n",
       "   -115.75225747872933,\n",
       "   -121.70534732990093,\n",
       "   -126.52008298407067,\n",
       "   -127.37515316534513,\n",
       "   -116.27920009176384,\n",
       "   -113.62307470500046,\n",
       "   -103.94188149296588,\n",
       "   -95.84607495823575,\n",
       "   -84.39894537367928,\n",
       "   -81.88755344670429,\n",
       "   -77.43860111935136,\n",
       "   -75.84331804707507,\n",
       "   -69.45487037554048,\n",
       "   -67.3181501448977,\n",
       "   -71.48243647620893,\n",
       "   -71.15838581188453,\n",
       "   -71.00351861689407,\n",
       "   -67.22536571928816,\n",
       "   -64.83763629411251,\n",
       "   -60.59212298519939,\n",
       "   -63.120105547182916,\n",
       "   -58.63525521134162,\n",
       "   -57.74146899534808,\n",
       "   -57.615483550378805,\n",
       "   -56.610426065791515,\n",
       "   -56.83933124979038,\n",
       "   -55.17577011574703,\n",
       "   -56.20927005826401,\n",
       "   -56.27645102669612,\n",
       "   -55.1942316083678,\n",
       "   -53.67166647602149,\n",
       "   -49.335261433922376,\n",
       "   -48.813084443008094,\n",
       "   -49.45794472427312,\n",
       "   -44.44588022348757,\n",
       "   -44.196415960554084,\n",
       "   -41.60830517776515,\n",
       "   -38.63323600341094,\n",
       "   -36.028739022331116,\n",
       "   -36.07456010773759,\n",
       "   -34.57090282959952,\n",
       "   -31.171036933778836,\n",
       "   -28.654259972190395,\n",
       "   -27.586272838751082,\n",
       "   -28.632069589766232,\n",
       "   -28.85431174106425,\n",
       "   -27.953070375005417,\n",
       "   -26.577025377569882,\n",
       "   -26.59256867841778,\n",
       "   -26.29660959302015,\n",
       "   -24.91868193705973,\n",
       "   -23.640389959471303,\n",
       "   -22.72684463842493,\n",
       "   -21.707436367988134,\n",
       "   -20.811000620868633,\n",
       "   -21.206025393055988,\n",
       "   -21.520419798120646,\n",
       "   -22.18103839233485,\n",
       "   -21.73940712358642,\n",
       "   -21.433694724460693,\n",
       "   -20.324251975076574,\n",
       "   -20.951047758077245,\n",
       "   -21.149485145671928,\n",
       "   -20.676104778002866,\n",
       "   -20.538051888860586,\n",
       "   -20.340102320365684,\n",
       "   -20.650590475780923,\n",
       "   -20.51137202671952,\n",
       "   -20.3872343327399,\n",
       "   -21.059209650271118,\n",
       "   -21.491825693545536,\n",
       "   -22.128779508156306,\n",
       "   -21.31854558201509,\n",
       "   -20.78523059790161,\n",
       "   -21.248564563361516,\n",
       "   -21.496117223222335,\n",
       "   -22.380601192621267,\n",
       "   -22.317672260270417,\n",
       "   -20.65669311758732,\n",
       "   -20.313953228427053,\n",
       "   -19.391210185418682,\n",
       "   -18.59661874044369,\n",
       "   -18.876831212515857,\n",
       "   -18.170300821770567,\n",
       "   -17.825206995113028,\n",
       "   -17.69537358178415,\n",
       "   -17.483891135705477,\n",
       "   -17.141424732880573,\n",
       "   -17.166751378400072,\n",
       "   -17.33919998954687,\n",
       "   -16.79567244059894,\n",
       "   -17.012922813748048,\n",
       "   -16.922714331421595,\n",
       "   -17.09620553528452,\n",
       "   -16.25708393121933,\n",
       "   -17.020444017521093,\n",
       "   -16.064594669369548,\n",
       "   -16.505102835149945,\n",
       "   -15.976763841382876,\n",
       "   -15.605435855849304,\n",
       "   -15.844454488006313,\n",
       "   -15.498951973013153,\n",
       "   -15.490163115943368,\n",
       "   -15.894063491809781,\n",
       "   -15.818311087653878,\n",
       "   -16.03403137594224,\n",
       "   -15.957203672558599,\n",
       "   -16.12772095496386,\n",
       "   -16.26532070501736,\n",
       "   -16.074205470759487,\n",
       "   -15.957958481875252,\n",
       "   -15.921722247930894,\n",
       "   -15.885976515468322,\n",
       "   -15.981751222256374,\n",
       "   -15.88308622492358,\n",
       "   -16.576241638353036,\n",
       "   -15.9434238731044,\n",
       "   -15.860059300400476,\n",
       "   -15.300192653895422,\n",
       "   -15.053080356572472,\n",
       "   -15.116891373468643,\n",
       "   -15.534586815758697,\n",
       "   -15.693477973085741,\n",
       "   -15.533255175900077,\n",
       "   -15.575754669299684,\n",
       "   -15.845883981334506,\n",
       "   -16.06475016160787,\n",
       "   -17.251679319232593,\n",
       "   -16.77777957106088,\n",
       "   -16.07874572073992,\n",
       "   -16.187542980074443,\n",
       "   -15.96421139944615,\n",
       "   -15.688255355748748,\n",
       "   -16.068484958982776,\n",
       "   -16.474120070344615,\n",
       "   -16.316877114738077,\n",
       "   -16.15526069571391,\n",
       "   -15.514927180020987,\n",
       "   -16.518643762986777,\n",
       "   -16.08129206974694,\n",
       "   -16.327887126078622,\n",
       "   -15.395402538843644,\n",
       "   -15.075792654586323,\n",
       "   -14.733329629815795,\n",
       "   -14.461724265506193,\n",
       "   -14.264168965915676,\n",
       "   -13.939019800508694,\n",
       "   -13.502900218156892,\n",
       "   -14.234578831594641,\n",
       "   -14.147491579465747,\n",
       "   -14.359928676087343,\n",
       "   -14.724754641705712,\n",
       "   -14.721721952143689,\n",
       "   -14.466035960393016,\n",
       "   -14.988036418666086,\n",
       "   -15.014057550183956,\n",
       "   -14.464034648283137,\n",
       "   -14.528674146500935,\n",
       "   -14.61647066963406,\n",
       "   -14.543637801648648,\n",
       "   -14.162577085292222,\n",
       "   -14.680040059134122,\n",
       "   -14.721175289626316,\n",
       "   -14.50522430397853,\n",
       "   -14.61294103193636,\n",
       "   -14.452126911655453,\n",
       "   -14.44121911749141,\n",
       "   -14.092292013801767,\n",
       "   -14.155625951019388,\n",
       "   -13.936322829306201,\n",
       "   -14.078825787767528,\n",
       "   -14.449521255745111,\n",
       "   -15.04889097863908,\n",
       "   -14.76963617504687,\n",
       "   -14.443265496448914,\n",
       "   -14.744628648369494,\n",
       "   -14.75785041822472,\n",
       "   -14.546365080646472,\n",
       "   -14.423284059367656,\n",
       "   -14.604077530788919,\n",
       "   -15.081516312338142,\n",
       "   -15.331913416963246,\n",
       "   -15.43149352983685,\n",
       "   -15.67799832134936,\n",
       "   -15.450612141234672,\n",
       "   -15.523022520885064,\n",
       "   -14.958686616355166,\n",
       "   -15.434163336672569,\n",
       "   -15.404716540214718,\n",
       "   -15.226925896460632,\n",
       "   -15.600105001431828,\n",
       "   -15.348198845372426,\n",
       "   -15.437927751414122,\n",
       "   -15.133801197410968,\n",
       "   -14.895875552790905,\n",
       "   -14.944947671695108,\n",
       "   -14.421681114898082,\n",
       "   -14.137113060778821,\n",
       "   -13.60165154293852,\n",
       "   -13.44964685456308,\n",
       "   -12.842678617163907,\n",
       "   -12.81315476341748,\n",
       "   -13.331784586803309,\n",
       "   -13.29981946032658,\n",
       "   -13.005265161014295,\n",
       "   -12.877257236262253,\n",
       "   -13.150820149238996,\n",
       "   -13.488505888028074,\n",
       "   -13.373067795071105,\n",
       "   -13.97445616723491,\n",
       "   -13.966279758635766,\n",
       "   -13.608284474754042,\n",
       "   -14.061262919048268,\n",
       "   -14.236422868282453,\n",
       "   -14.166904479863604,\n",
       "   -13.978927063314694,\n",
       "   -14.152454109144491,\n",
       "   -13.43338600132729,\n",
       "   -12.821240754383963,\n",
       "   -12.729766468611441,\n",
       "   -12.40521603821723,\n",
       "   -11.78788013557027,\n",
       "   -11.391274002768265,\n",
       "   -10.888170157493962,\n",
       "   -10.742074346599932,\n",
       "   -10.95139291051807,\n",
       "   -10.799041726352392,\n",
       "   -11.089418119779204,\n",
       "   -10.798630006484737,\n",
       "   -10.855971042182938,\n",
       "   -11.03533341966772,\n",
       "   -11.54641044251522,\n",
       "   -11.794103828416366,\n",
       "   -11.613874451955395,\n",
       "   -11.828511001879487,\n",
       "   -11.93680637696129,\n",
       "   -11.788018858472068,\n",
       "   -11.651976815700833,\n",
       "   -11.58748082937406,\n",
       "   -11.639291166842485,\n",
       "   -11.69962812555821,\n",
       "   -11.414041952787313,\n",
       "   -11.250726988850927,\n",
       "   -11.504582772543142,\n",
       "   -11.526326963531865,\n",
       "   -11.896517506922013,\n",
       "   -11.766582040182854,\n",
       "   -11.308189675771414,\n",
       "   -11.09546511331093,\n",
       "   -11.429078384797325,\n",
       "   -11.69462359441768,\n",
       "   -11.459614775328667,\n",
       "   -11.725183073713403,\n",
       "   -11.927686584451779,\n",
       "   -11.458326482250682,\n",
       "   -11.166947124727699,\n",
       "   -10.995157435287776,\n",
       "   -10.975520285667201,\n",
       "   -11.061541594699282,\n",
       "   -11.162996573003738,\n",
       "   -10.948548103353499,\n",
       "   -10.98701585343068,\n",
       "   -11.151486229333624,\n",
       "   -11.589895957578634,\n",
       "   -12.013755686109878,\n",
       "   -12.181984624972513,\n",
       "   -12.397470574841483,\n",
       "   -12.493077438433579,\n",
       "   -12.777003930296036,\n",
       "   -13.123060890343675,\n",
       "   -13.63484223224262,\n",
       "   -13.375296898202768,\n",
       "   -13.11088285955857,\n",
       "   -13.38019890467997,\n",
       "   -13.548050685088795,\n",
       "   -13.44174559334449,\n",
       "   -13.746897289415545,\n",
       "   -13.586467191758524,\n",
       "   -13.805017143668366,\n",
       "   -14.486070893217239,\n",
       "   -14.896758682457348,\n",
       "   -14.699055203528717,\n",
       "   -14.385905784862,\n",
       "   -14.651443959205768,\n",
       "   -15.632198807531424,\n",
       "   -16.23083281021178,\n",
       "   -16.757149455608715,\n",
       "   -16.690012303173692,\n",
       "   -16.49848552942365,\n",
       "   -16.679529582920473,\n",
       "   -16.94787038296492,\n",
       "   -18.016412936268058,\n",
       "   -17.86075230197567,\n",
       "   -18.370617569410584,\n",
       "   -18.179997669351202,\n",
       "   -18.233499211799995,\n",
       "   -18.2104101318277,\n",
       "   -18.15020958957987,\n",
       "   -17.612331084712327,\n",
       "   -16.712726988236707,\n",
       "   -16.753270620433526,\n",
       "   -16.971464053048198,\n",
       "   -16.575120288824504,\n",
       "   -16.204119245983097,\n",
       "   -16.468183663438552,\n",
       "   -16.490662020140682,\n",
       "   -16.811245395082505,\n",
       "   -17.356725445776945,\n",
       "   -17.774124151526518,\n",
       "   -18.047800913440277,\n",
       "   -17.737120756267025,\n",
       "   -17.438964195582983,\n",
       "   -17.104906165891038,\n",
       "   -16.72649764235805,\n",
       "   -16.714416498463947,\n",
       "   -16.86331115406753,\n",
       "   -17.055980199551737,\n",
       "   -16.93402062303382,\n",
       "   -16.71670620762181,\n",
       "   -16.465010834303417,\n",
       "   -16.520786055985813,\n",
       "   -16.961540884304203,\n",
       "   -16.53211574253189,\n",
       "   -16.19923315128468,\n",
       "   -16.555786067206324,\n",
       "   -16.628500271683453,\n",
       "   -16.239331078502847,\n",
       "   -15.759636713348613,\n",
       "   -15.432942088505797,\n",
       "   -15.226970628996646,\n",
       "   -15.164571629531526,\n",
       "   -15.200285465556075,\n",
       "   -15.205313803426364,\n",
       "   -15.304549578530622,\n",
       "   -14.882431788272859,\n",
       "   -15.477196522603666,\n",
       "   -15.495603771063351,\n",
       "   -16.020875028787138,\n",
       "   -16.2474762072416,\n",
       "   -16.145242192099047,\n",
       "   -16.27360659992669,\n",
       "   -16.582665731111277,\n",
       "   -16.86257868636472,\n",
       "   -16.929164656005725,\n",
       "   -17.22229564630966,\n",
       "   -17.5689962364876,\n",
       "   -17.874622889708988,\n",
       "   -17.56459295217592,\n",
       "   -17.54146640929863,\n",
       "   -17.289633964886185,\n",
       "   -17.536448268646197,\n",
       "   -17.69161805917638,\n",
       "   -17.60123671946852,\n",
       "   -17.216360572716617,\n",
       "   -17.53446454923655,\n",
       "   -17.368056293892337,\n",
       "   -17.165883786173286,\n",
       "   -16.905523871890935,\n",
       "   -16.266765298894114,\n",
       "   -15.99554110261063,\n",
       "   -16.0993941383189,\n",
       "   -16.464234654661006,\n",
       "   -16.28633550986765,\n",
       "   -16.671937847173552,\n",
       "   -17.02393149258247,\n",
       "   -17.759279573323937,\n",
       "   -18.043088756847506,\n",
       "   -18.292641134417106,\n",
       "   -18.31695819248154,\n",
       "   -18.560948044951118,\n",
       "   -18.841319949238244,\n",
       "   -19.39338640458756,\n",
       "   -19.33800482639573,\n",
       "   -19.385520448614177,\n",
       "   -20.01922075817238,\n",
       "   -19.432587080506508,\n",
       "   -19.830041924966373,\n",
       "   -19.788311674307412,\n",
       "   -21.271159528503166,\n",
       "   -21.424268390947994],\n",
       "  'ff_r2': [0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -455.24457887577967,\n",
       "   -118.26005515949572,\n",
       "   -52.761539715423844,\n",
       "   -27.55223235221797,\n",
       "   -18.981407421973795,\n",
       "   -14.754168673820102,\n",
       "   -11.294510866136852,\n",
       "   -10.308307985996052,\n",
       "   -9.510142296532639,\n",
       "   -8.516692347125295,\n",
       "   -7.938069966444191,\n",
       "   -7.201431390630994,\n",
       "   -6.459437134720041,\n",
       "   -6.074354582674795,\n",
       "   -5.921040731859847,\n",
       "   -5.794925933813514,\n",
       "   -5.732975664642514,\n",
       "   -5.917989644019088,\n",
       "   -6.102884387271708,\n",
       "   -6.3419033460562515,\n",
       "   -6.815426874959597,\n",
       "   -7.253818752458196,\n",
       "   -8.18176679457013,\n",
       "   -8.625662294822078,\n",
       "   -8.550235062593046,\n",
       "   -8.895586090992278,\n",
       "   -9.250092459815196,\n",
       "   -9.552028821250953,\n",
       "   -9.650404808624659,\n",
       "   -9.846323846442449,\n",
       "   -9.94301892095295,\n",
       "   -10.460421522248845,\n",
       "   -10.395779506303914,\n",
       "   -11.235602334597631,\n",
       "   -11.485389726790148,\n",
       "   -10.699184109553451,\n",
       "   -10.51396223005636,\n",
       "   -10.57670351647131,\n",
       "   -10.094051900699526,\n",
       "   -9.770343475104037,\n",
       "   -9.485249189888766,\n",
       "   -9.658575589093614,\n",
       "   -10.361024380431584,\n",
       "   -11.200920708289772,\n",
       "   -12.339777008461335,\n",
       "   -13.052991046052323,\n",
       "   -13.918908118167655,\n",
       "   -14.011065476011561,\n",
       "   -14.503530665220252,\n",
       "   -15.522394678027908,\n",
       "   -16.00105269608909,\n",
       "   -16.64143949936757,\n",
       "   -16.6796822197241,\n",
       "   -17.41370961865619,\n",
       "   -17.854112707170394,\n",
       "   -18.387782602621517,\n",
       "   -19.481277044792563,\n",
       "   -20.37370579371531,\n",
       "   -20.20761073083369,\n",
       "   -20.784956419766925,\n",
       "   -20.04067557237533,\n",
       "   -21.062432608486255,\n",
       "   -22.08940334852267,\n",
       "   -22.208045912372466,\n",
       "   -22.3943694702336,\n",
       "   -22.791457408109355,\n",
       "   -23.237713245771594,\n",
       "   -23.63382080860161,\n",
       "   -24.824655189750256,\n",
       "   -25.126474112646555,\n",
       "   -26.257483132337022,\n",
       "   -27.50880268136892,\n",
       "   -29.084764675004084,\n",
       "   -27.632366749101944,\n",
       "   -25.952613859602586,\n",
       "   -26.259464136673458,\n",
       "   -24.88845308886431,\n",
       "   -24.3379201844862,\n",
       "   -24.472272155389973,\n",
       "   -23.927650910862553,\n",
       "   -25.130501703333014,\n",
       "   -25.92298745563159,\n",
       "   -25.485110104101633,\n",
       "   -25.38702226469504,\n",
       "   -27.341138583358198,\n",
       "   -27.68758491721319,\n",
       "   -26.539526038181076,\n",
       "   -25.45432466687406,\n",
       "   -25.1817713045223,\n",
       "   -24.730062853376744,\n",
       "   -23.153243493839078,\n",
       "   -23.550328562447152,\n",
       "   -22.59541827541508,\n",
       "   -23.92291661863449,\n",
       "   -24.537742058999257,\n",
       "   -25.74153565656423,\n",
       "   -25.32346940394706,\n",
       "   -27.71957514994278,\n",
       "   -28.949563264218433,\n",
       "   -29.324706780782748,\n",
       "   -30.70235088575163,\n",
       "   -32.17481675290414,\n",
       "   -33.41376917195666,\n",
       "   -34.784741024961825,\n",
       "   -35.69170435668123,\n",
       "   -35.9914376566258,\n",
       "   -38.511237919299035,\n",
       "   -37.95992660561602,\n",
       "   -36.831321699747086,\n",
       "   -35.45545761071447,\n",
       "   -34.63218846493659,\n",
       "   -34.40905535523827,\n",
       "   -32.51635239290764,\n",
       "   -31.96308782944866,\n",
       "   -30.720988574409912,\n",
       "   -30.777249050989564,\n",
       "   -31.326540249705488,\n",
       "   -30.12794293494624,\n",
       "   -30.614102619296446,\n",
       "   -29.53917646311655,\n",
       "   -32.57191596184289,\n",
       "   -33.42574673645434,\n",
       "   -33.72436956351232,\n",
       "   -34.78126918290214,\n",
       "   -35.193257392147586,\n",
       "   -34.91659578865353,\n",
       "   -34.5641195954526,\n",
       "   -34.71327737415565,\n",
       "   -34.65146243545412,\n",
       "   -33.79893416600927,\n",
       "   -33.90319475675264,\n",
       "   -32.69444188197668,\n",
       "   -32.08895151606819,\n",
       "   -31.99425713844726,\n",
       "   -32.33726517245793,\n",
       "   -31.83486304052355,\n",
       "   -31.242640167346465,\n",
       "   -30.724941824688262,\n",
       "   -29.919123115054802,\n",
       "   -30.177204501417922,\n",
       "   -28.509081392913156,\n",
       "   -28.269707916020682,\n",
       "   -28.895945022029995,\n",
       "   -28.527797852022417,\n",
       "   -27.458734281871614,\n",
       "   -27.0767324322065,\n",
       "   -27.476113842559037,\n",
       "   -26.07973648029057,\n",
       "   -26.13378994662242,\n",
       "   -26.339792185619807,\n",
       "   -25.401295750709277,\n",
       "   -24.44116774864227,\n",
       "   -23.98784583492497,\n",
       "   -23.895966625501856,\n",
       "   -24.10921734931926,\n",
       "   -23.53272054331543,\n",
       "   -24.212430216701907,\n",
       "   -25.391784262604467,\n",
       "   -25.675280777750274,\n",
       "   -25.987140973857475,\n",
       "   -26.578377826489824,\n",
       "   -26.2387878945874,\n",
       "   -25.544149891530314,\n",
       "   -26.43674743402072,\n",
       "   -25.405674259632626,\n",
       "   -25.554590045614223,\n",
       "   -25.355100529666508,\n",
       "   -26.275276526492803,\n",
       "   -26.23937901940985,\n",
       "   -27.12771542957443,\n",
       "   -26.202948079303564,\n",
       "   -27.57960821074341,\n",
       "   -28.84633027109034,\n",
       "   -28.940425759648463,\n",
       "   -28.76358979911351,\n",
       "   -28.732348469273372,\n",
       "   -29.408314719184087,\n",
       "   -28.901336894127564,\n",
       "   -28.881164844638985,\n",
       "   -28.459358046678734,\n",
       "   -27.915315368940092,\n",
       "   -26.822907250529525,\n",
       "   -26.012360366745803,\n",
       "   -25.873177695032386,\n",
       "   -25.912038728405037,\n",
       "   -25.86399058511888,\n",
       "   -24.46317186257562,\n",
       "   -25.932459419632938,\n",
       "   -26.361064313982432,\n",
       "   -26.003317195602026,\n",
       "   -26.447552805515574,\n",
       "   -25.832196795965853,\n",
       "   -25.48312340950488,\n",
       "   -25.90312710041582,\n",
       "   -27.164269873715988,\n",
       "   -27.848949263246563,\n",
       "   -29.571591737382004,\n",
       "   -29.415569189246614,\n",
       "   -29.8495018662873,\n",
       "   -30.7133236632714,\n",
       "   -30.734419496596413,\n",
       "   -31.507655642156884,\n",
       "   -30.55967879941671,\n",
       "   -29.91896783763681,\n",
       "   -29.263603283896064,\n",
       "   -29.352486702382862,\n",
       "   -29.446500259676878,\n",
       "   -29.235513118113555,\n",
       "   -28.168753156571302,\n",
       "   -27.891717558473935,\n",
       "   -26.357570847243892,\n",
       "   -24.854649623370047,\n",
       "   -23.72746162082865,\n",
       "   -23.38615432543824,\n",
       "   -22.557870237823433,\n",
       "   -22.005435927472494,\n",
       "   -21.380076901835626,\n",
       "   -20.75770922705498,\n",
       "   -21.221640525041124,\n",
       "   -20.956644671004785,\n",
       "   -21.69709711382517,\n",
       "   -21.39025335496604,\n",
       "   -21.005046724725485,\n",
       "   -21.829227786496613,\n",
       "   -21.366892673347124,\n",
       "   -22.213593212643694,\n",
       "   -22.382085032068705,\n",
       "   -22.12718965178785,\n",
       "   -22.26416265851378,\n",
       "   -22.102784142149346,\n",
       "   -21.92301790630303,\n",
       "   -21.905823153490978,\n",
       "   -21.561054734339585,\n",
       "   -21.402376969613663,\n",
       "   -21.479923361444303,\n",
       "   -21.573249219576976,\n",
       "   -21.38710694567727,\n",
       "   -21.780099276798502,\n",
       "   -21.713890249741816,\n",
       "   -21.741702829992573,\n",
       "   -22.460298970121286,\n",
       "   -23.395713030437943,\n",
       "   -23.154641209266135,\n",
       "   -22.926154174771387,\n",
       "   -23.296409382061217,\n",
       "   -22.281299787374287,\n",
       "   -21.80750800299977,\n",
       "   -22.238718651093368,\n",
       "   -23.1307901175776,\n",
       "   -23.34057500142002,\n",
       "   -23.441833400286065,\n",
       "   -23.321128482027596,\n",
       "   -23.192949320410786,\n",
       "   -22.984452657623567,\n",
       "   -23.70864086051811,\n",
       "   -22.903355910758272,\n",
       "   -23.31007501039733,\n",
       "   -23.27339691405554,\n",
       "   -22.1284967716792,\n",
       "   -21.709061882516608,\n",
       "   -20.976544831197394,\n",
       "   -20.989927314878635,\n",
       "   -20.531484672578962,\n",
       "   -20.752149393951257,\n",
       "   -20.210420771423298,\n",
       "   -19.708936681474814,\n",
       "   -19.75156267772794,\n",
       "   -20.063558006740305,\n",
       "   -20.098606237623095,\n",
       "   -20.37536672421651,\n",
       "   -19.89145859721422,\n",
       "   -19.935489523105044,\n",
       "   -20.228493217245692,\n",
       "   -20.28816705048787,\n",
       "   -20.34078210617373,\n",
       "   -20.5068868360718,\n",
       "   -20.55396366195639,\n",
       "   -19.84803521398772,\n",
       "   -19.82953596239158,\n",
       "   -19.810467042432624,\n",
       "   -18.79813866552032,\n",
       "   -18.58433121329137,\n",
       "   -18.244167076460126,\n",
       "   -18.135897218475733,\n",
       "   -17.534645444146275,\n",
       "   -18.137189798478065,\n",
       "   -18.116190512812164,\n",
       "   -18.00404441858321,\n",
       "   -18.586861665858507,\n",
       "   -18.561976808632004,\n",
       "   -17.854676896922783,\n",
       "   -16.857564681512525,\n",
       "   -16.973120237639925,\n",
       "   -17.052829804371054,\n",
       "   -16.771045144699023,\n",
       "   -16.863700102320728,\n",
       "   -16.516900072494504,\n",
       "   -16.339942204558408,\n",
       "   -16.40324804828878,\n",
       "   -16.443030311453445,\n",
       "   -16.72145138219186,\n",
       "   -16.63040816029047,\n",
       "   -16.840452037354513,\n",
       "   -17.063974234745853,\n",
       "   -16.880540922789017,\n",
       "   -16.831521788761133,\n",
       "   -16.97067035437866,\n",
       "   -17.18621364116581,\n",
       "   -18.002037658834713,\n",
       "   -17.92945022879577,\n",
       "   -18.033213246373325,\n",
       "   -18.523204098391194,\n",
       "   -18.756660530693512,\n",
       "   -18.575906526247017,\n",
       "   -19.121967096212956,\n",
       "   -18.72538004585158,\n",
       "   -18.102403109545055,\n",
       "   -19.151080426488132,\n",
       "   -19.311441018639126,\n",
       "   -20.395759707642515,\n",
       "   -20.022819621355847,\n",
       "   -19.75621035016942,\n",
       "   -20.250383895188893,\n",
       "   -19.929077404576287,\n",
       "   -19.230494647255128,\n",
       "   -19.111957031500474,\n",
       "   -19.2023696117603,\n",
       "   -20.117004864572454,\n",
       "   -20.698012187014747,\n",
       "   -20.49658448506294,\n",
       "   -20.320593740717907,\n",
       "   -20.449919920816033,\n",
       "   -20.121357719863543,\n",
       "   -20.617035787488565,\n",
       "   -20.77582680497727,\n",
       "   -20.46749416623785,\n",
       "   -20.548052847832675,\n",
       "   -20.622659965946866,\n",
       "   -20.190429374959802,\n",
       "   -20.01013689843625,\n",
       "   -19.88300729336927,\n",
       "   -19.800295143677957,\n",
       "   -19.17706422422257,\n",
       "   -18.923641466631544,\n",
       "   -18.884580785990803,\n",
       "   -18.570994305945383,\n",
       "   -18.84238500483508,\n",
       "   -18.478266531921953,\n",
       "   -18.22070498567718,\n",
       "   -17.840546441080377,\n",
       "   -17.15844748700862,\n",
       "   -16.73157881369332,\n",
       "   -16.284024975570834,\n",
       "   -15.838800391112994,\n",
       "   -15.044132988000424,\n",
       "   -15.119838697290962,\n",
       "   -15.085293034607645,\n",
       "   -15.087360819827982,\n",
       "   -14.967993328881583,\n",
       "   -15.083964978464643,\n",
       "   -14.983003110098705,\n",
       "   -15.123304182990552,\n",
       "   -14.640660334836275,\n",
       "   -14.516983407626688,\n",
       "   -15.052456403703232,\n",
       "   -15.155945232391801,\n",
       "   -16.224887741618055,\n",
       "   -16.37968799184249,\n",
       "   -16.907450892696193,\n",
       "   -17.351411446226987,\n",
       "   -17.943679262631182,\n",
       "   -17.956094052162804,\n",
       "   -17.74481383206354,\n",
       "   -17.403267854183404,\n",
       "   -17.84099492925728,\n",
       "   -17.79149990474691,\n",
       "   -17.89175920149151,\n",
       "   -17.50176688198268,\n",
       "   -17.64248866824565,\n",
       "   -17.489424341991946,\n",
       "   -17.553014681898606,\n",
       "   -17.877005738938127,\n",
       "   -18.143402803585822,\n",
       "   -18.667173072220457,\n",
       "   -19.077170542668192,\n",
       "   -18.89677477109448,\n",
       "   -18.8075993660084,\n",
       "   -18.548016494094604,\n",
       "   -18.25615345701185,\n",
       "   -17.613987764152856,\n",
       "   -17.396879043435796,\n",
       "   -17.87175524918768,\n",
       "   -17.48237720599282,\n",
       "   -18.402614261355396,\n",
       "   -18.36465050681533,\n",
       "   -18.202693076489922,\n",
       "   -18.23007415376475,\n",
       "   -18.03773389484553,\n",
       "   -18.255856931228315,\n",
       "   -18.177338560268915,\n",
       "   -17.81834822242089,\n",
       "   -17.33452256941279,\n",
       "   -17.07978698229964,\n",
       "   -16.817766015020283,\n",
       "   -16.746857453117734,\n",
       "   -16.547688160306226,\n",
       "   -16.206774955577842,\n",
       "   -16.148602132939903,\n",
       "   -15.986128805735042,\n",
       "   -15.63122169001996,\n",
       "   -15.712948014954438,\n",
       "   -15.789660811305499,\n",
       "   -15.728394372702173,\n",
       "   -15.386028380503113,\n",
       "   -15.427845038857772,\n",
       "   -15.70062246999657,\n",
       "   -16.096049605707528,\n",
       "   -16.15732666549217,\n",
       "   -16.110967189606328,\n",
       "   -16.10221782098323,\n",
       "   -16.104543777598863,\n",
       "   -15.984907684461529,\n",
       "   -15.808446849630897,\n",
       "   -15.874383489062339,\n",
       "   -16.09573420877776,\n",
       "   -16.576774420412736,\n",
       "   -17.1583723777064,\n",
       "   -17.777076244504748,\n",
       "   -17.939601000806544,\n",
       "   -17.97987986672697,\n",
       "   -18.040808387084017,\n",
       "   -19.071417779516565,\n",
       "   -19.609868257350456,\n",
       "   -19.425867250969944,\n",
       "   -19.26228902204263,\n",
       "   -19.32783800649632,\n",
       "   -19.216481667524494,\n",
       "   -19.340836798821144,\n",
       "   -19.687513292725054,\n",
       "   -20.366959082371164,\n",
       "   -20.159411156389204,\n",
       "   -19.836180761670665,\n",
       "   -19.64977326379972,\n",
       "   -20.11485227419418,\n",
       "   -19.5140798426162,\n",
       "   -19.252344951612187,\n",
       "   -18.722152546005745,\n",
       "   -18.710810478739017,\n",
       "   -18.57874014286262,\n",
       "   -18.30442338795442,\n",
       "   -17.499799144948913,\n",
       "   -17.20688264725513,\n",
       "   -17.610680307757676,\n",
       "   -17.73707609325312,\n",
       "   -17.792823906804877,\n",
       "   -17.792639795110478,\n",
       "   -17.31048188969208,\n",
       "   -17.319083107428717,\n",
       "   -17.717568370769804,\n",
       "   -17.346836218485233,\n",
       "   -17.17320389724128,\n",
       "   -17.386835703642316,\n",
       "   -17.425390426075907,\n",
       "   -17.074126659084936,\n",
       "   -17.439672027038046,\n",
       "   -17.851110187132456,\n",
       "   -18.028709075656543,\n",
       "   -17.89250265250379,\n",
       "   -17.31722754330387,\n",
       "   -17.28106868715701,\n",
       "   -17.273521561638805,\n",
       "   -16.706996725910997,\n",
       "   -16.798748222548905,\n",
       "   -17.09802734319903,\n",
       "   -16.74759820197683,\n",
       "   -16.73438842811117,\n",
       "   -16.932678594257112,\n",
       "   -16.62541823413742,\n",
       "   -16.64232467362875,\n",
       "   -16.75787432957425,\n",
       "   -16.904049732290837,\n",
       "   -16.51379193067448,\n",
       "   -15.79395243450871,\n",
       "   -15.783855615140535,\n",
       "   -15.365475708592928,\n",
       "   -14.97774961082997,\n",
       "   -14.388081886573344,\n",
       "   -13.96794257536069,\n",
       "   -13.726990807178103],\n",
       "  'test_r2s': [-2.1237821299410347,\n",
       "   -0.6501478104296095,\n",
       "   -0.23914513015862982,\n",
       "   -0.06462948877492436,\n",
       "   0.01737317122665938,\n",
       "   0.04379679500966627,\n",
       "   0.000663670493137869,\n",
       "   -0.1130418938379556,\n",
       "   -0.2677639192705601,\n",
       "   -0.4679328371946916,\n",
       "   -0.6328003133394884,\n",
       "   -1648.4300597138495,\n",
       "   -321.3326261459364,\n",
       "   -214.93917159561747,\n",
       "   -164.56885279381706,\n",
       "   -147.93464235474391,\n",
       "   -134.02924499273732,\n",
       "   -122.26265785181234,\n",
       "   -122.32581674821256,\n",
       "   -114.27519931042468,\n",
       "   -1063112.1035443416,\n",
       "   -8199.22260947164,\n",
       "   -2626.582155577546,\n",
       "   -1362.583907922648,\n",
       "   -995.99714242575,\n",
       "   -692.6200416530008,\n",
       "   -492.33499688422,\n",
       "   -478.5659069943562,\n",
       "   -465.48290339851354,\n",
       "   -443.9363692472969,\n",
       "   -391.90168188413895,\n",
       "   -367.15945608811995,\n",
       "   -339.6096094656443,\n",
       "   -324.09874254946027,\n",
       "   -340.24847294914605,\n",
       "   -334.5575091926966,\n",
       "   -351.02559257355296,\n",
       "   -391.9083792934427,\n",
       "   -448.91975933284976,\n",
       "   -513.6815750909434,\n",
       "   -558.512851439088,\n",
       "   -542.5370064069177,\n",
       "   -535.8662556775815,\n",
       "   -509.11825075099875,\n",
       "   -473.7288899408757,\n",
       "   -451.12642658101964,\n",
       "   -398.20511558444906,\n",
       "   -377.36730311505,\n",
       "   -354.13538355967745,\n",
       "   -343.928132759511,\n",
       "   -325.98207274775314,\n",
       "   -321.4861452887605,\n",
       "   -296.4182526903748,\n",
       "   -288.41068684817384,\n",
       "   -283.77271247942224,\n",
       "   -270.46455472777654,\n",
       "   -254.0330345512916,\n",
       "   -263.47633234775043,\n",
       "   -259.7580975206884,\n",
       "   -277.54911681719085,\n",
       "   -283.3121033170661,\n",
       "   -282.6531972621384,\n",
       "   -253.34495160479833,\n",
       "   -233.1243232614534,\n",
       "   -222.54225152237512,\n",
       "   -210.24375637135714,\n",
       "   -234.60607495335142,\n",
       "   -252.86329408638835,\n",
       "   -245.07429363713618,\n",
       "   -233.04215533765128,\n",
       "   -225.2169517251629,\n",
       "   -236.79431658865258,\n",
       "   -221.07864027668424,\n",
       "   -237.03878593351868,\n",
       "   -257.6865269980047,\n",
       "   -252.777096503763,\n",
       "   -245.61138622399227,\n",
       "   -246.1250554198201,\n",
       "   -250.06134112720522,\n",
       "   -247.05618278402756,\n",
       "   -248.2378445834933,\n",
       "   -221.45278189362298,\n",
       "   -203.70101675244803,\n",
       "   -198.86185767167416,\n",
       "   -201.92075919336378,\n",
       "   -207.83624649442186,\n",
       "   -197.13731803598512,\n",
       "   -198.56150369630836,\n",
       "   -189.93801357955573,\n",
       "   -188.51332950678835,\n",
       "   -184.24551547753433,\n",
       "   -177.07614842041752,\n",
       "   -173.04800574533044,\n",
       "   -165.58394426047752,\n",
       "   -168.8074324072749,\n",
       "   -169.74434804603027,\n",
       "   -169.78170037492112,\n",
       "   -170.76526917731246,\n",
       "   -164.8048878420567,\n",
       "   -164.93073694106133,\n",
       "   -157.64012414792865,\n",
       "   -156.0633186081011,\n",
       "   -157.76709989344505,\n",
       "   -149.2981671540353,\n",
       "   -154.75048262067156,\n",
       "   -158.4091155048086,\n",
       "   -161.3865830574105,\n",
       "   -163.60354497380382,\n",
       "   -173.90878334928533,\n",
       "   -166.74822785184352,\n",
       "   -174.02681857679366,\n",
       "   -191.61724834222545,\n",
       "   -201.669228483586,\n",
       "   -203.55125681589934,\n",
       "   -209.84613018237832,\n",
       "   -209.06253338500335,\n",
       "   -205.55767183553286,\n",
       "   -216.17380081164748,\n",
       "   -216.2801227561847,\n",
       "   -220.23813364315112,\n",
       "   -242.85177149180691,\n",
       "   -248.0054419875541,\n",
       "   -249.5283065231654,\n",
       "   -249.66674823980975,\n",
       "   -237.81643186287428,\n",
       "   -230.9082137751454,\n",
       "   -221.49095553326237,\n",
       "   -206.33993190767146,\n",
       "   -199.3634912539197,\n",
       "   -193.33664800704645,\n",
       "   -190.1308174773575,\n",
       "   -186.65833086655016,\n",
       "   -183.85305178678493,\n",
       "   -178.28432118280574,\n",
       "   -193.92215481129787,\n",
       "   -200.78857556535922,\n",
       "   -196.26321414707638,\n",
       "   -193.50282185155086,\n",
       "   -190.14012839536792,\n",
       "   -176.82333668504054,\n",
       "   -176.7020458286266,\n",
       "   -167.88403766792925,\n",
       "   -169.86039960464132,\n",
       "   -169.21555958054344,\n",
       "   -165.89198437129244,\n",
       "   -159.3046340274018,\n",
       "   -154.0300409362812,\n",
       "   -154.43856945584758,\n",
       "   -154.67273273398186,\n",
       "   -151.6363601067807,\n",
       "   -146.2077452271964,\n",
       "   -139.3791947583004,\n",
       "   -141.14319627941367,\n",
       "   -139.10350969697632,\n",
       "   -135.94579854146207,\n",
       "   -136.1439336492051,\n",
       "   -133.6825431711376,\n",
       "   -133.04665461258952,\n",
       "   -131.41539546910877,\n",
       "   -134.4396158623915,\n",
       "   -133.39790034901822,\n",
       "   -131.23999567986377,\n",
       "   -126.95475624123898,\n",
       "   -125.50050091421035,\n",
       "   -130.8957500839922,\n",
       "   -131.564232376798,\n",
       "   -129.82590562770727,\n",
       "   -130.5822834325586,\n",
       "   -130.93879879433007,\n",
       "   -133.19206147501635,\n",
       "   -134.00558150203494,\n",
       "   -134.3744146579859,\n",
       "   -132.5971745427057,\n",
       "   -131.55983491227215,\n",
       "   -135.64167552291386,\n",
       "   -137.6225582859448,\n",
       "   -148.99948540113363,\n",
       "   -158.98442359317696,\n",
       "   -172.92101751746299,\n",
       "   -176.16520813681876,\n",
       "   -186.35678760018797,\n",
       "   -226.32879392948627,\n",
       "   -242.85571043115945,\n",
       "   -250.70163697257365,\n",
       "   -255.28272025509247,\n",
       "   -245.341758216045,\n",
       "   -243.05027133737,\n",
       "   -242.28842672596852,\n",
       "   -235.23995865644886,\n",
       "   -242.30385786857076,\n",
       "   -251.06492334807453,\n",
       "   -245.9366648906271,\n",
       "   -236.01410370322435,\n",
       "   -226.97030967261279,\n",
       "   -225.09827678429397,\n",
       "   -212.73760122822313,\n",
       "   -210.40312095333195,\n",
       "   -201.83586084175482,\n",
       "   -204.25288901719074,\n",
       "   -202.8922667239789,\n",
       "   -200.07128766252967,\n",
       "   -197.66347529625816,\n",
       "   -200.90245594722666,\n",
       "   -202.10726983072178,\n",
       "   -211.10134997693913,\n",
       "   -212.2906287752296,\n",
       "   -217.8089748057161,\n",
       "   -209.87516076497678,\n",
       "   -212.34516940204273,\n",
       "   -211.44362784561795,\n",
       "   -209.5012494831537,\n",
       "   -201.47137590198875,\n",
       "   -194.24306774890334,\n",
       "   -183.33772675757677,\n",
       "   -169.33959284048035,\n",
       "   -164.19955984466802,\n",
       "   -156.08549107901314,\n",
       "   -153.61546452821906,\n",
       "   -146.80175726674196,\n",
       "   -138.52155649414834,\n",
       "   -131.42061511352554,\n",
       "   -130.1867491880269,\n",
       "   -128.76980379440167,\n",
       "   -128.55419319246823,\n",
       "   -124.71511004951094,\n",
       "   -122.28989132025615,\n",
       "   -117.9370896009064,\n",
       "   -112.83035651411727,\n",
       "   -109.22875663274607,\n",
       "   -110.95891431063882,\n",
       "   -107.44915627333853,\n",
       "   -111.87868442710756,\n",
       "   -110.42737940988206,\n",
       "   -109.37119807386159,\n",
       "   -107.05139087242976,\n",
       "   -103.38645672713506,\n",
       "   -104.1890939689981,\n",
       "   -106.1652360155565,\n",
       "   -106.60287630347219,\n",
       "   -109.77238486686909,\n",
       "   -111.49967856378042,\n",
       "   -115.27989942449216,\n",
       "   -112.53644110867484,\n",
       "   -111.50073154586204,\n",
       "   -108.63395382308417,\n",
       "   -112.80441822023992,\n",
       "   -115.86605649857455,\n",
       "   -118.14085759006431,\n",
       "   -118.58242104457275,\n",
       "   -122.87376033966962,\n",
       "   -120.3300468082088,\n",
       "   -117.34392577239065,\n",
       "   -112.7477359510099,\n",
       "   -110.42870901532149,\n",
       "   -108.53564891168331,\n",
       "   -107.76430078559203,\n",
       "   -107.06855980083344,\n",
       "   -102.58261458266024,\n",
       "   -101.61586242160584,\n",
       "   -102.35717207081105,\n",
       "   -102.62441426138024,\n",
       "   -101.6546865634588,\n",
       "   -100.74248832287557,\n",
       "   -102.07928939726695,\n",
       "   -103.1448202179185,\n",
       "   -104.47502266181229,\n",
       "   -103.08942758781237,\n",
       "   -100.82176491424926,\n",
       "   -105.90156262033824,\n",
       "   -105.42257032276585,\n",
       "   -108.67161127495643,\n",
       "   -112.74275003473531,\n",
       "   -118.42684982315657,\n",
       "   -125.45547015894994,\n",
       "   -125.07679193682009,\n",
       "   -123.15740843182202,\n",
       "   -122.92256772620304,\n",
       "   -125.68199239159898,\n",
       "   -125.33136825988957,\n",
       "   -125.01695206575425,\n",
       "   -123.41743340563272,\n",
       "   -124.66910018301449,\n",
       "   -123.76461340022281,\n",
       "   -121.43554364194486,\n",
       "   -118.4145207601295,\n",
       "   -116.00350176816697,\n",
       "   -116.4568911370697,\n",
       "   -112.4615307079264,\n",
       "   -107.70875001865784,\n",
       "   -106.36293532600092,\n",
       "   -103.65580241768939,\n",
       "   -100.73148951039963,\n",
       "   -101.4118710785032,\n",
       "   -99.84156776983887,\n",
       "   -96.64357270748849,\n",
       "   -97.00473424750163,\n",
       "   -101.80890759249776,\n",
       "   -101.27851022422709,\n",
       "   -101.10554458950479,\n",
       "   -102.64552417040723,\n",
       "   -102.50988919425421,\n",
       "   -103.89980363026535,\n",
       "   -103.59031808833826,\n",
       "   -108.13785746941795,\n",
       "   -108.82362208487451,\n",
       "   -110.55139201799847,\n",
       "   -113.06431296943697,\n",
       "   -117.77376878552286,\n",
       "   -116.9449437895358,\n",
       "   -116.74468982498726,\n",
       "   -118.97292652979472,\n",
       "   -113.73572809417946,\n",
       "   -111.89704389524941,\n",
       "   -116.35105114722057,\n",
       "   -121.21891428940039,\n",
       "   -124.79992848638463,\n",
       "   -124.62871163381025,\n",
       "   -128.03550157815917,\n",
       "   -130.93467176003006,\n",
       "   -131.69322697879275,\n",
       "   -133.46819773525658,\n",
       "   -136.8112609205894,\n",
       "   -144.14742835931162,\n",
       "   -148.64053744828172,\n",
       "   -146.62551654713778,\n",
       "   -152.52128287439874,\n",
       "   -151.74251175575168,\n",
       "   -152.62332651568244,\n",
       "   -147.77513193271503,\n",
       "   -151.20090575957354,\n",
       "   -157.61453596294163,\n",
       "   -160.2121081430846,\n",
       "   -161.01393860511942,\n",
       "   -158.7614419153442,\n",
       "   -163.30809916400347,\n",
       "   -164.55723411364698,\n",
       "   -170.75712066766192,\n",
       "   -169.31314269965563,\n",
       "   -180.48434339570963,\n",
       "   -187.92706223010768,\n",
       "   -185.84047961766203,\n",
       "   -188.46443869534377,\n",
       "   -200.25465487061444,\n",
       "   -197.35133382252332,\n",
       "   -196.01758732008506,\n",
       "   -203.52818099579875,\n",
       "   -214.27126837940156,\n",
       "   -219.36067820854007,\n",
       "   -228.8038113508869,\n",
       "   -239.30636502935135,\n",
       "   -241.28219027043326,\n",
       "   -252.27953561001146,\n",
       "   -264.1142439007787,\n",
       "   -270.47052796767423,\n",
       "   -261.96292098919986,\n",
       "   -255.83351142796081,\n",
       "   -246.40473235841552,\n",
       "   -242.52388739255684,\n",
       "   -246.16040577110368,\n",
       "   -255.03906650501142,\n",
       "   -259.3797840972963,\n",
       "   -259.9878773560661,\n",
       "   -262.26028982535667,\n",
       "   -279.1548248922463,\n",
       "   -287.21384278008924,\n",
       "   -296.3344186207258,\n",
       "   -298.16648954561174,\n",
       "   -289.1575212001768,\n",
       "   -278.60558294045615,\n",
       "   -279.5513584832179,\n",
       "   -290.4456834695395,\n",
       "   -282.2872640203488,\n",
       "   -293.8307709358886,\n",
       "   -309.0777834252027,\n",
       "   -296.1610142592966,\n",
       "   -301.77812108598556,\n",
       "   -298.54267542697715,\n",
       "   -287.65145273324424,\n",
       "   -279.4234407775518,\n",
       "   -283.8707924969288,\n",
       "   -265.7751588936699,\n",
       "   -278.74082407379774,\n",
       "   -293.4382390761404,\n",
       "   -304.8502535545366,\n",
       "   -321.5087848082944,\n",
       "   -380.22103775046406,\n",
       "   -415.240644190139,\n",
       "   -418.9565367491309,\n",
       "   -419.26609131960095,\n",
       "   -454.7003227867025,\n",
       "   -445.3663668417099,\n",
       "   -526.3765519878351,\n",
       "   -578.7917291254067,\n",
       "   -625.8262162706525,\n",
       "   -712.1786579957744,\n",
       "   -774.1214788846451,\n",
       "   -862.0116065967082,\n",
       "   -867.2895103924554,\n",
       "   -935.7544186152197,\n",
       "   -865.5559321229786,\n",
       "   -730.2084311719443,\n",
       "   -663.5656290938342,\n",
       "   -582.7223570656561,\n",
       "   -490.25245566874247,\n",
       "   -441.91639358655675,\n",
       "   -431.1740286420004,\n",
       "   -373.1467047495997,\n",
       "   -360.8927100178626,\n",
       "   -328.9053861888431,\n",
       "   -316.6028925139338,\n",
       "   -319.074940690578,\n",
       "   -324.7528535978141,\n",
       "   -323.6511620027753,\n",
       "   -340.25070270102526,\n",
       "   -311.79265961718784,\n",
       "   -306.47269679919026,\n",
       "   -287.23900052833164,\n",
       "   -294.4918864127143,\n",
       "   -304.9321542353283,\n",
       "   -296.9496563562038,\n",
       "   -292.00344100573847,\n",
       "   -286.0149219347793,\n",
       "   -280.9791107284829,\n",
       "   -265.68915160995607,\n",
       "   -267.3441043961342,\n",
       "   -259.00184279782815,\n",
       "   -256.42039248340626,\n",
       "   -258.96476559067736,\n",
       "   -260.124827094056,\n",
       "   -261.2754590605764,\n",
       "   -264.582806525101,\n",
       "   -253.38545266531645,\n",
       "   -266.6766958984047,\n",
       "   -259.518627528393,\n",
       "   -253.65030643825503,\n",
       "   -245.34266552687848,\n",
       "   -248.20908643618984,\n",
       "   -256.21577349091115,\n",
       "   -282.83280227796007,\n",
       "   -282.2748551495909,\n",
       "   -286.4135129908569,\n",
       "   -297.45727171644364,\n",
       "   -289.96611569863353,\n",
       "   -292.0836580018587,\n",
       "   -310.73941313436734,\n",
       "   -298.66122106382664,\n",
       "   -307.57828410453715,\n",
       "   -303.47822792727504,\n",
       "   -287.73075880167545,\n",
       "   -276.1391155859199,\n",
       "   -263.7997321613868,\n",
       "   -271.3274546157638,\n",
       "   -272.2876370138507,\n",
       "   -271.5239250749809,\n",
       "   -277.9022670789441,\n",
       "   -268.6193528046204,\n",
       "   -283.8789342749408,\n",
       "   -297.44307997654175,\n",
       "   -281.3350712844274,\n",
       "   -283.3279340181053,\n",
       "   -296.35385851381557,\n",
       "   -285.7381961405581,\n",
       "   -291.6139709502946,\n",
       "   -315.1390661581536,\n",
       "   -329.8580635055602,\n",
       "   -347.911277272794,\n",
       "   -345.0229499744963,\n",
       "   -324.0167750262991,\n",
       "   -335.5902547995606,\n",
       "   -364.28730208989145,\n",
       "   -363.25174698630167,\n",
       "   -383.12549567758515,\n",
       "   -394.6971583091027,\n",
       "   -384.2450215808063,\n",
       "   -348.6230114603954,\n",
       "   -346.2557794804886,\n",
       "   -303.5079741967042,\n",
       "   -283.33059440272706,\n",
       "   -289.72595603511917,\n",
       "   -279.32551314679364,\n",
       "   -268.87221146961343,\n",
       "   -279.1755091158624,\n",
       "   -280.1592013610039,\n",
       "   -270.1188380900193,\n",
       "   -257.74765567599155,\n",
       "   -263.0589503800116,\n",
       "   -271.2853749939179,\n",
       "   -266.66523272764306,\n",
       "   -280.23898617140475,\n",
       "   -275.22958683384627,\n",
       "   -292.31651871922156,\n",
       "   -285.5298767383869,\n",
       "   -291.557297060977,\n",
       "   -297.2996935599929,\n",
       "   -295.8809352203789,\n",
       "   -317.0870410230639,\n",
       "   -310.97621248718207,\n",
       "   -310.92395706445654,\n",
       "   -305.48443383576017,\n",
       "   -293.54482620861876],\n",
       "  'train_pce_loss': [0.6565540432929993,\n",
       "   0.6143036484718323,\n",
       "   0.48741045594215393,\n",
       "   0.539883553981781,\n",
       "   0.4874548017978668,\n",
       "   0.4666203558444977,\n",
       "   0.49134668707847595,\n",
       "   0.46360811591148376,\n",
       "   0.42477691173553467,\n",
       "   0.4429374635219574,\n",
       "   0.3983035981655121,\n",
       "   0.4035813808441162,\n",
       "   0.3507363200187683,\n",
       "   0.3717101216316223,\n",
       "   0.36925628781318665,\n",
       "   0.4009959101676941,\n",
       "   0.37376633286476135,\n",
       "   0.3645991384983063,\n",
       "   0.34545132517814636,\n",
       "   0.3644440770149231,\n",
       "   0.3889624774456024,\n",
       "   0.3346457779407501,\n",
       "   0.33777087926864624,\n",
       "   0.3880006968975067,\n",
       "   0.3703552782535553,\n",
       "   0.3745463490486145,\n",
       "   0.31891459226608276,\n",
       "   0.3293968439102173,\n",
       "   0.3451025187969208,\n",
       "   0.28963685035705566,\n",
       "   0.3263576924800873,\n",
       "   0.36674222350120544,\n",
       "   0.32568982243537903,\n",
       "   0.3562811315059662,\n",
       "   0.3251723349094391,\n",
       "   0.3922084867954254,\n",
       "   0.3301298916339874,\n",
       "   0.3464187681674957,\n",
       "   0.29582086205482483,\n",
       "   0.3736921548843384,\n",
       "   0.37262624502182007,\n",
       "   0.32865414023399353,\n",
       "   0.356086403131485,\n",
       "   0.3009314239025116,\n",
       "   0.3481960892677307,\n",
       "   0.3308825194835663,\n",
       "   0.33178818225860596,\n",
       "   0.31062033772468567,\n",
       "   0.3123549818992615,\n",
       "   0.3182671070098877,\n",
       "   0.30796515941619873,\n",
       "   0.31560301780700684,\n",
       "   0.31648582220077515,\n",
       "   0.31228819489479065,\n",
       "   0.29560044407844543,\n",
       "   0.320257306098938,\n",
       "   0.3389560282230377,\n",
       "   0.32635030150413513,\n",
       "   0.3362310826778412,\n",
       "   0.2921485900878906,\n",
       "   0.333528608083725,\n",
       "   0.30957385897636414,\n",
       "   0.3021245002746582,\n",
       "   0.333597868680954,\n",
       "   0.31117454171180725,\n",
       "   0.2903776466846466,\n",
       "   0.3140825927257538,\n",
       "   0.324104368686676,\n",
       "   0.30310797691345215,\n",
       "   0.3099600672721863,\n",
       "   0.3012218177318573,\n",
       "   0.28021320700645447,\n",
       "   0.29037848114967346,\n",
       "   0.26972946524620056,\n",
       "   0.30338796973228455,\n",
       "   0.3300984799861908,\n",
       "   0.2992430627346039,\n",
       "   0.3146747946739197,\n",
       "   0.32340478897094727,\n",
       "   0.2853935956954956,\n",
       "   0.3037036657333374,\n",
       "   0.3142722547054291,\n",
       "   0.2733082175254822,\n",
       "   0.27927902340888977,\n",
       "   0.28060171008110046,\n",
       "   0.282876193523407,\n",
       "   0.29030466079711914,\n",
       "   0.32041412591934204,\n",
       "   0.27235138416290283,\n",
       "   0.27833330631256104,\n",
       "   0.28898724913597107,\n",
       "   0.30920982360839844,\n",
       "   0.29565727710723877,\n",
       "   0.284515380859375,\n",
       "   0.27113500237464905,\n",
       "   0.26158106327056885,\n",
       "   0.29494044184684753,\n",
       "   0.27496927976608276,\n",
       "   0.32398495078086853,\n",
       "   0.2719489336013794,\n",
       "   0.2692874073982239,\n",
       "   0.28405219316482544,\n",
       "   0.2728838622570038,\n",
       "   0.28753724694252014,\n",
       "   0.2969857156276703,\n",
       "   0.2340300977230072,\n",
       "   0.292509526014328,\n",
       "   0.3057788908481598,\n",
       "   0.2718384563922882,\n",
       "   0.3027923107147217,\n",
       "   0.25552842020988464,\n",
       "   0.2709289789199829,\n",
       "   0.30374816060066223,\n",
       "   0.2867397367954254,\n",
       "   0.28119340538978577,\n",
       "   0.25565311312675476,\n",
       "   0.27155426144599915,\n",
       "   0.27306851744651794,\n",
       "   0.287963330745697,\n",
       "   0.25561001896858215,\n",
       "   0.2842046022415161,\n",
       "   0.2488732486963272,\n",
       "   0.26717013120651245,\n",
       "   0.26106926798820496,\n",
       "   0.2702675461769104,\n",
       "   0.26125141978263855,\n",
       "   0.3015361726284027,\n",
       "   0.257759153842926,\n",
       "   0.24329636991024017,\n",
       "   0.24367906153202057,\n",
       "   0.2784217894077301,\n",
       "   0.2423650026321411,\n",
       "   0.23438233137130737,\n",
       "   0.26514121890068054,\n",
       "   0.27990853786468506,\n",
       "   0.24311265349388123,\n",
       "   0.2763681411743164,\n",
       "   0.263737291097641,\n",
       "   0.25383561849594116,\n",
       "   0.2484210729598999,\n",
       "   0.29212409257888794,\n",
       "   0.2142135351896286,\n",
       "   0.21857161819934845,\n",
       "   0.23965884745121002,\n",
       "   0.2485685646533966,\n",
       "   0.2395719438791275,\n",
       "   0.24043166637420654,\n",
       "   0.24200944602489471,\n",
       "   0.25711384415626526,\n",
       "   0.27233415842056274,\n",
       "   0.21891753375530243,\n",
       "   0.23856490850448608,\n",
       "   0.25016194581985474,\n",
       "   0.2447882890701294,\n",
       "   0.24204087257385254,\n",
       "   0.24708446860313416,\n",
       "   0.22163374722003937,\n",
       "   0.22826656699180603,\n",
       "   0.24183805286884308,\n",
       "   0.21162165701389313,\n",
       "   0.2426801174879074,\n",
       "   0.23975376784801483,\n",
       "   0.2483246624469757,\n",
       "   0.25906893610954285,\n",
       "   0.24668146669864655,\n",
       "   0.21022319793701172,\n",
       "   0.24833904206752777,\n",
       "   0.23775310814380646,\n",
       "   0.24017740786075592,\n",
       "   0.24251702427864075,\n",
       "   0.24561189115047455,\n",
       "   0.24024474620819092,\n",
       "   0.22747235000133514,\n",
       "   0.21976684033870697,\n",
       "   0.228465735912323,\n",
       "   0.2437584400177002,\n",
       "   0.22810031473636627,\n",
       "   0.23429825901985168,\n",
       "   0.21059608459472656,\n",
       "   0.20719021558761597,\n",
       "   0.22538155317306519,\n",
       "   0.23292097449302673,\n",
       "   0.22731031477451324,\n",
       "   0.22252698242664337,\n",
       "   0.23853826522827148,\n",
       "   0.22445903718471527,\n",
       "   0.2348712682723999,\n",
       "   0.22149546444416046,\n",
       "   0.2352231740951538,\n",
       "   0.23345980048179626,\n",
       "   0.21243813633918762,\n",
       "   0.20876121520996094,\n",
       "   0.2291005402803421,\n",
       "   0.21406428515911102,\n",
       "   0.2067660689353943,\n",
       "   0.2285957783460617,\n",
       "   0.21426701545715332,\n",
       "   0.2301836907863617,\n",
       "   0.21893973648548126,\n",
       "   0.21676036715507507,\n",
       "   0.20323848724365234,\n",
       "   0.21122056245803833,\n",
       "   0.21128711104393005,\n",
       "   0.1982080340385437,\n",
       "   0.2239925116300583,\n",
       "   0.22534283995628357,\n",
       "   0.20926955342292786,\n",
       "   0.20702329277992249,\n",
       "   0.22157153487205505,\n",
       "   0.2137739211320877,\n",
       "   0.19575220346450806,\n",
       "   0.21510501205921173,\n",
       "   0.21479834616184235,\n",
       "   0.1969839483499527,\n",
       "   0.21637311577796936,\n",
       "   0.23247309029102325,\n",
       "   0.1857796013355255,\n",
       "   0.2273462861776352,\n",
       "   0.23325420916080475,\n",
       "   0.20162074267864227,\n",
       "   0.20895598828792572,\n",
       "   0.21380653977394104,\n",
       "   0.21618233621120453,\n",
       "   0.20151777565479279,\n",
       "   0.20703516900539398,\n",
       "   0.20798127353191376,\n",
       "   0.20055922865867615,\n",
       "   0.20615214109420776,\n",
       "   0.2068987935781479,\n",
       "   0.20369082689285278,\n",
       "   0.21820849180221558,\n",
       "   0.21668337285518646,\n",
       "   0.21520084142684937,\n",
       "   0.20004114508628845,\n",
       "   0.18114665150642395,\n",
       "   0.20567066967487335,\n",
       "   0.1944824606180191,\n",
       "   0.18274907767772675,\n",
       "   0.20624805986881256,\n",
       "   0.18658329546451569,\n",
       "   0.19912394881248474,\n",
       "   0.2198558896780014,\n",
       "   0.2045493721961975,\n",
       "   0.20141752064228058,\n",
       "   0.19826601445674896,\n",
       "   0.19382940232753754,\n",
       "   0.19408655166625977,\n",
       "   0.2124500572681427,\n",
       "   0.19880138337612152,\n",
       "   0.18661923706531525,\n",
       "   0.1971096694469452,\n",
       "   0.18179917335510254,\n",
       "   0.17673207819461823,\n",
       "   0.20605763792991638,\n",
       "   0.17184951901435852,\n",
       "   0.21184797585010529,\n",
       "   0.18163470923900604,\n",
       "   0.20038653910160065,\n",
       "   0.1684579998254776,\n",
       "   0.19841453433036804,\n",
       "   0.18167616426944733,\n",
       "   0.18263188004493713,\n",
       "   0.21039022505283356,\n",
       "   0.2101454734802246,\n",
       "   0.199880450963974,\n",
       "   0.19054998457431793,\n",
       "   0.17646652460098267,\n",
       "   0.16846001148223877,\n",
       "   0.17596299946308136,\n",
       "   0.1696893870830536,\n",
       "   0.1946820318698883,\n",
       "   0.17956632375717163,\n",
       "   0.19381730258464813,\n",
       "   0.19307522475719452,\n",
       "   0.18791814148426056,\n",
       "   0.1755448281764984,\n",
       "   0.16668778657913208,\n",
       "   0.17634008824825287,\n",
       "   0.19428321719169617,\n",
       "   0.17243899405002594,\n",
       "   0.17386965453624725,\n",
       "   0.17068693041801453,\n",
       "   0.19848254323005676,\n",
       "   0.18422071635723114,\n",
       "   0.18610860407352448,\n",
       "   0.17378635704517365,\n",
       "   0.1834307610988617,\n",
       "   0.18415413796901703,\n",
       "   0.15531741082668304,\n",
       "   0.1873345673084259,\n",
       "   0.17059215903282166,\n",
       "   0.1498756855726242,\n",
       "   0.184874027967453,\n",
       "   0.18694253265857697,\n",
       "   0.18968471884727478,\n",
       "   0.1770343780517578,\n",
       "   0.1747473031282425,\n",
       "   0.19096265733242035,\n",
       "   0.16556569933891296,\n",
       "   0.1744931936264038,\n",
       "   0.17999213933944702,\n",
       "   0.15910565853118896,\n",
       "   0.17993181943893433,\n",
       "   0.1988532841205597,\n",
       "   0.16183000802993774,\n",
       "   0.17126859724521637,\n",
       "   0.19055426120758057,\n",
       "   0.17881976068019867,\n",
       "   0.18186086416244507,\n",
       "   0.18582305312156677,\n",
       "   0.16966257989406586,\n",
       "   0.16133703291416168,\n",
       "   0.1860942393541336,\n",
       "   0.17622493207454681,\n",
       "   0.17899800837039948,\n",
       "   0.17069534957408905,\n",
       "   0.15854156017303467,\n",
       "   0.17413276433944702,\n",
       "   0.18235917389392853,\n",
       "   0.16347180306911469,\n",
       "   0.17209887504577637,\n",
       "   0.17798128724098206,\n",
       "   0.1585550606250763,\n",
       "   0.1568908840417862,\n",
       "   0.18718016147613525,\n",
       "   0.16242678463459015,\n",
       "   0.17482389509677887,\n",
       "   0.15595369040966034,\n",
       "   0.1576075404882431,\n",
       "   0.16740325093269348,\n",
       "   0.18876439332962036,\n",
       "   0.17966584861278534,\n",
       "   0.16515643894672394,\n",
       "   0.1635816991329193,\n",
       "   0.1890619546175003,\n",
       "   0.17717388272285461,\n",
       "   0.15958747267723083,\n",
       "   0.14904290437698364,\n",
       "   0.16898927092552185,\n",
       "   0.18095165491104126,\n",
       "   0.15584848821163177,\n",
       "   0.15744003653526306,\n",
       "   0.16177421808242798,\n",
       "   0.18342795968055725,\n",
       "   0.1582770198583603,\n",
       "   0.17173618078231812,\n",
       "   0.16802695393562317,\n",
       "   0.15734076499938965,\n",
       "   0.15902552008628845,\n",
       "   0.1651977300643921,\n",
       "   0.16007599234580994,\n",
       "   0.16818584501743317,\n",
       "   0.16592983901500702,\n",
       "   0.15727263689041138,\n",
       "   0.14844325184822083,\n",
       "   0.1689496636390686,\n",
       "   0.17255711555480957,\n",
       "   0.1637551188468933,\n",
       "   0.14999765157699585,\n",
       "   0.14658494293689728,\n",
       "   0.16087403893470764,\n",
       "   0.17961564660072327,\n",
       "   0.17273260653018951,\n",
       "   0.17536374926567078,\n",
       "   0.1675070822238922,\n",
       "   0.1675785928964615,\n",
       "   0.16921348869800568,\n",
       "   0.15915510058403015,\n",
       "   0.15035958588123322,\n",
       "   0.1493101269006729,\n",
       "   0.163762629032135,\n",
       "   0.17053276300430298,\n",
       "   0.15590815246105194,\n",
       "   0.17183706164360046,\n",
       "   0.15283049643039703,\n",
       "   0.15502318739891052,\n",
       "   0.15813471376895905,\n",
       "   0.15447579324245453,\n",
       "   0.1566757708787918,\n",
       "   0.16832268238067627,\n",
       "   0.16352123022079468,\n",
       "   0.15819022059440613,\n",
       "   0.14415135979652405,\n",
       "   0.13864848017692566,\n",
       "   0.16656123101711273,\n",
       "   0.17947623133659363,\n",
       "   0.14963489770889282,\n",
       "   0.16062650084495544,\n",
       "   0.15109820663928986,\n",
       "   0.1482287496328354,\n",
       "   0.15786969661712646,\n",
       "   0.1488024741411209,\n",
       "   0.16538605093955994,\n",
       "   0.14892041683197021,\n",
       "   0.1528211385011673,\n",
       "   0.15244866907596588,\n",
       "   0.159527987241745,\n",
       "   0.16148172318935394,\n",
       "   0.15511643886566162,\n",
       "   0.14678505063056946,\n",
       "   0.16160371899604797,\n",
       "   0.160699263215065,\n",
       "   0.16363461315631866,\n",
       "   0.15720264613628387,\n",
       "   0.153115913271904,\n",
       "   0.18642018735408783,\n",
       "   0.15658405423164368,\n",
       "   0.165329709649086,\n",
       "   0.15591926872730255,\n",
       "   0.1560439020395279,\n",
       "   0.15604013204574585,\n",
       "   0.15326562523841858,\n",
       "   0.1411481499671936,\n",
       "   0.14559990167617798,\n",
       "   0.15528737008571625,\n",
       "   0.13771957159042358,\n",
       "   0.14018546044826508,\n",
       "   0.14600788056850433,\n",
       "   0.14965885877609253,\n",
       "   0.1693824976682663,\n",
       "   0.1459101289510727,\n",
       "   0.1558360457420349,\n",
       "   0.14605595171451569,\n",
       "   0.163119375705719,\n",
       "   0.15070274472236633,\n",
       "   0.17137642204761505,\n",
       "   0.1506851613521576,\n",
       "   0.1405804306268692,\n",
       "   0.14995045959949493,\n",
       "   0.1568012237548828,\n",
       "   0.15413179993629456,\n",
       "   0.15559498965740204,\n",
       "   0.14650531113147736,\n",
       "   0.1561196744441986,\n",
       "   0.14692406356334686,\n",
       "   0.1551586389541626,\n",
       "   0.1403571516275406,\n",
       "   0.14143073558807373,\n",
       "   0.13266818225383759,\n",
       "   0.12866656482219696,\n",
       "   0.12321237474679947,\n",
       "   0.15095314383506775,\n",
       "   0.149751216173172,\n",
       "   0.13309574127197266,\n",
       "   0.1493455320596695,\n",
       "   0.16042481362819672,\n",
       "   0.13386750221252441,\n",
       "   0.1436815708875656,\n",
       "   0.16369932889938354,\n",
       "   0.146552175283432,\n",
       "   0.12978947162628174,\n",
       "   0.1406758427619934,\n",
       "   0.1393408626317978,\n",
       "   0.15442001819610596,\n",
       "   0.14967618882656097,\n",
       "   0.14329442381858826,\n",
       "   0.14340610802173615,\n",
       "   0.13258646428585052,\n",
       "   0.13586914539337158,\n",
       "   0.13326705992221832,\n",
       "   0.13359634578227997,\n",
       "   0.13205201923847198,\n",
       "   0.13840380311012268,\n",
       "   0.1522543579339981,\n",
       "   0.14555120468139648,\n",
       "   0.15073160827159882,\n",
       "   0.12847892940044403,\n",
       "   0.1488349437713623,\n",
       "   0.1455894112586975,\n",
       "   0.14720658957958221,\n",
       "   0.1431792676448822,\n",
       "   0.15288878977298737,\n",
       "   0.14248763024806976,\n",
       "   0.1282397359609604,\n",
       "   0.14458370208740234,\n",
       "   0.13582201302051544,\n",
       "   0.1262092888355255,\n",
       "   0.13290522992610931,\n",
       "   0.1397855430841446,\n",
       "   0.13966557383537292,\n",
       "   0.13954336941242218,\n",
       "   0.14070408046245575,\n",
       "   0.15499478578567505,\n",
       "   0.1380261480808258,\n",
       "   0.12966707348823547,\n",
       "   0.14147083461284637,\n",
       "   0.13799132406711578,\n",
       "   0.131888747215271,\n",
       "   0.13484510779380798,\n",
       "   0.14972639083862305,\n",
       "   0.1420828253030777,\n",
       "   0.14585493505001068,\n",
       "   0.1306891143321991,\n",
       "   0.1356515884399414,\n",
       "   0.135051891207695,\n",
       "   0.13592883944511414,\n",
       "   0.14951591193675995,\n",
       "   0.14063282310962677,\n",
       "   0.14825528860092163,\n",
       "   0.14638766646385193],\n",
       "  'train_voc_loss': [0.5020980834960938,\n",
       "   0.591620147228241,\n",
       "   0.5009838342666626,\n",
       "   0.4726666808128357,\n",
       "   0.48168474435806274,\n",
       "   0.4565575420856476,\n",
       "   0.5356860160827637,\n",
       "   0.4837203323841095,\n",
       "   0.5004178285598755,\n",
       "   0.43218451738357544,\n",
       "   0.46807727217674255,\n",
       "   0.46432948112487793,\n",
       "   0.44559869170188904,\n",
       "   0.48973050713539124,\n",
       "   0.4521598219871521,\n",
       "   0.45908239483833313,\n",
       "   0.4664580523967743,\n",
       "   0.5108609199523926,\n",
       "   0.4359521269798279,\n",
       "   0.46569663286209106,\n",
       "   0.4596099853515625,\n",
       "   0.4351699650287628,\n",
       "   0.4441957175731659,\n",
       "   0.39283010363578796,\n",
       "   0.43126827478408813,\n",
       "   0.47117504477500916,\n",
       "   0.452081561088562,\n",
       "   0.4217982888221741,\n",
       "   0.40663817524909973,\n",
       "   0.4698096513748169,\n",
       "   0.4454842805862427,\n",
       "   0.46144306659698486,\n",
       "   0.4461924135684967,\n",
       "   0.42700451612472534,\n",
       "   0.4162215292453766,\n",
       "   0.41350242495536804,\n",
       "   0.41393226385116577,\n",
       "   0.44115591049194336,\n",
       "   0.42707714438438416,\n",
       "   0.4234231412410736,\n",
       "   0.42175158858299255,\n",
       "   0.3724331855773926,\n",
       "   0.38439467549324036,\n",
       "   0.38785693049430847,\n",
       "   0.3906041085720062,\n",
       "   0.38416793942451477,\n",
       "   0.3939666152000427,\n",
       "   0.37672609090805054,\n",
       "   0.3814238905906677,\n",
       "   0.3757206201553345,\n",
       "   0.41512030363082886,\n",
       "   0.39528006315231323,\n",
       "   0.38562649488449097,\n",
       "   0.39544954895973206,\n",
       "   0.3988732099533081,\n",
       "   0.3747590184211731,\n",
       "   0.3960455358028412,\n",
       "   0.3459336757659912,\n",
       "   0.37221306562423706,\n",
       "   0.3844737708568573,\n",
       "   0.3681226372718811,\n",
       "   0.3471706807613373,\n",
       "   0.3912321627140045,\n",
       "   0.35914528369903564,\n",
       "   0.3851015269756317,\n",
       "   0.3511373698711395,\n",
       "   0.3687818646430969,\n",
       "   0.35801827907562256,\n",
       "   0.34763631224632263,\n",
       "   0.34124818444252014,\n",
       "   0.33882418274879456,\n",
       "   0.3943376839160919,\n",
       "   0.39821621775627136,\n",
       "   0.35863783955574036,\n",
       "   0.34442952275276184,\n",
       "   0.3626132011413574,\n",
       "   0.40354984998703003,\n",
       "   0.35390225052833557,\n",
       "   0.33962273597717285,\n",
       "   0.3626146912574768,\n",
       "   0.3349388837814331,\n",
       "   0.353731632232666,\n",
       "   0.36094164848327637,\n",
       "   0.34510475397109985,\n",
       "   0.3467343747615814,\n",
       "   0.362466424703598,\n",
       "   0.3383741080760956,\n",
       "   0.34581518173217773,\n",
       "   0.39051851630210876,\n",
       "   0.33748823404312134,\n",
       "   0.32097718119621277,\n",
       "   0.34612834453582764,\n",
       "   0.31678110361099243,\n",
       "   0.34487685561180115,\n",
       "   0.3033735454082489,\n",
       "   0.33376696705818176,\n",
       "   0.3583182990550995,\n",
       "   0.31628304719924927,\n",
       "   0.3327083885669708,\n",
       "   0.3379039168357849,\n",
       "   0.3476145565509796,\n",
       "   0.32084551453590393,\n",
       "   0.3411867022514343,\n",
       "   0.33016011118888855,\n",
       "   0.34100914001464844,\n",
       "   0.3268972337245941,\n",
       "   0.3359670639038086,\n",
       "   0.3265552222728729,\n",
       "   0.34193143248558044,\n",
       "   0.3255503177642822,\n",
       "   0.34015369415283203,\n",
       "   0.348358154296875,\n",
       "   0.3242192268371582,\n",
       "   0.31866344809532166,\n",
       "   0.3391249179840088,\n",
       "   0.31504687666893005,\n",
       "   0.3169100880622864,\n",
       "   0.32708895206451416,\n",
       "   0.3600935935974121,\n",
       "   0.34048762917518616,\n",
       "   0.3468782305717468,\n",
       "   0.3529229760169983,\n",
       "   0.31027746200561523,\n",
       "   0.3299422264099121,\n",
       "   0.31528621912002563,\n",
       "   0.3044033646583557,\n",
       "   0.334918737411499,\n",
       "   0.345537930727005,\n",
       "   0.33416837453842163,\n",
       "   0.3243570029735565,\n",
       "   0.33341699838638306,\n",
       "   0.3113463222980499,\n",
       "   0.34678661823272705,\n",
       "   0.3196832239627838,\n",
       "   0.3394787609577179,\n",
       "   0.3277367055416107,\n",
       "   0.35154303908348083,\n",
       "   0.31470999121665955,\n",
       "   0.30857139825820923,\n",
       "   0.31990042328834534,\n",
       "   0.28966769576072693,\n",
       "   0.3136817514896393,\n",
       "   0.30625471472740173,\n",
       "   0.3175508975982666,\n",
       "   0.30668529868125916,\n",
       "   0.32894834876060486,\n",
       "   0.3220827877521515,\n",
       "   0.31033211946487427,\n",
       "   0.3193327784538269,\n",
       "   0.2884034216403961,\n",
       "   0.3056006133556366,\n",
       "   0.33554425835609436,\n",
       "   0.29599806666374207,\n",
       "   0.32268863916397095,\n",
       "   0.3181203305721283,\n",
       "   0.3109194040298462,\n",
       "   0.328959584236145,\n",
       "   0.2918190062046051,\n",
       "   0.3089519441127777,\n",
       "   0.30146512389183044,\n",
       "   0.28421932458877563,\n",
       "   0.281301885843277,\n",
       "   0.3130016624927521,\n",
       "   0.31501778960227966,\n",
       "   0.29412758350372314,\n",
       "   0.31007319688796997,\n",
       "   0.3099050223827362,\n",
       "   0.2896239459514618,\n",
       "   0.3186492323875427,\n",
       "   0.27878618240356445,\n",
       "   0.305745929479599,\n",
       "   0.29214808344841003,\n",
       "   0.3027920722961426,\n",
       "   0.29915085434913635,\n",
       "   0.3080183267593384,\n",
       "   0.33144697546958923,\n",
       "   0.29677584767341614,\n",
       "   0.3072790801525116,\n",
       "   0.3102814853191376,\n",
       "   0.3023536205291748,\n",
       "   0.29018434882164,\n",
       "   0.3145357668399811,\n",
       "   0.2787432372570038,\n",
       "   0.2977008819580078,\n",
       "   0.31270283460617065,\n",
       "   0.2714431583881378,\n",
       "   0.29527097940444946,\n",
       "   0.31284090876579285,\n",
       "   0.30105292797088623,\n",
       "   0.30255624651908875,\n",
       "   0.30147838592529297,\n",
       "   0.2830815315246582,\n",
       "   0.2911215126514435,\n",
       "   0.29263168573379517,\n",
       "   0.282936155796051,\n",
       "   0.2969760000705719,\n",
       "   0.27400723099708557,\n",
       "   0.2793930470943451,\n",
       "   0.29733821749687195,\n",
       "   0.29930299520492554,\n",
       "   0.2968141734600067,\n",
       "   0.31365078687667847,\n",
       "   0.29379647970199585,\n",
       "   0.28403013944625854,\n",
       "   0.26361095905303955,\n",
       "   0.2803095877170563,\n",
       "   0.27855241298675537,\n",
       "   0.2789424955844879,\n",
       "   0.27826637029647827,\n",
       "   0.2854803502559662,\n",
       "   0.2792612612247467,\n",
       "   0.29170510172843933,\n",
       "   0.28619083762168884,\n",
       "   0.2875741124153137,\n",
       "   0.2803327143192291,\n",
       "   0.3068615198135376,\n",
       "   0.29094386100769043,\n",
       "   0.282812237739563,\n",
       "   0.30483004450798035,\n",
       "   0.28405269980430603,\n",
       "   0.2968650460243225,\n",
       "   0.27650538086891174,\n",
       "   0.2869669198989868,\n",
       "   0.27971383929252625,\n",
       "   0.2784869372844696,\n",
       "   0.28859907388687134,\n",
       "   0.29277488589286804,\n",
       "   0.28102678060531616,\n",
       "   0.2943134009838104,\n",
       "   0.2859976887702942,\n",
       "   0.26454421877861023,\n",
       "   0.2946358919143677,\n",
       "   0.2893388867378235,\n",
       "   0.28587138652801514,\n",
       "   0.2794070839881897,\n",
       "   0.2849690914154053,\n",
       "   0.26294779777526855,\n",
       "   0.2942318916320801,\n",
       "   0.3053891658782959,\n",
       "   0.26233452558517456,\n",
       "   0.277268648147583,\n",
       "   0.28185081481933594,\n",
       "   0.2835211157798767,\n",
       "   0.2667849361896515,\n",
       "   0.2674160599708557,\n",
       "   0.27910155057907104,\n",
       "   0.26466602087020874,\n",
       "   0.27246764302253723,\n",
       "   0.2934734523296356,\n",
       "   0.27668747305870056,\n",
       "   0.2657164931297302,\n",
       "   0.2687709927558899,\n",
       "   0.29823702573776245,\n",
       "   0.27795568108558655,\n",
       "   0.26721855998039246,\n",
       "   0.2839615046977997,\n",
       "   0.2720785439014435,\n",
       "   0.2773035764694214,\n",
       "   0.2742224633693695,\n",
       "   0.26810771226882935,\n",
       "   0.28420138359069824,\n",
       "   0.2874270975589752,\n",
       "   0.2636583745479584,\n",
       "   0.27740877866744995,\n",
       "   0.2895941734313965,\n",
       "   0.26558196544647217,\n",
       "   0.30420011281967163,\n",
       "   0.2566699683666229,\n",
       "   0.2780364453792572,\n",
       "   0.28280800580978394,\n",
       "   0.268654465675354,\n",
       "   0.26462695002555847,\n",
       "   0.27140572667121887,\n",
       "   0.28572478890419006,\n",
       "   0.255513459444046,\n",
       "   0.26929107308387756,\n",
       "   0.27504780888557434,\n",
       "   0.28080108761787415,\n",
       "   0.2690572142601013,\n",
       "   0.27121463418006897,\n",
       "   0.27032217383384705,\n",
       "   0.2846951484680176,\n",
       "   0.26738348603248596,\n",
       "   0.28396332263946533,\n",
       "   0.26257598400115967,\n",
       "   0.2743048369884491,\n",
       "   0.25964662432670593,\n",
       "   0.2790455222129822,\n",
       "   0.2542553246021271,\n",
       "   0.25980880856513977,\n",
       "   0.2692824900150299,\n",
       "   0.2639087438583374,\n",
       "   0.27217409014701843,\n",
       "   0.26835572719573975,\n",
       "   0.28368890285491943,\n",
       "   0.26952433586120605,\n",
       "   0.2741115689277649,\n",
       "   0.2615162432193756,\n",
       "   0.2565045952796936,\n",
       "   0.26615235209465027,\n",
       "   0.2792251706123352,\n",
       "   0.26990777254104614,\n",
       "   0.26219475269317627,\n",
       "   0.24192115664482117,\n",
       "   0.27489519119262695,\n",
       "   0.2683461308479309,\n",
       "   0.26392093300819397,\n",
       "   0.2755926251411438,\n",
       "   0.2646707594394684,\n",
       "   0.273293137550354,\n",
       "   0.25001591444015503,\n",
       "   0.2752131521701813,\n",
       "   0.26185739040374756,\n",
       "   0.27143236994743347,\n",
       "   0.27449512481689453,\n",
       "   0.2751499116420746,\n",
       "   0.27057984471321106,\n",
       "   0.2717244029045105,\n",
       "   0.2584637999534607,\n",
       "   0.24689346551895142,\n",
       "   0.28224441409111023,\n",
       "   0.2536610960960388,\n",
       "   0.2638968825340271,\n",
       "   0.2731556296348572,\n",
       "   0.26030248403549194,\n",
       "   0.25664588809013367,\n",
       "   0.2646421790122986,\n",
       "   0.2629467844963074,\n",
       "   0.2631259560585022,\n",
       "   0.2591070830821991,\n",
       "   0.254734069108963,\n",
       "   0.25592419505119324,\n",
       "   0.2746257483959198,\n",
       "   0.26462796330451965,\n",
       "   0.27031925320625305,\n",
       "   0.264660507440567,\n",
       "   0.26072797179222107,\n",
       "   0.25107869505882263,\n",
       "   0.26330050826072693,\n",
       "   0.24789926409721375,\n",
       "   0.2492486983537674,\n",
       "   0.2767908275127411,\n",
       "   0.27287524938583374,\n",
       "   0.24557897448539734,\n",
       "   0.2546141743659973,\n",
       "   0.2535245418548584,\n",
       "   0.2871704697608948,\n",
       "   0.2726486921310425,\n",
       "   0.25235357880592346,\n",
       "   0.2521056532859802,\n",
       "   0.2646765112876892,\n",
       "   0.2708517611026764,\n",
       "   0.2570718228816986,\n",
       "   0.2824534773826599,\n",
       "   0.25100910663604736,\n",
       "   0.2564982771873474,\n",
       "   0.25447556376457214,\n",
       "   0.25299394130706787,\n",
       "   0.2439875602722168,\n",
       "   0.2627497613430023,\n",
       "   0.2546522617340088,\n",
       "   0.2538503408432007,\n",
       "   0.2484820932149887,\n",
       "   0.2632402777671814,\n",
       "   0.24925987422466278,\n",
       "   0.26091477274894714,\n",
       "   0.24315159022808075,\n",
       "   0.2415306717157364,\n",
       "   0.2746473252773285,\n",
       "   0.2518131136894226,\n",
       "   0.2571727931499481,\n",
       "   0.25628936290740967,\n",
       "   0.23484152555465698,\n",
       "   0.25206291675567627,\n",
       "   0.25858059525489807,\n",
       "   0.24061478674411774,\n",
       "   0.24787169694900513,\n",
       "   0.24373115599155426,\n",
       "   0.2534177899360657,\n",
       "   0.259832501411438,\n",
       "   0.256521075963974,\n",
       "   0.26159512996673584,\n",
       "   0.2536463141441345,\n",
       "   0.24646210670471191,\n",
       "   0.23363950848579407,\n",
       "   0.25335875153541565,\n",
       "   0.24663253128528595,\n",
       "   0.23916110396385193,\n",
       "   0.25805211067199707,\n",
       "   0.25534307956695557,\n",
       "   0.2586844563484192,\n",
       "   0.26816269755363464,\n",
       "   0.25028038024902344,\n",
       "   0.24539867043495178,\n",
       "   0.25652340054512024,\n",
       "   0.24649150669574738,\n",
       "   0.2524219751358032,\n",
       "   0.2569131553173065,\n",
       "   0.24049484729766846,\n",
       "   0.25797176361083984,\n",
       "   0.22722509503364563,\n",
       "   0.2425728440284729,\n",
       "   0.24166862666606903,\n",
       "   0.24677997827529907,\n",
       "   0.2555775046348572,\n",
       "   0.25480473041534424,\n",
       "   0.252259224653244,\n",
       "   0.24289493262767792,\n",
       "   0.25101110339164734,\n",
       "   0.25448837876319885,\n",
       "   0.24262893199920654,\n",
       "   0.2370026558637619,\n",
       "   0.2497600018978119,\n",
       "   0.24611961841583252,\n",
       "   0.2535078525543213,\n",
       "   0.24889889359474182,\n",
       "   0.25066736340522766,\n",
       "   0.23588833212852478,\n",
       "   0.24483591318130493,\n",
       "   0.24918745458126068,\n",
       "   0.25190895795822144,\n",
       "   0.24153172969818115,\n",
       "   0.23785161972045898,\n",
       "   0.24870239198207855,\n",
       "   0.23990735411643982,\n",
       "   0.25631651282310486,\n",
       "   0.26002418994903564,\n",
       "   0.24957554042339325,\n",
       "   0.24932971596717834,\n",
       "   0.25816699862480164,\n",
       "   0.25726184248924255,\n",
       "   0.2464314103126526,\n",
       "   0.2587686777114868,\n",
       "   0.25736570358276367,\n",
       "   0.24044953286647797,\n",
       "   0.24984349310398102,\n",
       "   0.23798997700214386,\n",
       "   0.2455586940050125,\n",
       "   0.24227267503738403,\n",
       "   0.25419607758522034,\n",
       "   0.23774167895317078,\n",
       "   0.24118107557296753,\n",
       "   0.23935577273368835,\n",
       "   0.24618090689182281,\n",
       "   0.22442059218883514,\n",
       "   0.23651038110256195,\n",
       "   0.23922798037528992,\n",
       "   0.23514872789382935,\n",
       "   0.25167199969291687,\n",
       "   0.23500904440879822,\n",
       "   0.24262206256389618,\n",
       "   0.237091526389122,\n",
       "   0.2371663749217987,\n",
       "   0.23612844944000244,\n",
       "   0.23734763264656067,\n",
       "   0.23624959588050842,\n",
       "   0.2374185174703598,\n",
       "   0.2283792793750763,\n",
       "   0.24440424144268036,\n",
       "   0.259909063577652,\n",
       "   0.22869554162025452,\n",
       "   0.23934659361839294,\n",
       "   0.23123331367969513,\n",
       "   0.24467380344867706,\n",
       "   0.23711466789245605,\n",
       "   0.24746115505695343,\n",
       "   0.2359500378370285,\n",
       "   0.23121660947799683,\n",
       "   0.2319803535938263,\n",
       "   0.22755879163742065,\n",
       "   0.24248385429382324,\n",
       "   0.23482564091682434,\n",
       "   0.23988023400306702,\n",
       "   0.24004895985126495,\n",
       "   0.2441052496433258,\n",
       "   0.22491002082824707,\n",
       "   0.23905307054519653,\n",
       "   0.23982761800289154,\n",
       "   0.2435535490512848,\n",
       "   0.2284032106399536,\n",
       "   0.24395966529846191,\n",
       "   0.238768070936203,\n",
       "   0.23318755626678467,\n",
       "   0.24388551712036133,\n",
       "   0.23459133505821228,\n",
       "   0.2434873878955841,\n",
       "   0.23201991617679596,\n",
       "   0.24896536767482758,\n",
       "   0.24186652898788452,\n",
       "   0.2409272938966751,\n",
       "   0.24676723778247833,\n",
       "   0.22401367127895355,\n",
       "   0.2482519894838333,\n",
       "   0.24005387723445892,\n",
       "   0.24079959094524384,\n",
       "   0.2210051566362381,\n",
       "   0.2462111860513687,\n",
       "   0.22520633041858673,\n",
       "   0.22608180344104767,\n",
       "   0.23283208906650543],\n",
       "  'train_jsc_loss': [0.4798218905925751,\n",
       "   0.49610552191734314,\n",
       "   0.4982779920101166,\n",
       "   0.49105343222618103,\n",
       "   0.4956119954586029,\n",
       "   0.4961042106151581,\n",
       "   0.49130210280418396,\n",
       "   0.48666730523109436,\n",
       "   0.48935163021087646,\n",
       "   0.5006192326545715,\n",
       "   0.5109988451004028,\n",
       "   0.4842309057712555,\n",
       "   0.4848657250404358,\n",
       "   0.4953363835811615,\n",
       "   0.48825836181640625,\n",
       "   0.4888220727443695,\n",
       "   0.4999691843986511,\n",
       "   0.4943283796310425,\n",
       "   0.4841368496417999,\n",
       "   0.4943985044956207,\n",
       "   0.49266722798347473,\n",
       "   0.481105774641037,\n",
       "   0.48864126205444336,\n",
       "   0.4885861575603485,\n",
       "   0.48727813363075256,\n",
       "   0.49087923765182495,\n",
       "   0.48902666568756104,\n",
       "   0.489590048789978,\n",
       "   0.4842269718647003,\n",
       "   0.49233710765838623,\n",
       "   0.48659318685531616,\n",
       "   0.48055359721183777,\n",
       "   0.5006546974182129,\n",
       "   0.4803733229637146,\n",
       "   0.49010729789733887,\n",
       "   0.4891003668308258,\n",
       "   0.4864658713340759,\n",
       "   0.4849673807621002,\n",
       "   0.4737246632575989,\n",
       "   0.4902092218399048,\n",
       "   0.4898611605167389,\n",
       "   0.4838411808013916,\n",
       "   0.4825912117958069,\n",
       "   0.48537907004356384,\n",
       "   0.48953449726104736,\n",
       "   0.4939666986465454,\n",
       "   0.48403802514076233,\n",
       "   0.48388999700546265,\n",
       "   0.49109306931495667,\n",
       "   0.49484142661094666,\n",
       "   0.4805259108543396,\n",
       "   0.48126038908958435,\n",
       "   0.5002570152282715,\n",
       "   0.5025898218154907,\n",
       "   0.48282670974731445,\n",
       "   0.4940035045146942,\n",
       "   0.5033878684043884,\n",
       "   0.490322083234787,\n",
       "   0.4941709637641907,\n",
       "   0.4998983144760132,\n",
       "   0.48023319244384766,\n",
       "   0.48590371012687683,\n",
       "   0.47768762707710266,\n",
       "   0.49587881565093994,\n",
       "   0.48828357458114624,\n",
       "   0.4813651144504547,\n",
       "   0.49840477108955383,\n",
       "   0.4788750112056732,\n",
       "   0.4843946099281311,\n",
       "   0.4864943325519562,\n",
       "   0.4830748736858368,\n",
       "   0.4860987365245819,\n",
       "   0.48860299587249756,\n",
       "   0.4873451888561249,\n",
       "   0.486330509185791,\n",
       "   0.4847436547279358,\n",
       "   0.49134382605552673,\n",
       "   0.48077934980392456,\n",
       "   0.4900873899459839,\n",
       "   0.4845713973045349,\n",
       "   0.4945893883705139,\n",
       "   0.48414698243141174,\n",
       "   0.4956488311290741,\n",
       "   0.4798421561717987,\n",
       "   0.4797958731651306,\n",
       "   0.48065435886383057,\n",
       "   0.4822161793708801,\n",
       "   0.46816590428352356,\n",
       "   0.4764192998409271,\n",
       "   0.47993820905685425,\n",
       "   0.4891751706600189,\n",
       "   0.48220500349998474,\n",
       "   0.4835170805454254,\n",
       "   0.48289623856544495,\n",
       "   0.4835074245929718,\n",
       "   0.4783920645713806,\n",
       "   0.4892010986804962,\n",
       "   0.48309028148651123,\n",
       "   0.48243167996406555,\n",
       "   0.4910958707332611,\n",
       "   0.4793347120285034,\n",
       "   0.483637273311615,\n",
       "   0.4859674572944641,\n",
       "   0.48886653780937195,\n",
       "   0.4885505735874176,\n",
       "   0.474698930978775,\n",
       "   0.4974057972431183,\n",
       "   0.48954886198043823,\n",
       "   0.4909951388835907,\n",
       "   0.48744234442710876,\n",
       "   0.4704183042049408,\n",
       "   0.4823717176914215,\n",
       "   0.4900451600551605,\n",
       "   0.4749468266963959,\n",
       "   0.4797045588493347,\n",
       "   0.48476117849349976,\n",
       "   0.47461771965026855,\n",
       "   0.4801577627658844,\n",
       "   0.4771313965320587,\n",
       "   0.4782301187515259,\n",
       "   0.4855464696884155,\n",
       "   0.4860874116420746,\n",
       "   0.48774105310440063,\n",
       "   0.4749765396118164,\n",
       "   0.48966577649116516,\n",
       "   0.4873761236667633,\n",
       "   0.4669322073459625,\n",
       "   0.4690382480621338,\n",
       "   0.48737776279449463,\n",
       "   0.47892457246780396,\n",
       "   0.48931413888931274,\n",
       "   0.47880443930625916,\n",
       "   0.4765363037586212,\n",
       "   0.5026613473892212,\n",
       "   0.4747132360935211,\n",
       "   0.4791618585586548,\n",
       "   0.4952670931816101,\n",
       "   0.4777340590953827,\n",
       "   0.46774551272392273,\n",
       "   0.4877173900604248,\n",
       "   0.4848998486995697,\n",
       "   0.47145015001296997,\n",
       "   0.47059839963912964,\n",
       "   0.4770563244819641,\n",
       "   0.47258973121643066,\n",
       "   0.4786050617694855,\n",
       "   0.48025521636009216,\n",
       "   0.4741784632205963,\n",
       "   0.48301541805267334,\n",
       "   0.4891884922981262,\n",
       "   0.465319961309433,\n",
       "   0.4719342887401581,\n",
       "   0.4775319993495941,\n",
       "   0.46979838609695435,\n",
       "   0.45822879672050476,\n",
       "   0.4888612926006317,\n",
       "   0.46788516640663147,\n",
       "   0.47817009687423706,\n",
       "   0.467423677444458,\n",
       "   0.46537402272224426,\n",
       "   0.45259878039360046,\n",
       "   0.46294230222702026,\n",
       "   0.47128015756607056,\n",
       "   0.4710566997528076,\n",
       "   0.4635932445526123,\n",
       "   0.4588008522987366,\n",
       "   0.47231659293174744,\n",
       "   0.4927186965942383,\n",
       "   0.48880019783973694,\n",
       "   0.4611048996448517,\n",
       "   0.4708089828491211,\n",
       "   0.46844860911369324,\n",
       "   0.4711742103099823,\n",
       "   0.4679611027240753,\n",
       "   0.4731418490409851,\n",
       "   0.4624442756175995,\n",
       "   0.4662615656852722,\n",
       "   0.4733574390411377,\n",
       "   0.46773532032966614,\n",
       "   0.4679114520549774,\n",
       "   0.478212833404541,\n",
       "   0.4742341935634613,\n",
       "   0.49606794118881226,\n",
       "   0.47746387124061584,\n",
       "   0.47521692514419556,\n",
       "   0.4623650014400482,\n",
       "   0.4512978494167328,\n",
       "   0.4736418128013611,\n",
       "   0.46044015884399414,\n",
       "   0.46003615856170654,\n",
       "   0.4530121088027954,\n",
       "   0.46759122610092163,\n",
       "   0.46796247363090515,\n",
       "   0.4779072403907776,\n",
       "   0.473322331905365,\n",
       "   0.45781150460243225,\n",
       "   0.4588465690612793,\n",
       "   0.4644400179386139,\n",
       "   0.46683672070503235,\n",
       "   0.45569413900375366,\n",
       "   0.4695577919483185,\n",
       "   0.46681201457977295,\n",
       "   0.46261778473854065,\n",
       "   0.4585619270801544,\n",
       "   0.46388277411460876,\n",
       "   0.45059603452682495,\n",
       "   0.4600904583930969,\n",
       "   0.473991721868515,\n",
       "   0.4477871060371399,\n",
       "   0.4659233093261719,\n",
       "   0.4604969024658203,\n",
       "   0.45139944553375244,\n",
       "   0.46743834018707275,\n",
       "   0.46170687675476074,\n",
       "   0.45403751730918884,\n",
       "   0.46878188848495483,\n",
       "   0.4659423828125,\n",
       "   0.4543919861316681,\n",
       "   0.4635547399520874,\n",
       "   0.45835214853286743,\n",
       "   0.44746702909469604,\n",
       "   0.4586125910282135,\n",
       "   0.4430476725101471,\n",
       "   0.45667001605033875,\n",
       "   0.4612422585487366,\n",
       "   0.45992541313171387,\n",
       "   0.4472438097000122,\n",
       "   0.46188583970069885,\n",
       "   0.4615885317325592,\n",
       "   0.4599153995513916,\n",
       "   0.46425387263298035,\n",
       "   0.4482158124446869,\n",
       "   0.4469543695449829,\n",
       "   0.4672330617904663,\n",
       "   0.4396015405654907,\n",
       "   0.4585987329483032,\n",
       "   0.43337446451187134,\n",
       "   0.4660392999649048,\n",
       "   0.44639500975608826,\n",
       "   0.45261669158935547,\n",
       "   0.4479143023490906,\n",
       "   0.4331137239933014,\n",
       "   0.43240034580230713,\n",
       "   0.45003199577331543,\n",
       "   0.44437479972839355,\n",
       "   0.4413711726665497,\n",
       "   0.464723140001297,\n",
       "   0.4488086998462677,\n",
       "   0.4287964999675751,\n",
       "   0.46177536249160767,\n",
       "   0.46907687187194824,\n",
       "   0.43745121359825134,\n",
       "   0.4503360390663147,\n",
       "   0.4457794427871704,\n",
       "   0.44124290347099304,\n",
       "   0.4424368143081665,\n",
       "   0.44240087270736694,\n",
       "   0.43912482261657715,\n",
       "   0.4263595640659332,\n",
       "   0.437163382768631,\n",
       "   0.44183582067489624,\n",
       "   0.46020251512527466,\n",
       "   0.44973573088645935,\n",
       "   0.43048882484436035,\n",
       "   0.4403548836708069,\n",
       "   0.43493786454200745,\n",
       "   0.44788551330566406,\n",
       "   0.4473912715911865,\n",
       "   0.4318409860134125,\n",
       "   0.4206583499908447,\n",
       "   0.43107202649116516,\n",
       "   0.4339320659637451,\n",
       "   0.44957253336906433,\n",
       "   0.4553010165691376,\n",
       "   0.4355451762676239,\n",
       "   0.44010141491889954,\n",
       "   0.4220324456691742,\n",
       "   0.42453238368034363,\n",
       "   0.4501557946205139,\n",
       "   0.4159844219684601,\n",
       "   0.4452682137489319,\n",
       "   0.4108608365058899,\n",
       "   0.4472777545452118,\n",
       "   0.41885465383529663,\n",
       "   0.3989965617656708,\n",
       "   0.4400174915790558,\n",
       "   0.426315575838089,\n",
       "   0.4378821551799774,\n",
       "   0.4606800973415375,\n",
       "   0.4235736131668091,\n",
       "   0.4328545331954956,\n",
       "   0.4399389326572418,\n",
       "   0.4233444333076477,\n",
       "   0.4279913008213043,\n",
       "   0.43153008818626404,\n",
       "   0.4069024324417114,\n",
       "   0.441376268863678,\n",
       "   0.4299558699131012,\n",
       "   0.4302506148815155,\n",
       "   0.4163672924041748,\n",
       "   0.423130601644516,\n",
       "   0.4321093261241913,\n",
       "   0.41385915875434875,\n",
       "   0.420874685049057,\n",
       "   0.4249218702316284,\n",
       "   0.4268406629562378,\n",
       "   0.4234313368797302,\n",
       "   0.4130694270133972,\n",
       "   0.4298512637615204,\n",
       "   0.4223399758338928,\n",
       "   0.41964876651763916,\n",
       "   0.41690075397491455,\n",
       "   0.4332119822502136,\n",
       "   0.4136046767234802,\n",
       "   0.42785292863845825,\n",
       "   0.4299464821815491,\n",
       "   0.4152308702468872,\n",
       "   0.42175573110580444,\n",
       "   0.42844143509864807,\n",
       "   0.42453742027282715,\n",
       "   0.4202899634838104,\n",
       "   0.40305471420288086,\n",
       "   0.42729663848876953,\n",
       "   0.4056392312049866,\n",
       "   0.39405563473701477,\n",
       "   0.4153025448322296,\n",
       "   0.4097793698310852,\n",
       "   0.42628389596939087,\n",
       "   0.41145655512809753,\n",
       "   0.4186827540397644,\n",
       "   0.42296111583709717,\n",
       "   0.42914703488349915,\n",
       "   0.42765918374061584,\n",
       "   0.41358715295791626,\n",
       "   0.4060945212841034,\n",
       "   0.40104272961616516,\n",
       "   0.4062858521938324,\n",
       "   0.4148152768611908,\n",
       "   0.4087798297405243,\n",
       "   0.4105471074581146,\n",
       "   0.43588879704475403,\n",
       "   0.43085628747940063,\n",
       "   0.40527448058128357,\n",
       "   0.405467689037323,\n",
       "   0.4146585166454315,\n",
       "   0.40405383706092834,\n",
       "   0.41016653180122375,\n",
       "   0.40790948271751404,\n",
       "   0.42506399750709534,\n",
       "   0.4185798466205597,\n",
       "   0.399291455745697,\n",
       "   0.4039832651615143,\n",
       "   0.41131311655044556,\n",
       "   0.4010023772716522,\n",
       "   0.406432181596756,\n",
       "   0.41385146975517273,\n",
       "   0.4257451295852661,\n",
       "   0.40797165036201477,\n",
       "   0.40416380763053894,\n",
       "   0.41067832708358765,\n",
       "   0.41125255823135376,\n",
       "   0.3884621858596802,\n",
       "   0.4123120605945587,\n",
       "   0.40357306599617004,\n",
       "   0.38490015268325806,\n",
       "   0.402166485786438,\n",
       "   0.3902229070663452,\n",
       "   0.39448970556259155,\n",
       "   0.39766156673431396,\n",
       "   0.3998350203037262,\n",
       "   0.4109678566455841,\n",
       "   0.39432284235954285,\n",
       "   0.40159693360328674,\n",
       "   0.4035443961620331,\n",
       "   0.4005852937698364,\n",
       "   0.4040173888206482,\n",
       "   0.39921143651008606,\n",
       "   0.3925093114376068,\n",
       "   0.39260149002075195,\n",
       "   0.41135358810424805,\n",
       "   0.3800060749053955,\n",
       "   0.39681103825569153,\n",
       "   0.4123856723308563,\n",
       "   0.39099013805389404,\n",
       "   0.39838868379592896,\n",
       "   0.3927995562553406,\n",
       "   0.39323824644088745,\n",
       "   0.3964415490627289,\n",
       "   0.3869449496269226,\n",
       "   0.4241158664226532,\n",
       "   0.393326997756958,\n",
       "   0.417035847902298,\n",
       "   0.395835816860199,\n",
       "   0.39288991689682007,\n",
       "   0.40455520153045654,\n",
       "   0.40092846751213074,\n",
       "   0.3869478404521942,\n",
       "   0.3763696253299713,\n",
       "   0.376114159822464,\n",
       "   0.3974396884441376,\n",
       "   0.38231930136680603,\n",
       "   0.3860709071159363,\n",
       "   0.3791782259941101,\n",
       "   0.3978649973869324,\n",
       "   0.3993500769138336,\n",
       "   0.3980081081390381,\n",
       "   0.3734647035598755,\n",
       "   0.38982170820236206,\n",
       "   0.39575695991516113,\n",
       "   0.3662704825401306,\n",
       "   0.3763011693954468,\n",
       "   0.3938879370689392,\n",
       "   0.3839888870716095,\n",
       "   0.37649908661842346,\n",
       "   0.38362520933151245,\n",
       "   0.3974345922470093,\n",
       "   0.37748080492019653,\n",
       "   0.37479037046432495,\n",
       "   0.40015503764152527,\n",
       "   0.37661316990852356,\n",
       "   0.39107850193977356,\n",
       "   0.39077919721603394,\n",
       "   0.38048475980758667,\n",
       "   0.38817012310028076,\n",
       "   0.36247408390045166,\n",
       "   0.38117367029190063,\n",
       "   0.3971714377403259,\n",
       "   0.37002143263816833,\n",
       "   0.3914038836956024,\n",
       "   0.3919074535369873,\n",
       "   0.39838072657585144,\n",
       "   0.36328935623168945,\n",
       "   0.37519869208335876,\n",
       "   0.3750247061252594,\n",
       "   0.3836444914340973,\n",
       "   0.3716201186180115,\n",
       "   0.3737522065639496,\n",
       "   0.392180472612381,\n",
       "   0.3656362295150757,\n",
       "   0.3663545846939087,\n",
       "   0.3633909523487091,\n",
       "   0.39745986461639404,\n",
       "   0.3573068678379059,\n",
       "   0.37327080965042114,\n",
       "   0.3879508376121521,\n",
       "   0.36318889260292053,\n",
       "   0.37891125679016113,\n",
       "   0.3862742483615875,\n",
       "   0.38875654339790344,\n",
       "   0.3879387676715851,\n",
       "   0.36736729741096497,\n",
       "   0.38966670632362366,\n",
       "   0.37931323051452637,\n",
       "   0.3819914162158966,\n",
       "   0.38823720812797546,\n",
       "   0.38401228189468384,\n",
       "   0.3766423165798187,\n",
       "   0.36392489075660706,\n",
       "   0.3761431872844696,\n",
       "   0.3863288164138794,\n",
       "   0.36623620986938477,\n",
       "   0.3729998469352722,\n",
       "   0.3726550340652466,\n",
       "   0.38125860691070557,\n",
       "   0.3698829710483551,\n",
       "   0.37029871344566345,\n",
       "   0.36109548807144165,\n",
       "   0.38124996423721313,\n",
       "   0.3662913143634796,\n",
       "   0.3692433536052704,\n",
       "   0.3718089461326599,\n",
       "   0.3673749566078186,\n",
       "   0.37404555082321167,\n",
       "   0.3799634873867035,\n",
       "   0.37824663519859314,\n",
       "   0.3791956305503845,\n",
       "   0.35939061641693115,\n",
       "   0.37174326181411743,\n",
       "   0.37841036915779114,\n",
       "   0.3780941069126129,\n",
       "   0.3624986410140991,\n",
       "   0.3736717402935028,\n",
       "   0.3672041893005371,\n",
       "   0.3746510148048401,\n",
       "   0.3749678432941437,\n",
       "   0.3680649697780609,\n",
       "   0.3685222566127777,\n",
       "   0.38117125630378723,\n",
       "   0.366440087556839,\n",
       "   0.34763914346694946,\n",
       "   0.35857054591178894,\n",
       "   0.3705117404460907,\n",
       "   0.3803540766239166,\n",
       "   0.3689422011375427,\n",
       "   0.37575817108154297,\n",
       "   0.37090158462524414,\n",
       "   0.37003275752067566,\n",
       "   0.3594782054424286,\n",
       "   0.3632930815219879,\n",
       "   0.36175736784935],\n",
       "  'train_ff_loss': [0.34193313121795654,\n",
       "   0.36485013365745544,\n",
       "   0.3321242332458496,\n",
       "   0.34536662697792053,\n",
       "   0.33953890204429626,\n",
       "   0.34444257616996765,\n",
       "   0.3275085389614105,\n",
       "   0.34486669301986694,\n",
       "   0.3194233179092407,\n",
       "   0.3538297116756439,\n",
       "   0.36347129940986633,\n",
       "   0.3525407016277313,\n",
       "   0.3348267674446106,\n",
       "   0.352783203125,\n",
       "   0.3341963291168213,\n",
       "   0.3311341404914856,\n",
       "   0.35478413105010986,\n",
       "   0.3348154127597809,\n",
       "   0.3316667973995209,\n",
       "   0.33805689215660095,\n",
       "   0.34178492426872253,\n",
       "   0.33747950196266174,\n",
       "   0.32744723558425903,\n",
       "   0.3496261537075043,\n",
       "   0.3295432925224304,\n",
       "   0.3298415541648865,\n",
       "   0.31840240955352783,\n",
       "   0.35710665583610535,\n",
       "   0.33376604318618774,\n",
       "   0.34240564703941345,\n",
       "   0.3235214054584503,\n",
       "   0.3410579562187195,\n",
       "   0.3056190609931946,\n",
       "   0.31836116313934326,\n",
       "   0.3384849727153778,\n",
       "   0.33255499601364136,\n",
       "   0.3434406816959381,\n",
       "   0.3288435637950897,\n",
       "   0.31336912512779236,\n",
       "   0.3275196850299835,\n",
       "   0.32569387555122375,\n",
       "   0.30958816409111023,\n",
       "   0.3095112144947052,\n",
       "   0.30046185851097107,\n",
       "   0.31822678446769714,\n",
       "   0.3195529580116272,\n",
       "   0.3043109178543091,\n",
       "   0.3602236807346344,\n",
       "   0.289150595664978,\n",
       "   0.3134894371032715,\n",
       "   0.33752813935279846,\n",
       "   0.31751298904418945,\n",
       "   0.2982999086380005,\n",
       "   0.31537383794784546,\n",
       "   0.3171975612640381,\n",
       "   0.298544317483902,\n",
       "   0.31755387783050537,\n",
       "   0.3189510405063629,\n",
       "   0.3312399387359619,\n",
       "   0.32851168513298035,\n",
       "   0.302782267332077,\n",
       "   0.3095787465572357,\n",
       "   0.33656343817710876,\n",
       "   0.3259272873401642,\n",
       "   0.3301716148853302,\n",
       "   0.32892125844955444,\n",
       "   0.2956206500530243,\n",
       "   0.33533143997192383,\n",
       "   0.3195369839668274,\n",
       "   0.3336266577243805,\n",
       "   0.3102511763572693,\n",
       "   0.31382909417152405,\n",
       "   0.3246411383152008,\n",
       "   0.30214181542396545,\n",
       "   0.292725533246994,\n",
       "   0.2912900447845459,\n",
       "   0.338279128074646,\n",
       "   0.30297309160232544,\n",
       "   0.31522518396377563,\n",
       "   0.3339298963546753,\n",
       "   0.31197696924209595,\n",
       "   0.33350953459739685,\n",
       "   0.3122621774673462,\n",
       "   0.30901986360549927,\n",
       "   0.31627148389816284,\n",
       "   0.32718023657798767,\n",
       "   0.2925781309604645,\n",
       "   0.3109968900680542,\n",
       "   0.3258507549762726,\n",
       "   0.3208749294281006,\n",
       "   0.30132022500038147,\n",
       "   0.2889535427093506,\n",
       "   0.3140904903411865,\n",
       "   0.3083404302597046,\n",
       "   0.32901281118392944,\n",
       "   0.31326812505722046,\n",
       "   0.2876620888710022,\n",
       "   0.32282742857933044,\n",
       "   0.3181508779525757,\n",
       "   0.32302719354629517,\n",
       "   0.30390027165412903,\n",
       "   0.3026092052459717,\n",
       "   0.3235490918159485,\n",
       "   0.3073841333389282,\n",
       "   0.2949821650981903,\n",
       "   0.28723278641700745,\n",
       "   0.3106497526168823,\n",
       "   0.293808251619339,\n",
       "   0.2986457049846649,\n",
       "   0.3092409074306488,\n",
       "   0.3007330894470215,\n",
       "   0.3094225227832794,\n",
       "   0.3029683530330658,\n",
       "   0.2767498791217804,\n",
       "   0.2826794683933258,\n",
       "   0.3130355179309845,\n",
       "   0.29435092210769653,\n",
       "   0.28910067677497864,\n",
       "   0.2988729774951935,\n",
       "   0.2876163423061371,\n",
       "   0.30358871817588806,\n",
       "   0.2849663197994232,\n",
       "   0.28208425641059875,\n",
       "   0.30147650837898254,\n",
       "   0.28971216082572937,\n",
       "   0.29642829298973083,\n",
       "   0.28911110758781433,\n",
       "   0.27561458945274353,\n",
       "   0.28624430298805237,\n",
       "   0.3034079372882843,\n",
       "   0.30065083503723145,\n",
       "   0.28446877002716064,\n",
       "   0.29774728417396545,\n",
       "   0.28291720151901245,\n",
       "   0.30108392238616943,\n",
       "   0.2951887547969818,\n",
       "   0.28526073694229126,\n",
       "   0.2933277189731598,\n",
       "   0.32241958379745483,\n",
       "   0.2900104224681854,\n",
       "   0.29141131043434143,\n",
       "   0.2886670231819153,\n",
       "   0.3177843987941742,\n",
       "   0.2915671467781067,\n",
       "   0.2858335077762604,\n",
       "   0.27500250935554504,\n",
       "   0.28853094577789307,\n",
       "   0.2762671113014221,\n",
       "   0.29563775658607483,\n",
       "   0.27063167095184326,\n",
       "   0.3066471815109253,\n",
       "   0.30378207564353943,\n",
       "   0.29758772253990173,\n",
       "   0.30341947078704834,\n",
       "   0.2776942849159241,\n",
       "   0.2794419229030609,\n",
       "   0.2848834991455078,\n",
       "   0.30034494400024414,\n",
       "   0.2957339286804199,\n",
       "   0.29445570707321167,\n",
       "   0.2791719436645508,\n",
       "   0.31492602825164795,\n",
       "   0.29033902287483215,\n",
       "   0.29467636346817017,\n",
       "   0.2818138897418976,\n",
       "   0.28828176856040955,\n",
       "   0.276550829410553,\n",
       "   0.270275741815567,\n",
       "   0.2802603840827942,\n",
       "   0.2842768728733063,\n",
       "   0.28756028413772583,\n",
       "   0.30601805448532104,\n",
       "   0.2705073356628418,\n",
       "   0.2869206368923187,\n",
       "   0.2819329500198364,\n",
       "   0.28121545910835266,\n",
       "   0.2796027362346649,\n",
       "   0.2656231224536896,\n",
       "   0.2790859639644623,\n",
       "   0.27255862951278687,\n",
       "   0.2980082035064697,\n",
       "   0.27422869205474854,\n",
       "   0.27694398164749146,\n",
       "   0.28335240483283997,\n",
       "   0.2879982888698578,\n",
       "   0.2884620130062103,\n",
       "   0.2768878638744354,\n",
       "   0.2835789620876312,\n",
       "   0.2837545573711395,\n",
       "   0.2857309579849243,\n",
       "   0.274068683385849,\n",
       "   0.28712180256843567,\n",
       "   0.27754563093185425,\n",
       "   0.2852730453014374,\n",
       "   0.27359727025032043,\n",
       "   0.28353413939476013,\n",
       "   0.2711285948753357,\n",
       "   0.27355268597602844,\n",
       "   0.2816729247570038,\n",
       "   0.2718229591846466,\n",
       "   0.2714482843875885,\n",
       "   0.2667728364467621,\n",
       "   0.2799275815486908,\n",
       "   0.26091933250427246,\n",
       "   0.28785669803619385,\n",
       "   0.2686287462711334,\n",
       "   0.2755204737186432,\n",
       "   0.26510706543922424,\n",
       "   0.28904545307159424,\n",
       "   0.2894381284713745,\n",
       "   0.28130581974983215,\n",
       "   0.27963122725486755,\n",
       "   0.2783622145652771,\n",
       "   0.2789047360420227,\n",
       "   0.27472272515296936,\n",
       "   0.2821294367313385,\n",
       "   0.25991693139076233,\n",
       "   0.2913360297679901,\n",
       "   0.2564602792263031,\n",
       "   0.2641032338142395,\n",
       "   0.26550164818763733,\n",
       "   0.264367014169693,\n",
       "   0.284517765045166,\n",
       "   0.27020347118377686,\n",
       "   0.2695646584033966,\n",
       "   0.2661932110786438,\n",
       "   0.262989342212677,\n",
       "   0.2783750295639038,\n",
       "   0.27619636058807373,\n",
       "   0.24804691970348358,\n",
       "   0.2847674489021301,\n",
       "   0.27722853422164917,\n",
       "   0.28339749574661255,\n",
       "   0.26861122250556946,\n",
       "   0.2791462540626526,\n",
       "   0.26386046409606934,\n",
       "   0.28308045864105225,\n",
       "   0.28001025319099426,\n",
       "   0.25312724709510803,\n",
       "   0.26730629801750183,\n",
       "   0.28732115030288696,\n",
       "   0.27306073904037476,\n",
       "   0.29618188738822937,\n",
       "   0.2623569369316101,\n",
       "   0.25776225328445435,\n",
       "   0.2670183777809143,\n",
       "   0.28171128034591675,\n",
       "   0.2587403357028961,\n",
       "   0.29444172978401184,\n",
       "   0.2598212957382202,\n",
       "   0.23783345520496368,\n",
       "   0.28601521253585815,\n",
       "   0.2658461928367615,\n",
       "   0.26579350233078003,\n",
       "   0.25281891226768494,\n",
       "   0.2585030496120453,\n",
       "   0.28189051151275635,\n",
       "   0.2718372344970703,\n",
       "   0.2716643810272217,\n",
       "   0.26254746317863464,\n",
       "   0.2730807363986969,\n",
       "   0.28426873683929443,\n",
       "   0.274148553609848,\n",
       "   0.262310266494751,\n",
       "   0.280656099319458,\n",
       "   0.25849881768226624,\n",
       "   0.28977295756340027,\n",
       "   0.2681332528591156,\n",
       "   0.2759056091308594,\n",
       "   0.2760891020298004,\n",
       "   0.2613990604877472,\n",
       "   0.2702333331108093,\n",
       "   0.26441487669944763,\n",
       "   0.2827305793762207,\n",
       "   0.27885639667510986,\n",
       "   0.25397324562072754,\n",
       "   0.2687513828277588,\n",
       "   0.28452351689338684,\n",
       "   0.26389697194099426,\n",
       "   0.26241880655288696,\n",
       "   0.2749347686767578,\n",
       "   0.25699278712272644,\n",
       "   0.26027873158454895,\n",
       "   0.258984237909317,\n",
       "   0.2692871391773224,\n",
       "   0.276699960231781,\n",
       "   0.25928106904029846,\n",
       "   0.2647552192211151,\n",
       "   0.26892149448394775,\n",
       "   0.24947157502174377,\n",
       "   0.2618788182735443,\n",
       "   0.2477848380804062,\n",
       "   0.2611468434333801,\n",
       "   0.27100446820259094,\n",
       "   0.2576008141040802,\n",
       "   0.2886603772640228,\n",
       "   0.2694719433784485,\n",
       "   0.24844171106815338,\n",
       "   0.2622778117656708,\n",
       "   0.2593654990196228,\n",
       "   0.26331546902656555,\n",
       "   0.2576071619987488,\n",
       "   0.2651667892932892,\n",
       "   0.2710554003715515,\n",
       "   0.25332555174827576,\n",
       "   0.24985536932945251,\n",
       "   0.24806644022464752,\n",
       "   0.267900288105011,\n",
       "   0.25804686546325684,\n",
       "   0.2640395164489746,\n",
       "   0.2587502598762512,\n",
       "   0.2541356086730957,\n",
       "   0.27131813764572144,\n",
       "   0.2476232796907425,\n",
       "   0.26292991638183594,\n",
       "   0.2621394395828247,\n",
       "   0.2664526402950287,\n",
       "   0.2543032169342041,\n",
       "   0.25974076986312866,\n",
       "   0.2515272796154022,\n",
       "   0.2593636214733124,\n",
       "   0.2613019049167633,\n",
       "   0.2642793655395508,\n",
       "   0.26106876134872437,\n",
       "   0.2610972821712494,\n",
       "   0.25982317328453064,\n",
       "   0.24768079817295074,\n",
       "   0.26723095774650574,\n",
       "   0.2580661177635193,\n",
       "   0.2678198516368866,\n",
       "   0.2503255605697632,\n",
       "   0.27382397651672363,\n",
       "   0.2696627378463745,\n",
       "   0.24667268991470337,\n",
       "   0.26411762833595276,\n",
       "   0.25938156247138977,\n",
       "   0.2620921730995178,\n",
       "   0.24747999012470245,\n",
       "   0.25221267342567444,\n",
       "   0.23870770633220673,\n",
       "   0.24265943467617035,\n",
       "   0.25174611806869507,\n",
       "   0.25163769721984863,\n",
       "   0.26061514019966125,\n",
       "   0.23204955458641052,\n",
       "   0.238566592335701,\n",
       "   0.2684157192707062,\n",
       "   0.2497943490743637,\n",
       "   0.27018359303474426,\n",
       "   0.253727525472641,\n",
       "   0.25583818554878235,\n",
       "   0.24930039048194885,\n",
       "   0.26296356320381165,\n",
       "   0.25719672441482544,\n",
       "   0.2554292380809784,\n",
       "   0.25422045588493347,\n",
       "   0.2577851414680481,\n",
       "   0.26208436489105225,\n",
       "   0.24334470927715302,\n",
       "   0.24953754246234894,\n",
       "   0.25002822279930115,\n",
       "   0.25382277369499207,\n",
       "   0.2366093397140503,\n",
       "   0.2463783323764801,\n",
       "   0.24788013100624084,\n",
       "   0.23344756662845612,\n",
       "   0.24047023057937622,\n",
       "   0.2524328827857971,\n",
       "   0.250721275806427,\n",
       "   0.26029667258262634,\n",
       "   0.25461098551750183,\n",
       "   0.26253145933151245,\n",
       "   0.26848718523979187,\n",
       "   0.24009586870670319,\n",
       "   0.24238012731075287,\n",
       "   0.2542148530483246,\n",
       "   0.2370927780866623,\n",
       "   0.24350908398628235,\n",
       "   0.2497672438621521,\n",
       "   0.254899263381958,\n",
       "   0.2539382874965668,\n",
       "   0.2445230931043625,\n",
       "   0.25248607993125916,\n",
       "   0.24766552448272705,\n",
       "   0.24268391728401184,\n",
       "   0.2538103461265564,\n",
       "   0.24134454131126404,\n",
       "   0.24239639937877655,\n",
       "   0.24662669003009796,\n",
       "   0.24304592609405518,\n",
       "   0.24847352504730225,\n",
       "   0.24261555075645447,\n",
       "   0.23147167265415192,\n",
       "   0.2481888234615326,\n",
       "   0.25167056918144226,\n",
       "   0.2524012327194214,\n",
       "   0.24499545991420746,\n",
       "   0.2575584352016449,\n",
       "   0.2503393888473511,\n",
       "   0.24876809120178223,\n",
       "   0.2526782155036926,\n",
       "   0.24939921498298645,\n",
       "   0.2472122609615326,\n",
       "   0.23968258500099182,\n",
       "   0.23893271386623383,\n",
       "   0.24510557949543,\n",
       "   0.23366989195346832,\n",
       "   0.2468993067741394,\n",
       "   0.24456895887851715,\n",
       "   0.24208363890647888,\n",
       "   0.24080827832221985,\n",
       "   0.2562224864959717,\n",
       "   0.2591356039047241,\n",
       "   0.24676387012004852,\n",
       "   0.2498731017112732,\n",
       "   0.2525729537010193,\n",
       "   0.23486587405204773,\n",
       "   0.2599550485610962,\n",
       "   0.24920004606246948,\n",
       "   0.25318068265914917,\n",
       "   0.24231141805648804,\n",
       "   0.22830407321453094,\n",
       "   0.251154363155365,\n",
       "   0.25055041909217834,\n",
       "   0.23843884468078613,\n",
       "   0.23360978066921234,\n",
       "   0.2523307502269745,\n",
       "   0.24018967151641846,\n",
       "   0.2569120228290558,\n",
       "   0.25899791717529297,\n",
       "   0.24781645834445953,\n",
       "   0.2452637106180191,\n",
       "   0.23200882971286774,\n",
       "   0.25283366441726685,\n",
       "   0.242919459939003,\n",
       "   0.2438308298587799,\n",
       "   0.2377462238073349,\n",
       "   0.24437640607357025,\n",
       "   0.25187596678733826,\n",
       "   0.23396193981170654,\n",
       "   0.24666747450828552,\n",
       "   0.24282017350196838,\n",
       "   0.24552461504936218,\n",
       "   0.2369796186685562,\n",
       "   0.2624448537826538,\n",
       "   0.24984106421470642,\n",
       "   0.23756030201911926,\n",
       "   0.23858642578125,\n",
       "   0.258393794298172,\n",
       "   0.2437473088502884,\n",
       "   0.24586018919944763,\n",
       "   0.255248099565506,\n",
       "   0.22830848395824432,\n",
       "   0.24727480113506317,\n",
       "   0.23760071396827698,\n",
       "   0.2617252469062805,\n",
       "   0.25451207160949707,\n",
       "   0.23626849055290222,\n",
       "   0.24228070676326752,\n",
       "   0.22707030177116394,\n",
       "   0.24229656159877777,\n",
       "   0.22983138263225555,\n",
       "   0.23333962261676788,\n",
       "   0.2443895787000656,\n",
       "   0.23392608761787415,\n",
       "   0.22960224747657776,\n",
       "   0.25881868600845337,\n",
       "   0.24830293655395508,\n",
       "   0.22739827632904053,\n",
       "   0.24660001695156097,\n",
       "   0.23164846003055573,\n",
       "   0.23499177396297455,\n",
       "   0.24885326623916626,\n",
       "   0.24296438694000244,\n",
       "   0.2310810685157776,\n",
       "   0.2453330159187317,\n",
       "   0.22153961658477783,\n",
       "   0.24526473879814148,\n",
       "   0.25476205348968506,\n",
       "   0.2333824783563614,\n",
       "   0.23250289261341095,\n",
       "   0.23293481767177582,\n",
       "   0.248500794172287,\n",
       "   0.2518850862979889,\n",
       "   0.25157666206359863,\n",
       "   0.227802574634552,\n",
       "   0.22943304479122162,\n",
       "   0.23106038570404053,\n",
       "   0.2455151528120041,\n",
       "   0.23157067596912384,\n",
       "   0.23132812976837158,\n",
       "   0.23250830173492432,\n",
       "   0.23836606740951538,\n",
       "   0.24365417659282684,\n",
       "   0.23425792157649994,\n",
       "   0.24321335554122925,\n",
       "   0.23548822104930878,\n",
       "   0.2222801148891449,\n",
       "   0.23016880452632904,\n",
       "   0.23490795493125916]}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./json/20200811_OPVNN1_CV-2.json', 'w') as fp:\n",
    "    json.dump(cv_fits, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lr', 'best_loss_epoch', 'best_acc_epoch', 'best_r2_epoch', 'pce_loss', 'voc_loss', 'jsc_loss', 'ff_loss', 'test_losses', 'pce_acc', 'voc_acc', 'jsc_acc', 'ff_acc', 'test_accs', 'pce_r2', 'voc_r2', 'jsc_r2', 'ff_r2', 'test_r2s', 'train_pce_loss', 'train_voc_loss', 'train_jsc_loss', 'train_ff_loss'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./json/20200811_OPVNN1_CV-2.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "data['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_results(fit_dict):\n",
    "    lr = float(fit_dict['lr'])\n",
    "    best_loss_epoch = int(fit_dict['best_loss_epoch'])\n",
    "    best_acc_epoch = int(fit_dict['best_acc_epoch'])\n",
    "    best_r2_epoch = int(fit_dict['best_r2_epoch'])\n",
    "    \n",
    "    test_loss = [float(i) for i in fit_dict['test_losses']]\n",
    "    pce_loss = [float(i) for i in fit_dict['pce_loss']]\n",
    "    voc_loss = [float(i) for i in fit_dict['voc_loss']]\n",
    "    jsc_loss = [float(i) for i in fit_dict['jsc_loss']]\n",
    "    ff_loss = [float(i) for i in fit_dict['ff_loss']]\n",
    "    \n",
    "    test_acc = [float(i) for i in fit_dict['test_accs']]\n",
    "    pce_acc = [float(i) for i in fit_dict['pce_acc']]\n",
    "    voc_acc = [float(i) for i in fit_dict['voc_acc']]\n",
    "    jsc_acc = [float(i) for i in fit_dict['jsc_acc']]\n",
    "    ff_acc = [float(i) for i in fit_dict['ff_acc']]\n",
    "    \n",
    "    test_r2 = [float(i) for i in fit_dict['test_r2s']]\n",
    "    pce_r2 = [float(i) for i in fit_dict['pce_r2']]\n",
    "    voc_r2 = [float(i) for i in fit_dict['voc_r2']]\n",
    "    jsc_r2 = [float(i) for i in fit_dict['jsc_r2']]\n",
    "    ff_r2 = [float(i) for i in fit_dict['ff_r2']]\n",
    "    \n",
    "    train_pce_loss = [float(i) for i in fit_dict['train_pce_loss']]\n",
    "    train_voc_loss = [float(i) for i in fit_dict['train_voc_loss']]\n",
    "    train_jsc_loss = [float(i) for i in fit_dict['train_jsc_loss']]\n",
    "    train_ff_loss = [float(i) for i in fit_dict['train_ff_loss']]\n",
    "\n",
    "    epochs = np.arange(0, (len(test_loss)), 1)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12, 6))\n",
    "    ax1.plot(epochs, pce_loss, c = 'r', label = 'pce loss')\n",
    "    ax1.plot(epochs, voc_loss, c = 'g', label = 'voc loss')\n",
    "    ax1.plot(epochs, jsc_loss, c = 'b', label = 'jsc loss')\n",
    "    ax1.plot(epochs, ff_loss, c = 'c', label = 'ff loss')\n",
    "#     ax1.plot(epochs, test_loss, c = 'k', label = 'total loss')\n",
    "    ax1.plot(epochs, train_pce_loss, c = 'r', linestyle = '-.', label = 'pce train loss')\n",
    "    ax1.plot(epochs, train_voc_loss, c = 'g', linestyle = '-.', label = 'voc train loss')\n",
    "    ax1.plot(epochs, train_jsc_loss, c = 'b', linestyle = '-.', label = 'jsc train loss')\n",
    "#     ax1.plot(epochs, train_ff_loss, c = 'c', linestyle = '-.', label = 'ff train loss')\n",
    "    ax1.scatter(best_loss_epoch, min(test_loss), s = 64, c = 'c')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Mean Squared Error Loss')\n",
    "    ax1.legend(loc = 'best')\n",
    "    ax1.set_title(f'MSE Loss with lr = {lr}')\n",
    "\n",
    "    ax2.plot(epochs, pce_acc, c = 'r', label = 'pce acc')\n",
    "    ax2.plot(epochs, voc_acc, c = 'g', label = 'voc acc')\n",
    "    ax2.plot(epochs, jsc_acc, c = 'b', label = 'jsc acc')\n",
    "    ax2.plot(epochs, ff_acc, c = 'c', label = 'ff acc')\n",
    "#     ax2.plot(epochs, test_acc, c = 'k', label = 'total acc')\n",
    "    ax2.scatter(best_acc_epoch, min(test_acc), s = 64, c = 'c')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Mean Absolute Percent Error')\n",
    "    ax2.legend(loc = 'best')\n",
    "    ax2.set_title(f'MAPE with lr = {lr}')\n",
    "\n",
    "    ax3.plot(epochs, pce_r2, c = 'r', label = 'pce R$^2$')\n",
    "    ax3.plot(epochs, voc_r2, c = 'g', label = 'voc R$^2$')\n",
    "    ax3.plot(epochs, jsc_r2, c = 'b', label = 'jsc R$^2$')\n",
    "    ax3.plot(epochs, ff_r2, c = 'c', label = 'ff R$^2$')\n",
    "#     ax3.plot(epochs, test_r2, c = 'k', label = 'total R$^2$')\n",
    "    ax3.scatter(best_r2_epoch, max(test_r2), s = 64, c = 'c')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('R$^2$')\n",
    "    ax3.legend(loc = 'best')\n",
    "    ax3.set_title(f'R$^2$ with lr = {lr}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key, fit_dict in data.items():\n",
    "    plot_fit_results(fit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
